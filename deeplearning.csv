,Contents,Date,Summary,Title,URL
0,"Computers are playing spot the difference in the Serengeti. An image-recognition algorithm that can identify different species could make it easier to track animals in the wild.  Using a database of 3.2 million photos taken by hidden camera traps in the Serengeti National Park in Tanzania, Jeff Clune at the University of Wyoming in Laramie and his colleagues trained the deep-learning system to distinguish between 48 animal species, such as elephants, giraffes and gazelles. In tests, it correctly identified the species present in an image 92 per cent of the time.  Camera traps automatically take pictures of passing animals when triggered by heat and motion. This produces thousands or millions of photographs for ecologists to study, but people usually have to go through and label what each picture shows by hand, says Ali Swanson, who worked on the project while at the University of Oxford. If an algorithm could categorise at least some of the images, it could save a lot of time.  In 2010, Swanson set up 225 camera traps in the Serengeti, inviting an army of 70,000 online volunteers to help label the images. When Clune heard about this, he saw a perfect opportunity for deep learning  so he and Swanson arranged to team up on the project.  Right now in AI and deep learning, one of the hardest things to come by is a very good, large labelled dataset, says Clune.  His team started by teaching a neural network to recognise whether an image contained an animal, which 75 per cent of the Serengeti images lack. The researchers then trained it to differentiate between species.  The system is much better at identifying the most common animals in the data set, such as wildebeest, says Clune. It has trouble with more rare species like the zorilla, a type of polecat that only appears in the images a few dozen times.  Clune says the system could be used to classify most of the photographs and researchers could work on any it wasnt sure about. It could then be further trained on these hand-labelled images to get better at recognising rarer species. The team also plans to test whether the system can identify animal behaviour in images.  This is very exciting, says Chris Carbone at the Zoological Society of London. Automatic species recognition could help us learn more about the distribution of species and get a better idea of the impact humans are having on them, he says.  An ideal system would provide live tracking information about animals as they pass traps, says Swanson. But the challenge would be transmitting the data from the device in real time for the system to analyse, rather than the current method of using an SD card to store the data on the device until a researcher comes along to collect it.  One difficulty is that hyenas and elephants have a habit of damaging the cameras, which are thus kept in heavy-duty plastic cases with no space for an antenna that can transmit data. And if you do put an antenna on a camera, it wont last very long at all, says Swanson. Something will come along and chew it off pretty quick.",2017-04-11,"When Clune heard about this, he saw a perfect opportunity for deep learning – so he and Swanson arranged to team up on the project.",Deep learning tells giraffes from gazelles in the Serengeti,https://www.newscientist.com/article/2127541-deep-learning-tells-giraffes-from-gazelles-in-the-serengeti/
1,"A new deep learning method, DeepCpG, designed by researchers at EMBL-EBI, the Babraham Institute and the Sanger Institute helps scientists better understand the epigenome  the biochemical activity around the genome. Published in Genome Biology, DeepCpG leverages deep neural networks, a multi-layered machine learning model inspired by the brain. Machine learning provides a valuable tool for research into health and disease.  Deep learning is one of the most active fields in machine learning, which has led to recent advancements in computer image classification, text translation and speech recognition. But deep learning also has major potential in computational biology, particularly for regulatory genomics and cellular imaging.  Nice book. But what does it mean  We now have this amazing book of the human genome, thanks to projects like 1000 Genomes, divided up nicely into chapters and annotated in parts. But what does it mean If we want to really understand how life works, we need to decipher both the genome  the set of instructions repeated in every cell  and the epigenome, the part that varies wildly between cells, explains Oliver Stegle group leader at EMBL-EBI.  To better understand how DNA sequences relate to biological changes, the genomics community is turning to artificial neural networks  a class of machine learning methods first introduced in the 1980s and inspired by the wiring of the brain. More recently, these models have been rebranded as deep neural networks, which form the field of deep learning.  A recent review of deep learning for Molecular Systems Biology provides a user guide to how deep learning can be applied in genomics  an area of rapid technological change.  Single-cell genomics allows us to generate a huge amount of highly detailed information about the genome and all the activity happening around it, in many different types and subtypes of cells. The complexity is simply staggering, and the idea of explicitly probing each of these potential interactions individually is not really workable, says Stegle.  Most existing methods require you to know a lot up front, for example which patterns in the DNA sequence are informative for a specific task. However, there is a huge number of possible patterns in the genome that we could explore, so these existing methods are not practical for genomics, adds Christof Angermueller, PhD candidate at EMBL-EBI. With deep learning, you do not have to spend your time on manually crafting features that capture these patterns. Instead, the model uses raw DNA sequences as input and discovers relevant patters itself.  The team leveraged the capacity of deep learning to fill in the gaps in single-cell genomics, an emerging technology that offers a close-up view on epigenetics.  DeepCpG was designed to help scientists learn about the connections between DNA sequences and DNA methylation  a biochemical modification of the genome sequence that can act like an off-switch for individual genes. Methylation plays a key part in important biological processes, including cell development, ageing and cancer progression.  The new method uses genomic and epigenomic data to make predictions about DNA methylation in single cells. This is important because current technologies provide incomplete information about this. With DeepCpG, researchers can obtain a more complete picture of DNA methylation. The model can also be used to obtain new biological insights, for example on the connection between the DNA sequence and methylation.  DeepCpG actually learns meaningful features in a data-driven manner, says Angermueller. It has major advantages over previous methods, including the ability to more accurately predict DNA methylation and to study intercellular differences. By studying the wiring of the learnt network, we can understand how the biology of DNA methylation works. This has allowed us to recover known DNA sequence motifs that are important for methylation changes, as well as to discover new motifs, which are the starting point for future studies.  We have demonstrated that DeepCpG enables to accurately predict and analyse DNA methylation in single cells. However, DeepCpG is just one example of how we can apply deep learning to genomics and single-cell technologies, says Stegle. It is exciting to see the versatile applications deep learning has already found in genomics. I am looking forward to seeing more deep learning techniques come online. I believe it will make a big difference to how we study biology and has the potential to yield new answers about how life works.  Single cell epigenomics methods provide exciting insights into cell heterogeneity in development, ageing and disease; however if you are just dealing with two genomes (in a single cell) bits of information are often lost during the experiment, explains Wolf Reik of the Babraham Institute Wolf Reik of the Babraham Institute and Associate Faculty member at the Sanger Institute. This new method recognises patterns of the epigenome in single cells and then reconstructs lost information, returning a data-rich single cell epigenome.  ""Deep learning is now the state-of-the art in many fields. We are exploring its utility for making sense of large scale biological data. Pioneering studies, such as the one by Angermueller and colleagues, prove that there is lot to be gained by using deep learning methods in computational biology, concludes Leopold Parts, Group Leader at the Sanger Institute.  Angermueller, C., Lee, H., Reik, W., & Stegle, O. (2017). Accurate prediction of single-cell DNA methylation states using deep learning. bioRxiv, 055715.  ",2017-04-12,"A 'new deep learning' method, DeepCpG, designed by researchers at EMBL-EBI, the Babraham Institute and the Sanger Institute helps ...",Deep Learning: Accelerating Single-cell Genomics,https://www.technologynetworks.com/tn/news/deep-learning-tech-will-open-up-the-epigenome-287581
2,"April 11, 2017 Nimbix announced today the immediate availability of high-performance NVIDIA PascalGPUs using the NVIDIA DGX-1AI supercomputer in the Nimbix Cloud. For an on-demand rate, customers gain access to the industry-leading native bandwidth of eight NVIDIA NVLinkinterconnected NVIDIA Tesla P100 GPUs to launch or develop state-of the-art machine learning workflows, accelerated analytics and a host of other GPU-powered applications.  The Nimbix Cloud offers the most diverse set of GPU-powered machines available from a public cloud provider, spanning the NVIDIA portfolio and supporting configurations for both Intel x86 and IBM Power8 processors to deliver the best performance and economics available for both enterprises and developers. Nimbix Cloud machines are interconnected with industry-leading 56Gbps FDR and 100Gbps EDR Infiniband for optimal GPU cluster performance.  Nimbix has tremendous experience in GPU cloud computing, going all the way back to NVIDIAs Fermi architecture, said Steve Hebert, CEO of Nimbix. We are looking forward to accelerating deep learning and analytics applications for customers seeking the latest generation GPU technology available in a public cloud.  Combining the optimized performance of NVIDIA DGX-1 with Nimbixs cloud platform provides customers a flexible option to run their most challenging deep learning and AI workloads in an easy to use cloud system, said Charlie Doyle, senior director for DGX-1, NVIDIA.  In addition to the rich catalog of DGX turn-key workflows for deep learning, developers can use the PushToComputeTM feature of the JARVICE platform to import the latest versions of their custom applications into the Nimbix Cloud and make them available for consumption at scale immediately. Each application, along with its dependencies, executes in JARVICEs container runtime environment, which provides superior performance and scale. This includes sub-second launch times, faster execution, seamless access to supercomputing GPUs, automated heterogeneous data management, and rapid workflow deployment across multiple compute nodes either in a parallel or distributed paradigm. PushToComputeTM also facilitates continuous integration and continuous deployment (CI/CD) for the entire life cycles of containerized applications.  Nimbix is the leading provider of purpose-built cloud computing for machine learning, AI and HPC applications. Powered by JARVICE, the Nimbix Cloud provides high-performance software as a service, dramatically speeding up data processing for Energy, Life Sciences, Manufacturing, Media and Analytics applications. Nimbix delivers unique accelerated high-performance systems and applications from its world-class data centers as a pay-per-use service.",2017-04-11,We are looking forward to accelerating deep learning and analytics applications for customers seeking the latest generation GPU technology ...,Nimbix Cloud Adds Nvidia DGX-1 Servers for Deep Learning,https://www.hpcwire.com/off-the-wire/nimbix-announces-next-gen-gpus-cloud-based-deep-learning/
3,"Todays financial services companies must continually strive to gain a competitive edge in a highly data-intensive industry. With the emergence of big data, firms are struggling to manage the onslaught of complex data from many sources, stay on top of evolving regulations, and boost data security. High performance computing (HPC) technologies are not only helping financial firms ease the pains associated with explosive data growth, but they are also becoming absolutely essential to survival.  Emergent technologies and new HPC innovations to hit the financial sector were the focus of the 14th annual HPC on Wall Street event held earlier this week in New York City. The one-day show and conference brings together key FinTech players, financial industry experts, and capital markets IT providers to promote a discussion of the latest technological advances in the realm of financial HPC.  Each year, the show features a variety of networking opportunities, exhibits, expert panel discussions, and keynote speeches meant to help financial CIOs learn about new technologies, discuss industry trends and developments, and collaborate with experts that can help them address their biggest IT challenges.  This year, Hewlett Packard Enterprise (HPE) teamed up with SUSE and Intel to host a keynote session entitled HPC Innovation for the FSI Market. The keynote, which was delivered by Lacee McGee, Senior Product Manager for FSI Vertical Solutions at HPE and Natalia Vassilieva, a Senior Research Manager at HP Labs, as well as representatives from SUSE and Intel, concentrated on ways that financial organizations can begin their journeys into game-changing technologies like artificial intelligence (AI) and deep learning. Adoption of these emerging technologies is beginning to proliferate among companies in this sector, a fact which served as a persistent theme throughout the conference.  As McGee recently explained to EnterpriseTech, simply getting started can be the most difficult part for financial companies when it comes to implementing AI and deep learning technologies. While larger enterprises may have broader capital budgets and IT expertise, small and mid-sized businesses (SMBs) sometimes hesitate to take the plunge for a variety of reasons, including cost, lack of IT expertise, shortage of trained staff, or insufficient server infrastructure. McGees advice for SMBs was to start slow, and begin by having a trusted IT vendor package small amounts of AI and deep learning on familiar hardware. This gradual approach, meant to allow SMBs to grow with the technology and progressively increase their familiarity with these technologies, can help them ease the transition to more optimal solutions in the future.  The presentation also touched on the broadening demand for code modernization tools that can help quantitative analysts circumvent the steep learning curve associated with todays multi-core processor architectures. The HPE Quantitative Finance Library was recently introduced to address these challenges, a solution that modernizes application software using a productivity tool that generates highly parallelized source code for multi-core, multi-thread platforms. This helps quants exploit the advantages of the latest Intel Xeon Phi processors more quickly and with less time spent away from critical business activities.  Theres no denying that HPC has become essential to survival in the financial services industry. As data growth continues to bring new challenges and increased complexity to the financial services industry, HPC solutions are offering unprecedented levels of speed and efficiency, as well as whole new ways of doing business. Visit the HPC for Wall Street website for additional conference information and press coverage. You can also follow me on Twitter at @VineethRam to stay informed about the latest technologies and HPC finance innovations impacting the industry. We hope to see you at next years event!",2017-04-10,"McGee's advice for SMBs was to start slow, and begin by having a trusted IT vendor package small amounts of AI and deep learning on familiar hardware.",AI and Deep Learning Come to Wall Street,https://www.hpcwire.com/solution_content/hpe/financial-services/ai-deep-learning-come-wall-street/
4,"Japan joined the artificial intelligence race late. It was not until the mid-1980s, three decades later than countries like the United States, that researchers began pondering the idea of thinking machines. Japanese companies and universities are now increasingly pouring resources into unlocking the secrets of deep learning. The countrys slow start and scarcity of AI experts, however, has hindered progress, as illustrated by the lack of breakthroughs to date.  In March 2016 AlphaGo became the algorithm heard around the globe. In a best-of-five series in Seoul, South Korea, the go-playing program developed by Googles DeepMind soundly defeated Lee Sedol, a leading international player, 41, effectively dethroning humanity inone of the few remaining arenas where it ruled over artificial intelligence. While the ground-breaking achievement has shown a spotlight on the rapid advances in machine learning, AI is actually a rather mature concept with a long history encompassing a broad variety of technologies.  The desire to build machines with humanlike intelligence began with the accomplishments of British mathematician Alan Turing, who during World War II laid the groundwork for digital computing by developing the first electromechanical calculating device. His code-breaking machine and seminal 1947 work Intelligent Machinery inspired scientists to consider the potentials of AI. The term artificial intelligence was coined in 1956 at the pivotal Dartmouth Artificial Intelligence Conference. Japanese researchers were not among the diverse experts in attendance at that influential gathering, and it was not until 1986, three decades later, that the Japanese Society for Artificial Intelligence was formed to study AI.  Turings notions of machine intelligence encompassed such broad notions as neural networks, something I call first-generation AI. However, the JSAI was created in response to a fresh wave of debate focusing specifically on signal and information processing. More than 90% of AI researchersincluding those in Japan, where funding has steadily increasedare engaged in exploring these systems, whichI term second-generation AI. This rapidly developing field has already produced such innovations as the World Wide Web and Googles search engine.  One rapidly developing area of AI is machine learning. The idea of constructing algorithms able to analyze data and learn to make decisions and predictions based on results is not new. However, the field sat on the back burner of AI research for want of computers capable of making the necessary computations. Since the early 2000s, though, the emergence of increasingly powerful computers have enabled great strides in what is known as deep learning, making it possible to create large, multilayered artificial neural networks able to process vast amounts of data. I consider this to be third-generation AI.  While computers have improved, the lack of activity in machine learning during the 1990s has resulted in a grave shortage of experts in the field. The scarcity is particularly acute with deep learning and has forced tech giants like Alphabet (Google), Facebook, Microsoft, and Baidu to actively court and handsomely compensatesuitably capable researchers.  Japan has especially felt the pinch of the diamondlike rarity of deep learning experts. The situation is so dire, in fact, that there is hardly a domestic university or research organization with a dedicated researcher on staff.  Efforts are underway to address this paucity and a rapidly growing number of institutes are moving to offer research programs in deep learning. The problems, however, is that many of the top professors influencing the direction of programs are themselves little better than novices in the field, having spent most of their careers studying second-generation AI. In addition, views among those specializing in machine learning and deep learning frequently vary, making it difficult for institutes to decide how to best direct resources.  As a result, Japanese researchers seem stuck in neutral while their counterparts in China, Europe, and the United States rush ahead in producing a steady string of breakthroughs. My interactions with academia have left me wondering whetherthe rigid thinking at Japanese universities is promoting a derisive view of deep learning, robbing it of the serious consideration it deserves.  As an illustration, I was asked by the Ministry of Economy, Trade, and Industry to help plan two sessions on deep learning for the annual JSAI symposium in June 2016. During discussions about possible titles for the meetings, one academic quipped that no one would consider attending sessions featuring the term deep learning. As astonishing a statement as this was, it showed me the lack of regard many Japanese researchers have for cutting edge AI.  It is certainly true that deep learning represented only a small fraction of the topics covered at the symposium. One must also bear in mind that the JSAI gathering preceded Google releasing its revolutionary neural machine translation system.  Introduced in November 2016, the system foregoes standard models of natural language processing and instead uses an artificial neural network that over time learns from countless examples how to produce better translations. Although a relatively straightforward concept, the subsequent sharp improvement in the capabilities of Google Translate has left Japanese researchers gaping in awe.  In Japan the lions share of resources are directed toward studying natural language processing and machine translation. Researchers of second-generation AI have focused on unraveling the structure of intelligence with a view that it can be logically understood and recreated. As such, they have placed high value on analyzing the natural patterns of language as a means of unlocking the workings of the human mind.  The quality and accuracy of translations produced by the end-to-end approach of Googles neural machine translation system, however, have turned this method on its head. Researchers in Japan have to accept the possibility that Googles algorithm presents a revolutionary way forward or face the prospect of continuing down a dead-end road.  A similar ground-breaking dynamic can be seen behind DeepMinds approach in developing AlphaGo.  DeepMind developed the program to learn via two deep neural networks: a value network that evaluates the state of the game and a policy network that guides what actions to take. It then boosted the playing ability of the algorithm by having it study records of past matches and pitting the two networks against one other, a method it calls deep reinforcement learning.  Prior to AlphaGo, designers of go-playing AI took a fairly straightforward approach, arming programs with the rules and tactics and focusing on devising ways of winning against professionals. While significant progress was made, no AI capable of matching the wily skills of leading human players emerged.  Like a shot out of the blue, though, the relatively unknown DeepMind swooped in to claim the holy grail of computer go. Since then, leading IT companies in Japan and China like Dwango and Baidu, along with amateur programmers, have been emboldened to test their algorithms at various AI go competitions. On the other hand, Japans University of Electro-Communications shuttered its UEC Cup Computer Go tournament in the wake of AlphaGos victory, hosting the decade-old event for the final time in March 2017.  DeepMind has not grown indolent in its success but is maneuvering to take the lead globally with a string of newly developed technologies. Domestically, companies such as Yahoo Japan and Dwango, along with emerging firms like Preferred Networks, are pouring resources into deep learning. Meanwhile, major manufacturers including Toyota and FANUC are continuing to invest in AI. This is heartening to see, and I hope their efforts will net real dividends in the near future.",2017-04-11,Japanese companies and universities are now increasingly pouring resources into unlocking the secrets of deep learning. The country's slow ...,Deep Learning's Rise Leaves Japan Playing AI Catchup,http://www.nippon.com/en/currents/d00307/
5,"Science fiction has always provided an outlet where we can let our imaginations run wild about the possibilities of technological advances. Not long ago, science fiction concepts like self-aware robots, autonomous cars, or 3D printers may have been unimaginable, but were watching many of these things come to life before our eyes in the digital age.  Deep learning was previously a technology that seemed straight from the plot of the latest blockbuster, yet these days its no longer fiction and is proliferating across real-life applications. Deep learning, which falls under the umbrella of technologies known as artificial intelligence (AI), teaches machines to mimic the thought and decision-making processes of the human brain. Computers are trained using extremely large historical datasets to help them adapt and learn from prior experience, identify anomalous patterns in large datasets, and improve predictive analysis.  These techniques are becoming so popular that Gartner recently named AI and Advanced Machine Learning (which includes technologies such as deep learning) their #1 Strategic Technology Trend for 2017. The firm went on to predict that these technologies will begin to increasingly augment and extend virtually every technology-enabled service, thing, or application, and therefore will become the primary battleground for technology vendors through at least 2020.  The broadening availability of deep learning tools is helping to move these technologies beyond elite academic and research environments and into the everyday enterprise. Today, deep learning techniques are helping businesses across many industries leverage large amounts of unstructured data to drive better decision-making, enhance customer service, and build new revenue streams.  In any business situation where prediction is the objective, deep learning can be of use. While there are a variety of enterprise use cases involving deep learning, here are some of the most common:  Deep learning applications are by nature highly data-intensive, and this dramatically changes the compute infrastructure that is required to exploit these technologies. The core technologies required for deep learning are very similar to those necessary for high performance computing (HPC) applications, and include multi-core processors, software frameworks/toolkits, and HPC server platforms built to handle compute-intensive tasks.  As deep learning technologies become even more efficient, cost-effective, and accessible, businesses will be positioned to reap the full benefits of their datasets and fundamentally transform the way they operate. Savvy businesses will hop aboard this next innovative wave of computing capabilities in order to build more intelligent data platforms, improve predictive capabilities, and create business value.  Deep learning is finally moving away from science fiction and breaking through to the enterprise  is your business ready Follow me on Twitter at @Bill_Mannel for more insights into the disruptive technologies that are fueling business transformation.",2017-04-07,"Deep learning was previously a technology that seemed straight from the plot of the latest blockbuster, yet these days it's no longer fiction and ...",Deep Learning Technologies Breaking Through to the Enterprise,https://www.hpcwire.com/solution_content/hpe/government-academia/deep-learning-technologies-breaking-enterprise/
6,"The research team at IBM recently announcedthey've reached a new industry record in speech recognition with a word error rate of 5.5% using the SWITCHBOARD linguistic corpus. This brings it closer to what's considered to be the human error rate of 5.1%. Humans typically miss one to two words out of every 20 words they hear. In a five-minute conversation, that could be as many as 80 words.  The research project includes applying deep learning technologies and incorporating acoustic models. The speech recognition model used Long Short Term Memory (LSTM) and WaveNet language models with a score fusion of three acoustic models. The acoustic models included aLSTM with multiple feature inputs, another LSTM trained with speaker-adversarial multi-task learning and a third model with a residual net (ResNet) with 25 convolutional layers and time-dilated convolutions.The last model learns from positive examples but also takes advantage of negative examples, so it performs better where similar speech patterns are repeated.  Yoshua Bengiofrom Montreal Institute for Learning Algorithms (MILA) Lab at University of Montreal commented about the speech recognition.  In spite of impressive advances in recent years, reaching human-level performance in AI tasks such as speech recognition or object recognition remains a scientific challenge. Indeed, standard benchmarks do not always reveal the variations and complexities of real data. For example, different data sets can be more or less sensitive to different aspects of the task, and the results depend crucially on how human performance is evaluated, for example using skilled professional transcribers in the case of speech recognition.  He also said IBM research helps with advancing speech recognition by applying neural networks and deep learning into acoustic and language models.  In other speech processing news, IBM added Diarization to their Watson Speech to Text service which helps with use cases like distinguishing individual speakers in a conversation. All these achievements help with introducing technologies that will match the complexity of how the human ear, voice and brain interact.",2017-03-31,The research project includes applying deep learning technologies and ... speech recognition by applying neural networks and deep learning ...,Using Deep Learning Technologies IBM Reaches a New Milestone ...,https://www.infoq.com/news/2017/03/ibm-speech-recognition
7,"The chip maker Nvidiais riding the current artificial-intelligence boom with hardware designed to power cutting-edge learning algorithms. And the company sees health care and medicine as the next big market for its technology.  Kimberly Powell, who leads Nvidias efforts in health care, says the company is working with medical researchers in a range of areas and will look to expand these efforts in coming years.  Theres this amazing surge in medical imaging research, Powell said at MIT Technology Reviews EmTech Digital conference in San Francisco on Monday. More and more were visiting providers at hospitals today, and theyre imagining new artificial-intelligence applications.  Most notably, a machine-learning technique called deep learning is being applied to processing medical images and sifting through large amounts of medical data. Deep learning, which is very loosely inspired by the way neurons in the brain seem to work, has already provedincredibly useful for finding images and processing audio files (see 10 Breakthrough Technologies: Deep Learning).  This AI technique certainly seems to be gaining acolytes in medical research. Last year a team from Google showed that deep learning can be used to automate the diagnosis of eye disease. Meanwhile, a group from Stanford University published a paper in the journal Nature that showed the technique can spot skin cancer as well as a trained dermatologist. A group from Mount Sinai Hospital in New York used the approach to analyze patients electronic health records and predict, with surprisingly high accuracy, what disease a person would go on to develop.  These are just a few high-profile examples. Powell noted during her talk that large medical-imaging conferences have become dominated by deep-learning papers.  The graphics processors made by Nvidia are very well suited to performing the parallel calculations required for deep learning, and the chip maker has already built a sizable business supplying hardware to deep-learning researchers in academia and industry. Nvidiamakes a growing number of specialized deep-learning products, including a powerful research computer called the DGX-1and a system for self-driving vehicles called the Drive PX.  Powell believes the companys hardware will increasingly be found in hospitals and medical research centers, too. The approach could help improve the reliability of diagnosis, she said, and might significantly boost standards of care in developing countries, where expertise is scarce. Powell added that drug discovery would likely be another big area for deep learning in the future.  But deep learning might also help doctors find patterns that would otherwise be invisible. Nvidia is, for example, working with Bradley Erickson, a neuro-radiologist at the Mayo Clinic, to apply deep learning to brain images. Erickson has had some success in identifying genetic factors related to brain disease from images, Powell said.  Earlier, at the same event, Gary Marcus, a professor from NYU, singled out medicine as the area in which AI could have its biggest impact. Think about cancer, Marcus said. The risk factors that might indicate the likelihood of such a disease may be hard for a person to identify, but they could be uncovered by an algorithm, he said. The killer app [for AI] might be major advances in how we treat medicine.  There are, however, significant challenges in applying techniques like deep learning to medicine. The approach is so complex and opaque that it may not be clear to a doctor why an algorithm comes up with a particular diagnosis. Powell acknowledged this challenge but said that solutions, such as new ways of visualizing the behavior of deep-learning networks, were emerging. Its a big topic in research right now, she said.",2017-03-28,The chip maker Nvidia is riding the current artificial-intelligence boom with hardware designed to power cutting-edge learning algorithms.,Nvidia's Deep-Learning Chips May Give Medicine a Shot in the Arm,https://www.technologyreview.com/s/603917/nvidias-deep-learning-chips-may-give-medicine-a-shot-in-the-arm/
8,"Artificially intelligent algorithms can learn to identify amazingly subtle information, enabling them to distinguish between people in photos or to screen medical images as well as a doctor. But in most cases their ability to perform such feats relies on training that involves thousands to trillions of data points. This means artificial intelligence doesnt work all that well in situations where there is very little data, such as drug development.  Vijay Pande, professor of chemistry at Stanford University, and his students thought that a fairly new kind of deep learning, called one-shot learning, that requires only a small number of data points might be a solution to that low-data problem.  Were trying to use machine learning, especially deep learning, for the early stage of drug design, said Pande. The issue is, once you have thousands of examples in drug design, you probably already have a successful drug.  The group admitted the idea of applying one-shot learning to drug design problems was farfetched  the data was likely too limited. However, theyd had success in the past with machine learning methods requiring only hundreds of data points, and they had data available to test the one-shot approach. It seemed worth a try.  Much to their surprise, their results, published April 3 in ACS Central Science, show that one-shot learning methods have potential as a helpful tool for drug development and other areas of chemistry research.  Other researchers have successfully applied one-shot learning to image recognition and genomics, but applying it to problems relevant to drug development is a bit different. Whereas pixels and bases are fairly natural types of data to feed into an algorithm, properties of small molecules arent.  To make molecularinformation more digestible, the researchers first represented each molecule in terms of the connections between atoms (what a mathematician would call a graph). This step highlighted intrinsic properties of the chemical in a form that an algorithm could process.  With these graphical representations, the group trained an algorithm on two different datasets  one with information about the toxicity of different chemicals and another that detailed side effects of approved medicines. From the first dataset, they trained the algorithm on six chemicals and had it make predictions about the toxicity of the other three. Using the second dataset, they trained it to associate drugs with side effects in 21 tasks, testing it on six more.  In both cases, the algorithm was better able to predict toxicity or side effects than would have been possible by chance.  We worked on some prototype algorithms and found that, given a few data points, they were able to make predictions that were pretty accurate, said Bharath Ramsundar, who is a graduate student in the Pande lab and co-lead author of the study.  However, Ramsundar cautioned that this isnt a magical technique. It was built off of several recent advances in a particular style of one-shot learning and it works by relying on the closeness of different molecules, as indirectly indicated by their formula. For example, when the researchers trained their algorithm on the toxicity data and tested it on the side effect data, the algorithm completely collapsed.  People concerned about AI taking jobs from humans have nothing to fear from this work. The researchers envision this as groundwork for a potential tool for chemists who are early in their research and trying to choose which molecule to pursue from a set of promising candidates.  Right now, people make this kind of choice by hunch, Ramsundar said. This might be a nice compliment to that: an experimentalists helper.  Beyond giving insight into drug design, this tool would be broadly applicable to molecular chemistry. Already, the Pande lab is testing these methods on different chemical compositions for solar cells. They have also made all of the code they used for the experiment open source, available as part of the DeepChem library.  This paper is the first time that one-shot has been applied to this space and its exciting to see the field of machine learning move so quickly, Pande said. This is not the end of this journey  its the beginning.  Han Altae-Tran, Massachusetts Institute of Technology, is also lead author on this paper. Aneesh S. Pappu, Stanford University, is co-author. Pande is also a member of Stanford Bio-X and the Stanford Child Health Research Institute; and a fellow at Stanford ChEM-H.  This research was funded by the Fannie and John Hertz Foundation.",2017-04-03,"Vijay Pande, professor of chemistry at Stanford University, and his students thought that a fairly new kind of deep learning, called one-shot ...",Stanford researchers create deep learning algorithm that could ...,http://news.stanford.edu/2017/04/03/deep-learning-algorithm-aid-drug-development/
9,"Intel and Japan-based Preferred Networks have announced a collaboration that will focus on the development of Chainer, an open source Python-based deep learning framework developed by the Preferred Networks.  The partnership, announced at Intels Tokyo AI Day, aims to boost the performance of deep learning applications on Intels products and forms part of Intels strategy for gaining ground in the artificial intelligence (AI) space. The collaboration will also focus on easing AI development and affordability, with results of the collaboration published on Intels GitHub repository.  Chainer was made open-source in June 2015 and the framework has since been adopted for the development of deep learning in both research and real-world applications. The framework will utilize Intels open source library  Intel Math Kernel Library and Intel Math Kernel Library Deep Neural Network as a fundamental building block.  We believe in an open, standards-based ecosystem, shared Intels senior director for AI products Nidhi Chappell (pictured), who added that HPC use cases in the academic community have contributed significantly to the development of AI.  Intel is investing heavily in AI across several verticals, including financial services, healthcare, retail and manufacturing. The chipmaker recently formed a new AI business unit and its AI portfolio currently includes its Xeon and Xeon Phi processors, Arria 10 FPGAs and deep learning technology Nervana, which the chipmaker acquired last year.  The chipmaker maintains a bullish outlook on AI, betting that the technology will eventually account for one of the largest workloads in corporate data centers. The nature of AI requires that training be done both at the edge and in the data center, said Chapelle. Many of our customers do not wish to share their data on the public cloud, so deep learning will still be kept on premise.  An October 2016 report from research firm IDC estimates that the widespread adoption of cognitive systems and AI across various industries will drive revenues from nearly $8 billion in 2016 to more than $47 billion in 2020. IDC expects cognitive/AI solutions is expected to experience a compound annual growth rate of 55.1% between 2016  2020.",2017-04-06,"The partnership, announced at Intel's Tokyo AI Day, aims to boost the performance of deep learning applications on Intel's products and forms ...","Intel bets on AI, partners Preferred Networks for open source deep ...",http://www.networksasia.net/article/intel-bets-ai-partners-preferred-networks-open-source-deep-learning.1491492305
10,"Apples director of artificial intelligence, Ruslan Salakhutdinov, believes that the deep neural networks that have produced spectacular results in recent years could be supercharged in coming years by the addition of memory, attention, and general knowledge.  Speaking at MIT Technology Reviews EmTech Digital conference in San Francisco on Tuesday, Salakhutdinov said these attributes could help solve some of the outstanding problems in artificial intelligence.  Salakhutdinov, who retains a post as an associate professor at Carnegie Mellon University in Pittsburgh, pointed in his talk to limitations with deep-learning-driven machine vision and natural-language understanding.  Deep learninga technique that involves using vast numbers of roughly simulated neurons arranged in many interconnected layershas produced dramatic progress in machine perception over recent years, but there are many ways in which these networks are limited.  Salakhutdinov showed, for example, how image captioning systems based on the technology can label images incorrectly because they tend to focus on everything in the image. He then pointed to a solution in the form of so-called attention mechanisms, a tweak to deep learning that has been developed in the last few years. The approach can remedy these errors by having a system focus on specific parts of an image when applying different words in a caption. The same approach can help improve natural-language understanding, too, by enabling a machine to focus on the relevant part of a sentence in order to infer its meaning.  A technique called memory networks, developed by researchers at Facebook, can improve how machines talk with people. As the name suggests, the approach adds a component of long-term memory to neural networks so that they remember the history of a chat.  Memory networks have been shown to improve another kind of AI as well, known as reinforcement learning. For example, two researchers at CMU recently showed how this could create a smarter game-playing algorithm. Researchers at DeepMind, an AI-focused subsidiary of Alphabet, have also demonstrated ways for deep-learning systems to build and access a form of memory.  Reinforcement learning is rapidly emerging as a valuable way to solve hard-to-program problems in robotics and automated driving. It was one of MIT Technology Reviews 10 Breakthrough Technologies of 2017.  Another exciting area of future research, Salakhutdinov said, would be finding ways to combine hand-built sources of knowledge with deep learning. He pointed to general-knowledge databases like Freebase and word-meaning repositories like WordNet.  Just as humans rely heavily on general knowledge when parsing language or interpreting a visual scene, this could help make AI systems smarter, Salakhutdinov said. How can we incorporate all that prior knowledge into deep learning he said during his talk. Thats a big challenge.  Salakhutdinov spoke during a session that brought together researchers from several different schools of AI. A common theme among the speakers was the need for different approaches in order to take AI to the next level.  During the session Pedro Domingos, a professor at the University of Washington who studies different machine-learning approaches, said there is also a need to keep searching for completely new approaches to AI. Theres a school of thought in machine learning that we dont need fancy new algorithms, we just need more data, he said. I think there are really deep, fundamental ideas that need to be discovered before we can really solve AI.",2017-03-29,Deep learning—a technique that involves using vast numbers of roughly simulated neurons arranged in many interconnected layers—has ...,Apple's AI Director: Here's How to Supercharge Deep Learning,https://www.technologyreview.com/s/603912/apples-ai-director-heres-how-to-supercharge-deep-learning/
11,"Are you getting tired of hearing about artificial intelligence It seems we must be reaching peak hype cycle around AI when almost every article written about it rehashes the same tropes around self-driving cars, the latest game that has been mastered by computers, or the next house appliance to get speech recognition.  There is so much noise around AI that its hard to find a signal. And the real work of AI is happening behind the scenes of mainstream press coverage. After all, the places that machine learning can have the most impact havent yet been touched by AIlike automation.  But wait, I hear your moans already. Isnt artificial intelligence almost the same as automation No.  Computer automation is as old as the computer itself. The Turing Machine is by definition a self-replicating mathematical automaton. In other words, an algorithm that can automatically generate itself. But who programmed the Turing Machine to replicate itself We did.  Humans are behind the overwhelming majority of computer automation. For example, take the lowly underappreciated cron job, arguably the bedrock of computer automation. You specify a time (every five minutes) and a command (empty the trash), and the job will get done. Still, the computer didnt decide to empty the trash on its own. There was no computer sentience. You told it what to do.  But times are changing, and machine learning has opened opportunities for computers to automate and program themselves in more advanced ways than ever before.  One of the biggest macro trends in IT operations today is the movement away from humans running and managing software systems to computers running and managing software systems.  Consider one of the most successful and popular web server technologies in history: Apache. In the 1990s, the running and management of Apache was almost purely done by humans. If a server crashed, a human IT operations pro would have to fix it.  The next step forward was computer systems like Nagios, which would monitor Apache for crashes and restart it as necessary. But Nagios wasnt smart enough to get under the hood and actually manage the configuration of the software it ran.  Devops and PaaS represented the next leap, as configuration management software like Puppet and Chef could not only start and stop Apache but manage the settings as well.  Still, with devops and PaaS, the systems were being built piecemeal. If you needed to run a new application, you ad-hoc deployed it into the system. And what if the whole system crashed Could you re-create the entire thing with all of the applications and dependencies intact Not easily.  Right now, the state of the art for IT operations is declarative (as opposed to ad-hoc) systems. What do I mean by declarative I mean creating an entire IT topology that is described in a single file or set of files. Then its the declarative systems job to sync the real IT topology with the description specified in the files.  How is that different than devops or anything before Because there is no ad-hoc adding or removing from the system without modifying the declared system architecture. You dont simply deploy a few more websites or applications into the system (from the bottom up). You declare them (top-down) and let the system become consistent with your top-down declaration.  Some leaders in declarative IT include the Kubernetes project from Google and the Docker container platform. These technologies represent a paradigm shift in the operation of complex systems, but fundamentally there is still a human creating the architecture itself.  If the system needs to scale up or down, the architecture file needs to be modified to reflect that (or a human needs to specify the scaling rules).  Still, the holistic, encapsulating nature of declarative IT systems creates a new opportunity. Since the total state of the complex system is defined in a single place, the changes of state can be tracked over timebefore and after ... every improvement. Every scale up and down is logged in a centralized way.  The next step is to train a neural network on the topology and its changes over time. A recurrent neural network would then be able to predict and react to the state of the IT system based on its own assessments.  At first, the output of the learning system might be used as mere recommendations (it looks like your systems are about to be overloaded), but eventually, as the recommendations become more reliable than even human intuition, the deep learning system might take control and put all of IT management on autopilot.  We all have blind spots. Some things are so obvious to us that we cant see them. It astounds me the amount of resources that computer programmers have put into teaching computers to understand human languages when there is something so much more obvious: teaching computers to understand code itself.  If youre a programmer, youre probably rolling your eyes at this statement. After all, a compilers job is to understand and execute code. But simply following code instructions is not what I mean. I mean understanding the intentions behind code, understanding the difference between good code and bad code, intuitively being able to fix buggy code to do what it was supposed to.  Impossible Maybe, or maybe not. After all, GitHub has more than 20 million active repositories and more than 66 million pull requests. Every pull request (in theory) is a change of state from bad code to good code, from featureless code to featured code, from security vulnerable code to patched code.  The structure and volume of the data set for training a neural network on GitHub data is ideal. A recurrent neural network could be used as the ultimate code autocompletion tool. A Kohenon neural network could automatically figure out the purpose of the program you are trying to code as you write it, the ultimate code pattern identifier.  Oh, is that an abstract factory you are trying to build there You really shouldnt be doing it that way. I also noticed you are trying to build a web server. May I finish it off for you  Deep learning applied to computer programming itself is an almost completely unexplored area right now and perfectly ripe for discovery. One of the few examples of work in this direction is the bug predictor system built by Google engineers, which watches code repositories and their associated project management systems. The more frequently a piece of code shows up in a bug report, the higher the score it gets for being problematic. There is also an exponential time decay to make sure the most recent problems show up higher in the ranking. Although this isnt quite applying deep learning principles to code, it is certainly a step in that direction.  Another example is Siris successor, Viv. A Wired article includes an infographic that shows how Viv builds and executes a computer program based on the speech it receives from the user. This is very different to the way existing natural language processors work. Right now, systems like Siri and Alexa work by matching the language patterns of the request to the available set of skills. Adding new skills means adding new patterns. Once the pattern is matched, a single script is run for each pattern.  With Viv, the process resembles a more sophisticated compiler than a simple pattern matching system. In essence the human language is compiled into a set of algorithms, rather than reduced to simple patterns that can be matched to algorithms. The team at Viv seems to be creating a generic compiler for English to machine language.  The quantity of information going through IP networks is staggering. In 2013, there were around 200TB of data transferred every single minute. Today that number is almost double and growing. Ninety percent of the worlds network data has been created in the last two years.  The only way to keep up with this kind of scale is through automation. As seen in the previous examples of IT operations and the building of software itself, artificial intelligence is about to play a much more prominent role in the way our networks are managed and run.  Every action you take on the internet leaves a network fingerprint. Those fingerprints create patterns. Even encrypted traffic leaves a trail of which IP addresses were accessed at what times and with what frequency. And wherever there are patterns, artificial intelligence will eventually be able to understand them far better than humans can.  A recent Wired article covered the Darpa Cyber Grand Challenge, where AI-based hackers autonomously attacked one another and defended themselves. As you might expect, AIs can run through a cookbook of penetration testing techniques faster than their computer counterparts. The surprise was their success in finding the sorts of subtle and complex security holes that have always required human researchers to discover.  A group of network and security startups are built from the ground up to address the need to automate the discovery and patching of vulnerabilities. Among them is StackPath, a startup run by the founder of SoftLayer. Traditional security software isnt built for the age of IoT where the number of internet-connected devices is exploding.  Site licenses on a per-device basis made sense when the majority of devices were desktop computers, but with the proliferation of smartphones, laptops, pads, surfaces, and Raspberry Pis, not to mention thermostats and major appliances, the surface area for network security attacks is growing out of hand.  After all, the security of your entire network is only as good as the security of its weakest link.  The applications of machine learning to computer automation itself might be the most valuable, underused, and underhyped application of artificial intelligence to date. Why Because, more than any other use of AI, it is a force multiplier.  Using AI to assist in the running and managing of computers themselves offers improvements and opportunities to all other forms of computation, including the AI that runs our cars and powers our robo-investments. Improvements in computers playing chess arguably doesnt make a direct difference to other applications of AI, but improvements to automation do.  Thats why the next steps in the artificial intelligence revolution will be in automation.  New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth. The selection is subjective, based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers. InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content. Send all inquiries tonewtechforum@infoworld.com.",2017-03-30,Are you getting tired of hearing about artificial intelligence? It seems we must be reaching peak hype cycle around AI when almost every article ...,How deep learning will transform automation,http://www.infoworld.com/article/3184837/analytics/how-deep-learning-will-transform-automation.html
12,"Seeks boost in performance, development and affordability of AI products Last week, Intel said that it would begin collaborating with Japan based Preferred Networks (PFN) on the development of a Python-based deep learning framework to boost the performance, development and affordability of AI applications on the market.  The partnership, announced at Intels Tokyo AI Today, will focus on the companys deep learning framework called Chainer, with the results now available in Intels GitHub repository. The framework was made open source in June 2015 and has since been adopted for both research and real-world application environments. It uses Intels open source libraries (Intel Math Kernel Library and Intel Math Kernel Library Deep Neural Network) as fundamental building blocks for AI training and development.  Recently, Intel has been investing heavily in AI across several key groups  financial services, healthcare, retail, and manufacturing. As we mentioned last week, competition in the AI platform market is heating up in 2017 with key players including Intel, IBM, Qualcomm, Nvidia and AMD. Intels recent $400 million investment in buying startup Nervana Systems has contributed to the companys ability to optimize its latest Xeon and Xeon Phi processor lineups specifically for neural networks. The result is more compute density at the hardware level that allows targeted performance increases for inference tasks and machine learning workloads.  While Intel continues to remain bullish on AI, it is betting that the technology will eventually account for one of the largest workloads in corporate data centers. The nature of AI requires that training be done both at the edge and in the data center, said Chapelle. Many of our customers do not wish to share their data on the public cloud, so deep learning will still be kept on premise.  Many other multinational companies including Samsung, Google and HTC have invested heavily in AI as a means to boost the functionality of their digital voice assistant platforms for smartphones. While digital assistants are not a vertical industry that Intel has focused on, they represent a quickly growing market that enhances the intrinsic value of data but is likely bound for an eventual bubble.",2017-04-10,"The partnership, announced at Intel's Tokyo AI Today, will focus on the company's deep learning framework called Chainer, with the results ...",Intel collaborates with Preferred Networks on deep learning,http://www.fudzilla.com/news/43340-intel-collaborates-with-preferred-networks-on-deep-learning
13,"Here at The Next Platform, we tend to focus on deep learning as it relates to hardware and systems versus algorithmic innovation, but at times, it is useful to look at the co-evolution of both code and machines over time to see what might be around the next corner.  One segment of the deep learning applications area that has generated a great deal of work is in speech recognition and translationsomething weve described in detail via efforts from Baidu, Google, Tencent, among others. While the application itself is interesting, what is most notable is how codes and systems have shifted to meet the needs of new ways of thinking about some of the hardest machine learning problems. And when we stretch back to the underpinnings of machine translation and speech recognition, IBM has some of the longest historyeven if that history doesnt have a true deep learning element in relatively recently.  In his 36 years at IBM focusing on speech and language algorithms, Michael Picheny, senior manager for IBMs Watson Multimodal division (an area that focuses on language and image recognition, among other areas), much has changed for both code and the systems required to push speech recognition. While IBM, like many others doing deep learning at scale, has also landed in the large-scale deployment of GPUs for neural networks, the path to that point was long and complex. It is just in the last few years that the critical combination of advanced neural network models and the hardware to run them in real-time and at scale have been made available. And this one-two punch has shifted IBMs approach to speech algorithmic development and deployment.  Picheney says that back when he joined IBM, they were the only company doing speech analysis and recognition with statistical and computational approaches. Others were focused on physical modeling of the underlying processes in speech. They were the only ones solving the problem with compute and mathematical techniques, it was the neatest thing Id ever seen. His early speech recognition work at IBM was done on large mainframes entirely offline, an effort that was later deployed on three separate IBM minicomputers working in parallel to achieve real-time performancea first of its kind capability. Then, in the early 1980s, came the IBM PCs, which could have custom accelerators lashed on until the 1990s, when the work could be done entirely on a CPU. Speech recognition teams in Pichenys group have come full circle to accelerator cards with a reliance on GPUs, which he says are a spot-on architecture for speech, even if there are some limitations hardware-wise on the horizon for the next level of speech recognition using deep learning models.  Code-wise, much has changed for IBM when it comes to speech recognition. Picheny says that earlier systems were a combination of four componentsa feature extractor, acoustic model, language model, and speech recognition engine or decoder. As neural networks evolved, both internally and in the larger ecosystem (Caffe, TensorFlow, etc.), these components have been fused and merged, creating a master modelone that requires significant computational resources and scalability from both hardware and software. Back then, he explains that it was difficult to create an architecture that would run all of these different elements efficiently since they all had their own optimization and other features.  What we are seeing over time is that deep learning methods seem to be taking over more of the speech recognition functions. Bit by bit, deep learning structures and mechanisms are replacing older mechanisms that made it harder to do the heavy duty scaling. In the next couple of years, perhaps longer, we will see deep learning architectures used for all components of speech recognitionand image as well.  He notes that over time too, it might be possible for these many functions to be packaged into a single special purpose chip to make it even easier to scale.  People working in deep learning have become very agile in what theyve learned to do. The field is moving so fast; there are advances in one framework, another in a different oneall of the deep learning packages have their pros and cons, especially for speechWeve worked with all the major packages, some are better than others, but I dont want to say which. But we also have built a good bit of our own code, Picheny says.  For speech, IBM has its own custom-developed neural network models that feed into Watson (which is still a nebulous thing hardware and software-wise, were still working on that story), For these models, the drivers are computation speed and memory. As it turns out, these are the biggest limitations as well, particularly on the memory front.  GPUs are very fast but limited in memory. Thats where the bottleneck is for training on huge quantities of speech. Theres advantage in keeping everything on local memory on the GPU versus fetching it off the chip. And there are algorithms where people are trying to combine results across multiple GPUs, which means making performance tradeoffs for that level of parallelization. What we really want are faster GPUs with more memory.  Aside from custom ASICs just for speech, we asked Picheny about other architectures that seem to be getting traction in deep learning outside of pure-play deep learning chips from companies like Nervana Systems (now part of Intel). One of the likely candidates for speech acceleration could be neuromorphic devicesand IBM has developed its own (TrueNorth). There is a lot of work on neuromorphic architectures but the limitation of these chips, even though they do fascinating things, is that you have to program them far differently than a GPU. And with a GPU there are big communities of people writing special purpose libraries. The limitation is that the people developing their algorithms dont want a new way of programming what theyre already doing.  FPGAs have a similar problem, Picheny says, but are more of an intermediate solution, but programming is still not easy. He says the availability of a rich array of libraries in the GPU CUDA ecosystem is the reason deep learning will continue to be done on GPUs in this middle period before we might see specialized chips just for this task.  On the note above about Watson being nebulous: Pichney agrees that it is hard to pin down just how many different frameworks and models are part of Watsons overall AI umbrella, but this is not by design. Everything is evolving so quickly, especially in the last two years. He says that what Watson was in the beginning, just as with his own speech recognition algorithms and clusters, isnt the same now. While we might not get answers from IBM about just how many components Watson encapsulates and what hardware is required to make it all work for key applications, one can image that Pichenys story about the merging and fusing of various components into master networks with specialized functionality might ring equally true for Watson.",2017-04-07,"Here at The Next Platform, we tend to focus on deep learning as it relates to hardware and systems versus algorithmic innovation, but at times, ...",From Mainframes to Deep Learning Clusters: IBM's Speech Journey,https://www.nextplatform.com/2017/04/07/mainframes-deep-learning-clusters-ibms-speech-journey/
14,"If brands and marketers want to get the right message to the right customer at the right time, they need to prepare  Take for instance, the tired marketing mantra of getting the right message to the right customer at the right time.  While that's long been a goal of marketers, Brandon Purcell, a senior analyst at Forrester, said at Forrester's AI Summit in New York yesterday that the the traditional marketing machine was only capable of identifying the right customer and right time, not the right message.  There is something, however, that could make this into reality. Deep learning, according to Purcell, is a fast evolving set of technologies and algorithms used by researchers, data scientists, and/or developers to build, train and test artificial neural networks that can be used as predictive models to probabilistically predict outcomes and/or identify complex patterns in data.  Quite simply, deep learning is the marketing red pill. And with deep learning, Purcell says, brands and marketers should use these AI-marketing innovations to deliver the right message.  Well, deep learning is used to unlock insight from unstructured data, such as image and video analytics, speech analytics, facial recognition, and text analytics. It's these capabilities, Purcell believes, that will change the way brands interact with consumers in the future.  Deep learning will eventually help automate and optimize the entire customer journey, said Purcell. For marketers, AI will fulfill the promise of customer analytics, giving brands and marketers the ability to get the right message to the right customer at the right time.  Purcell highlighted that certain platforms, such as deep learning and machine learning, will give marketers the ability to train their own machine to learn algorithms from scratch. However, if marketers and brands, were more interested in preparing with an automatic approach with a fully-baked AI solution, Purcell spoke about the following platforms and their strengths:  Of course, Purcell cautioned, the models are only as good as the data, so it's better to be aware and prepared of these changes, if brands and marketers want to get the right message to the right customer at the right time.",2017-04-06,"Well, deep learning is used to unlock insight from unstructured data, such as image and video analytics, speech analytics, facial recognition, ...",How Deep Learning Will Disrupt Marketing,http://www.dmnews.com/marketing-strategy/how-deep-learning-will-disrupt-marketing/article/649067/
15,"Computer scientist David Blaauw pulls a small plastic box from his bag. He carefully uses his fingernail to pick up the tiny black speck inside and place it on the hotel caf table. At 1 cubic millimeter, this is one of a line of the worlds smallest computers. I had to be careful not to cough or sneeze lest it blow away and be swept into the trash.  Blaauw and his colleague Dennis Sylvester, both IEEE Fellows and computer science professors at the University of Michigan, were in San Francisco in February to present 10 papers related to these micromote computers at the IEEE International Solid-State Circuits Conference (ISSCC). Theyve been presenting different variations on the tiny devices for a few years now.  The broad goal of the Michigan Micro Mote (M3) initiative is to make smarter, smaller sensors for medical devices and the Internet of Thingssensors that can do more with less energy. Many of the microphones, cameras, and other sensors that make up the eyes and ears of smart devices are always on alert, and frequently they beam personal data into the cloud because they cant analyze it themselves. Some have predicted that by 2035, there will be 1 trillion such devices. If youve got a trillion devices producing readings constantly, were going to drown in data, says Blaauw. By developing tiny, energy-efficient computing sensors that can do analysis on board, Blaauw and Sylvester hope to make these devices more secure, while also saving energy and bandwidth.  In San Francisco, they described micromote designs that use only a few nanowatts of power to perform tasks such as distinguishing the sound of a passing car and measuring temperature and light levels. They showed off a compact radio that can send data from the small computers to receivers 20meters awaya considerable boost compared with the 50-centimeter range they reported last year. They also described their work with TSMC (Taiwan Semiconductor Manufacturing Co.) on embedding flash memory into the devices and a project to bring on board dedicated, low-power hardware for running artificial intelligence algorithms called deep neural networks.  Blaauw and Sylvester say they take a holistic approach to adding these new features without ramping up power consumption. Theres no one answer to how they and their engineers do it, says Sylvester. If anything, its smart circuit design, Blaauw adds. (They pass ideas back and forth rapidly, not finishing each others sentences but something close to it.)  The memory research is a good example of how the right trade-offs can improve performance, says Sylvester. Previous versions of the micromotes used 8 kilobytes of SRAM, which makes for a pretty low-performance computer. To record video and sound, the tiny computers need more memory. So the group worked with TSMC to bring flash memory on board. Now they can make tiny computers with 1 megabyte of storage.  Flash can store more data in a smaller footprint than SRAM, but it takes a big burst of power to write to the memory. With TSMC, the group designed a new memory array that uses a more efficient charge pump for the writing process. The memory arrays wind up being a bit less dense than TSMCs commercial products, but still much better than SRAM. We were able to get huge gains with small trade-offs, says Sylvester.  Another micromote they presented at ISSCC incorporates a deep-learning processor that can operate a neural network while using just 288 microwatts. Neural networks are artificial intelligence algorithms that perform well at tasks such as face and voice recognition. They typically demand both large memory banks and intense processing power, and so theyre usually run on banks of servers often powered by advanced GPUs. Some researchers have been trying to lessen the size and power demands of deep-learning AI with dedicated hardware thats specially designed to run these algorithms. But even those processors still use over 50 milliwatts of powerfar too much for a micromote. The Michigan group brought down the power requirements by redesigning the chip architecture, for example by situating four processing elements within the memory (in this case, SRAM) to minimize data movement.  The idea is to bring neural networks to the Internet of Things. A lot of motion detection cameras take pictures of branches moving in the windthats not very helpful, says Blaauw. Security cameras and other connected devices are not smart enough to tell the difference between a burglar and a tree, so they waste energy sending uninteresting footage to the cloud for analysis. Onboard deep-learning processors could make better decisions, but only if they dont use too much power. The Michigan group imagines that deep-learning processors could be integrated into many other Internet-connected things besides security systems. For example, an HVAC system could decide to turn the air-conditioning down if it sees several people putting on their coats.  After demonstrating many variations on these micromotes in an academic setting, the Michigan group hopes they will be ready for market in a few years. Blaauw and Sylvester say their startup company, CubeWorks, is currently prototyping devices and researching markets. The company was quietly incorporated in late 2013. Last October, Intel Capital announced it had invested an undisclosed amount in the tiny computer company.  A version of this article appeared in the Tech Talk blog on 10 February 2017.",2017-03-28,Photo: University of Michigan and TSMC Mighty and Mini: One of several varieties of the University of Michigan millimeter-scale computers ...,Speck-Size Computers: Now With Deep Learning,http://spectrum.ieee.org/semiconductors/processors/specksize-computers-now-with-deep-learning
16,"Deep learning has become the technology du jour of late and few companies have advanced the field as much across as many areas or integrated the technology as completely into their operations as Google and its Alphabet affiliates. In keeping with Google's push to externalize its innovations, the company's Next 17 cloud conference featured a number of AI-related announcements and a general theme of democratizing access to the worlds most powerful deep learning systems.  In recent years Google and its sister companies have become synonymous with advancing the AI revolution at a frenzied pace and infusing deep learning across the companys services. Perhaps most famously, last year Deep Minds AlphaGo became the first machine to beat a top Go player, while Waymos driverless cars have become symbols of the autonomous driving revolution. But, it has been the quiet AI revolution, shaping everything from Google Translate to Google Search that has had the greatest impact on Google itself, bringing the power of automated reasoning to bear on almost everything the company does. As it has built up the massive infrastructure to train and run these AI systems, Google has begun bringing these same tools to the masses.  Some companies have built their own AI research units and need to build highly customized models for specific applications. Yet, in doing so they quickly run up against the immense hardware requirements of building large deep learning models, often requiring entire accelerator farms for rapid iteration. In Googles case it offers a hosted deep learning platform called Cloud Machine Learning Engine that takes care of the hardware needs of deep learning development, allowing companies to focus on building their models and offload the computing requirements to Google. After all, few companies have invested so much in AI that they have built their own custom accelerator hardware like Google did with its Tensor Processing Units (TPUs).  Of course, while algorithmic and hardware advances play a significant role in the AI revolution, it is difficult to make true progress in the field without data. Current AI systems require vast volumes of data to learn a new concept. Whereas a human can see a single picture of a new object and instantly recognize it from there forward, a similar AI system requires a tremendous corpus of images depicting the object from many angles to properly build a robust internal representation of it. This means that companies like Google have a significant advantage in being able to muster hundreds of millions of images to build a visual representation of the planet for applications likegeolocation.  In short, the deep learning revolution is powered by data and few companies have as much data as Google. This means that when it comes to deep learning systems, it is easy to find tools, but hard to find pretrained models that you can actually use. I personally experienced this in my pursuit of a system that was robust enough to catalog global news imagery  after trying countless systems over the last several years, I found many that offered some really incredible technology, but none that offered rich prebuilt cataloging with tens of thousands of labels and which worked well on imagery sourced from the non-Western world, until I came across Googles Cloud Vision system.  In fact, this is a common need of many companies  they are interested in building services for their customers, not conducting AI research. In following its externalization trend, Google has risen to this challenge by releasing many of its internal AI systems as public cloud APIs. Cloud Vision accepts any arbitrary image and catalogs objects and activities, OCRs text, recognizes the location depicted, estimates the emotion of human faces and even flags whether the image depicts violence. All with a single API call and with results returned in just a few seconds and infinitely scalable. Cloud Speech performs live speech to text in over 80 languages and, unlike legacy speech transcription systems, requires no training and is incredibly robust to noise. Cloud Natural Language accepts arbitrary text in English, Spanish and Japanese and outputs a robust dependency parse tree, recognizes key entities and even performs sentiment analysis. At Next 17, Google expanded this lineup with itslatest tool, Cloud Video Intelligence, which takes a video and segments it into scenes and identifies the major topics and activities in each scene, allowing one to take a massive video archive and instantly index it to make it topically searchable.  What makes these APIs so powerful is that they are exposed as a simple API and really do just work right out of the box. You simply make an API call with your dataand after a few seconds get back the results of pretrained algorithms built by some of the top AI researchers in the world. The massive complexity of deep learning is all hidden behind a simple API call and you can even string API calls together to build unimaginably complex workflows with just a few lines of code.  Teowakis Javier Ramirez offers a glimpse at just how easy it is to rapidly build an entire workflow(and an immensely powerful one at that) from these APIs with just a few minutes of time and a few lines of code. In his tutorial, he takes a YouTube video of British Prime Minister Theresa Mays inaugural speech and feeds it through the Cloud Speech API to instantly generate a high quality textual transcript. He then feeds that transcript through Cloud Natural Language to extract the primary mentioned entities (along with links to their Wikipedia pages for more information) and calculate a general sentiment of the speech. It took just a few lines of code making a few API calls to take a YouTube video, transcribe it, extract key entities and sentiment code it. Even more amazingly, the entire workflow could be scaled up to run across millions of videos without a single change. This is the power of cloud.  Putting this all together, just as Google is externalizing its services and security models, it has also been opening the doors to its incredible AI advances, offering both a hosted AI environment for companies looking to build their own models and an ever-growing array of pretrained models that just work out of the box and allow companies to build complex applications with just a few API calls. It was clear from the number of sessions at Next that involved AI and its heavy presence in the keynotes that Google is betting big on rolling AI into the enterprise. In the end, Google has effectively democratized access to some of the worlds most advanced AI algorithms by making them so easy to use (literally just an API call away) that even the smallest businesses can now leverage the full power of deep learning to revolutionize how they do business.",2017-03-24,Deep learning has become the technology du jour of late and few companies have advanced the field as much across as many areas or ...,Recapping Google NEXT 2017: Deep Learning As A Service,http://www.forbes.com/sites/kalevleetaru/2017/03/24/recapping-google-next-2017-deep-learning-as-a-service/
17,"Artificial intelligence has reached peak hype. News outletsreport thatcompanies have replaced workers with IBM Watson and that algorithms are beating doctors at diagnoses. New AI startups pop up everyday, claiming to solve all your personal and businessproblems with machine learning.  Ordinary objects like juicers and Wi-Firouters suddenly advertise themselves as powered by AI. Not only can smart standing desks remember your height settings, they can also order you lunch.  Much of the AIhubbubis generated by reporters whove never trained a neural network and by startups or those hoping to be acqui-hired for engineering talent despite not having solved any real business problems. No wonder there are so many misconceptions about what AI can and cannot do.  Neural networks were invented in the 60s, butrecent boosts in big data and computational power made them actually useful. A new discipline called deep learning has arisen that can apply complex neural network architectures to model patterns in data more accurately than ever before.  The results are undeniably impressive. Computers can now recognize objects in images and video and transcribe speech to text better than humans can. Google replaced Google Translates architecture with neural networks, and now machine translation is also closing in on human performance.  The practical applications are mind-blowing as well. Computers can predict crop yield better than the USDA and indeed diagnose cancermore accurately than elite physicians.  John Launchbury, a director at DARPA, describes three waves of artificial intelligence:  1. Handcrafted knowledge, or expert systems like IBMs Deep Blue or Watson  3. Contextual adaption, which involves constructing reliable, explanatory models for real-world phenomena using sparse data, like humans do  As part of thecurrent second wave of AI, deep learning algorithms work wellbecause of what Launchbury calls the manifold hypothesis (see below). Insimplified terms, this refers to howdifferent types of high-dimensionalnaturaldata tend to clump and be shaped differently when visualized in lower dimensions.  By mathematically manipulating and separating data clumps, deep neural networks can distinguishdifferent data types. While neural nets can achieve nuanced classification and predication capabilities, they are essentially what Launchbury calls spreadsheets on steroids.  At the recent AI By The Bay conference, Francois Chollet emphasized that deep learning is simply more powerful pattern recognition than previous statistical and machine learning methods. The most important problem for AI today is abstraction and reasoning, explains Chollet, an AI researcher at Google and famed inventor of widely used deep learning library Keras. Current supervised perception and reinforcement learning algorithms require lots of data, are terrible at planning, and are only doing straightforward pattern recognition.  By contrast, humans learn from very few examples, can do very long-term planning, and are capable of forming abstract models of a situation and [manipulating] these models to achieve extreme generalization.  Even simple human behaviors are laborious to teach to a deep learning algorithm. Lets examine a situation such as avoiding being hit by a car as you walk down the road. If you go the supervised learning route, youd need huge data sets of car situations with clearly labeled actions to take, such as stop or move. Then youd need to train a neural network to learn the mapping between the situation and the appropriate action.  If you go the reinforcement learning route, where you give an algorithm a goal and let it independently determine theideal actions to take, the computer would need to die thousands of times before learning to avoid cars in different situations.  You cannot achieve general intelligence simply by scaling up todays deep learning techniques, warns Chollet.  Humans only need to be told once to avoid cars. Were equipped with the ability to generalize from just a few examples and are capable of imagining (i.e. modeling) the dire consequences of being run over. Without losing life or limb, most of us quickly learn to avoid being overrun by motor vehicles.  While neural networks achieve statistically impressive results across large sample sizes, they are individually unreliable and often make mistakes humans would never make, such as classifying a toothbrush as a baseball bat.  Your results are only as good as your data. Neural networks fed inaccurate or incomplete data will simplyproduce the wrong results. The outcomes can be bothembarrassing and damaging.In two major PR debacles, Google Images incorrectly classified African Americans as gorillas, while Microsofts Tay learned to spew racist, misogynistichate speech after only hours training on Twitter.  Undesirable biases are implicit in our input data. Googles massive Word2Vec embeddings are built off of 3 million words from Google News. The data set makes associations such as father is to doctor as motheris to nurse, which reflect gender bias in our language. Researchers such asTolga Bolukbasi of Boston University have taken to human ratings onMechanical Turk to perform hard de-biasing to undo such associations.  Such tactics are essential since, according to Bolukbasi,word embeddings not only reflect stereotypes but can also amplify them. If the term doctor is more associated with men than women, then an algorithm might prioritize male job applicants over female job applicants for open physician positions.  Finally,Ian Goodfellow, inventor of generative adversarial networks (GANs), showed that neural networks can be deliberatelytricked with adversarial examples. By mathematically manipulating an image in a way that is undetectable to the human eye, sophisticatedattackers can trick neural networks into grossly misclassifying objects.  The dangers such adversarial attacks pose to AI systems are alarming, especially since adversarial images and original images seem identical to us. Self-driving cars could be hijacked with seemingly innocuous signage and secure systems could be compromised by data that initially appears normal.  How can we overcome the limitations of deep learning and proceed toward general artificial intelligence Chollets initial plan of attack involves using super-human pattern recognition, like deep learning, to augment explicit search and formal systems, starting with the field of mathematical proofs. Automated Theorem Provers (ATPs) typically use brute force searchand quickly hit combinatorial explosions in practical use. In the DeepMath project, Chollet and his colleagues used deep learning to assist the proof search process, simulating a mathematicians intuitions about what lemmas(a subsidiary or intermediate theorem in an argument or proof) might be relevant.  Another approach is to develop more explainable models. In handwriting recognition,neural nets currentlyneed to be trained on tens to hundreds of thousands of examples to perform decent classification. Instead of looking at just pixels, however, Launchbury of DARPA explains that generativemodels can be taught the strokes behind any given character and can use this physical construction information to disambiguate between similar numbers, such as a 9 or a 4.  Yann LeCun, inventor of convolutional neural networks (CNNs) and director of AI research at Facebook, proposes energy-based models as a method of overcoming limits in deep learning. Typically, a neural network is trained to produce a single output, such as an image label or sentence translation. LeCuns energy-based models instead give an entire set of possible outputs, such as the many ways a sentence could be translated, along with scores for each configuration.  Geoffrey Hinton, widely called the father of deep learning wants to replace neurons in neural networks with capsules that he believesmore accurately reflect the cortical structure in thehuman mind. Evolution must have found an efficient way to adapt features that are early in a sensory pathway so that they are more helpful to features that are several stages later in the pathway, Hinton explains. He hopes that capsule-based neural network architectures will be more resistant to the adversarial attacks that Goodfellow illuminated above.  Perhaps all of these approaches to overcoming the limits of deep learning have truth value. Perhaps none of them do. Only time and continued investment in AI research will tell.",2017-04-02,A new discipline called deep learning has arisen that can apply complex neural network architectures to model patterns in data more ...,Understanding the limits of deep learning,https://venturebeat.com/2017/04/02/understanding-the-limits-of-deep-learning/
18,"Its been almost two years since Google liquefied our brains with its Deep Dream neural network and the nightmare-inducing images the technology created. But now, a team from the University of California, Berkeley is sort of doing the oppositeemphasis on sort of.  The Berkeley team built a new type of deep learning software, dubbed CycleGAN, that can convert impressionist paintings into photorealistic images. With a few tweaks, the tool can also turn horses into zebras, apples into oranges, and winter into summer. Why would you ever want to do these kinds of tricks Who knows. At the very least, CycleGAN is an interesting proof of concept.  Like other deep learning techniques, the Berkeley software works by drawing relationships between two sets of common images. The primary example that the researchers discuss in their paper about the software deals with Monet paintings and landscape photography. The researchers trained CycleGAN with a set of the master impressionists paintings as well as a random batch of landscape photos pulled from Flickr, and then applied algorithms to mimic the style in either direction. The end result looks something like a photo of the landscapes Monet painted, even though weve never seen a photo of that exact landscape. Likewise, the software can take a random landscape photo and make it look like a Monet painting    The Berkeley researchers also applied the CycleGAN software to other artists work, including Cezanne, van Gogh, and Renoir. In the examples included in the paper, however, they converted photographs into painting-like images:    Perhaps obviously, the effect also works with stills from movies, including the popular and seemingly pointless live-action remake of Disneys Beauty and the Beast. Heres Belle and Gaston in the style of Renoir:  Youre probably thinking that this is just some kind of advanced Photoshop filter, and you wouldnt be completely wrong. Adobe is constantly developing new technology to make your images look crazy, like the Deep Photo Style transfer technique the company built with Cornell researchers and showed off a few days ago. That software enabled researchers to transfer attributes from one photo to another, and quite frankly, it looks pretty damn impressive:  This is all a very roundabout way of saying that the future of truth is doomed, since even a photograph can become a lie with a few keystrokes. You can even install the new Berkeley software yourself since the researchers put the code on Github. Just bear in mind it doesnt work perfectly all the time. Just look at these failures:",2017-04-03,"The Berkeley team built a new type of deep learning software, dubbed CycleGAN, that can convert impressionist paintings into photorealistic ...",Someone Finally Hijacked Deep Learning Tech to Create More ...,http://gizmodo.com/someone-finally-hijacked-deep-learning-tech-to-create-m-1793957126
19,"In 2011 Google realized they had a problem. They were getting serious about deep learning networks with computational demands that strained their resources. Google calculated they would have to have twice as manydata centers as they already had if people used their deep learning speech recognition models for voice search for just three minutes a day. They needed more powerful and efficient processing chips.  What kind of chip did they needCentral processing units (CPUs) are built to handle a wide variety of computational tasks very efficiently. Their limitation is that they can only handle a relatively small number of tasks at one time. GPUs, on the other hand, are less efficient at carrying out a single task and they can handle a much smaller variety of tasks. Their strength is that they can carry out many tasks at the same time. If you have to multiply three floating point numbers, a CPU will crush a GPU; if you have to multiply a million sets of three floating point numbers, the GPU will blow the CPU out of the room.  GPUs are ideal for deep learning applications because sophisticated deep learning networks perform millions of computations that can be carried out at the same time. Nvidia is the go-to chip company for GPUs that are designed for machine learning.Google uses Nvidia GPUs but they needed something faster. They also needed a chip that is more efficient. A single GPU doesnt consume a lot of energy but when you have millions of servers running 24/7 as Google does, energy consumption becomes a serious problem. Google decided to build a chip of their own.  Google told the world they had designed their own chip in May of last year. They called it a Tensor processing unit (TPU) because its custom designed to work with Tensorflow, Googles open-source software library for machine learning. A good deal of speculation about what Google had in mind for the TPU accompanied the announcement because Google did not provide very much information about the chips architecture or what it could do. That information came in a paper Google released last week and it is very compelling.  The paper compares performance of the first generation TPU (which has been running in Googles data centers since 2015) with Intels Haswell CPUs and Nvidias K80 Kepler dual GPU. A deep learning network must be trained before it can be used to infer information from data. Googles first-generation TPU was designed for inference and thus the performance comparison was limited to inference operations.  Google compared the chips in terms of speed and efficiency. Speed was measured as tera (trillion)-operations performed per second as a function of memory bandwidth. The TPU was 15x to 30x faster than the CPUs and GPUs. Efficiency was measured as tera-operations performed per Watt of energy consumed. The TPU was 30x to 80x more efficient than the CPUs and GPUs.  These are extraordinary numbers but several caveats must be noted before jumping to the conclusion that the TPU is the future of deep learning computing. Googles tests were carried out using chips that were contemporary in early 2015. Google, Nvidia and Intel have all improved their chips since then and it is unknown how todays chips compare. Still, the TPUs advantages were so great two years ago that its unlikely Intel and Nvidia have completely closed the gap.  A more important consideration is the nature of the chips being compared. Intels CPUs are general purpose chips designed for flexibility and speed running a limited number of processes at one time. Nvidias GPUs are general purpose chips designed for running many neural net computations at one time. Googles TPU is an ASIC (application specific integrated circuit) that is custom designed to carry out specific functions in Tensorflow.  The CPU has maximum flexibility. It can run a wide variety of programs including deep learning networks performing both learning and inference using many software libraries. The GPU is not as flexible as the CPU but it is better at deep learning computation, it can carry out both learning and inference and it is also not limited to a singlesoftware library. The tested TPU has almost no flexibility. It does one thing, inference in Tensorflow, but it does it brilliantly.  Chip deployment in deep learning computing is not a zero-sum game. Real-world deep learning networks need a GPU in the system to communicate with either GPUs or ASICs like Googles TPU. GPUs are ideal for work environments where deep learning flexibility is required or the necessary ASICs have yet to be built. ASICs are ideal when a full commitment has been made to a software library or platform.  Google has obviously made that commitment to Tensorflow and the TPUs superior performance makes it highly likely that Tensorflow and the TPU will evolve together. The tight connection between the TPU and specific functions within particular builds of Tensorflow makes it unclear whether it's sensiblefor Google to market their chips outside the company. However, third parties that make useof Googles cloud services for machine learning solutionscan reap the benefits of the TPUs exceptional performance metrics.  Kevin Murnane coversscience, technology and video games for Forbes. You can find more of his writing at The Info Monkeyand Tuned In To Cycling. Follow on Twitter@TheInfoMonkey.",2017-04-09,In 2011 Google realized they had a problem. They were getting serious about deep learning networks with computational demands that ...,The Great Strengths and Important Limitations Of Google's Machine ...,https://www.forbes.com/sites/kevinmurnane/2017/04/10/the-great-strengths-and-important-limitations-of-googles-machine-learning-chip/
20,"Last year, a strange self-driving car was released onto the quiet roads of Monmouth County, New Jersey. The experimental vehicle, developed by researchers at the chip maker Nvidia, didnt look different from other autonomous cars, but it was unlike anything demonstrated by Google, Tesla, or General Motors, and it showed the rising power of artificial intelligence. The car didnt follow a single instruction provided by an engineer or programmer. Instead, it relied entirely on an algorithm that had taught itself to drive by watching a human do it.  Getting a car to drive this way was an impressive feat. But its also a bit unsettling, since it isnt completely clear how the car makes its decisions. Information from the vehicles sensors goes straight into a huge network of artificial neurons that process the data and then deliver the commands required to operate the steering wheel, the brakes, and other systems. The result seems to match the responses youd expect from a human driver. But what if one day it did something unexpectedcrashed into a tree, or sat at a green light As things stand now, it might be difficult to find out why. The system is so complicated that even the engineers who designed it may struggle to isolate the reason for any single action. And you cant ask it: there is no obvious way to design such a system so that it could always explain why it did what it did.  The mysterious mind of this vehicle points to a looming issue with artificial intelligence. The cars underlying AI technology, known as deep learning, has proved very powerful at solving problems in recent years, and it has been widely deployed for tasks like image captioning, voice recognition, and language translation. There is now hope that the same techniques will be able to diagnose deadly diseases, make million-dollar trading decisions, and do countless other things to transform whole industries.  But this wont happenor shouldnt happenunless we find ways of making techniques like deep learning more understandable to their creators and accountable to their users. Otherwise it will be hard to predict when failures might occurand its inevitable they will. Thats one reason Nvidias car is still experimental.  Already, mathematical models are being used to help determine who makes parole, whos approved for a loan, and who gets hired for a job. If you could get access to these mathematical models, it would be possible to understand their reasoning. But banks, the military, employers, and others are now turning their attention to more complex machine-learning approaches that could make automated decision-making altogether inscrutable. Deep learning, the most common of these approaches, represents a fundamentally different way to program computers. It is a problem that is already relevant, and its going to be much more relevant in the future, says Tommi Jaakkola, a professor at MIT who works on applications of machine learning. Whether its an investment decision, a medical decision, or maybe a military decision, you dont want to just rely on a black box method.  Theres already an argument that being able to interrogate an AI system about how it reached its conclusions is a fundamental legal right. Starting in the summer of 2018, the European Union may require that companies be able to give users an explanation for decisions that automated systems reach. This might be impossible, even for systems that seem relatively simple on the surface, such as the apps and websites that use deep learning to serve ads or recommend songs. The computers that run those services have programmed themselves, and they have done it in ways we cannot understand. Even the engineers who build these apps cannot fully explain their behavior.  This raises mind-boggling questions. As the technology advances, we might soon cross some threshold beyond which using AI requires a leap of faith. Sure, we humans cant always truly explain our thought processes eitherbut we find ways to intuitively trust and gauge people. Will that also be possible with machines that think and make decisions differently from the way a human would Weve never before built machines that operate in ways their creators dont understand. How well can we expect to communicateand get along withintelligent machines that could be unpredictable and inscrutable These questions took me on a journey to the bleeding edge of research on AI algorithms, from Google to Apple and many places in between, including a meeting with one of the great philosophers of our time.  In 2015, a research group at Mount Sinai Hospital in New York was inspired to apply deep learning to the hospitals vast database of patient records. This data set features hundreds of variables on patients, drawn from their test results, doctor visits, and so on. The resulting program, which the researchers named Deep Patient, was trained using data from about 700,000 individuals, and when tested on new records, it proved incredibly good at predicting disease. Without any expert instruction, Deep Patient had discovered patterns hidden in the hospital data that seemed to indicate when people were on the way to a wide range of ailments, including cancer of the liver. There are a lot of methods that are pretty good at predicting disease from a patients records, says Joel Dudley, who leads the Mount Sinai team. But, he adds, this was just way better.  At the same time, Deep Patient is a bit puzzling. It appears to anticipate the onset of psychiatric disorders like schizophrenia surprisingly well. But since schizophrenia is notoriously difficult for physicians to predict, Dudley wondered how this was possible. He still doesnt know. The new tool offers no clue as to how it does this. If something like Deep Patient is actually going to help doctors, it will ideally give them the rationale for its prediction, to reassure them that it is accurate and to justify, say, a change in the drugs someone is being prescribed. We can build these models, Dudley says ruefully, but we dont know how they work.  Artificial intelligence hasnt always been this way. From the outset, there were two schools of thought regarding how understandable, or explainable, AI ought to be. Many thought it made the most sense to build machines that reasoned according to rules and logic, making their inner workings transparent to anyone who cared to examine some code. Others felt that intelligence would more easily emerge if machines took inspiration from biology, and learned by observing and experiencing. This meant turning computer programming on its head. Instead of a programmer writing the commands to solve a problem, the program generates its own algorithm based on example data and a desired output. The machine-learning techniques that would later evolve into todays most powerful AI systems followed the latter path: the machine essentially programs itself.  At first this approach was of limited practical use, and in the 1960s and 70s it remained largely confined to the fringes of the field. Then the computerization of many industries and the emergence of large data sets renewed interest. That inspired the development of more powerful machine-learning techniques, especially new versions of one known as the artificial neural network. By the 1990s, neural networks could automatically digitize handwritten characters.  But it was not until the start of this decade, after several clever tweaks and refinements, that very largeor deepneural networks demonstrated dramatic improvements in automated perception. Deep learning is responsible for todays explosion of AI. It has given computers extraordinary powers, like the ability to recognize spoken words almost as well as a person could, a skill too complex to code into the machine by hand. Deep learning has transformed computer vision and dramatically improved machine translation. It is now being used to guide all sorts of key decisions in medicine, finance, manufacturingand beyond.  The workings of any machine-learning technology are inherently more opaque, even to computer scientists, than a hand-coded system. This is not to say that all future AI techniques will be equally unknowable. But by its nature, deep learning is a particularly dark black box.  You cant just look inside a deep neural network to see how it works. A networks reasoning is embedded in the behavior of thousands of simulated neurons, arranged into dozens or even hundreds of intricately interconnected layers. The neurons in the first layer each receive an input, like the intensity of a pixel in an image, and then perform a calculation before outputting a new signal. These outputs are fed, in a complex web, to the neurons in the next layer, and so on, until an overall output is produced. Plus, there is a process known as back-propagation that tweaks the calculations of individual neurons in a way that lets the network learn to produce a desired output.  The many layers in a deep network enable it to recognize things at different levels of abstraction. In a system designed to recognize dogs, for instance, the lower layers recognize simple things like outlines or color; higher layers recognize more complex stuff like fur or eyes; and the topmost layer identifies it all as a dog. The same approach can be applied, roughly speaking, to other inputs that lead a machine to teach itself: the sounds that make up words in speech, the letters and words that create sentences in text, or the steering-wheel movements required for driving.  Ingenious strategies have been used to try to capture and thus explain in more detail whats happening in such systems. In 2015, researchers at Google modified a deep-learning-based image recognition algorithm so that instead of spotting objects in photos, it would generate or modify them. By effectively running the algorithm in reverse, they could discover the features the program uses to recognize, say, a bird or building. The resulting images, produced by a project known as Deep Dream, showed grotesque, alien-like animals emerging from clouds and plants, and hallucinatory pagodas blooming across forests and mountain ranges. The images proved that deep learning need not be entirely inscrutable; they revealed that the algorithms home in on familiar visual features like a birds beak or feathers. But the images also hinted at how different deep learning is from human perception, in that it might make something out of an artifact that we would know to ignore. Google researchers noted that when its algorithm generated images of a dumbbell, it also generated a human arm holding it. The machine had concluded that an arm was part of the thing.  Further progress has been made using ideas borrowed from neuroscience and cognitive science. A team led by Jeff Clune, an assistant professor at the University of Wyoming, has employed the AI equivalent of optical illusions to test deep neural networks. In 2015, Clunes group showed how certain images could fool such a network into perceiving things that arent there, because the images exploit the low-level patterns the system searches for. One of Clunes collaborators, Jason Yosinski, also built a tool that acts like a probe stuck into a brain. His tool targets any neuron in the middle of the network and searches for the image that activates it the most. The images that turn up are abstract (imagine an impressionistic take on a flamingo or a school bus), highlighting the mysterious nature of the machines perceptual abilities.  We need more than a glimpse of AIs thinking, however, and there is no easy solution. It is the interplay of calculations inside a deep neural network that is crucial to higher-level pattern recognition and complex decision-making, but those calculations are a quagmire of mathematical functions and variables. If you had a very small neural network, you might be able to understand it, Jaakkola says. But once it becomes very large, and it has thousands of units per layer and maybe hundreds of layers, then it becomes quite un-understandable.  In the office next to Jaakkola is Regina Barzilay, an MIT professor who is determined to apply machine learning to medicine. She was diagnosed with breast cancer a couple of years ago, at age 43. The diagnosis was shocking in itself, but Barzilay was also dismayed that cutting-edge statistical and machine-learning methods were not being used to help with oncological research or to guide patient treatment. She says AI has huge potential to revolutionize medicine, but realizing that potential will mean going beyond just medical records. She envisions using more of the raw data that she says is currently underutilized: imaging data, pathology data, all this information.  After she finished cancer treatment last year, Barzilay and her students began working with doctors at Massachusetts General Hospital to develop a system capable of mining pathology reports to identify patients with specific clinical characteristics that researchers might want to study. However, Barzilay understood that the system would need to explain its reasoning. So, together with Jaakkola and a student, she added a step: the system extracts and highlights snippets of text that are representative of a pattern it has discovered. Barzilay and her students are also developing a deep-learning algorithm capable of finding early signs of breast cancer in mammogram images, and they aim to give this system some ability to explain its reasoning, too. You really need to have a loop where the machine and the human collaborate, -Barzilay says.  The U.S. military is pouring billions into projects that will use machine learning to pilot vehicles and aircraft, identify targets, and help analysts sift through huge piles of intelligence data. Here more than anywhere else, even more than in medicine, there is little room for algorithmic mystery, and the Department of Defense has identified explainability as a key stumbling block.  David Gunning, a program manager at the Defense Advanced Research Projects Agency, is overseeing the aptly named Explainable Artificial Intelligence program. A silver-haired veteran of the agency who previously oversaw the DARPA project that eventually led to the creation of Siri, Gunning says automation is creeping into countless areas of the military. Intelligence analysts are testing machine learning as a way of identifying patterns in vast amounts of surveillance data. Many autonomous ground vehicles and aircraft are being developed and tested. But soldiers probably wont feel comfortable in a robotic tank that doesnt explain itself to them, and analysts will be reluctant to act on information without some reasoning. Its often the nature of these machine-learning systems that they produce a lot of false alarms, so an intel analyst really needs extra help to understand why a recommendation was made, Gunning says.  This March, DARPA chose 13 projects from academia and industry for funding under Gunnings program. Some of them could build on work led by Carlos Guestrin, a professor at the University of Washington. He and his colleagues have developed a way for machine-learning systems to provide a rationale for their outputs. Essentially, under this method a computer automatically finds a few examples from a data set and serves them up in a short explanation. A system designed to classify an e-mail message as coming from a terrorist, for example, might use many millions of messages in its training and decision-making. But using the Washington teams approach, it could highlight certain keywords found in a message. Guestrins group has also devised ways for image recognition systems to hint at their reasoning by highlighting the parts of an image that were most significant.  One drawback to this approach and others like it, such as Barzilays, is that the explanations provided will always be simplified, meaning some vital information may be lost along the way. We havent achieved the whole dream, which is where AI has a conversation with you, and it is able to explain, says Guestrin. Were a long way from having truly interpretable AI.  It doesnt have to be a high-stakes situation like cancer diagnosis or military maneuvers for this to become an issue. Knowing AIs reasoning is also going to be crucial if the technology is to become a common and useful part of our daily lives. Tom Gruber, who leads the Siri team at Apple, says explainability is a key consideration for his team as it tries to make Siri a smarter and more capable virtual assistant. Gruber wouldnt discuss specific plans for Siris future, but its easy to imagine that if you receive a restaurant recommendation from Siri, youll want to know what the reasoning was. Ruslan Salakhutdinov, director of AI research at Apple and an associate professor at Carnegie Mellon University, sees explainability as the core of the evolving relationship between humans and intelligent machines. Its going to introduce trust, he says.  Just as many aspects of human behavior are impossible to explain in detail, perhaps it wont be possible for AI to explain everything it does. Even if somebody can give you a reasonable-sounding explanation [for his or her actions], it probably is incomplete, and the same could very well be true for AI, says Clune, of the University of Wyoming. It might just be part of the nature of intelligence that only part of it is exposedto rational explanation. Some of it is just instinctual, or subconscious, or inscrutable.  If thats so, then at some stage we may have to simply trust AIs judgment or do without using it. Likewise, that judgment will have to incorporate social intelligence. Just as society is built upon a contract of expected behavior, we will need to design AI systems to respect and fit with our social norms. If we are to create robot tanks and other killing machines, it is important that their decision-making be consistent with our ethical judgments.  To probe these metaphysical concepts, I went to Tufts University to meet with Daniel Dennett, a renowned philosopher and cognitive scientist who studies consciousness and the mind. A chapter of Dennetts latest book, From Bacteria to Bach and Back, an encyclopedic treatise on consciousness, suggests that a natural part of the evolution of intelligence itself is the creation of systems capable of performing tasks their creators do not know how to do. The question is, what accommodations do we have to make to do this wiselywhat standards do we demand of them, and of ourselves he tells me in his cluttered office on the universitys idyllic campus.  He also has a word of warning about the quest for explainability. I think by all means ifwere going to use these things and rely on them, then lets get as firm a grip on how and why theyre giving us the answers as possible, he says. But since there may be no perfect answer, we should be as cautious of AI explanations as we are of each othersno matter how clever a machine seems.If it cant do better than us at explaining what its doing, he says, then dont trustit.",2017-04-10,"The car's underlying AI technology, known as deep learning, has proved very powerful at solving problems in recent years, and it has been ...",The Dark Secret at the Heart of AI,https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/
21,"It is one thing to scale a neural network on a single GPU or even a single system with four or eight GPUs. But it is another thing entirely to push it across thousands of nodes. Most centers doing deep learning have relatively small GPU clusters for training and certainly nothing on the order of the Titan supercomputer at Oak Ridge National Laboratory.  The emphasis on machine learning scalability has often been focused on node counts in the past for single-model runs. This is useful for some applications, but as neural networks become more integrated into existing workflows, including those in HPC, there is another way to consider scalability. Interestingly, the lesson comes from an HPC application area like weather modeling where, instead of one monolithic model to predict climate, an ensemble of forecasts run in parallel on a massive supercomputer are meshed together for the best result. Using this ensemble method on deep neural networks allows for scalability across thousands of nodes, with the end result being derived from an average of the ensemblesomething that is acceptable in an area that does not require the kind of precision (in more ways than one) that some HPC calculations do.  This approach has been used on the Titan supercomputer at Oak Ridge, which is a powerhouse for deep learning training given its high GPU counts. Titans 18,688 Tesla K20X GPUs have proven useful for a large number of scientific simulations and are now pulling double-duty on deep learning frameworks, including Caffe, to boost the capabilities of HPC simulations (classification, filtering of noise, etc.). The next generation supercomputer at the lab, the future Summit machine (expected to be operational at the end of 2017) will provide even more GPU power with the Volta generation Tesla graphics coprocessors from Nvidia, high-bandwidth memory, NVLink for faster data movement, and IBM Power9 CPUs.  ORNL researchers used this ensemble approach to neural networks and were able to stretch these across all of the GPUs in the machine. This is a notable feat, even for the types of large simulations that are built to run on big supercomputers. What is interesting is that while the frameworks might come from the deep learning (Caffe in ORNLs case), the node to node communication is rooted in HPC. As we have described before, MPI is still the best method out there for fast communication across InfiniBand-connected nodes and like researchers elsewhere, ORNL has adapted it to deep learning at scale.  Right now, the team is using each individual node to train an individual deep learning network, but all of those different networks need to have the same data if training from the same set. The question is how to feed that same data to over 18,000 different GPUs at almost the same timeand on a system that wasnt designed with that in mind The answer is in a custom MPI-based layer that can divvy up the data and distribute it. With the coming Summit supercomputerthe successor to Titan, which will sport six Volta GPUs per nodethe other problem is multi-GPU scaling, something application teams across HPC are tackling as well.  Ultimately, the success of MPI for deep learning at such scale will depend on how many messages the system and MPI can handle since there is both results between nodes in addition to thousands of synchronous updates for training iterations. Each iteration will cause a number of neurons within the network to be updated, so if the network is spread across multiple nodes, all of that will have to be communicated. That is large enough task on its ownbut also consider the delay of the data that needs to be transferred to and from disk (although a burst buffer can be of use here). There are also new ways of looking at MPIs guarantees for robustness, which limits certain communication patterns. HPC needs this, but neural networks are more fault-tolerant than many HPC applications, Patton says. Going forward, that the same I/O is being used to communicate between the nodes and from disk, so when the datasets are large enough the bandwidth could quickly dwindle.  In addition to their work scaling deep neural networks across Titan, the team has also developed a method of automatically designing neural networks for use across multiple datasets. Before, a network designed for image recognition could not be reused for speech, but their own auto-designing code has scaled beyond 5,000 (single GPU) nodes on Titan with up to 80 percent accuracy.  The algorithm is evolutionary, so it can take design parameters of a deep learning network and evolve those automatically,Robert Patton, a computational analytics scientist at Oak Ridge, tells The Next Platform. We can take a dataset that no one has looked at before and automatically generate a network that works well on that dataset.  Since developing the auto-generating neural networks, Oak Ridge researchers have been working with key application groups that can benefit from the noise filtering and data classification that large-scale neural nets can provide. These include high-energy particle physics, where they are working with Fermi National Lab to classify neutrinos and subatomic particles. Simulations produce so much data and its too hard to go through it all or even keep it all on disk, says Patton. We want to identify things that are interesting in data in real time in a simulation so we can snapshot parts of the data in high resolution and go back later.  It is with an eye onSummit and the challenges to programming the systemthat teams at Oak Ridge are swiftly figuring out where deep learning fits into existing HPC workflows and how to maximize the hardware theyll have on hand.  We started taking notice of deep learning in 2012 and things really took off then, in large part because of the move of those algorithms to the GPU, which allowed researchers to speed the development process, Patton explains. There has since been a lot of progress made toward tackling some of the hardest problems and by 2014, we started seeing that if one GPU is good for deep learning, what could we do with 18,000 of them on the Titan supercomputer.  While large supercomputers like Titan have the hybrid GPU/CPU horsepower for deep learning at scale, they are not built for these kinds of workloads. Some hardware changes in Summit will go a long way toward speeding through some bottlenecks, but the right combination of hardware might include some non-standard accelerators like neuromorphic devices and other chips to bolster training or inference. Right now, if we were to use machine learning in real-time for HPC applications, we still have the problem of training. We are loading the data from disk and the processing cant continue until the data comes off disk, so we are excited for Summit, which will give us the ability to get the data off disk faster in the nodes, which will be thicker, denser and have more memory and storage, Patton says.  It takes a lot of computation on expensive HPC systems to find the distinguishing features in all the noise, says Patton. The problem is, we are throwing away a lot of good data. For a field like materials science, for instance, its not unlikely for them to pitch more than 90 percent of their data because its so noisy and they lack the tools to deal with it. He says this is also why his teams are looking at integrating novel architectures to offload to, including neuromorphic and quantum computerssomething we will talk about more later this week in an interview with ORNL collaborator, Thomas Potok.",2017-03-28,Most centers doing deep learning have relatively small GPU clusters for training and certainly nothing on the order of the Titan supercomputer ...,Scaling Deep Learning on an 18000 GPU Supercomputer,https://www.nextplatform.com/2017/03/28/scaling-deep-learning-beyond-18000-gpus/
22,"Over the course of March of the Machines, there has been a lot of talk about machine learning and deep learning, and the jobs arising from them, but what is it like to work in that field  When we talk about emerging technologies and the future of tech, deep learning is an area that crops up again and again. It will be the driving force behind the development of AI and robotics, and already plays an essential part in the creation of tech we use on a daily basis. But what is it like to work in this evolving sector  We asked Kevin McGuinness, research fellow at the Insight Centre for Data Analytics, Dublin City University(DCU), about what hes doing with deep learning and how the area is changing.  I am a research fellow working on topics in computer vision and deep learning. I also teach graduate-level data analysis and machine learning for the School of Electronic Engineering. My research is funded by Science Foundation Ireland under the Starting Investigator Research Grant (SIRG).  My team is interested in developing novel methods for applying complex models in situations where there is limited training data available. We are looking at a variety of approaches, including unsupervised and semi-supervised learning, multi-objective deep learning, transfer learning and domain adaptation. Our goal is to apply these techniques to solve real-world problems in areas such as information retrieval, image understanding, segmentation and medical image analysis.  I work on many different projects, so a day at the office for me could involve a variety of activities, including discussing research experiments and theory with my PhD students, creating slides and lecture notes for teaching, presenting papers onour own research internally and externally, working on project and grant proposals, attending project meetings, collaborating with Irish and international researchers, and reviewing papers for academic journals and conferences.  I also ensure to include as much time as I can to work on my own research ideas, which typically involves writing scientific software, training machine-learning models or evaluating performance on benchmark data sets.  Some examples of projects that I am currently involved in include:  Strong mathematical and software development skills are essential for my role. I use Python and C++ for development and training models. For deep learning, I use Theano and Keras, although I have also worked with several other packages, such as Caffe and TensorFlow. I use tools from linear algebra, calculus and statistics for understanding and implementing algorithms.  Good communication and presentation skills are also critical for communicating research and teaching.  The diversity of activities each day can make some days a little frantic. There is a lot of mental context-switching necessary to jump between writing code, composing lectures, reviewing papers and trying to stay on top of the latest research. Many of these tasks require a very different mental frame and it can take time to shift gears.  It is important to decide which tasks are the most important for you (and not just the most urgent), and set aside time for these in your calendar early. If you do not schedule time for these activities, meetings will consume your time and you will end up trying to fit in these activities between meetings, which results in a lot of context-switching overhead.  Also, keep a to-do list. There are usually far too many tasks to remember, and trying to keep them all in your head is stressful.  The importance of communicating and publicising your research was something I had not really considered much before starting my academic career. It turns out this stuff is important.  Time management is also more important than I would have thought.  The recent advances in machine learning and deep learning have been widely covered by the media. The public now seems to have a better appreciation of what makes some problems in AI challenging.  Deep learning has also had a strong impact and unifying impact on many research communities. There is now a lot of cross-pollination of ideas from computer vision, natural language understanding and speech recognition.  I very much enjoy working on challenging problems and coming up with novel solutions. The process of doing this can be frustrating at times, but that makes it all the more gratifying when you finally develop a solution that improves the state-of-the-art.",2017-03-24,"Over the course of March of the Machines, there has been a lot of talk about machine learning and deep learning, and the jobs arising from ...",Deep learning is about more than AI – it has unified research,https://www.siliconrepublic.com/people/deep-learning-research-insight-centre-dcu
23,"Deep learning owes its rising popularity to its vast applications across an increasing number of fields. From healthcare to finance, automation to e-commerce, the REWORK Deep Learning Summit (27-28 April) will showcase the deep learning landscape and its impact on business and society.  Of notable interest is speaker Jeffrey De Fauw, Research Engineer at DeepMind. Prior to joining DeepMind, De Fauw developed a deep learning model to detect Diabetic Retinopathy (DR) in fundus images, which he will be presenting at the Summit. DR is a leading cause of blindness in the developed world and diagnosing it is a time-consuming process. De Fauw's model was designed to reduce diagnostics time and to accurately identify patients at risk, to help them receive treatment as early as possible.  Joining De Fauw will be Brian Cheung, A PhD student from UC Berkeley, and currently working at Google Brain. At the event, he will explain how neural network models are able to extract relevant features from data with minimal feature engineering. Applied in the study of physiology, his research aims to use a retinal lattice model to examine retinal images.  Deep learning's extended applications in privacy and security will be touched on by Nicolas Papernot, a Google PhD Fellow in Security from Penn State University. Machine intelligence and deep learning exposes a new and current risk to security and privacy. He will describe the process where an ""attacker"" tricks deep learning models into make wrong predictions.  Learn more from De Fauw, Cheung and Papernot at the Deep Learning Summit in Singapore. Joining them will be 30 extraordinary speakers, including representatives from Facebook, P&G and Visa. Topics covered include Predictive Intelligence, Autonomous Vehicles, Natural Language Understanding, Deep Learning Algorithms, Computer Vision, AI for Business Efficiency, and Deep Learning for Enterprise. Held concurrently will be the Deep Learning in Finance Summit with notable speakers from HSBC, BNP Paribas, AXA, NTUC Income and Societe Generale.  Click here for more information and to register for the Deep Learning Summit or here for the finance track. View our comprehensive brochure here.  Summit attendees will also gain access to presentation slides, summit recordings and exclusive interviews on the REWORK Video Hub. Can't attend On-demand passes are also available.  For further enquiries, please email hello@re-work.co .",2017-04-02,"Deep learning owes its rising popularity to its vast applications across an increasing number of fields. From healthcare to finance, automation to ...",Where is Deep Learning Being Applied? More from RE•WORK ...,http://finance.yahoo.com/news/where-deep-learning-being-applied-003000424.html
24,,2017-04-03,"The work is presented in a paper (A Study of Complex Deep Learning Networks on High Performance, Neuromorphic, and Quantum ...","ORNL Studies Quantum, HPC, and Neuromorphic Computing for ...",https://www.hpcwire.com/2017/04/03/ornl-studies-quantum-hpc-neuromorphic-computing-deep-learning/
25,"What is the point of Twitter The 11-year-old microblogging platform is a social network, a broadcasting tool, a public relations platform, a joke incubator, and a news aggregator. Its a daunting medium, but with the help of a little AI, it doesnt have to be. At least, thats the premise of Post Intelligence, a social media assistant tool launched this week by a pair of former Google executives. Were dubbing it the worlds first AI-based media assistant, says Bindu Reddy, co-founder and CEO of Post Intelligence. It focuses on problems in the realm of social media which humans find difficult to do, that require a lot of brain energy, like simply establishing a social media presence, getting followers, writing engaging things, making insightful comments. To this end, a user signs into Post Intelligence with a social media account, and within two minutes it scoops up a users data, and then suggests what sorts of things a user might want to share online. In my conversation with Reddy and in my own experience, I focused primarily on Twitter, but the tool is also configured for Facebook, and could be expanded to other social networks. Post Intelligence (or PI for short; the company features a cartoon of a smiling robot with the greek letter on its torso) was born out of MyLikes, a tool that delivered sponsored content and advertising for celebrities on Twitter and other social media. To figure out what celebrities should share, what both matched the brand and follower interests, MyLife used AI and deep learning. PI is designed to take those tools and apply them more broadly, rather than just curating an online presence for a brand.  We create a deep learning model for every user who joins, and thats created pretty much on the fly, it takes 2 minutes from when you sign up, and it reads all your posts, whether theyre facebook or on twitter, and then it takes as much data as it can grab, and then it trains a model specific to you, says Reddy. Its trying to learn what you post about and what resonates with your audience, if youre a sports personality, its going to look through all your tweets and now we know you post about baseball and whos succeeding and whos failing. To demonstrate, Reddy showed a selection of recommendations from PI. Our call was on March 2nd, and so the suggestions hit the high notes of the day: the looming SnapChat IPO, the announcement that Attorney General Jeff Sessions would recuse himself from any investigations related to the last presidential campaign, and a viral video of turkeys circling a dead cat. This mercurial mix of topics is both the appeal and the barrier for entry to Twitter: all these conversations are happening at the same time, which is fascinating and confusing, and then by the next day its all gone. Coming up with what to say in the moment can be hard, especially for people who dont spend hours and hours of every day checking Twitter. Reddy, tapping into the zeitgeist, used PI to pull the video of the circling turkeys, and made a joke about Sessions recusing himself. I was intrigued. So, in preparation for the launch, I decided to turn my Twitter over to Post Intelligence, and see how, exactly, an AI could help me Tweet. I set a few rules for myself: for the two-day trial period, I would only using PI to tweet during work hours. I would keep this up for the two days, and I couldnt let anyone besides my editor know that this was what I was doing. Ive been on Twitter for a long time, and have developed what I like to think of as a somewhat distinct voice, so I was curious to see what changed.  The short version of the experiment is that Post Intelligence told me to tweet less. The day before I started the experiment, I sent 44 tweets. The first day of the experiment, PI recommended I tweet just 4 times (I ultimately tweeted 7, adding a few others through the tool). It recommended I tweet at 2:30, 3:30, 7:00, and 10:00pm, and when I asked it to schedule a fifth tweet, it put it at 3:00. One of the neat tools in PI is a prediction score, where it looks at the words and attached images or links to a tweet, and gives a score from 1 to 10 on how well it thinks that tweet will do. PI preferred the straightforward description for a story about a comet to my dated meme description for a tweet about anchors. The second day, I leaned more into the suggestions. A couple gaps in PIs processing were immediately apparent. It recommended I share tweets from a couple different accounts that Id muted, and even let me schedule a retweet of a post from an account that I knew had me blocked. (That tweet did not go through, so it looks like Twitters own blocking tools caught it before it went live). Instead, I shared suggested tweets from people outside my normal feed, which I might not have seen otherwise, and had about the same level of engagement as if Id shared from within my normal timeline. For my second day, too, PI recommended I tweet just four time a say, which was a frequency I matched back when I was posting tweets via text message from a flip-phone. In that respect, the scheduling was a nice break: I felt like I was broadcasting observations on the world, rather than living and breathing with the pulse of a social network every second that news happened. Which brought me to the first major understanding of what Post Intelligence does in practice. Its a tool for those new to Twitter, and those with limited time to spend on tweets, to broadcast thoughts into the general news stream as it happens. But its not a great tool for interacting with others. Whenever someone replied to one of my tweets, there was no way to see that through the PI interface, and so no way to respond directly.  When I asked Reddy about mentions and notifications in our call before my trial, she suggested it as a possible future feature for PI. Without notifications, PI offers feedback on a few different metrics: first, theres the likes and retweets of sent tweets themselves, displayed below each published tweet in a column in PI, just like they are on the Twitter app itself. And then theres a whole analytics section, tracking Follower Growth, a Word Cloud, a Relationship Graph, Posting Patterns, and Sentiment. Sentiment is by far the most interesting, as it breaks tweets down into either positive or negative (with some falling in-between) and then displays a graph of how well tweets of each type performs. 'Trump is a very funny guy, haha.' Is that a negative sentiment or a positive one, says Reddy. To tackle sentiment, Post Intelligence has their own API to try and infer context. Its a task thats hard for AI and for people, too. Thats something that social media struggles with, when Im being sarcastic, people think Im being literal. If youre being tongue-in-cheek, people take it literally. In my brief trial, it wasnt sentiment that tripped me up, but just the lack of interaction with followers. A joke made in a moment loses potency the next day, and Im sorry, it was funny, but I was testing a tool for work isnt the greatest excuse for answering a question a day late. Still, I think theres value to a tool like PI, especially for people who arent glued to the internet for over eight hours every day. The freedom to plan a days tweets in five minutes, with automatically supplied topical content, meant I could focus my attention elsewhere, confident that my online presence was intact. Twitter is very addicting, and it is very important, even as a company it may be only worth a few billion dollars, says Reddy, but its really important to the culture of humanity, in some way I know thats a strong way to say, its proven itself as recently as November 9th, it can change the world. I think more people want to do well on it but dont, because its just so difficult to do well on it. Viewed as the only way to experience Twitter, Post Intelligence is a little underwhelming, but as a tool to get into Twitter, without needing to spend hours a day following the news looking for good enough jokes and news to share, Post Intelligence makes a pretty good set of training wheels.",2017-03-27,"To figure out what celebrities should share, what both matched the brand and follower interests, MyLife used AI and deep learning.",AI and deep learning can now help you be more popular on Twitter,http://www.popsci.com/ai-deep-learning-better-twitter
26,"Few people on the planet know more about building computers for Artificial Intelligence than Rob Ober. As the top technology exec at Nvidias Accelerated Computing Group, hes the chief platform architect behind Tesla, the most powerful GPU on the market for Machine Learning, which is the most widespread type of AI today.  GPUs, or Graphics Processing Units, take their name from their original purpose, but their applications today stretch far beyond that. Supercomputer designers have found them ideal for offloading huge chunks of workloads from CPUs in the systems they build; theyve also proven to be super-efficient processors for a Machine Learning approach called Deep Learning. Thats the type of AI Google uses to serve targeted ads and Amazon Alexa taps into for instantaneous answers to voice queries.  Creating algorithms that enable computers to learn by observation and iteration is undoubtedly complex; also incredibly complex is designing computer systems to execute those instructions and data center infrastructure to power and cool those systems. Ober has seen this firsthand working with Nvidias hyper-scale customers on their data center systems for deep learning.  Weve been working with a lot of the hyper-scales  really all of the hyper-scales  in the large data centers, he said in an interview with Data Center Knowledge. Its a really hard engineering problem to build a system for GPUs for deep learning training. Its really, really hard. Even the big guys like Facebook and Microsoft struggled.  See also:Tencent Cranks up Cloud AI Power with Nvidias Mightiest GPUs Yet  Big Basin, Facebooks latest AI server. Each of the eight heat sinks hides a GPU. (Photo: Facebook)  Training is one type of computing workload involved in deep learning (or rather a category of workloads, since the field is evolving, and there are several different approaches to training). Its purpose is to teach a deep neural network  a network of computing nodes aiming to mimic the way neurons interact in the human brain  a new capability from existing data. For example, a neural net can learn to recognize dogs in photos by repeatedly looking at various images that have dogs in them, where dogs are tagged as dogs.  The other category of workloads is inference, which is where a neural net applies its knowledge to new data (e.g. recognizes a dog in an image it hasnt seen before).  Nvidia makes GPUs for both categories, but training is the part thats especially difficult in the data center, because hardware for training requires extremely dense clusters of GPUs, or interconnected servers with up to eight GPUs per server. One such cabinet can easily require 30kW or more  power density most data centers outside of the supercomputer realm arent designed to support. Even though thats the low end of the range, about 20 such cabinets need as much power as the Dallas Cowboys jumbotron at the AT&T stadium, the worlds largest 1080p video display, which contains 30 million lightbulbs.  We put real stresses on a lot of data center infrastructure, Ober said about Nvidias GPUs. With deep learning training you typically want to make as dense a compute pool as possible, and that becomes incredibly power-dense, and thats a real challenge. Another problem is controlling the voltage in these clusters. GPU computing, by its nature, produces lots of power transients (sudden spikes in voltage), and those are difficult to deal with.  Interconnecting the nodes is another big challenge. Depending on where your training data comes from it can be an incredible load on the data center network, Ober said. You can be creating a real intense hot spot. Power density and networking are probably the two biggest design challenges in data center systems for deep learning, according to him.  Hyper-scale data center operators  the likes of Facebook and Microsoft  mostly address the power density challenge by spreading their deep learning clusters over many racks, although some dabble in liquid cooling or liquid-assist, Ober said. Liquid cooling is when chilled water is delivered directly to the chips on the motherboard (a common approach to cooling supercomputers), while liquid-assist cooling is when chilled water is brought to a heat exchanger attached to an IT cabinet to cool air that is then pushed through the servers.  Not everybody that needs to support high-density deep learning hardware has the luxury of hundreds of thousands of square feet of data center space, and those who dont, such as the few data center providers that have chosen to specialize in high density, have gone the liquid-assist route. Recently, these providers have seen a spike in demand for their services, driven to a large extent by the growing interest in machine learning.  Both startups and large companies are looking for ways to leverage the technology that is widely predicted to drive the next big wave of innovation, but most of them dont have the infrastructure necessary to support this development work. Right now the GPU-enabled workloads are the ones where were seeing the largest amount of growth, and its definitely the enterprise sector, Chris Orlando, co-founder of high-density data center provider ScaleMatrix, said in an interview. The enterprise data center is not equipped for this.  That spike in growth started only recently. Orlando said his company has seen a hockey stick-shaped growth trajectory with the knee somewhere around the middle of last year. Other applications driving the spike have been computing for life sciences and genomics (one of the biggest customers at ScaleMatrixs flagship data center outside of San Diego, a hub for that types of research, is the genomics powerhouse J. Craig Venter Institute), geospacial research, and big data analytics. In Houston, its second data center location, most of the demand comes from the oil and gas industry whose exploration work requires some high-octane computing power.  Another major ScaleMatrix customer in San Diego is Cirrascale, a hardware maker and cloud provider that specializes in infrastructure for Deep Learning. Read our feature on Cirrascale here.  Each ScaleMatrix cabinet can support up to 52kW by bringing chilled water from a central plant to cool air in the fully enclosed cabinet. The custom-designed systems chilled-water loop is on top of the cabinet, where hot exhaust air from the servers rises to get cooled and pushed back over the motherboards. Seeing growing enterprise demand for high-density computing, the company recently started selling this technology to companies interested in deploying it in-house.  Colovore, a data center provider in Silicon Valley, also specializes in high-density colocation. It is using the more typical rear-door heat exchanger to provide up to 20kW per rack in the current first phase, and 35kW in the upcoming second phase. At least one of its customers is interested in pushing beyond 35kW, so the company is exploring the possibility of a supercomputer-like system that brings chilled water directly to the motherboards.  Today a large percentage of Colovores data center capacity is supporting GPU clusters for machine learning, Sean Holzknecht, the companys co-founder and president, said in an interview. Like ScaleMatrix, Colovore is in a good location for what it does. Silicon Valley is a hotbed for companies that are pushing the envelope in machine learning, self-driving cars, and bioinformatics, and theres no shortage of demand for the boutique providers high-density data center space.  Read our feature on Colovore and its niche play in Silicon Valley here.  A look beneath the floor tiles at Colovore displays the infrastructure to support water cooled doors. (Photo: Colovore)  And demand for the kind of infrastructure Colovore and ScaleMatrix provide is likely to continue growing. Machine learning is only in the early innings, and few companies outside of the large cloud platforms, the likes of Google, Facebook, Microsoft, and Alibaba, are using the technology in production. Much of the current activity in the field today consists of development, but that work still requires a lot of GPU horsepower.  Nvidia says demand for AI hardware is surging, a lot of it driven by enterprise cloud giants like Amazon Web Services, Google Cloud Platform, and Microsoft Azure, who offer both machine learning-enhanced cloud services and raw GPU power for rent. Theres hunger for the most powerful cloud GPU instances available. The cloud vendors who currently have GPU instances are seeing unbelievable consumption and traction, Nvidias Ober said. It really is telling that people are drifting to the largest instances they can get.",2017-03-27,"It's a really hard engineering problem to build a system for GPUs for deep learning training. It's really, really hard. Even the big guys like ...",Deep Learning Driving Up Data Center Power Density,http://www.datacenterknowledge.com/archives/2017/03/27/deep-learning-driving-up-data-center-power-density/
27,"It is difficult to shed a tear for Moores Law when there are so many interesting architectural distractions on the systems horizon.  While the steady tick-tock of the tried and true is still audible, the last two years have ushered a fresh wave of new architectures targeting deep learning and other specialized workloads, as well as a bevy of forthcoming hybrids with FPGAs, zippier GPUs, and swiftly emerging open architectures. None of this has been lost on system architects at the bleeding edge, where the rush is on to build systems that can efficiently chew through ever-growing datasets with better performance, lower power consumption, while maintaining programmability and scalability.  Yesterday, we profiled how researchers at Oak Ridge National Lab are scaling deep learning by dividing up thousands of networks to run on over 18,000 GPUs using an MPI-based approach. As we discussed, this effort was driven by HPCs recognition that adding deep learning and machine learning into the supercomputing application mix can drive better use of large wells of scientific dataand bolster the complexity and eventually, capabilities of simulations.  One of the researchers involved in that companion effort we described to create auto-generating neural networks for scientific data took neural network hardware investigations one step further. Thomas Potok and his team built a novel deep learning workflow that uses the best of three worldssupercomputers, neuromorphic devices, and quantum computing.  At a high level, they evaluated the benefits of all three compute platforms (well get to those in a moment) and found that they could use HPC simulation data as the baseline for a convolutional neural network generated using their auto-network tools on the Titan machine, then shift elements of that network to the quantum (using the 1,000 qubit machine at USC/Lockheed) and neuromorphic (developed at ORNL and described here) devices to handle the elements they are best at handling.  It is difficult to compare the performance of HPC, quantum, and neuromorphic devices on deep learning applications since the measurements are differentas are the areas where each excel. A quantum system can provide deeply connect networks and a greater representation of the information without the computational cost of conventional machines, Potok says, but again, they cannot span the entire deep learning workflow in the way ORNL envisions it with the HPC layer.  Neuromorphic devices, particularly those with a spiking neural network like the one developed at ORNL (DANNA) can take some of the offload of neural networks that incorporate a time series element. In other words, the HPC simulation and origin of the network can be done best on a supercomputer, the higher order functions of a convolutional neural net can be addressed by a quantum machine, and the results can be further analyzed with a temporal aspect from neuromorphic devices.  Three or four years ago when we started looking at all of this, Andrew Ng and others were trying to scale neural networks across nodes. The problem was, people were not having much luck scaling past 64 nodes or so before the performance gains went away, Potok explains. We took a different approach; rather than building a massive 18,000 node deep learning system, we took the biggest challengeconfiguring the network, building the topologies and parameters, and using evolutionary optimization to automatically configure itwe made it an optimization problem.  Hyperscale companies have been able to do things like label images automatically or recognize speech, but there are a lot of people working on these networks. In HPC, there are a lot of datasets but there are not a lot of people working on themand even fewer who understand them, says Potok. Trying to build a deep learning network that works there would be laborious; thats where HPC comes in. We can fairly quickly tailor a deep learning network to a new dataset and get results quickly. And what is most important is that it is scalable on Titan, and then to Summit or an exascale machine.  All of this might sound like a major integration challenge with so many data types working toward the same end resulteven if the inputs and outputs are generated as steps in the overall problem on distributed networks. Potok says the above shown architecture does work, but the real test will be merging the CNN and spiking neural networks to see what results might be achieved. Ultimately, the biggest question is just how hybrid architectures will need to be to sate compute and application demands. It will be beneficial to have a new, more robust, scalable, way to create more complex results from simulations, but there is a lot of overheadand on devices that are not yet produced at any scale.  Big theory work is part of what national labs do best, of course, but this could move beyond concept in the next generations of machines. Potok doesnt see such a massively hybrid architecture in the next five years, but does think all three modes of computing hold promise in the next decade. For example, he says, if you look at a field like material science where there is an image of a material at the atomic level (with its associated terabytes of data), what new things could be learned from systems that can take this data, learn from it as a static and temporal record, and then create entirely new ways of considering the material. In other words, not only will the compute platform be new, so too will be the ways problems are approached.",2017-03-29,"Yesterday, we profiled how researchers at Oak Ridge National Lab are scaling deep learning by dividing up thousands of networks to run on ...","Neuromorphic, Quantum, Supercomputing Mesh for Deep Learning",https://www.nextplatform.com/2017/03/29/neuromorphic-quantum-supercomputing-mesh-deep-learning/
28,"People are constantly trying to find more human ways to interact with technology. Could the same be achieved for how we approach our health too  After the recent Deep Learning in Healthcare conference held in London, it is clear that technology has the capacity to transform healthcare as we know it.  Deep learning (also known as machine learning or artificial intelligence) refers to a set of algorithms whereby a computational system is trained to process data by being tested on some information, which it then learns from. This results in a computer which has the ability to assimilate, process and present data to humans, eliminating work for us humans in the process.  Several speakers at the event described how deep learning can be used in diagnostic systems, including Oladimeji Farri, Senior Research Scientist from Philips, who described the system they have developed that could give a diagnosis based on a patients symptoms by drawing in information from the signs and symptoms sections of Wikipedia and using titles of the articles as the diagnosis.  Viktor Kazakov introduced Skin Scanner  a convolutional neural network algorithm that can classify patients' skin diseases. Patients can submit a picture of an area of skin that is troubling them, then the system will visually scan it to match it to one of 23 skin disease classes that it has learned to recognize. The system has been shown to classify 67% of images by the correct disease category.  With the current lengthy drug discovery process resulting in a 95% fail rate, Polina Mamoshina, Research Scientist from Insilico Medicine mentioned how 21st century deep learning algorithms can help to overcome this. Insilico Medicine aim to extend health and longevity by using artificial intelligence for drug discovery and aging research. Polina described how the development of their new nutraceutical supplement to slow down, reduce and even reverse aging, involved using deep neural networks to screen for naturally occurring compounds that turn back the clock at the cellular level.  Pearse Keane, NIHR Clinician Scientist, Moorfields Eye Hospital, is confident that ophthalmology will be the first area of medicine to fully utilize deep learning. If a deep learning algorithm could look at an optical coherence tomography and issue a diagnosis by looking at the scan, it would completely reinvent the eye exam as we know it.  Machine learning could even enhance sight itself, as shown by Stephen Hicks, Founder and Head of Innovation of OxSight. If you can symbolize the world in a reduced form, it will be possible for visually impaired people to interact with the world more meaningfully.  Deep learning can help to determine what the most important object to pay attention to is. This is crucial for visually impaired people, as sometimes it is not the closest object. Neural networks could be changed to recognize objects  not only where they are, but also cropping to find borders of the objects.  Our obsession with wearable fitness trackers was shown to have some clinical applications, as Johanna Ernst (Wearable-Technology researcher, University of Oxford) presented research from Cook et al, which showed that when looking at the recovery of patients undergoing major surgery, the number of steps tracked by wearable devices correlated with the length of time in the hospital and by looking at this they were able to tell by day 2 (post-surgery), how long these individuals would be in the hospital for.  Deep learning solutions can be incorporated into this to be able to classify more activities more accurately and identify meaningful markers. Johanna's work is focusing on predicting heart failure hospitalisation using data from wearable technologies.  Gilles Wainrib, Co-Founder of Owkin Science, emphasized the importance of transfer learning for the success deep learning as it brings the power of machine learning to small datasets. Transfer learning is a domain of artificial intelligence which focuses on the ability for a machine learning algorithm to improve learning capacities on one given dataset through the previous exposure to a different one.  This can be seen in their platform, Deepscope - a platform that can apply algorithms to analyze new images and make predictions on cancer diagnosis or chemotherapy response predictions. This platform also has the ability to share algorithms to the marketplace, meaning that by using a transfer learning technology like this, algorithms can cross-fertilize and therefore circumvent the data sharing problem.  Valentin Tablan, Principal Scientist showed that neural networks also have a place in mental health. Ieso Health's online talking therapy conists of patients talking to a human therapist, via on-line messaging. The system was shown to help mental health patients get better faster compared to results for face-to-face therapy, averaged nationally from NHS Digital public data. The neural networks work presented was about building tools to support the therapists, making them more effective and efficient.  The concept of computers and algorithms replacing doctors was also covered by Daniel Nathrath, Co-Founder of Ada Digital Health. Here, a symptom checker chat-bot app has been created to help the public to identify what is wrong with them based on their symptoms.  The app supports doctors too as patients can share their assessment with their doctor. The technology in the app benefits from this as Ada have created a continuous feedback loop which means that the doctors analysis of the diagnosis means that the app gets smarter with every case that it works on as it continuously learns from the feedback.  Although deep learning in healthcare comes with its challenges, such as difficulties teaching the system to learn the right features and learning how to discriminate, detecting when the system goes wrong and difficulties going beyond human level performance, as summarized by Ben Glocker, Lecturer in Medical Image Computing at Imperial College London, it is apparent that deep learning has a massive potential to transform healthcare.  This is through improving patient quality of life, diagnosis, treatment and assessing response to treatment. Although it is very unlikely that algorithms will completely replace human doctors, it can at least relieve the pressure on doctors for basic diagnosis and administrative tasks.",2017-03-15,"After the recent Deep Learning in Healthcare conference held in London, it is clear that technology has the capacity to transform healthcare as ...",Deep learning in healthcare: a move towards algorithmic doctors,http://www.news-medical.net/news/20170315/Deep-learning-in-healthcare-a-move-towards-algorithmic-doctors.aspx
29,"In a previous post I discussed the promising applications for deep learning in the enterprise. The greatest potential for deep learning is in adding business-relevant structure to less-structured, sense-like data -- such as images, audio and other sensor data.  How quickly does the tone and affect of a support call from a frustrated customer change, broken down by support rep It's that time-to-mollification that matters to your business, not the raw sound data.  Generally when training machine learning algorithms (and deep nets are an extreme example of this), the more data the better. There's a persistent danger of ""overfitting"" your data -- performing very well on the training set, but poorly on new data. If the algorithm has overfit, it has failed to generalize and is thus not that useful.  Practitioners guard against this by holding out some of the data set before model training and then confirming that the model performs about as well on this hold-out set as it did on the rest of the samples. The more data you have available for training, and the more diverse it is, the less likely you are to overfit.  So if you're sitting on a massive pile of sense-like data, you're in a good position to build quality deep learning models.  When it comes to the most broadly applicable deep learning problems -- object recognition in images, identification of people and their activities in video, natural language processing -- companies like Google and Facebook already sit atop a tremendous amount of relevant image, video, audio and text data. The average Fortune 500 company has nowhere near this data scale.  Thus, I expect artificial intelligence as a service (AIaaS) to be the dominant delivery vehicle for these high-value, broadly applicable use cases for deep learning.  Homegrown algorithms will generally be expensive to build and perform more poorly than their cloud-borne peers. If you just need to know how many people entered your store, which areas they visited and how long they spent in each area, you're much better off piping your video feeds to an AIaaS offering than rolling that analysis yourself.  It's not just Google and Facebook that get to have all the fun, however. Any company that naturally aggregates sense-like data has the opportunity to dominate the A.I. use cases trained on those data. Have a vast trove of medical imaging data, or decades of sensor readings from tunnel-boring machines Then you're in an advantaged position for building A.I. services in those domains.  This is why Tesla, with tens of thousands of vehicles on the road phoning data home, has a significant leg up in the autonomous vehicle race over competitors like Google and Apple.  Typically, though, companies store much more structured data -- financial transactions, purchase histories, customer lists. And here, classic machine learning techniques for classification, regression or clustering are more appropriate.  The already data-rich are well positioned to get richer in the emerging field of AIaaS. For most companies, though, the interesting A.I. work starts once business-relevant information has been extracted out of raw sensor streams; it's not particularly valuable or efficient to build the deep learning component itself.  And that business-relevant information is potentially powerful: A single image will soon be sufficient to determine a vending machine restocking order, eliminating the need for more complicated sensors or manual stock checks. As the difficulty of extracting these data approaches zero with AIaaS, all businesses stand to reap significant benefit from deep learning.  This article is published as part of the IDG Contributor Network. Want to Join",2017-03-21,In a previous post I discussed the promising applications for deep learning in the enterprise. The greatest potential for deep learning is in ...,"With deep learning, the data-rich get richer",http://www.computerworld.com/article/3181736/internet-of-things/with-deep-learning-the-data-rich-get-richer.html
