,Contents,DirectLink,Summary,Title,ProcessedContents,dbscan_labels
0,"| by Srini Penchikala on     Mar 31, 2017. Estimated reading time:         1   minute | The research team at IBM recently announced they've reached a new industry record in speech recognition with a word error rate of 5.5% using the SWITCHBOARD linguistic corpus . This brings it closer to what's considered to be the human error rate of 5.1%. Humans typically miss one to two words out of every 20 words they hear. In a five-minute conversation, that could be as many as 80 words. The research project includes applying deep learning technologies and incorporating acoustic models . The speech recognition model used Long Short Term Memory (LSTM) and WaveNet language models with a score fusion of three acoustic models. The acoustic models included aLSTM with multiple feature inputs, another LSTM trained with speaker-adversarial multi-task learning and a third model with a residual net (ResNet) with 25 convolutional layers and time-dilated convolutions.The last model learns from positive examples but also takes advantage of negative examples, so it performs better where similar speech patterns are repeated. Yoshua Bengio from Montreal Institute for Learning Algorithms (MILA) Lab at University of Montreal commented about the speech recognition. In spite of impressive advances in recent years, reaching human-level performance in AI tasks such as speech recognition or object recognition remains a scientific challenge. Indeed, standard benchmarks do not always reveal the variations and complexities of real data. For example, different data sets can be more or less sensitive to different aspects of the task, and the results depend crucially on how human performance is evaluated, for example using skilled professional transcribers in the case of speech recognition. He also said IBM research helps with advancing speech recognition by applying neural networks and deep learning into acoustic and language models. In other speech processing news, IBM added Diarization to their Watson Speech to Text service which helps with use cases like distinguishing individual speakers in a conversation. All these achievements help with introducing technologies that will match the complexity of how the human ear, voice and brain interact. ",https://www.infoq.com/news/2017/03/ibm-speech-recognition,The research team at IBM recently announced they've reached a new industry record in speech recognition with a word error rate of 5.5% using ...,Using Deep Learning Technologies IBM Reaches a New Milestone ...,Srini Penchikala Mar IBM SWITCHBOARD Humans Long Term Memory LSTM WaveNet LSTM ResNet Yoshua Bengio Montreal Institute Learning Algorithms MILA Lab University Montreal AI IBM IBM Diarization Watson Speech Text,0
1,"This story was delivered to BI Intelligence Apps and Platforms Briefing subscribers. To learn more and subscribe, please click here . IBM has taken the lead in the race to create speech recognition that's in parity with the error rate of human speech recognition. As of last week, IBMs speech-recognition team achieved a 5.5% error rate, a vast improvement from its previous record of 6.9%. Digital voice assistants like Apple's Siri and Microsoft's Cortana must meet or outdo human speech recognition  which is estimated to have anerror rate of 5.1%, according to IBM  in order to see wider consumer adoption. Voice assistants are expected to be the next major computing interface for smartphones, wearables, connected cars, and home hubs. While digital voice assistants are far from perfect,competition among tech companies is bolstering overall voice-recognition capabilities, as tech companiesvie to outdo one another. IBM is locked in a race with Microsoft, which last year developed a voice-recognition system with an error rate of 5.9%, accordingto Microsoft's Chief Speech Scientist Xuedong Huang; this beat IBM by an entire percentage point. Despite progress, however, existing methods to study voice recognition lack an industry standard. This makes it difficult to truly gauge advances in the technology. IBM tested a combination of Long Short-Term Memory (LSTM), a type of artificial neural network, and Google-owned DeepMinds WaveNet language models, against SWITCHBOARD, which is a series of recorded human discussions. And while SWITCHBOARD has been regarded as a benchmark for speech recognition for more than two decades, there are other measurements that can be used that are regarded as more difficult, like CallHome,"" which are more difficult for machines to transcribe, IBM notes. Using CallHome, the company achieved a 10.3% error rate. Moreover, voice assistants need to overcome several hurdles before mass adoption occurs: They need to surpass as close as humanly possible.""Despite recent advancements, speech recognition needs to reach roughly 95% for voice to be considered the most efficient form of computing input,accordingto Kleiner Perkins analyst Mary Meeker. Thats because expectations for automated services are much less forgiving than they are for human error. In fact, when a panel of US smartphone owners were asked what they most wantedvoice assistants todo better, ""understand the words I am saying"" received 44% of votes,accordingto MindMeld. Consumer behavior needs to change.For voice to truly replace text or touch as the primary interface, consumers need to be more willing to use the technology in all situations. Yet relatively few consumers regularly employ voice assistants; just 33% of consumers aged 14-17 regularly used voice assistants in 2016,according to an AccentureReport. Voice assistants need to be more helpful.Opening third-party apps to voice assistants will be key in providing consumers with a use case more in line with future expectations of a truly helpful assistant. Voice assistants like Siri, Google Assistant, and Echo, are only just beginning to gain access to these apps, enablingusers to carry out more actions like ordering a car. The Internet of Things (IoT) is growing rapidly as companies around the world connect thousands of devices every day. But behind those devices, theres a sector worth hundreds of billions of dollars supporting the IoT. Platforms are the glue that holds the IoT together, allowing users to take full advantage of the disruptive potential of connected devices. These platforms allow the IoT to achieve its transformational potential, letting businesses manage devices, analyze data, and automate the workflow. Peter Newman, research analyst for BI Intelligence , Business Insider's premium research service, has compiled a detailed report on the evolving IoT platform ecosystem . This reportsizes the market and identifies the primary growth drivers that will power the IoT platform space in the next five years. It also profiles many of the top IoT platforms by discussing key trends in the platform industry such asplatform consolidation. Here are some of the key takeaways: The IoT platforms market is set to expand rapidly in the years to come, with current leading platforms expanding and others entering the space. We define the key categories into which IoT platforms fall: building block open platforms, closed high-end platforms, and product management platforms. We highlight the ways platforms can help companies reach the full five stage potential of the IoT. Explains the coming growth of the IoT platforms. Profiles a number of leading platforms. Highlights the central role platforms play in the IoT. Looks to the future of the IoT platforms market. Interested in getting the full report Here are twoways to access it: Subscribe to anAll-Accesspass to BI Intelligence and gain immediate access to this report and over 100 other expertly researched reports. As an added bonus, you'll also gain access to all future reports and daily newsletters to ensureyou stay ahead of the curve and benefit personally and professionally. >> START A MEMBERSHIP Purchase & download the full report fromour research store. >> BUY THE REPORT ",http://www.businessinsider.com/ibm-edges-closer-to-human-speech-recognition-2017-3,"IBM has taken the lead in the race to create speech recognition that's in parity with the error rate of human speech recognition. As of last week, ...",IBM edges closer to human speech recognition,BI Intelligence Apps Platforms Briefing IBM IBMs Digital Apple Siri Microsoft Cortana IBM IBM Microsoft Microsoft Chief Speech Scientist Xuedong Huang IBM IBM Long Short-Term Memory LSTM DeepMinds WaveNet SWITCHBOARD SWITCHBOARD CallHome IBM CallHome Kleiner Perkins Mary Meeker Thats US MindMeld Consumer AccentureReport Voice Voice Siri Google Assistant Echo Internet Things IoT IoT IoT IoT Peter Newman BI Intelligence Business Insider IoT IoT IoT IoT IoT IoT IoT IoT IoT Subscribe BI Intelligence START A MEMBERSHIP Purchase BUY THE REPORT,-1
2,"IBM recently announced that its speech recognition system achieved an industry record of 5.5% word error rate, coming closer to human parity. Here's what it means for enterprises. By Alison DeNisco | March 13, 2017, 9:39 AM PST IBM recently announced that it reached a new industry record in conversational speech recognition, which could have big implications for the future of artificial intelligence (AI). The IBM team's system achieved a 5.5% word error ratedown from 6.9% last year. The benchmark was measured on a difficult speech recognition task, with the machine deciphering recorded conversations between humans discussing day-to-day topics such as buying a car. This recording is known as SWITCHBOARD, and has been used for more than two decades to test speech recognition systems, according to a blog post by George Saon, a principal research scientist at IBM. IBM used deep learning technologies to reach the 5.5% record. Researchers combined Long Short Term Memory (LSTM) and WaveNet language models with three acoustic models, according to the blog post. ""Within the acoustic models used, the first two were six-layer bidirectional LSTMs. One of these has multiple feature inputs, while the other is trained with speaker-adversarial multi-task learning,"" Saon wrote. ""The unique thing about the last model is that it not only learns from positive examples but also takes advantage of negative examples - so it gets smarter as it goes and performs better where similar speech patterns are repeated."" The previous record was set by Microsoft's Artificial Intelligence and Research group in October 2016 , when researchers developed a system that they claimed recognized speech as accurately as a professional human transcriptionist, with a word error rate of 5.9%. However, Saon argued in his post that human parity is actually a 5.1% word error ratelower than any company has yet to achieve. ""We're not popping the champagne yet,"" Saon wrote. ""While our breakthrough of 5.5% is a big one, this discovery of human parity at 5.1 percent proved to us we have a way to go before we can claim technology is on par with humans."" SEE: How we learned to talk to computers, and how they learned to answer back Reaching human-level performance in AI tasks such as speech or object recognition remains a scientific challenge, according to Yoshua Bengio, leader of the University of Montreal's Montreal Institute for Learning Algorithms (MILA) Lab, as quoted in the blog post. Standard benchmarks do not always reveal the variations and complexities of real data, he added. ""For example, different data sets can be more or less sensitive to different aspects of the task, and the results depend crucially on how human performance is evaluated, for example using skilled professional transcribers in the case of speech recognition,"" Bengio said. Saon also noted that finding a standard measurement for human parity is a complex task as well. While many use SWITCHBOARD, another corpus called CallHome offers a different set of linguistic data created from colloquial conversations between family members, on topics that are not pre-arranged. These conversations are more difficult for machines to transcribe than those from SWITCHBOARD. IBM achieved a 10.3% error rate on this measure, but determined that human parity would be 6.8%. ""The ability to recognize speech as well as humans do is a continuing challenge, since human speech, especially during spontaneous conversation, is extremely complex,"" said Julia Hirschberg, a professor and Chair at the Department of Computer Science at Columbia University, in the blog post. ""It's also difficult to define human performance, since humans also vary in their ability to understand the speech of others. When we compare automatic recognition to human performance it's extremely important to take both these things into account: the performance of the recognizer and the way human performance on the same speech is estimated."" IBM's breakthrough could have major implications for the future of AI and the Internet of Things (IoT) in the enterprise, according to Mark Hung, research vice president and lead analyst of Internet of Things at Gartner. ""With the proliferation of conversational AI platforms such as Alexa and Google Assistant, continued reductions in error rate will be imperative to drive greater adoption of speech as the UI for consumer and enterprise applications,"" Hung said. IBM has recently made major investments in its Watson division, with a new $200 million global headquarters for Watson Internet of Things opening recently in Munich, Germany as part of a $3 billion investment in IoT that IBM pledged in 2014. IBM also recently added diarization to its Watson Speech to Text service, making it possible for the processor to distinguish individual speakers in a conversation. 1. Last week, IBM announced that it achieved a new industry record in speech recognition, with a 5.5% word error rate. 3. IBM's breakthrough could have implications for improving the use of artificial intelligence and the Internet of Things in the enterprise. Stay up on the latest in AI and machine learning. Click here to subscribe to TechRepublic's Next Big Thing newsletter. ",http://www.techrepublic.com/article/why-ibms-speech-recognition-breakthrough-matters-for-ai-and-iot/,"Why IBM's speech recognition breakthrough matters for AI and IoT ... that it reached a new industry record in conversational speech recognition, ...",Why IBM's speech recognition breakthrough matters for AI and IoT,IBM Alison DeNisco | March AM PST IBM AI IBM SWITCHBOARD George Saon IBM IBM Long Short Term Memory LSTM WaveNet LSTMs Saon Microsoft Artificial Intelligence Research October Saon Saon AI Yoshua Bengio University Montreal Montreal Institute Learning Algorithms MILA Lab Standard Bengio Saon SWITCHBOARD CallHome SWITCHBOARD IBM Julia Hirschberg Chair Department Computer Science Columbia University IBM AI Internet Things IoT Mark Hung Internet Things Gartner AI Alexa Google Assistant UI Hung IBM Watson Watson Internet Things Munich Germany IoT IBM IBM Watson Speech Text IBM IBM Internet Things AI Click TechRepublic Next Big Thing,0
3,"Notify me of new posts by email. It is a good idea but I dont think the technology is anywhere near good enough for this.  The false positives/negatives this will throw up will be just as inaccurate as a human although probably much quicker to reach a conclusion After the horse has bolted, Germany is trying to secure the stable. Merkels migration policy has fallen flat and made Europe unsafe. What is required is not vaccination but surgery, meaning thereby drastic measures like fencing, incarceration and deportation; in short the Australian model to stem the flow. And of course economic and diplomatic sanctions against Turkey ",http://www.politico.eu/article/germany-to-use-speech-recognition-to-establish-migrant-origins-refugees-asylum/,Germany's Federal Migration Office (BAMF) will use speech recognition software to establish where migrants entering the country have come ...,Germany to use speech recognition to establish migrant origins,Notify Germany Merkels Europe Turkey,-1
4,"Here at The Next Platform, we tend to focus on deep learning as it relates to hardware and systems versus algorithmic innovation, but at times, it is useful to look at the co-evolution of both code and machines over time to see what might be around the next corner. One segment of the deep learning applications area that has generated a great deal of work is in speech recognition and translationsomething weve described in detail via efforts from Baidu , Google , Tencent , among others. While the application itself is interesting, what is most notable is how codes and systems have shifted to meet the needs of new ways of thinking about some of the hardest machine learning problems. And when we stretch back to the underpinnings of machine translation and speech recognition, IBM has some of the longest historyeven if that history doesnt have a true deep learning element in relatively recently. In his 36 years at IBM focusing on speech and language algorithms, Michael Picheny, senior manager for IBMs Watson Multimodal division (an area that focuses on language and image recognition, among other areas), much has changed for both code and the systems required to push speech recognition. While IBM, like many others doing deep learning at scale, has also landed in the large-scale deployment of GPUs for neural networks, the path to that point was long and complex. It is just in the last few years that the critical combination of advanced neural network models and the hardware to run them in real-time and at scale have been made available. And this one-two punch has shifted IBMs approach to speech algorithmic development and deployment. Picheney says that back when he joined IBM, they were the only company doing speech analysis and recognition with statistical and computational approaches. Others were focused on physical modeling of the underlying processes in speech. They were the only ones solving the problem with compute and mathematical techniques, it was the neatest thing Id ever seen. His early speech recognition work at IBM was done on large mainframes entirely offline, an effort that was later deployed on three separate IBM minicomputers working in parallel to achieve real-time performancea first of its kind capability. Then, in the early 1980s, came the IBM PCs, which could have custom accelerators lashed on until the 1990s, when the work could be done entirely on a CPU. Speech recognition teams in Pichenys group have come full circle to accelerator cards with a reliance on GPUs, which he says are a spot-on architecture for speech, even if there are some limitations hardware-wise on the horizon for the next level of speech recognition using deep learning models. Last year, IBM announced a major milestone in conversational speech recognition: a system that achieved a 6.9 percent word error rate and as of 2017, theyve hit a new industry record of 5.5 percent. To reach this 5.5 percent mark (5.1% is human parity), IBM researchers combined LSTM (Long Short Term Memory) and WaveNet language models with three strong acoustic models. Within the acoustic models used, the first two were six-layer bidirectional LSTMs. One of these has multiple feature inputs, while the other is trained with speaker-adversarial multi-task learning. The unique thing about the last model is that it not only learns from positive examples but also takes advantage of negative examples  so it gets smarter as it goes and performs better where similar speech patterns are repeated. Code-wise, much has changed for IBM when it comes to speech recognition. Picheny says that earlier systems were a combination of four componentsa feature extractor, acoustic model, language model, and speech recognition engine or decoder. As neural networks evolved, both internally and in the larger ecosystem (Caffe, TensorFlow, etc.), these components have been fused and merged, creating a master modelone that requires significant computational resources and scalability from both hardware and software. Back then, he explains that it was difficult to create an architecture that would run all of these different elements efficiently since they all had their own optimization and other features. What we are seeing over time is that deep learning methods seem to be taking over more of the speech recognition functions. Bit by bit, deep learning structures and mechanisms are replacing older mechanisms that made it harder to do the heavy duty scaling. In the next couple of years, perhaps longer, we will see deep learning architectures used for all components of speech recognitionand image as well. He notes that over time too, it might be possible for these many functions to be packaged into a single special purpose chip to make it even easier to scale. People working in deep learning have become very agile in what theyve learned to do. The field is moving so fast; there are advances in one framework, another in a different oneall of the deep learning packages have their pros and cons, especially for speechWeve worked with all the major packages, some are better than others, but I dont want to say which. But we also have built a good bit of our own code, Picheny says. Deep learning architectures are now being used for all components of that speech processing system and merging these structures that learn everything to solve the problem as a whole. As you can imagine, its much easier to scale something based on a single component and a single architectureeven if that architectures is more complexthan if you have many components that need to be programmed separately. In the future, as that one architecture becomes more mature and standardized, you can see how CPUs might have some inherent assists or there will be chips with these single architectures for doing this in bulk. For speech, IBM has its own custom-developed neural network models that feed into Watson (which is still a nebulous thing hardware and software-wise, were still working on that story), For these models, the drivers are computation speed and memory. As it turns out, these are the biggest limitations as well, particularly on the memory front. GPUs are very fast but limited in memory. Thats where the bottleneck is for training on huge quantities of speech. Theres advantage in keeping everything on local memory on the GPU versus fetching it off the chip. And there are algorithms where people are trying to combine results across multiple GPUs, which means making performance tradeoffs for that level of parallelization. What we really want are faster GPUs with more memory. Aside from custom ASICs just for speech, we asked Picheny about other architectures that seem to be getting traction in deep learning outside of pure-play deep learning chips from companies like Nervana Systems (now part of Intel). One of the likely candidates for speech acceleration could be neuromorphic devicesand IBM has developed its own (TrueNorth). There is a lot of work on neuromorphic architectures but the limitation of these chips, even though they do fascinating things, is that you have to program them far differently than a GPU. And with a GPU there are big communities of people writing special purpose libraries. The limitation is that the people developing their algorithms dont want a new way of programming what theyre already doing. FPGAs have a similar problem, Picheny says, but are more of an intermediate solution, but programming is still not easy. He says the availability of a rich array of libraries in the GPU CUDA ecosystem is the reason deep learning will continue to be done on GPUs in this middle period before we might see specialized chips just for this task. On the note above about Watson being nebulous: Pichney agrees that it is hard to pin down just how many different frameworks and models are part of Watsons overall AI umbrella, but this is not by design. Everything is evolving so quickly, especially in the last two years. He says that what Watson was in the beginning, just as with his own speech recognition algorithms and clusters, isnt the same now. While we might not get answers from IBM about just how many components Watson encapsulates and what hardware is required to make it all work for key applications, one can image that Pichenys story about the merging and fusing of various components into master networks with specialized functionality might ring equally true for Watson. ",https://www.nextplatform.com/2017/04/07/mainframes-deep-learning-clusters-ibms-speech-journey/,"And when we stretch back to the underpinnings of machine translation and speech recognition, IBM has some of the longest history—even if ...",From Mainframes to Deep Learning Clusters: IBM's Speech Journey,Platform Baidu Google Tencent IBM IBM Michael Picheny IBMs Watson Multimodal IBM GPUs IBMs Picheney IBM Id IBM IBM IBM PCs CPU Pichenys GPUs IBM IBM LSTM Long Short Term Memory WaveNet LSTMs IBM Picheny Caffe TensorFlow Back Bit Picheny Deep CPUs IBM Watson GPUs GPU GPUs GPUs ASICs Picheny Nervana Systems Intel IBM TrueNorth GPU GPU FPGAs Picheny GPU CUDA GPUs Watson Watsons AI Watson IBM Watson Pichenys Watson,0
5,"Crescendo Systems received two prestigious healthcare awards at the recent Nuance Annual Partner Event in Barcelona. Crescendo Systems received two prestigious healthcare awards at the recent Nuance Annual Partner Event in Barcelona. As well as picking up the award for Healthcare Marketing Excellence, Crescendo were also delighted to be accorded Elite Healthcare Partner' status in recognition of their work with Nuance's Dragon Medical speech recognition product. ""We are thrilled with these Nuance Healthcare Awards"", said John Bendall, Operations Director of Crescendo Systems, ""Having stepped up our healthcare activity over the past year in response to the rapidly evolving NHS digital landscape, we feel that these awards are an endorsement of the high quality added value services that Crescendo bring to the Dragon Medical proposition"". Crescendo not only specialise in creating custom interfaces for Dragon Medical with leading clinical systems such as TPP SystmOne and EMIS Web but have also integrated Dragon Medical into their own digital dictation platform, allowing doctors the freedom to create clinical documents entirely by voice or send speech recognised text to their back office staff for editing. ""Our Dragon Medical portfolio has been designed to give doctors the ability to choose how speech recognition works best for them and then to reinforce these choices by providing expert implementation, training and advice,"" continued John, ""We want Crescendo customers to benefit in every possible way from knowing that they are working with a Nuance Elite Healthcare Partner and we will continue to work closely with Nuance to provide solutions that help healthcare organisations improve efficiency and channel those gains back into patient care"". For over 25 years, Crescendo Systems Corporation ( http://www.crescendosystems.co.uk ) has been delivering powerful clinical documentation, voice processing, speech recognition, transcription, workflow and referral management solutions to countless healthcare facilities around the world. In 2003, Crescendo Systems Ltd. was the first subsidiary to be opened in Europe and it is now the preferred supplier for tens of thousands of UK healthcare professionals. Crescendo Systems are a Nuance Elite Healthcare Partner, offering specialist added value and professional services to Nuance's Dragon Medical speech technology. Developed by Crescendo with care teams and for care teams, Centro ( http://www.trustcentro.co.uk ) is a revolutionary Clinical Documentation System aimed at Trusts embracing the NHS Digital Challenge. Designed to maximise efficiency gains and savings, Centro offers superior clinical documentation processing by combining a seamless, digitally-rich and mobile workflow with a collaborative care approach that delivers timely, more informed patient care. For the original version on PRWeb visit: http://www.prweb.com/releases/2017/04/prweb14207930.htm ",https://www.benzinga.com/pressreleases/17/04/p9256733/crescendo-recognised-for-excellence-in-healthcare-by-speech-recognition,Our Dragon Medical portfolio has been designed to give doctors the ability to choose how speech recognition works best for them and then to ...,Crescendo Recognised for Excellence in Healthcare by Speech ...,Crescendo Systems Nuance Annual Partner Event Barcelona Crescendo Systems Nuance Annual Partner Event Barcelona Healthcare Marketing Excellence Crescendo Elite Healthcare Partner Nuance Dragon Medical Nuance Healthcare Awards John Bendall Operations Director Crescendo Systems NHS Crescendo Dragon Medical Crescendo Dragon Medical TPP SystmOne EMIS Web Dragon Medical Dragon Medical John Nuance Elite Healthcare Partner Nuance Crescendo Systems Corporation Crescendo Systems Ltd. Europe UK Crescendo Systems Nuance Elite Healthcare Partner Nuance Dragon Medical Crescendo Centro Clinical Documentation System Trusts NHS Digital Challenge Centro PRWeb,1
6,"Sara Murphy and Hailey Gentzwill represent Osage High School at the IHSSA All-State Individual Speech Festival when Murphy performs were acting piece and Gentz her prose piece. Renae Jensen will represent Riceville High School at the IHSSA All-State Individual Speech Festival when she performs her LiteraryProgram. Sara Murphy and Hailey Gentzwill represent Osage High School at the IHSSA All-State Individual Speech Festival when Murphy performs were acting piece and Gentz her prose piece. Renae Jensen will represent Riceville High School at the IHSSA All-State Individual Speech Festival when she performs her LiteraryProgram. Forthree Mitchell County speech students, the journey started as being among 9,000 students in the state competing in one or more categories in the Iowa High School Speech Associations (IHSSA) Individual Events Speech Competition. On Monday, March 27, the journey ends for Hailey Gentz, junior, and Sara Murphy, senior, from Osage High School; and Renae Jensen, junior from Riceville High School, when they will perform at the IHSSA All-State Individual Events Speech Festival on the University of Northern Iowa campus. Reaching the level of All-State Speech designation means you are considered the best of the best in your performance category. Each student moved from district to state competition, where at least two of three judges determined they were one of the outstanding performers during the day-long contest. This is athird appearance for Murphy and second appearance for Gentz. Gentz, a double-qualifier in acting and prose, will present her original prose piece titled ""The Exception."" Gentz said she decided to write and perform an original piece because, ""I just couldn't find anything I could connect with."" The prose piece started out as a short story for an English assignment. ""I decided to cut it down and turn it into a prose piece,"" she said. ""I cried when I first heard I had been chosen. I am excited I was chosen."" For Murphy, this will be her fourth time as an all-state performer, with another original acting piece. ""I really like to write my own material,"" she said. ""I discovered it is something I really like to do."" I understand and agree that registration on or use of this site consitutes agreement to its user agreement and privacy policy. Murphy plans to expand on that talent, by majoring in communication and minoring in either journalism or creative writing in college. This is Jensens first time receiving an all-state nomination. ""I was honestly in disbelief,"" Jensen said when she found out she had been chosen. ""I didn't think I was good enough to get chosen. Since you have to be really good, I just didn't think it was going to be me."" Allthree agreed they were honored to represent their schools speech teams at the All-State Festival. ",http://globegazette.com/mcpress/news/local/students-receive-all-state-speech-recognition/article_d4012565-a271-58b0-ad4d-8b3ca3af7187.html,Sara Murphy and Hailey Gentz will represent Osage High School at the IHSSA All-State Individual Speech Festival when Murphy performs ...,Students receive All-State Speech recognition,Sara Murphy Hailey Gentzwill Osage High School IHSSA All-State Individual Speech Festival Murphy Gentz Renae Jensen Riceville High School IHSSA All-State Individual Speech Festival LiteraryProgram Sara Murphy Hailey Gentzwill Osage High School IHSSA All-State Individual Speech Festival Murphy Gentz Renae Jensen Riceville High School IHSSA All-State Individual Speech Festival LiteraryProgram Forthree Mitchell County Iowa High School Speech Associations IHSSA Individual Events Speech Competition Monday March Hailey Gentz Sara Murphy Osage High School Renae Jensen Riceville High School IHSSA All-State Individual Events Speech Festival University Northern Iowa All-State Speech Murphy Gentz Gentz Gentz Murphy Murphy Jensens Jensen All-State Festival,2
7,"Market Research Report on the Automatic Speech Recognition Market analyzed two aspects. One part is about its production and the other part is about its consumption. In terms of its production,analyze the production, revenue, gross margin of its main manufacturers and the unit price that they offer in different regions from 2012 to 2017. In terms of its consumption,analyze the consumption volume, consumption value, sale price, import and export in different regions from 2012 to 2017.Also make a prediction of its production and consumption in coming 2017-2022. At the same time,classify different Automatic Speech Recognition based on their definitions. Upstream raw materials, equipment and downstream consumers analysis is also carried out. Wha is more, the Automatic Speech Recognition industry development trends and marketing channels are analyzed. 1. To provide detailed analysis of the market structure along with forecast of the various segments and sub-segments of the global Automatic Speech Recognition market. 2. To provide insights about factors affecting the market growth. To analyze the Automatic Speech Recognition market based on various factors- price analysis, supply chain analysis, porte five force analysis etc. 3. To provide historical and forecast revenue of the market segments and sub-segments with respect to four main geographies and their countries- North America, Europe, Asia, and Rest of the World. 4. To provide country level analysis of the market with respect to the current market size and future prospective. 5. To provide country level analysis of the market for segment by application, product type and sub-segments. 6. To provide strategic profiling of key players in the market, comprehensively analyzing their core competencies, and drawing a competitive landscape for the market. 7. To track and analyze competitive developments such as joint ventures, strategic alliances, mergers and acquisitions, new product developments, and research and developments in the global Automatic Speech Recognition market. ",http://www.technologynewsextra.com/global-automatic-speech-recognition-market-forecast-research-report-2017-2022/10416.html,Automatic Speech Recognition Market Market Research Report on the Automatic Speech Recognition Market analyzed two aspects. One part ...,Global Automatic Speech Recognition Market Forecast Research ...,Market Research Report Market Automatic Upstream Wha Automatic Automatic North America Europe Asia Rest World,3
8,"The pricing starts at CNY 2,099 (roughly Rs. 20,000) Xiaomi at the beginning of 2017 showcased its new Mi TV 4 series at the CES 2017 trade show. The company is now upgrading the TV 4 series, and introducing its all-new Mi TV 4A series in China. The all-new Mi TV 4A will be marketed as a smart TV with AI-based speech recognition. The new MI TV 4A series include four screen-sizes - 43, 49, 55, and 65-inches, and they will be available in AI with speech recognition and non-AI variants. The basic Mi TV 4A 43-inch (1080p) model will be priced at CNY 2,099 (roughly Rs. 20,000) while the 49-inch (1080p) model will be priced at CNY 2,599 (roughly Rs. 24,750). The Mi TV 4A 55-inch and 65-inch 4K models have been priced at CNY 3,199 (roughly Rs. 30,500) and CNY 5,699 (roughly Rs. 54,000). The 49, 55, and 65-inch models will be also available in with AI Speech Recognition as well as come with new Mi Bluetooth Remote Control. The Mi TV 4A 49-inch (1080P) 32GB AI model has been priced at CNY 2,899 (roughly Rs. 27,500); Mi TV 4A 55-inch (4K) 32GB AI variant will be available at CNY 3,599 (roughly Rs. 34,000), and Mi TV 4A 65-inch (4K) 32 GB AI variant will be available at CNY 6,199 (roughly Rs. 59,000). The third-generation Mi Bluetooth Remote Control has been priced at CNY 99 (roughly Rs. 1,000), and will come with Bluetooth, Speech Recognition, Mi Touch, and Infrared features. At launch, Xiaomi talked how Mi Touch has replaced the traditional navigation button on the remote control, and lets you use gestures to control your TV. The Chinese company claimed that the feature will come in handy while searching or browsing on a long webpage. Much like the Mi TV 4 series , the new Mi TV 4A series also features AI (artificial intelligence) based recommendations UI called PatchWall. Notably, the PatchWall is a UI layer on top of the Android OS and it is based on deep learning AI technology. The Mi TV 4A series is powered by a quad-core Amlogic T962 64-bit processor coupled with 2GB of RAM and comes with 32GB ROM. Connectivity options include Wi-Fi 802.11ac (2.4/5 GHz dual band Wi-Fi), Bluetooth 4.2, Dolby, and DTS Audio. The Mi TV 4A also sports a blue light-reducing mode to reduce eye strain problem. The processor on the Mi TV 4A series also supports HDR 10 and HLG. In terms of screen resolution, the Mi TV 4A 1080p models offer screen resolution of 1080x1920 pixels, and will sport Samsung/ LG/ AUO/ CSOT panel while the 4K models will boast of a screen resolution of 3840x2160 pixels with Samsung/ LG/ CSOT panel. Xiaomi's Wang Chuan, Dirrector for Internet TV related products such as Mi TV and Mi Box, at the launch claimed that the company's deep learning AI system will understand user preferences and intelligently classify content. Chuan added that Xiaomi started working on AI speech recognition three years back, and it lets users control and interact with the TV. Xiaomi revealed that it uses Sogou ASR speech recognition technology which is claimed to have an accuracy of up to 97 percent. Unfortunately, the Chinese company hasn't revealed plans to launch the new Mi TV 4A series outside China. ",http://gadgets.ndtv.com/tv/news/xiaomis-mi-tv-4a-series-launched-with-ai-based-speech-recognition-1672577,The all-new Mi TV 4A will be marketed as a smart TV with AI-based speech recognition. The new MI TV 4A series include four screen-sizes - 43 ...,Xiaomi's Mi TV 4A Series Launched With AI-Based Speech ...,CNY Rs Mi CES Mi China Mi MI AI Mi CNY Rs CNY Rs Mi CNY Rs Rs AI Mi Bluetooth Remote Control Mi AI CNY Rs Mi AI CNY Rs Mi TV GB AI CNY Rs Mi Bluetooth Remote Control CNY Rs Bluetooth Mi Touch Infrared Xiaomi Mi Touch Mi Mi AI UI PatchWall PatchWall UI Android OS AI Mi Amlogic T962 RAM ROM Connectivity GHz Wi-Fi Bluetooth Dolby DTS Audio Mi Mi HDR HLG Mi Samsung/ LG/ AUO/ CSOT Samsung/ LG/ CSOT Xiaomi Wang Chuan Dirrector Internet TV Mi Mi Box AI Chuan Xiaomi AI Sogou ASR Mi China,4
9,"Large Municipal Transportation Authority to Implement Voice Self-Service and Speech Recognition Solutions from Verint Business Wire Thursday, March 23, 2017 Verint Systems Inc. ( VRNT ) today announced that a large municipal transportation authority in the U.S. will implement the voice self-service and speech recognition solutions from its Customer Engagement Optimization  portfolio. When this transportation authority sought new solutions to support its move from an on-premises to cloud environment, as well as to advance key compliance needsincluding functionality to support the Payment Card Industry Data Security Standard (PCI DSS)it turned to Verint.* The authority also wanted to ensure the technology it selected would enable it to enhance security and further the success of its prepaid card program that enables citizens to reload their transit cards through voice self-service. When deployed, the software solutions will be integrated with the authoritys contact center and back-end data systems, and will help deliver timely, high-quality voice self-service to improve citizen experiences, achieve higher completion rate goals, and decrease transfer rates and cost per contact. The authority and citizens it serves also will benefit from the solutions personalization capabilities, which help reduce typical consumer frustration with voice self-service systems. This U.S. transportation authority selected our solutions based on our cloud environmentwhich it deemed more secure than its legacy offeringand our long history and success working with state government programs, says Michael Southworth, general manager, voice self-service and government solutions, Verint. Today, we support more than 60 programs in 44 states, reinforcing our experience and dedication to the market and local government agencies. Verint ( VRNT ) is a global leader in Actionable Intelligence solutions with a focus on customer engagement optimization, security intelligence, and fraud, risk and compliance. Today, more than 10,000 organizations in approximately 180 countriesincluding over 80 percent of the Fortune 100count on intelligence from Verint solutions to make more informed, effective and timely decisions. Learn more about how were creating A Smarter World with Actionable Intelligence at www.verint.com . * The transportation authority purchased the Verint solutions in November 2016 and plans to roll the technology out in the first half of this year. This press release contains forward-looking statements, including statements regarding expectations, predictions, views, opportunities, plans, strategies, beliefs, and statements of similar effect relating to Verint Systems Inc. These forward-looking statements are not guarantees of future performance and they are based on management's expectations that involve a number of risks, uncertainties and assumptions, any of which could cause actual results to differ materially from those expressed in or implied by the forward-looking statements. For a detailed discussion of these risk factors, see our Annual Report on Form 10-K for the fiscal year ended January 31, 2016, our Quarterly Report on Form 10-Q for the quarter ended October 31, 2016, and other filings we make with the SEC. The forward-looking statements contained in this press release are made as of the date of this press release and, except as required by law, Verint assumes no obligation to update or revise them or to provide reasons why actual results may differ. VERINT, ACTIONABLE INTELLIGENCE, MAKE BIG DATA ACTIONABLE, CUSTOMER-INSPIRED EXCELLENCE, INTELLIGENCE IN ACTION, IMPACT 360, WITNESS, VERINT VERIFIED, KANA, LAGAN, VOVICI, GMT, VICTRIO, AUDIOLOG, CONTACT SOLUTIONS, OPINIONLAB, ADTECH, CUSTOMER ENGAGEMENT SOLUTIONS, CYBER INTELLIGENCE SOLUTIONS, VOICE OF THE CUSTOMER ANALYTICS, NEXTIVA, EDGEVR, RELIANT, VANTAGE, STAR-GATE, ENGAGE, CYBERVISION, FOCALINFO, SUNTECH, and VIGIA are trademarks or registered trademarks of Verint Systems Inc. or its subsidiaries. Other trademarks mentioned are the property of their respective owners. View source version on businesswire.com: http://www.businesswire.com/news/home/20170323005131/en/ ",http://finance.yahoo.com/news/large-municipal-transportation-authority-implement-123000881.html,... a large municipal transportation authority in the U.S. will implement the voice self-service and speech recognition solutions from its Customer ...,Large Municipal Transportation Authority to Implement Voice Self ...,Municipal Transportation Authority Implement Voice Self-Service Solutions Verint Business Wire Thursday March Verint Systems Inc. VRNT U.S. Customer Engagement Optimization Payment Card Industry Data Security Standard PCI DSS Verint U.S. Michael Southworth Verint Verint VRNT Intelligence Fortune Verint A Smarter World Verint November Verint Systems Inc Annual Report Form January Report Form October SEC Verint VERINT ACTIONABLE INTELLIGENCE MAKE BIG DATA ACTIONABLE CUSTOMER-INSPIRED EXCELLENCE INTELLIGENCE IN ACTION IMPACT WITNESS VERINT VERIFIED KANA LAGAN VOVICI GMT VICTRIO AUDIOLOG CONTACT SOLUTIONS OPINIONLAB ADTECH CUSTOMER ENGAGEMENT SOLUTIONS CYBER INTELLIGENCE SOLUTIONS VOICE THE CUSTOMER ANALYTICS NEXTIVA EDGEVR RELIANT VANTAGE STAR-GATE ENGAGE CYBERVISION FOCALINFO SUNTECH VIGIA Verint Systems Inc. View,-1
10,"Xiaomi has a lot in store for us. After putting the Redmi 4A with 4G VoLTE support on sale, Redmi 4X, Mi 5C , that selfie stick , and the Pinecone processor , Xiaomi has just rolled out a new Mi TV product. The Mi TV 4A is a new smart TV that comes with AI speech recognition.The Mi TV 4A starts with an affordable price of only 2099 which is roughly $304 in the US. This new TV takes advantage of an original Samsung/LG/AUO/CSOT panel and comes equipped with a high-performance Quad-core 64-bit processor, 2GB RAM, 8GB ROM/32GB ROM, WiFi, and Bluetooth 4.2 connectivity. The product already supports HDR 10 and HLG, Dolby and DTS Audio, and features a deep learning artificial intelligence (AI) system. The AI variants will even arrive with a special Mi Bluetooth Remote Control but is sold separately for only 99($14). The Xiaomi Mi TV 4A will be ready in four sizes: 43, 49, 55, and 65. Both the 43- and 49-inch models have 1080p displays but only the latter has a 32GB AI variant. The Mi TV 4A 43 costs 2099($304) while the 49 (1080P) costs 2599($377) or2899($420) for the AI variant. The bigger and 4K models, the 55-inch and 65-inch Mi TV 4As are priced 3199($467)/3599($522) and 5699($837)/6199($899), respectively. The Mi TV 4A is targeted for the whole family. Young kids and the elderly can enjoy it because it is easy to navigate and is fun to use. The AI system is important because it learns and understands your preferences and follows personalized recommendations. ",https://androidcommunity.com/xiaomi-unveils-new-mi-tv-4a-with-ai-speech-recognition-20170321/,The Mi TV 4A is a new smart TV that comes with AI speech recognition. The Mi TV 4A starts with an affordable price of only ￥2099 which is ...,Xiaomi unveils new Mi TV 4A with AI speech recognition,Redmi VoLTE Redmi Mi Pinecone Xiaomi Mi Mi AI .The Mi US Samsung/LG/AUO/CSOT RAM ROM/32GB ROM WiFi Bluetooth HDR HLG Dolby DTS Audio AI AI Mi Bluetooth Remote Control Xiaomi Mi AI Mi AI Mi Mi Young AI,4
11,"A researcher has found a vulnerability in the latest version of reCAPTCHA that could potentially let spambots bypass reCAPTCHA fields across millions of sites. Catalin Cimpanu over at Bleeping Computer has the details. The developer going under the name east-ee has developed a script that uses Googles own speech recognition API to solve audio challenges associated with the latest version (v2) of reCAPTCHA. The Python script is now available on Github, although east-ee claims he spotted the bug back in 2016. The script only works for the latest version, which uses audio challenges as a backup check if you click a button at the bottom of the reCAPTCHA pop-up. The fatal flaw in the design is that Google gives users on older browsers the option to download the audio. This allowed east-ee to download the audio to memory and then use Googles own speech recognition API to get the correct transcription, which could then be posted back to solve the captcha. Catalin notes that this is not the first time researchers have bypassed Google and Facebooks captcha tech. Three researchers in April last year were able to fool both tech giants in over 70% of cases. Google is however working on reCAPTCHA v3 which will hopefully patch the latest vulnerability. ",https://www.programmableweb.com/news/how-recaptcha-can-be-bypassed-using-googles-speech-recognition-api/elsewhere-web/2017/03/12,The developer going under the name 'east-ee' has developed a script that uses Google's own speech recognition API to solve audio ...,How reCAPTCHA Can be Bypassed Using Google's Speech ...,Catalin Cimpanu Bleeping Computer Googles API Python Github Google Googles API Catalin Google Facebooks April Google,4
12,"The Armonk-based tech giant claims it has achieved a 5.5% word error rate, beating Microsofts previous record of 5.9%. Last October, Microsoft revealed that its Speech Recognition technology achieved a 5.9% word error rate (WER) , setting a new world record. Now, IBM has managed to break that record, announcing in a blog post that it has achieved a 5.5% WER . Word error rate is a common metric of the performance of a speech recognition or machine translation system. The difficulty of measuring performance lies in the fact that the recognized word sequence can have a different length from the reference word sequence; i.e. supposedly the correct one. According to IBMs blog post, the company has reached 5.5% word error rate by combining Long Short-Term Memory (LSTM) and WaveNet language models. LSTM is a recurrent neural network architecture (an artificial neural network), which can compute anything a conventional computer can compute. An LSTM network is well-suited to learn from experience to classify, process and predict time series. WaveNet is a deep generative model of raw audio waveforms created by DeepMind Technologies. WaveNet is able to generate speech which mimics any human voice. According to DeepMind, WaveNets speech sounds more natural than the best existing Text-to-Speech systems. The noble competition between Microsoft and IBM in the Speech Recognition field is a long-standing one. Both companies have managed some impressive breakthrough at Speech Recognition, beating each others world records over the last couple of months. Back in September 2016, Microsoft announced it achieved a 6.3% word error rate , beating IBMs 6.9% WER. As mentioned before, in October Microsoft beat its own world record with a 5.9% word error rate, and now IBM has claimed the world record once more. Despite the fact that both Microsoft and IBM compete for the best WER, the companies views on reaching human parity are different. Reaching human parity  meaning an error rate on par with that of two humans speaking- has always been the ultimate industry goal. Back in October, when Microsoft achieved a 5.9% WER, the company claimed to have reached human parity in conversational speech recognition. Weve reached human parity, said Xuedong Huang, Microsofts chief speech scientist. This is a historic achievement,, he added. However, IBM claims to have determined human parity is lower than what anyone has yet achieved  at 5.1% WER. George Saon, an IBM Principal Research Scientist, says in the announcement blog post that Others in the industry are chasing this milestone alongside [IBM], and some have recently claimed reaching 5.9 percent as equivalent to human paritybut [IBM is] not popping the champagne yet. ",https://winbuzzer.com/2017/03/12/ibm-beats-microsofts-speech-recognition-accuracy-record-xcxwbn/,IBM Beats Microsoft's Speech Recognition Accuracy Record ... WaveNet is able to generate speech which mimics any human voice. According ...,IBM Beats Microsoft's Speech Recognition Accuracy Record,Microsofts October Microsoft WER IBM WER Word IBMs Long Memory LSTM WaveNet LSTM LSTM WaveNet DeepMind Technologies WaveNet DeepMind WaveNets Microsoft IBM Back September Microsoft IBMs WER October Microsoft IBM Microsoft IBM October Microsoft WER Weve Xuedong Huang Microsofts IBM WER George Saon IBM Principal Research Scientist [ IBM ] [ IBM,-1
13,"Getting to 100 percent, where Google or Alexa (or something else we havent met yet) understands words we mispronounce, or utter by mistake, is probably another few years away. But today, its possible to ask an assistant about almost anything, and people are asking: according to Hound, a voice-enabled platform built by Shazam competitor Soundhound, its active users pose Hound multiple questions every day, on a wide variety of topics. Andrew Ng, chief scientist at Baidu, says that half of all internet queries by 2020 will be done either through voice or speech. Another big piece of this puzzle thats already been solved is hardware. It could be years before Oculus Rift (which costs $600 and requires a powerful PC with serious graphics processors) become widely affordable and before smartphones that can handle live video streaming are ubiquitous. Meanwhile, an Echo Dot will set you back just $50, and pretty much every smartphone on the market can handle cloud-powered voice queries. And if you dont have your phone on you, thats not necessarily a problem, either. Its not just your phone, your Amazon Echo, says Beerud Sheth, founder of voice and chat development platform Gupshup. In addition to the Echo and Google Home, Ford, VW and BMW are all working on cars that have Amazons voice platform, Alexa, inside them. Google Assistant, which is expected to become standard on high-end Android smartphones, is also expected to make its way into Android Auto, an operating system available in a growing number of connected automobiles. And that, Sheth says, is just the beginning. Soon, voice platforms will be accessible from smart devices that we would never consider technologically advanced. Its your toaster oven, maybe even your table and chair, Sheth says.  Whenever a new medium emerges, it takes a while to figure out what its advertising will look like (or, in this case, sound like). But voice has a head start. Thanks to nearly a century of terrestrial radio advertising, publishers including The Washington Post are already monetizing their flash briefings on Alexa. And once Google and Amazon add the ability to programmatically target listeners, the money is going to flow. Audio advertising is already projected to account for nearly 12 percent of marketers budgets and programmers ad placements by the end of 2017, more than double the share it claimed in 2015, according to a survey conducted by Ad Age and The Trade Desk. Add in the ability to claim an offer, or make a purchase directly through a media companys skill, which is expected to be possible by the end of 2017, and its on: Suddenly, voice becomes a way to drive transactions, subscriptions, and other meaningful revenue sources What radios always been missing is a direct back channel, said Pat Higbie, the founder of XAPP Media, a digital ad developer and a top Alexa developer. What we have here is the intimacy of radio as well as the instantaneous feedback from users. Amazon has set aside up to $100 million to invest in companies that it thinks could boost voice. An accelerator program, created in partnership with the mentorship-focused accelerator firm Techstars, will launch in July. And while Google does not have a similar voice-oriented fund, partners at its venture capital arm, GV, have said that they think voice is going to be the future. They are very invested in identifying use cases theyre not thinking about internally, says Cody Simms, who heads accelerator programs for Techstars, said of Amazon. If theres one thing guaranteed to inject rocket fuel into a new idea or technology, its goliaths like Google, Amazon or Microsoft battling to own its ascent. Consider what competition between Facebook and Google did to increase the profile of streaming video. That fight will be good for speeding the innovation thats sure to occur on Google Assistant, Alexa et al. But what will be most interesting is seeing what happens when that innovation starts to really distinguish these nascent platforms from one another. For now, there is very little that separates Google from Amazon. But once Google allows users to control things like Gmail, or Gcal, or YouTube from Assistant, or starts using peoples search histories to personalize each listeners voice experience, its going to be very different from Amazon, which in turn will have access to an enormous trove of user purchase history, intent and other data. People arent going to want to have to interact differently, depending on what microphone theyre talking to, said David Beisel, a partner and co-founder at NextView Ventures. Theres a lot of complication there. But thats not a problem for right now. For now, the stage is set for voice to take off. And while Amazon, Google and Microsofts long-term strategic visions for voice may differ, theyre all going to be focused on the same thing, for now. If they want to foster a rich ecosystem, Beisel said, theyre going to have to reward folks for it. ",https://digiday.com/media/can-hear-now-voice-picks-steam-bot-hype-fades/,"While people expect artificial intelligence and virtual reality will be ready for prime time eventually, the speech recognition technology that ...",Can you hear me now?: Voice picks up steam as bot hype fades,Google Alexa Hound Shazam Soundhound Hound Andrew Ng Baidu Oculus Rift Echo Dot Amazon Echo Beerud Sheth Gupshup Echo Google Home Ford VW BMW Amazons Alexa Google Assistant Android Android Auto Sheth Soon Sheth Washington Post Alexa Google Amazon Audio Ad Age Trade Desk Add Pat Higbie XAPP Media Alexa Amazon Techstars July Google GV Cody Simms Techstars Amazon Google Amazon Microsoft Facebook Google Google Assistant Alexa Google Amazon Google Gmail Gcal Assistant Amazon David Beisel NextView Ventures Amazon Google Microsofts Beisel,4
14,"Shanghai, China, March 15, 2017 - STMicroelectronics (NYSE: STM), a global semiconductor leader serving customers across the spectrum of electronics applications and a top MEMS supplier, and iFLYTEK (SHE: 002230), one of the leading voice-recognition cloud service providers in China, have introduced the market's first IoT development platform that enables voice-recognition cloud services in Chinese. The joint solution is on display at electronica China 2017, Shanghai New International Expo Center, Hall E4 Booth 4102, March 14-16, 2017. The new platform combines ST's SensorTile multi-sensor module, STM32 ODE (Open Development Environment), and Open.software package with iFLYTEK's voice-recognition technology. It gives designers a complete toolset for the development of voice-enabled Smart-Home, Smart-Driving, IoT, and robotics applications. The SensorTile module captures voice inputs through the digital MEMS microphone (MP34DT04) and transmits them using the Bluetooth Low Energy network processor (BlueNRG-MS) to iFLYTEK's cloud through a smartphone with the voice-recognition result back within seconds. ""ST's SensorTile is a perfect match for developers integrating voice-control capabilities in applications across Smart-Home, Smart-Industry, and Smart-Driving segments. iFLYTEK has been empowering developers with the best performing and easy-to-use speech-recognition service,"" said Jidong YU, Senior Vice President of iFLYTEK Co., Ltd. ""We have been working with ST to enable the SensorTile platform with a high-performance Chinese-language recognition. Leveraging iFLYTEK's more than 270,000 developers on xfyun.cn and ST's smart IoT development tools, we look forward to creating more designs together in future."" ""The implementation of iFLYTEK's automatic speech-recognition services on SensorTile accelerates and simplifies voice-enabled IoT design,"" said Collins Wu, Marketing Director, Analog and MEMS Group, Greater China & South Asia, STMicroelectronics. ""Leveraging a powerful open-software ecosystem, including the STM32(TM) Open Development Environment, shortens time to market and makes IoT design simple and cool."" ST's Analog and MEMS Group has also played an active role in nurturing the Innovator Community and Smart Hardware Development Platform in China, establishing a Chinese-speaking engineer community, st_AMSchina, a service subscription on Wechat, as well as the MEMS QQ Group. STMicroelectronics' 13.5mm x 13.5mm SensorTile is currently the smallest turnkey sensor board of its type, containing ST's MEMS accelerometer, gyroscope, magnetometer, pressure sensor, and MEMS microphone. With the on-board low-power STM32L4 microcontroller, it can be used as a sensing and connectivity hub for developing products such as wearables, gaming accessories, and smart-home or Internet-of-Things (IoT) devices. ST is a global semiconductor leader delivering intelligent and energy-efficient products and solutions that power the electronics at the heart of everyday life. ST's products are found everywhere today, and together with our customers, we are enabling smarter driving and smarter factories, cities and homes, along with the next generation of mobile and Internet of Things devices. By getting more from technology to get more from life, ST stands for life.augmented. In 2016, the Company's net revenues were $6.97 billion, serving more than 100,000 customers worldwide. Further information can be found at www.st.com . IFLYTEK CO.,LTD.(iFLYTEK) is a national key software enterprise dedicated to the research of intelligent speech and language technologies, development of software and chip products, provision of speech information services, and integration of E-government systems. The intelligent speech technology of iFLYTEK, the core technology of the company, represents the top level in the world. Established in 1999, iFLYTEK was listed in the Shenzhen Stock Exchange in 2008 (stock code: 002230). With vigorous support from major shareholders including USTC Holdings Co., Ltd., Shanghai Guangxin, and Legend Capital, iFLYTEK boasts the longest fundamental research history, the largest professional research team, the greatest capital investment, the best evaluation results, and the largest market share among all business entities in the speech technology field in China. ",https://globenewswire.com/news-release/2017/03/15/937743/0/en/STMicroelectronics-Collaborates-with-iFLYTEK-for-Chinese-Voice-Recognition-Cloud-Services.html,"iFLYTEK has been empowering developers with the best performing and easy-to-use speech-recognition service, said Jidong YU, Senior Vice ...",STMicroelectronics Collaborates with iFLYTEK for Chinese Voice ...,Shanghai China March NYSE MEMS SHE China IoT Chinese China Shanghai New International Expo Center Hall E4 Booth March ST SensorTile STM32 ODE Open Development Environment Open.software Smart-Home Smart-Driving IoT SensorTile MEMS MP34DT04 Bluetooth Low Energy BlueNRG-MS ST SensorTile Smart-Home Smart-Industry Jidong YU Senior Vice President iFLYTEK Co. Ltd. ST SensorTile ST IoT SensorTile IoT Collins Wu Marketing Director Analog MEMS Group Greater China South Asia STMicroelectronics STM32 TM Open Development Environment IoT ST Analog MEMS Group Innovator Community Smart Hardware Development Platform China Wechat MEMS QQ Group STMicroelectronics SensorTile ST MEMS MEMS STM32L4 IoT ST ST Internet Things ST Company IFLYTEK CO. LTD Shenzhen Stock Exchange USTC Holdings Co. Ltd. Shanghai Guangxin Legend Capital China,-1
15,"A deep neural network architecture which can directly translate speech from one language into text in another language is being developed by Google researchers. The study , titled Sequence-to-Sequence Models Can Directly Transcribe Foreign Speech, describes using a modified sequence-to-sequence model, which has had previous success in speech recognition, to create a powerful encoder-decoder network for machine translation. The paper explains that the new model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the source language transcription during training. In testing, the research team reported state-of-the-art performance on conversational Spanish to English speech translation tasks. The experiments used the Fisher Callhome Spanish-English dataset and found that the proposed model could outperform cascades of speech recognition and machine translation technologies. Using the BLEU (bilingual evaluation understudy) scoring framework, which evaluates the quality of machine-translated text, the proposed system recorded 1.8 points over other translation models. According to the study, when Spanish transcripts were used as training data for additional supervision across independent automatic speech recognition (ASR) and speech translation (ST) decoders, additional improvements of at least 1.4 BLEU points were obtained. In future work, the Google researchers plan to construct a multilingual speech translation system in which a single decoder is shared across multiple languages. With big advances in deep learning, human versus AI competitions are springing up across the world. In February, human translators battled against AI machine translators in Seoul, South Korea. While Google Translate and Naver Papago, both based on Neural Machine Translation (NMT), won the high-profile battle in terms of speed, they fell considerably behind their human counterparts on quality. It is however expected that with further NMT developments, such as this latest Google research, machine translation will improve at a fast rate and could reach human-level accuracy very soon. ",https://thestack.com/big-data/2017/04/04/google-model-instantly-translates-speech-to-foreign-text-using-ai/,"... Foreign Speech, describes using a modified sequence-to-sequence model, which has had previous success in speech recognition, to create ...",Google model instantly translates speech to foreign text using AI,Google Directly Transcribe Foreign Speech Fisher Callhome BLEU ASR ST BLEU Google AI February AI Seoul South Korea Google Translate Naver Papago Neural Machine Translation NMT NMT Google,4
16,"Ever wondered what it would be like for artificial intelligence to trip-out while watching Bob Ross paint a pretty picture This video was created by Alexander Reben , an engineer turned artist who uses technology to explore how machines are changing the human world and vice versa. It features an episode of the stoner-favorite television show The Joy of Painting with Bob Ross through Googles neural network DeepDream. DeepDream is a convolutional neural network, a style of computing inspired by the brain, that identifies and recognizes images and patterns. Most of the time, it's used to create nightmarish visions like these, but its alsoa surprisingly insightful visualization that shows how computers think in regards to tasks like image classification and speech recognition.  Google made the code forDeepDream open-source, meaning there are plenty of videos, images, and apps that utilize it. Somebody has also published the code and a guide on GitHub , complete with a nightmarish rehash of Fear & Loathing in Las Vegas. ",http://www.iflscience.com/technology/watch-artificial-intelligence-lose-its-mind-while-watching-bob-ross/,... a surprisingly insightful visualization that shows how computers think in regards to tasks like image classification and speech recognition.,Watch Artificial Intelligence Lose Its Mind While Watching Bob Ross,Bob Ross Alexander Reben Joy Bob Ross Googles DeepDream DeepDream Google GitHub Fear Loathing Las Vegas,-1
17,"journalist, radio producer, contributing editor at MediaShift. fellow with the Fund for Investigative Journalism and Schuster Institute. likely #loitering. Voice Is the Next Big Platform, Unless You Have anAccent Its super funny that Alexa cant understand my mom  until we need Alexa to use the web, drive a car, and do pretty much anythingelse. My mother waited two months for her Amazon Echo to arrive. Then, she waited again  leaving it in the box until I came to help her install it. Her forehead crinkled as I download the Alexa app on her phone. Any device that requires vocal instructions makes my mother skeptical. She has bad memories of Siri. She could not understand me, my mom told me. My mother was born in the Philippines, my father in India. Both of them speak English as a third language. In the nearly 50 years theyve lived in the United States, theyve spoken English daily  fluently, but with distinct accents and sometimes different phrasings than a native speaker. In their experience, that means Siri, Alexa, or basically any device that uses speech technology will struggle to recognize their commands. My parents experience is hardly exclusive or unknown. (Its even been chronicled in comedy, with this infamous trapped-in-a-voice-activated elevator sketch .) My sister-in-law told me she gave up on using Siri after it failed to recognize the ethnic names of her friends and family. I can vouch for the frustration: The other day, my command of Text Zahir morphed into Text Zara here. Right now, its not much of a problem  but its slated to become more serious, given that we are in the middle of a voice revolution. Voice-based wearables , audio, and video entertainment systems are already here. Due in part to distracted drivers, voice control systems will soon be the norm in vehicles. Google Home and Amazons Alexa are radicalizing the idea of a smart home across millions of households in the US. Thats why it took so long for my mothers Echo to arrive  the Echo was among Amazons bestsellers this holiday season, with a 900 percent increase from 2016 sales . It was backordered for weeks. Overall, researchers estimate 24.5 million voice-driven devices will be delivered to Americans daily routines this year  evidence that underscores ComScores prediction that by 2020, half of all our searches will be performed by voice. But as technology shifts to respond to our vocal chords, what happens to the huge swath of people who cant be understood To train a machine to recognize speech, you need a lot of audio samples. First, researchers have to collect thousands of voices, speaking on a range of topics. They then manually transcribe the audio clips. This combination of data  audio clips and written transcriptions  allows machines to make associations between sound and words. The phrases that occur most frequently become a pattern for an algorithm to learn how a human speaks. But an AI can only recognize what its been trained to hear. Its flexibility depends on the diversity of the accents to which its been introduced. Governments, academics, and smaller startups rely on collections of audio and transcriptions, called speech corpora, to bypass doing labor-intensive transcriptions themselves. The University of Pennsylvanias Linguistic Data Consortium (LDC) is a powerhouse of these data sets, making them available under licensed agreements for companies and researchers. One of its most famous corpora is Switchboard . Texas Instruments launched Switchboard in the early 1990s to build up a repository of voice data, which was then distributed by the LDC for machine learning programs. Its a collection of roughly 2,400 telephone conversations , amassed from 543 people from around the US  a total of about 250 hours. Researchers lured the callers by offering them long-distance calling cards. A participant would dial in and be connected with another study participant. The two strangers would then chat spontaneously about a given topic  say, childcare or sports. Rural Filipinas have flipped the dating script: Now theyre the ones shopping for Western men.backchannel.com For years linguists have assumed that because the LDC is located in Philadelphia, the conversations skewed towards a Northeastern accent. But when Marsal Gavald, the director of machine intelligence at the messaging app Yik Yak, crunched the numbers in Switchboards demographic history , he found that the accent pool skewed more midwestern. South and North Midland accents comprised more than 40 percent of the voice data. Other corpora exist, but Switchboard remains a benchmark for the models used in voice recognition systems. Case in point: Both IBM and Microsoft use Switchboard to test the word error rates for their voice-based systems. From this set of just over 500 speakers, pretty much all engines have been trained, says Gavald. But building voice technology on a 26-year-old corpus inevitably lays a foundation for misunderstanding. English is professional currency in the linguistic marketplace, but numerous speakers learn it as a second, third, or fourth language. Gavald likens the process to drug trials. It may have been tried in a hundred patients, [but] for a narrow demographic, he tells me. You try to extrapolate that to the general population, the dosage may be incorrect. Larger companies, of course, have to think globally to stay competitive  especially because most sales of smartphones happen outside the US Technology companies like Apple, Google, and Amazon have private, in-house methods of collecting this data for the languages and accents theyd like to accommodate. And the more consumers use their products, the more their feedback will improve the products, through programs like Voice Training on the Alexa app. But even if larger tech companies are making headway in collecting more specific data, theyre motivated by the market to not share it with anyone  which is why it takes so long for the technology to trickle down. This secrecy also applied to my reporting of this piece. Amazon never replied to my request for comment, a spokesperson for Google directed me to a blog post outlining its deep learning techniques, and an Apple PR representative noted that Siri is now customized for 36 countries and supports 21 languages, language variants, and accents. Outside the US, companies are aware of the importance of catering to accents. The Chinese search engine company Baidu, for one, says its deep learning approach to speech recognition achieves accuracy in English and Mandarin better than humans , and its developing a deep speech algorithm that will recognize a range of dialects and accents. China has a fairly deep awareness of whats happening in the English-speaking world, but the opposite is not true, Baidu chief scientist Andrew Ng told The Atlantic. Amazons personal assistant is about to stretch beyond the Echo, and get downright chatty with everyone.backchannel.com Yet smaller companies and individuals who cant invest in collecting data on their own are beholden to cheaper, more readily available databases that may not be as diverse as their target demographics. [The datas] not really becoming more diverse, at least from my perspective, Arlo Faria, a speech researcher at the conference transcription startup Remeeting, tells me. Remeeting, for example, has used a corpus called Fisher that includes a group of non-native English speakers  but Fishers accents are largely left up to chance, depending on who happened to participate in the data collection. There are some Spanish and Indian accents, for instance, but very few British accents, Faria recalls. Thats why, very often, voice recognition technology reacts to accents differently than humans, says Anne Wootton, co-founder and CEO of the Oakland-based audio search platform Pop Up Archive, Oftentimes the software does a better job with like, Indian accents than deep Southern, like Shenandoah Valley accents, she says. I think thats a reflection of what the training data includes or does not include. Rachael Tatman, a PhD candidate at the University of Washingtons Department of Linguistics who focuses on sociolinguistics, noted that the underrepresented groups in these data sets tend to be groups that are marginalized in general. A typical database of American voices, for example, would lack poor, uneducated, rural, non-white, non-native English voices. The more of those categories you fall into, the worse speech recognition is for you, she says. Still, Jeffrey Kofman, the CEO and co-founder of Trint, another automated speech-to-text software based in the UK, is confident accent recognition is something speech science will be able to eventually solve. We video chatted on the Trint platform itself, where Australian English is now available alongside British and North American English as transcription accents. Trint also offers speech-to-text in a dozen European languages, and plans to add South Asian English sometime this year, he said. Collecting data is expensive and cumbersome, which is why certain key demographics take priority. For Kofman, thats South Asian accents, because there are so many people from India, Pakistan, and those countries here in England, in the US and Canada, who speak very clearly but with a distinct accent, he says. Next, he suspects, hell prioritize South African accents. Obviously, its not just technology that discriminates against people with accents. Its also other people. Mass media and globalization are having a huge effect on how people sound. Speech experts have documented the decline of certain regional American accents since as early as 1960 , for example, in favor of a more homogenous accent fit for populations from mixed geographic areas. This effect is exacerbated when humans deal with digital assistants or operators; they tend to use a voice devoid of colloquialisms and natural cadence. Or, in other words, a voice devoid of an identity and accent. As voice recognition technology becomes better, using a robotic accent to communicate with a device stands to be challenged  if people feel less of a need to talk to their devices as if they are machines, they can start talking to them as naturally as they would a friend. And while some accent reduction coaches find their clients use voice assistants to practice neutralizing their thick foreign or regional accents, Lisa Wentz, a public speaking coach in San Francisco who works in accent reduction, says that she doesnt recommend it. Thats because, she tells me, most of her clients are aiming for other people to understand them. They dont want to have to repeat themselves or feel like their accents prevent others from hearing them. Using devices that arent ready for different voices, then, only stands to make this feeling echo. My mother and I set up her Alexa app together. She wasnt very excited about it. I could already imagine her distrust and fear of a car purported to drive by the command of her voice. My mother would never ride in it; the risk of crashing would be too real. Still, she tried out a couple of questions on the Echo. Alexa, play Que sera sera, my mother said. My mom laughed, less out of frustration and more out of amusement. She tried again, this time speaking slower, as if she were talking to a child. Alexa, play Que sera sera. She sang out the syllables of sera in a slight melody, so that the device could clearly hear se-rah. Alexa understood, and found what my mom was looking for. Heres a sample of Que sera sera, by Doris Day, she said, pronouncing the sera a bit harsher  se-raw. The 1964 hit started to play, and my mother smiled at the pleasure of recognition. ",https://backchannel.com/voice-is-the-next-big-platform-unless-you-have-an-accent-6a787f7e8500,"To train a machine to recognize speech, you need a lot of audio ... says its deep learning approach to speech recognition achieves accuracy in ...","Voice Is the Next Big Platform, Unless You Have an Accent",MediaShift Fund Investigative Journalism Schuster Institute Voice Big Platform Unless Alexa Alexa Amazon Echo Alexa Siri Philippines India United States Siri Alexa Siri Text Zahir Text Zara Due Google Home Amazons Alexa US Thats Echo Echo Amazons Americans ComScores First AI University Pennsylvanias Linguistic Data Consortium LDC Switchboard Texas Instruments Switchboard LDC US Filipinas LDC Philadelphia Northeastern Marsal Gavald Yik Yak Switchboards South North Midland Switchboard Case IBM Microsoft Switchboard Gavald English Gavald ] US Technology Apple Google Amazon Voice Training Alexa Amazon Google Apple PR Siri US Baidu English Mandarin China Baidu Andrew Ng Atlantic Amazons Echo Yet Arlo Faria Remeeting Fisher Fishers Faria Thats Anne Wootton CEO Pop Up Archive Southern Shenandoah Valley Rachael Tatman PhD University Washingtons Department Linguistics Jeffrey Kofman CEO Trint UK Trint English British North English Trint South English Collecting Kofman South India Pakistan England US Canada Mass Lisa Wentz San Francisco Thats Alexa Echo Alexa Que My Alexa Que Alexa Que Doris Day,4
18,"AutoCorrect and speech recognition software should be like the other half of an old married couple, always finishing our sentences perfectly. But when I read what comes through my smartphone, Im left wondering whether talk-to-text and AutoCorrect know me at all. Speech recognition software was a godsend when it became a standard smartphone feature. For one thing, it allows for hands-free transmission of text messages, email and whatever else were doing when we should be watching the road. For another, it provides an alternative to using that tiny keyboard on smartphone screens, which only Tinkerbell could operate without inadvertently hitting three letters at once. The next thing you know, instead of texting your daughter that I want to see Granny, youre saying I want to seem grabby. Ummm Dad, thats like soooo gross. AutoCorrect should be a blessing to the spelling-challenged, but its a curse to anyone who hits the Send button without checking the screen first. I just took a bite of granola becomes I just took a bite of grandpa. Whats troubling is that even though the software is designed to recognize commonly used words, AutoCorrect doesnt get me. Perhaps its encouraging me to clean up my language when it replaces a certain four-letter word with duck, whether thats what I ducking said or not. Talk-to-text recognizes my friends names when I say them, yet somehow when I mention my home city, it never hears Baraboo. The screen reads bearable. Baraboo is indeed bearable, but talk-to-text isnt a true companion if it doesnt know where I live. Facebook sends me a coupon the instant I walk past P.F. Changs, yet my phone doesnt know where Ive lived for 20 years. Such situations leave me wondering whether I speak oddly. Do I mumble Do I stutter Have I failed to shed the Midwestern accent bestowed upon natives of Wis-CAHN-sin Oh yah, yer darn tootin! Ill be walking down the sidewalk, attempting to dictate a simple sentence, and what pops up on the screen is vegan apocalypse Chevy dismount. Theres a great rock band name in there somewhere. AutoCorrect isnt any more reliable than speech recognition. In fact, it makes unintentionally hilarious gaffes so easy, theres an entire website dedicated to them. Thanks to damnyouautocorrect.com , we know that in an instant a herniation can become a horny Asian. Your father may tell you he and your mother decided to separate, when what he meant was that they decided to stay up late. And somehow Harry Potter becomes hardy pornstar. Im no computer programmer, but I have to believe the odds of anyone purposely using the term hardy pornstar pale in comparison to someone referencing a hit series of books and movies. This phenomenon leaves me scratching my head or, as AutoCorrect might interpret that phrase, stashing undead. Given the technologys clear ability to pick up on frequently used words, I suspect the designers of AutoCorrect and speech recognition insert humorous glitches just to mess with us. Imagine their glee when I mean to tell my father Hello, its your dear son, but whats sent instead is Hello, its your dead son. Cue the theme from The Sixth Sense. Or maybe software engineers are merely reinforcing lessons we were taught in school. Haste makes waste. Check your work. Reread and revise. Otherwise, you may end up communicating that your birthday-boy husband is getting executed when he is in fact getting excited. If your smartphone truly knew you, it would know that your husband isnt on death row. Forget finishing our sentences: Our smartphones are finishing our loved ones life sentences. Perhaps someday our phones will understand us. Theyll always know what we mean, even when we dont say it quite right. And they certainly wont suggest, as a lover tries to tell a new beau her sheets smell like his cologne, that her bedroom now smells like his colon. I understand and agree that registration on or use of this site consitutes agreement to its user agreement and privacy policy. Ben Bromley has been named Wisconsins top small-town newspaper columnist two years running. ",http://lacrossetribune.com/news/local/ben-bromley-smartphone-can-t-finnish-my-sent-tenses/article_943eb98f-8215-552d-8a25-1fda9299c8bd.html,"AutoCorrect and speech recognition software should be like the other half of an old married couple, always finishing our sentences perfectly.",Ben Bromley: Smartphone can't Finnish my sent tenses,AutoCorrect Im AutoCorrect Tinkerbell Granny Ummm Dad AutoCorrect Send Whats AutoCorrect Baraboo Baraboo Facebook P.F Changs Ive Midwestern Wis-CAHN-sin Oh Ill Chevy AutoCorrect Asian Harry Potter Im AutoCorrect AutoCorrect Hello Hello Sixth Sense Theyll Ben Bromley Wisconsins,-1
19,"Physics Today magazine features From Sound to Meaning this month, a special article highlighting physics role in understanding language Newswise  WASHINGTON, D.C., April 10, 2017 -- Perhaps you have been thinking of taking a foreign language course and are undecided whether to take an evening or morning class. Adding to your indecision: You are concerned about your ability to understand someone speaking another language. Research provides partial guidance. In her April Physics Today feature, From Sound to Meaning, University of Connecticut cognitive scientist Emily Myers recounts her groups discovery that people retain what theyve learned in a language class better if they go to bed before they get the chance hear a lot of their own language during the rest of the day. Evening classes are better. That and other findings draw on big strides in a cross-disciplinary effort that is currently advancing understanding of how people derive meaning from sounds. Myers starts off by explaining phonemes, the abstract units of perception and production that, when swapped, produce a change in the word. Phonemes vary across cultures. For example, for an English speaker, the r and l are heard as distinct phonemes. In contrast, the Japanese do not have distinct r and l phonemes. To a Japanese listener, play and pray sound the same. Myers is not only talking about linguistics here: Physics is important too for turning vocalizations into understanding. For example, differences in voice onset time (VOT), the speed with which a sound is vocalized, bear on how the sound is interpreted. She describes how the human mind becomes adept at working with these and other variables so that speakers and their listeners can enter a community of understanding. It is a distinctly biological ability (at least for now), an observation that Myers supports by illustrating how Siri or Alexa speech-recognition interfaces go awry when they try to interpret rapid speech. Yet, the human system for producing and understanding speech is not terribly resilient. Myers describes how we develop a communicative ability through perceptual narrowing, which may lead to skill and perceptiveness for the inevitable situations where communication occurs amidst interference. What follows perceptual narrowing is perceptual entrenchment, also an inevitability. One not-so-fortunate byproduct is adults difficulties with new languages or unfamiliar accents. So, if you are puzzled why Siri sometimes misunderstands you or why children learn languages better than adults do, turn to Myerss article. Its freely available on Physics Todays website, and can be accessed directly here: http://physicstoday.scitation.org/doi/abs/10.1063/PT.3.3523 . Physics Todayis the flagship publication of the American Institute of Physics. Each month it includes a mix of in-depth feature articles, news coverage and analysis, and fresh perspectives on scientific advances and ground-breaking research. See: http://www.physicstoday.org  The American Institute of Physics is a federation of scientific societies in the physical sciences, representing scientists, engineers, educators, and students. AIP offers authoritative information, services, and expertise in physics education and student programs, science communication, government relations, career services, statistical research in physics employment and education, industrial outreach, and the history of the physical sciences. AIP is home to the Society of Physics Students and the Niels Bohr Library and Archives. AIP owns AIP Publishing LLC, a scholarly publisher in the physical and related sciences. More information: http://www.aip.org  ",http://www.newswise.com/articles/scientists-make-strides-explaining-how-we-discern-language,"It is a distinctly biological ability (at least for now), an observation that Myers supports by illustrating how Siri or Alexa speech-recognition ...",Scientists Make Strides Explaining How We Discern Language,Today Sound Meaning Newswise WASHINGTON D.C. April Research April Physics Today From Sound Meaning University Connecticut Emily Myers Phonemes Japanese Physics VOT Myers Siri Alexa Siri Myerss Physics Todays American Institute Physics American Institute Physics AIP AIP Society Physics Niels Bohr Library Archives AIP AIP Publishing LLC,-1
20,"What is A.I. and how it will affect our $200 billion Digital Ad Market Learn about how Gravity4 is bringing AI to the Digital Ad Market. Nowadays, the latest buzzword of attraction is Artificial Intelligence and its immediate impact on our advertising sector. As the CEO of Gravity4 , I thought it to be only appropriate to help dissect this new evolutionary phase of our industry as we apply it.  It is no doubt that Deep Learning is our future, and it is on course to have a huge impact on the lives of everyday consumers and business sectors. In the scientific world, deep learning is referred to as deep neural networks. These involve a family of artificial intelligence, popularly known as AI, something named way back in 1955, and something which Facebook, Google and Microsoft are all now pushing for with Herculean force. In fact, according to the international data corporation, it is estimated that from a global standpoint, by 2020, the artificial intelligence market could reach close to $50 billion. AI refers to a collection of tools and technologies, some of which are relatively new, and some of which are time-tested. The techniques that are employed allow computers to use these tools and technologies to imitate human intelligence. These include: machine learning such as deep learning, decision trees, if-then rules, and logic. Conversely, deep learning refers to many layers of deep neural networks. This form of machine learning comprises algorithms, which enable computer software to carry out tasks by training itself (without human assistance), by uncovering multi-layered neural networks comprising huge quantities of data. Good examples are image and speech recognition. Machine learning, on the other hand, is an AI subset, which allows machines to get better at the tasks they perform via experience. Deep learning is evolutionary phase in this category. Basically, AI enables machines to think the way that we do, but do things even more phenomenally better - think of sci-fi movies involving futuristic computers! For instance: AI can have a conversation with hundreds or thousands of people simultaneously and use real time data to make decisions to discover what is happening in every conversation. Or Tony Stark, having a jolly good conversation with his AI Jarvis. (Yeah! Siri , Alexa , and Google have pretty high standards set for sure). When it comes to marketing and advertising, these sectors are the ones most likely to take the biggest percentage of the general artificial intelligence market. In theory, AI has great potential for making consumer experiences more interactive and personable. This includes smarter recommendation and search results, commerce with a conversational twist courtesy of bots; identifying site visitors decision making patterns, and targeting programmatic advert buy ups. A 2016 survey undertaken by Narrative Science, showed that 38% of organizations already employ artificial intelligence, and that by 2018, it will rocket to in excess of 60%. When will AI be alive It may surprise some that Artificial Intelligence has been very much integrated in our daily lives for sometime already. There are a number of cutting-edge artificial technologies utilizing variations of AI in their workflow. These technologies just not in the mainstream full awareness of the consumers; these are just some of them: 1. Robotic Process Automation: This action supports cost-effective business procedures by using scripts to enable human action automation. It is perfect for cases where it is inefficient or too costly for a human to carry out a process or task. Some industries are already using the automation plans. You may be aware of some of them. Here, think the industrialized robotic machines used by companies to manufacture big equipments such as GE , Seismic and ABB . Or think automotive industry. Automation continues to help these verticals produce faster and efficient products. 2. Speech Recognition: this involves transforming and transcribing everyday human speech into a workable format, which can be employed for computer applications. It can already be found on mobile applications and systems with an interactive voice response. 3. Biometrics: At present, this system is mainly utilized in market research to provide better interactions between machines and humans. Biometrics includes elements such as body language, speech and image recognition. 4. NLP and Text Analytics: Text Analytics are supported and used by NLP (Natural language processing) through the understanding and facilitating of sentiment, sentence meaning and structure, and intent via machine learning and statistical methods. These days it is applied in a broad spectrum of automated assistants, the mining of unstructured data applications, and security and fraud detection. 5. Decision Management: This consists of engines, which introduce logic and rules into artificial intelligence systems. It is utilized for initial training/setup, and ongoing tuning and maintenance. It is a long established technology which is utilized in a broad spectrum of enterprise applications, performing or helping automated decision making. 6. Virtual Agents: These involve everything from superior systems that have the capacity to network with human beings, to simple chatbots. At the present time they are utilized for smart home managers, and customer support and service. They are currently used by Microsoft , IBM , Google , Apple and Amazon . Well, we all have had high hopes for Siri. Now that Bixy, Alexa and Google are making their way into our homes, I am sure Jarvis is not too far. 7. Machine Learning Platforms: These provide data, training and development tool kits, application programming interfaces, and algorithms. They also offer the computing power to distribute, train and design models, and convert them into other machines, processes and applications. Today, they are employed in a huge array of enterprise applications. These are mainly consist of classification or prediction. They are presently used by Microsoft, Google, and Amazon. Of course, for those are waiting for K-2S0 from the Star Wars Rogue One  well, then you may have to wait just a while more. How does this affect the $200 billion Global Digital Advertising Market Various companies already use machine learning for ad targeting, utilizing Real-Time Bidding (RTB). By utilizing deep learning, marketers have the ability to make stronger predictions of the next event in the consumers purchase cycle. With AI layered on, deep learning provides the platform for marketers to understand their customers need, to serve them better and sell products that consumers have yet to decide on. This strive towards knowing the intent of the customer before they do (yes, a bit freaky  but possible with sheer amount of data) is the ultimate prize towards enabling hyper-personalized consumer experience. Imagine a fully automated media platform, that is able to utilize machine learning to process the inflow of data stream, to create personalized digital ads for each unique customer  selecting not only the brands they will have best infinity towards, but also have pre-select the color and even the size of clothing based on the customers current trending choice. Artificial intelligence is no doubt going to have tremendous impact on digital marketing; hence, the race towards testing and implementing better measures of deep learning. AI, and particularly deep learning, are here and now. These will and are helping to revolutionize analytics, build smart cities, and prevent disease. Not so long ago, deep learning was just a concept - now organizations utilize it to magically change moonshots into reality. And, we hope to simplify this for our industry with our upcoming products built by our Gravity4 leadership team, across the globe. ",http://www.huffingtonpost.com/entry/what-is-ai-and-how-it-will-affect-our-200-billion_us_58eba189e4b0ea028d568b4c,"Speech Recognition: this involves transforming and transcribing everyday human speech into a workable format, which can be employed for ...",What is A.I. and how it will affect our $200 billion Digital Ad Market?,A.I Digital Ad Market Learn Gravity4 AI Digital Ad Market Nowadays Intelligence CEO Gravity4 Deep Learning AI Facebook Google Microsoft Herculean AI Machine AI Deep Basically AI Tony Stark AI Jarvis Siri Alexa Google AI Narrative Science AI Intelligence AI Process GE Seismic ABB Automation NLP Text Analytics NLP Natural Microsoft IBM Google Apple Amazon Well Siri Bixy Alexa Google Jarvis Microsoft Google Amazon K-2S0 Star Wars Rogue One Global Digital Advertising Market Various Bidding RTB AI AI Gravity4,-1
21,"This fintech CEO says AI-powered voice recognition and text transcription let traders communicate with one another without typing or picking up a phone. IfNader Shwayhat, CEO of fintech firm of GreenKey Technologies, has his way every conversationon the trading floor will be instantly recorded, transcribed and stored as soon as it happens. As banks and hedge funds pay ever-closer attention to bad behaviour after a litany of scandals, his firm uses an AI-driven speech recognition tool to keep tabs on the trading floor. Shwayhat is a veteran of venture-backed fintech businesses focused on trading and big-data analytics and holds patents on chat-based trading communication technologies. He said that advanced voice interfaces and AI-driven speech recognition represent the next wave of innovation on trading floors. When you look at the trader voice market, which is based on economic models and technologies that are decades old, we havent seen disruptive innovation yet, but were looking to change the entire definition of a voice turret, Shwayhat said. We essentially created an app that can be installed on a smartphone, tablet, laptop or desktop computer, which offers a lot more capability aside from being able to focus on the data that is in the phone calls, he said. Were going to be scaling and growing the business over the next 24 months, with a lot of growth in the New York City area, Singapore and Hong Kong, Shwayhat said. Well be scaling our support services and sales staff here in New York and internationally as well  in Europe, mainly London, and Asia, well primarily be hiring in the front office, and the development team managed out of Chicago will be expanding as well. GreenKey is looking to hire engineers and computer developers with experience working in JavaScript, Python and HTML5. WebRTC is what our core stack is based on, Shwayhat said. Were hiring developers in Chicago, and data science is a massive aspect of what we do, so well look to add PhDs in data science. Between full-time employees and advisory consultant staff our headcount is currently around 20, and I can envision that doubling or tripling within the next 12-to-18 months, with approximately two-thirds of the hiring to be in New York, London and Hong Kong. Back in 2004, Shwayhat co-founded Pivot, an instant-messaging startup geared toward financial services that was spun off from his previous employer Eze Castle Integration and eventually bought by CME Group, a derivatives exchange. Post-acquisition, he became the new parents global head of sales and marketing and co-founded CME Direct, an internal, acquisition-driven startup offering technology tools for traders. Most recently, Shwayhat was the global head of sales and business development for Novus Partners, a portfolio intelligence company specializing in big data and analytics for institutional investors and asset managers. Like many successful fintech entrepreneurs, Shwayhat is an engineer. He earned a Bachelors degree in mechanical engineering and a Masters degree in financial and operations engineering from the University of Michigan. What career advice can he give to tech-savvy students interested in a career in financial services or fintech Financial services is one of the largest employers of tech graduates in the country, specifically the market we play in, financial technology, Shwayhat said. Fintech is a massive area, and beyond the traditional banking, consulting and trading skills sets, finance and math are primary. Computer science probably one of the most valuable majors in financial services, because every trader is having to also be a quant developer in their own right, he said. Bankers have to have tech skills to evaluate M&A opportunities, beyond the finance and math backgrounds that people traditionally think about. ",http://news.efinancialcareers.com/us-en/278764/ai-speech-recognition-trading-greenkey-fintech,"As banks and hedge funds pay ever-closer attention to bad behaviour after a litany of scandals, his firm uses an AI-driven speech recognition ...",This firm is using AI to keep tabs on investment banks' traders,CEO IfNader Shwayhat CEO GreenKey Technologies Shwayhat AI-driven Shwayhat Were New York City Singapore Hong Kong Shwayhat New York Europe London Asia Chicago GreenKey JavaScript Python HTML5 WebRTC Shwayhat Were Chicago PhDs New York London Hong Kong Back Shwayhat Pivot Eze Castle Integration CME Group CME Direct Shwayhat Novus Partners Shwayhat Masters University Michigan Financial Shwayhat Fintech Computer M A,-1
22,"With an estimated 100 million speakers , Swahili is the second-most-widely-used language on the African continent, after Arabic. Yet services such as automatic speech recognition (ASR) arent commercially available in this language, denying many users with disabilities and those who aren't literate the information they desperately need in their daily lives. This could change very soon though, as academic research and technology startups are converging to provide localized technologies to Swahili speakers. One of these very promising innovations is about to be rolled out in Kenya. Uliza (meaning ask in Swahili) is a voice interface that allows users to access information from the Internet usinga basic mobile phone. All users need to do is call in andask a question in Swahili. Within 15 to 90 minutes, an answer agent (an actual person working behind the scenes) responds with a voice answer. At the moment, a crowd of around 50 agents treat the queries by transcribing the voice recordings, searching for answers online in multiple languages, translating the information and sending it back to the caller in Swahili. During the pilot carried out in the Kenyan capital Nairobi and in Western Kenya, some 600 beta users sent questions about their local representatives, asked for help with Swahili homework, and requested medical information that would be too delicate to bring up in person. Uliza will solve another problem for its future users: the lack of access to information hosted on the Internet. There are many overlapping reasons for this situation: unaffordable mobile data bundles, distance to the nearest cybercafe, illiteracy in languages of wider communication, compounded by a dearth of available content in local languages. Uliza's crowdsourcing model is admittedly labor-intensive but it has a major advantage: by having human beings handle the transcription and translation, it temporarily bypasses the lack of large voice datasets that typically constrains ASR efforts in African languages, while simultaneously collecting data from real speakers in a variety of accents and dialects. Uliza founder Grant Bridgman plans to use this database of short recordings and transcriptions to build machine learning capability and fully automate the system in the future. In this talk at Tufts University, Bridgman introduced the concept behind the project: A good amount of research has already gone into building automatic speech recognition software for Swahili and other widely spoken African languages, but its taking a while for the technology to find its way into peoples hands. In an interview with Global Voices, Bridgman explained: The technology exists and all of this is already available for first world languages, now we need to find a commercial model to make it viable for low-resource languages. Companies looking to set up helplines for a rural customer base at a reduced cost are potential customers for the initial phase of Ulizas growth. Eventually, a full service allowing mobile phone users without access to the Internet to find answers to their questions and upload their own voice content will be implemented. The cost to the user would be minimal  close to the price of an SMS. Ulizas model could be viable for other languages with a large enough number of speakers. But for the vast majority of the 2,000 languages spoken on the African continent, this isn't the case. But solutions might be coming froma research project led by Preethi Jyothi at the Beckman Institute, where ateam of researchers used a probabilistic method tocrowdsource transcriptions from non-native speakers. Once fine-tuned, probabilistic transcription couldopen up the possibility of ASR for less-represented languages, hopefully at a reasonable cost. ",https://globalvoices.org/2017/03/23/swahili-africas-second-top-language-gets-some-much-needed-recognition/,"Yet services such as automatic speech recognition (ASR) aren't commercially available in this language, denying many users with disabilities ...",These Techies Want Your Phone to Speak to You in Swahili,Swahili African Arabic ASR Swahili Kenya Uliza Swahili Internet Swahili Swahili Kenyan Nairobi Kenya Swahili Uliza Internet Uliza ASR Uliza Grant Bridgman Tufts University Bridgman Swahili Global Voices Bridgman Ulizas Internet SMS Ulizas African Preethi Jyothi Beckman Institute ASR,-1
23,"Noam Brown, a Ph.D. candidate at Carnegie Mellon University, chats with Alan Du, head of Team Dragons. Team Dragons challenged Lengpudashi, a poker bot develop by CMU professor Tuomas Sandholm and Brown to an exhibition in Heads-Up, No-Limit Texas Holdem in Hainan, China that concluded Monday, April 10. Carnegie Mellon University professor Tuomas Sandholm talks to Kai-Fu Lee, head of Sinovation Ventures, a Chinese venture capital firm, while he plays poker against Lengpudashi, a poker bot develop by Sandholm and Ph.D. candidate Noam Brown. Lengpudashi beat an all-star team of Chinese poker players in Heads-Up, No-Limit Texas Hold'em during an exhibition in Hainan, China that concluded Monday, April 10. Carnegie Mellon University professor Tuomas Sandholm watches Jack Xu of Team Dragons play poker against Lengpudashi, a poker bot develop by Sandholm and Ph.D. candidate Noam Brown. Lengpudashi beat an all-star team of Chinese poker players in Heads-Up, No-Limit Texas Hold'em during an exhibition in Hainan, China that concluded Monday, April 10. Yaxi Zhu of Team Dragons plays poker against Lengpudashi, a poker bot develop by Carnegie Mellon University professor Tuomas Sandholm and Ph.D. candidate Noam Brown. Lengpudashi beat an all-star team of Chinese poker players in Heads-Up, No-Limit Texas Hold'em during an exhibition in Hainan, China that concluded Monday, April 10. Sign up for one of our email newsletters. The humans never had a chance. As expected, the latest poker-playing bot powered by an artificial intelligence designed by a duo from Carnegie Mellon University beat a team of some of the best poker players in China. Lengpudashi , the AI developed by Professor Tuomas Sandholm and Noam Brown, a graduate student at CMU, finished five days of Heads-Up, No-Limit Texas Hold'em with nearly $800,000 in chips and walked away with $290,000. No one from Team Dragons finished in the black. ""We all knew the outcome. There was little chance for the humans,"" said Kai-Fu Lee, head of Sinovation Ventures , a Chinese venture capital firm that organized the winner-take-all exhibition match in Hainan, China. A screen capture of the results of the tournament pitting the AI Lengpudashi against the humans on Team Dragons. Lee, himself a CMU alum who worked on speech recognition in Pittsburgh in the 1990s, challenged Lengpudashi to about 30 hands in a celebrity match during the exhibition. ""It was fun watching me get butchered,"" Lee said. Sandholm and Brown said Lengpudashi showed the same superhuman performance as Libratus, an similar artificial intelligence designed by the duo that beat some of the best professional poker players in the world during the Brains Vs. AI challenge in January at Rivers Casino in Pittsburgh. Team Dragons, the computer's competition in China, tried to gain the upper hand by filling out its six-person rosters with three top poker players and three computer scientists. ""They thought they would understand the algorithms and come up with countermeasures,"" Lee said. The computer whizzes tried to get inside the bot's ""head,"" but they couldn't. ""Even though they came in well-prepared, they were not able to effectively exploit any weakness in the AI's strategy,"" Brown wrote. Lengpudashi, or ""cold poker master"" became known as Libratus' Chinese brother throughout the match. Brown said the two were similar with only minor tweaks. For example, Lengpudashi did not make improvements to its play overnight, studying its own strategy and fixing any weaknesses it found as Libratus did throughout the 20-day Brains Vs. AI challenge to stay one step ahead of the humans. Lengpudashi played the same strategy through the competition, Brown wrote. Lee, whose firm targets AI companies in the United States and China, said there were several reasons for inviting Lengpudashi, Sandholm and Brown to China even though there was nearly no chance they would lose. Lee wanted to increase the awareness of artificial intelligence in China. Artificial intelligence underscores most tech conversations in America and big names from Bill Gates to Elon Musk to Stephen Hawking serve as both evangelists and cautionary advisers, Lee said. That conversation about AI isn't happening in China, Lee said. He wanted to build on AI's recent win in Go, a popular game in China, with an exhibition of its possibilities in poker. The poker match was streamed on 30 different websites and attracted more than 50 million viewers, Lee said. In terms of academic research into AI, China is three to five years behind the United States, Lee said. China, however, could adopt the advantages of AI quickly. The country's financial system is outdated, providing an opportunity for it to leapfrog into using artificial intelligence to make decisions. Few people in China use credit cards but many people use their phones to make purchases, Lee said. That's why his firm invested in a start-up firm that uses artificial intelligence to approve small loans, $200 to $300, in a matter of seconds. Centralized control of health data might make it easier to aggregate that data for use in medical applications of artificial intelligence. And the central control of infrastructure might be the framework to pave the way for improvements for autonomous vehicles. Lee also used the poker match to promote his firm. Sinovation Ventures will be starting an artificial intelligence institute, and Lee hopes to attract top minds from around the world to work at it. Lee will be in Pittsburgh this summer and fall to talk with CMU students and faculty about joining him in China. Finally, Lee wanted to show off his alma mater. ""The fact that CMU is bar none the No. 1 AI university in the world is not as well known, especially in Asia, and I want them to know how great we are,"" Lee said. Aaron Aupperlee is a Tribune-Review staff writer. Reach Aupperlee at aaupperlee@tribweb.com or 412-336-8448. ",http://triblive.com/local/allegheny/12179279-74/poker-match-shows-off-pittsburghs-artificial-intelligence-dominance-in-china,"Lee, himself a CMU alum who worked on speech recognition in Pittsburgh in the 1990s, challenged Lengpudashi to about 30 hands in a ...",Poker match shows off Pittsburgh's artificial intelligence dominance ...,Noam Brown Ph.D. Carnegie Mellon University Alan Du Team Dragons Dragons Lengpudashi CMU Tuomas Sandholm Brown Heads-Up No-Limit Texas Holdem Hainan China Monday April Carnegie Mellon University Tuomas Sandholm Kai-Fu Lee Sinovation Ventures Lengpudashi Sandholm Ph.D. Noam Brown Lengpudashi Heads-Up No-Limit Texas Hold'em Hainan China Monday April Carnegie Mellon University Tuomas Sandholm Jack Xu Team Dragons Lengpudashi Sandholm Ph.D. Noam Brown Lengpudashi Heads-Up No-Limit Texas Hold'em Hainan China Monday April Yaxi Zhu Team Dragons Lengpudashi Carnegie Mellon University Tuomas Sandholm Ph.D. Noam Brown Lengpudashi Heads-Up No-Limit Texas Hold'em Hainan China Monday April human Carnegie Mellon University China Lengpudashi AI Professor Tuomas Sandholm Noam Brown CMU Heads-Up No-Limit Texas Hold'em Team Dragons Kai-Fu Lee Sinovation Ventures Hainan China AI Lengpudashi Team Dragons Lee CMU Pittsburgh Lengpudashi Lee Sandholm Brown Lengpudashi Libratus Brains Vs. AI January Rivers Casino Pittsburgh Team Dragons China Lee AI Brown Lengpudashi Libratus Brown Lengpudashi Libratus Brains Vs. AI Lengpudashi Brown Lee United States China Lengpudashi Sandholm Brown China Lee China America Bill Gates Elon Musk Stephen Hawking Lee AI China Lee AI Go China Lee AI China United States Lee China AI China Lee Centralized Lee Sinovation Ventures Lee Lee Pittsburgh CMU China Lee CMU No AI Asia Lee Aaron Aupperlee Reach Aupperlee @,5
24,"By Gulli Arnason in Media and Technology  March 28, 2017 Verint Systems Inc.s (Nasdaq: VRNT) voice self-service and speech recognition solutions from its Customer Engagement Optimization portfolio will be implemented a large municipal transportation authority in the US, the company said. When this transportation authority sought new solutions to support its move from an on-premises to cloud environment, as well as to advance key compliance needsincluding functionality to support the Payment Card Industry Data Security Standard (PCI DSS)it turned to Verint. The authority also wanted to ensure the technology it selected would enable it to enhance security and further the success of its prepaid card program that enables citizens to reload their transit cards through voice self-service. When deployed, the software solutions will be integrated with the authoritys contact center and back-end data systems, and will help deliver timely, high-quality voice self-service to improve citizen experiences, achieve higher completion rate goals, and decrease transfer rates and cost per contact. The authority and citizens it serves also will benefit from the solutions personalization capabilities, which help reduce typical consumer frustration with voice self-service systems. Verint is a global leader in actionable intelligence solutions with a focus on customer engagement optimization, security intelligence, and fraud, risk and compliance. ",http://www.financial-news.co.uk/40436/2017/03/verint-speech-recognition-solutions-to-be-implemented-i/,Verint Systems Inc.´s (Nasdaq: VRNT) voice self-service and speech recognition solutions from its Customer Engagement Optimization portfolio ...,Verint speech recognition solutions to be implemented in MTA,Gulli Arnason Media Technology March Verint Systems Inc.s Nasdaq VRNT Customer Engagement Optimization US Payment Card Industry Data Security Standard PCI DSS Verint Verint,2
25,"Intelligent Virtual Assistant Market report, published by Allied Market Research, forecasts that the global market is expected to garner $3.6 billion by 2020, registering a CAGR of 35.2% during the period 2015-2020. The global intelligent virtual assistant market size is the revenue generated from the summation of speech recognition solutions and text-to-speech solutions. Intelligent Virtual Assistant Market by Verticle (Automotive, Healthcare, and e-commerce) and Technology (Text-to-Speech and Speech Recognition) - Global Opportunity Analysis and Industry Forecast, 2014 - 2020. North America dominates the global intelligent virtual assistant market owing to the increased adoption of IVA in countries such as the U.S. However, Asia-Pacific region is expected to exhibit a faster growth over the forecast period 2015-2020. The technology segment comprises speech recognition and text-to-speech technology. Speech recognition technology segment dominated the market in 2014, and is expected to dominate throughout the forecast period 2015-2020. Intelligent virtual assistant (IVA) is used in various industry verticals such as automotive, healthcare, BFSI, retail and others. Among these verticals, BFSI was the highest revenue-generating segment, accounting for 39.9% share in 2014. IVA solutions are widely used in the BFSI sector, owing to faster response time, improved customer handling and high customer satisfaction. However, the automotive segment would witness the fastest growth during the forecast period. The growth would be driven by the fast growing in-car infotainment systems market, as IVA is an important part of these systems, which enables the vehicle driver and passengers, to surf the internet, navigate and make calls through speech recognition technology. Further, theIVA market based on geography is bifurcated into North America, Europe, Asia-Pacific and LAMEA. North America dominated the global intelligent virtual assistant (IVA) market, accounting for 39.5% market share in 2014. The region would maintain its dominance during the forecast period. However, Asia- Pacific would witness the highest CAGR of 38.97% during the same period. Key Findings of Intelligent Virtual Assistant (IVA) Market: Automotive segment is expected to exhibit huge growth potential throughout the analysis period. In the Intelligent Virtual Assistant market, speech recognition is the most popular technology since it enables interactive and intuitive communication. BFSI industry vertical would exhibit the highest adoption of IVA throughout the forecast period and would continue to dominate the market throughout the analysis period. The report also outlines the competitive scenario of the intelligent virtual assistant (IVA) market, providing a comprehensive study of the key strategies adopted by companies operating in the IVA market. Key companies profiled in the report include Next IT, Nuance Communications Inc., IntelliResponse Systems Inc., CodeBaby Corporation, Anboto Group and others. Company profile: Allied Market Research is a global market research and business consulting wing of Allied Analytics LLP based in Portland, Oregon. Allied Market Research provides global enterprises as well as medium and small businesses with unmatched quality of Market Research Reports and Business Intelligence Solutions. AMR has a targeted view to provide business insights and consulting to assist its clients to make strategic business decisions and achieve sustainable growth in their respective market dom ... ",https://www.whatech.com/market-research/it/285473-global-intelligent-virtual-assistant-market-forecast-by-2020-just-published,The global intelligent virtual assistant market size is the revenue generated from the summation of speech recognition solutions and ...,Global intelligent virtual assistant market forecast by 2020 just ...,Virtual Assistant Market Allied Market Research CAGR Intelligent Virtual Assistant Market Verticle Automotive Healthcare Technology Text-to-Speech Opportunity Analysis Industry Forecast North America IVA U.S IVA BFSI BFSI IVA BFSI IVA Further North America Europe Asia-Pacific LAMEA North America IVA Asia- Pacific Key Findings Intelligent Virtual Assistant IVA Intelligent Virtual Assistant BFSI IVA IVA IVA IT Nuance Communications Inc. IntelliResponse Systems Inc. CodeBaby Corporation Anboto Group Market Research Allied Analytics LLP Portland Oregon Allied Market Research Market Research Reports Business Intelligence Solutions AMR,3
26,"By using this website, you consent to our use of cookies. For more information on cookies see our Cookie Policy . Belfast-based start-up Liopa  is developing software and systems which can read lips  and dramatically improve the accuracy of speech-controlled systems. Remember that time, way back in the early 2000s, when Ozzy Osbourne got angry with his new BMW because its voice-control system couldnt understand him Today, we are all Ozzy  constantly rephrasing and e-nun-ci-ating to try and get Siri, Cortana, Alexa or whoever to understand us. While speech-controlled systems can be okay in a quiet environment, when theyre fighting against the background noise of tyres, engines and other people talking, it all goes to pot. Experts in the field regularly quote a  possibly apocryphal  tale of someone sending a speech-to-text message which incorrectly informed a family member that their mother had arrived, dead, at her destination. Help has arrived in the shape of Liopa (Irish for lips), a Belfast-based start-up, borne out of research at Queens University Belfast. The company is developing software and systems which can read your lips, in a way unaffected by background noise, which can dramatically improve the accuracy of speech-controlled systems. Were setup to commercialise some research that had been done in Queens, around eight to 10 years of research, on what we call viseme, says Liopa chief executive Liam McQuillan, who is a telecoms veteran. That is the way your lips move when you speak. Theyve found that someones lip movements are very speaker-specific, much in the way that your fingerprints are, so the idea was to develop this technique as a way of online user verification. So anywhere you could train a camera on someones face and get them to say something, you could track their lip movement, once theyre pre-enrolled, and you can compare that movement against a challenge phrase or a series of numbers to be read out. The initial work was in turning the lip-reading system into a security protocol, but that plan ran up against two hurdles: the fact that the biometric security market is already crowded, and that the consequences of getting a false positive result could be disastrous. As McQuillan points out: You make a false match and someone can get into your bank account. So for the last nine months we kind of pivoted a little, and started to develop the speech recognition from where it is today, which is digit recognition, through to where its at limited vocabulary or menu control and eventually on to natural speech. So where we see this really playing is as a supporting technology to audio speech recognition. All the big guys are investing heavily in that area  tens of millions into personal assistants, such as Cortana, Siri, Amazon with Alexa, all of which are based on audio speech recognition. We see the car thing as a good initial use case, where we can train a camera on the drivers face. Today its an RGB camera, but were developing an infrared one to take the outside illumination out of the equation. So where theres a lot of background noise, road noise, engine revs, winding down the window, as long as you can see the drivers face and read their lip movements you can combine the two techniques, and our software will improve audio speech recognition platforms. The idea is to combine both the audio and the visual inputs to create a more accurate system, as Fabian Campbell-West, Liopas head of research and development explained: A lot of the audio command-and-control stuff is fairly clear. Where it can be difficult at the moment is when you try and dictate a number, for instance. So what were focusing on right now are the commands that control the in-car environment, and were confident we can get very good accuracy with those. What the system cant do well, yet, is deal with people using free speech or speaking colloquially. Right now, the Liopa system has to first be trained into your lip movements, and then will require you to stick to a specific set of vocabulary and grammar. Where everyone wants to get to is free speech, which were not at yet, says Richard McConnell, Liopas chief operating officer and technical officer. You create whats called a model, so you train the system to your lip movements. Then theres the fine vocabulary and grammar model. Free speech is definitely on our road map though. The key thing for us is that in a noisy environment were getting 70-80 per cent accuracy at a point where the audio is almost unusable. The research here has identified that theres about 30 specific visemes, and the more combinations of those that you capture the better. There are two aspects to the system that will be exceptionally tempting to the car makers who may become Liopas customers. First, the technology needs only a decent camera which can see the drivers face, and a bit of software, all of which McConnell says can be readily accommodated by existing onboard systems: At the minute it just needs an ordinary camera, and the computing power required we think is quite small. Theres enough compute power in a car already to deal with what we need. And when you add things like gesture control, it all helps. The granularity of image that were looking at only needs to be 16x16 pixels, but we can add accuracy with infrared cameras and depth sensors, a bit like the Kinect system in a gaming console. But generally theres no special hardware required. We are moving towards using AI and neural network-type systems; but, again, many cars have systems which can already cope with that. Secondly, theres a chance for car makers, and their software and interface designers, to catch up with the likes of Apple and Googles Android. With the introduction of Apple CarPlay and Android Auto , the two smartphone software giants are increasingly taking over the space within the car, and many vehicle manufacturers are known to be concerned over the transfer of user data and experience. Developing a superior voice-control system could be a useful hook in getting drivers to switch out of CarPlay and back to the proprietary vehicle software. Weve met with a number of car companies, and Toyota was especially keen on it, says McQuillan. So a couple of months ago we closed a pre-seed round, which takes us up to the summer and allows us to develop an in-car system, and then well go out on a proper funding round, and well be talking to the car manufacturers. Given the enormous concerns over driver distraction and accidents caused by phone use behind the wheel, anything that can make a voice-controlled system easier and more accurate to use, and which might keep drivers away from their keypads, will surely snag the interest of car makers. And possibly a few long-haired rock gods too. ",http://www.irishtimes.com/business/innovation/read-my-lips-irish-start-up-gets-teeth-into-verification-tools-1.3032289,"So for the last nine months we kind of pivoted a little, and started to develop the speech recognition from where it is today, which is digit ...",Read my lips: Irish start-up gets teeth into verification tools,Policy Liopa Ozzy Osbourne BMW Today Ozzy Siri Cortana Alexa Help Liopa Irish Queens University Belfast Were Queens Liopa Liam McQuillan Theyve McQuillan Cortana Siri Amazon Alexa Today RGB Campbell-West Liopas Liopa Richard McConnell Liopas Liopas First McConnell Kinect AI Apple Googles Android Apple CarPlay Android Auto CarPlay Weve Toyota McQuillan,4
27,"Tickets on sale for the GeekWire Awards! Amazon Web Services just unveiled a new service for running callcenters, dubbed Amazon Connect , leveragingthe same technology used by Amazon.coms own customer service systemto route and manage calls using automatic speech recognition and artificial intelligence. The announcement is the latest move by the cloud giant beyond its core infrastructure technologies and into higher-level cloud services. Amazon saysthe service incorporates its Lex technology, an artificial intelligence service for speech recognition and natural language processing, which also powers the companys Alexa virtual assistant. The company says Amazon Connect works with existingAWS services such as DynamoDB, Amazon Redshift, or Amazon Aurora, as well as third-party CRM and analytics services. Salesforce says its integrating its Service Cloud Einstein with Amazon Connect. It uses a graphical interface to let companies set up a workflow for calls without coding. Amazon Connectuses a traditional cloud pay-as-you go pricing model, charging companies depending on their usage, rather than an up-front licensing fee. AWSchief evangelist Jeff Barr does a deep dive on the technology in this post , and AWS GM Matt Wood provides an overview in this video, noting that the service will ultimately expand beyond the U.S. and Europe to other markets around the world. The call-center technologymarket includes many existing players such as Seattle-based Spoken , as well as Zendesk , Cisco Systems , Avaya , Genesys and many others. Ten years ago, we made the decision to build our own customer contact center technology from scratch because legacy solutions did not provide the scale, cost structure, and features we needed to deliver excellent customer service for our customers around the world, said Tom Weiland, Amazons vice president of worldwide customer service, in a news release . This choice has been a differentiator for us, as it is used today by our agents around the world in the millions of interactions they have with our customers.Were excited to offer this technology to customers as an AWS service  with all of the simplicity, flexibility, reliability, and cost-effectiveness of the cloud. Amazons announcement confirms a report by The Information news site last month. Theres code behind the cloud: lots of it. Running 247, data-intensive, distributed services at scale requires some serious engineering. Follow Salesforce Architect Ian Varley as he traces the solutions to those architectural puzzles in his series, The Architecture Files on our Salesforce Engineering Medium blog . ",http://www.geekwire.com/2017/amazon-web-services-jumps-call-center-market-new-amazon-connect-service/,"Amazon says the service incorporates its Lex technology, an artificial intelligence service for speech recognition and natural language ...",Amazon Web Services jumps into call-center market with new ...,GeekWire Awards Amazon Web Services Amazon Connect Amazon.coms Amazon Lex Alexa Amazon Connect DynamoDB Amazon Redshift Amazon Aurora CRM Salesforce Service Cloud Einstein Amazon Connect Amazon AWSchief Jeff Barr AWS GM Matt Wood U.S. Europe Spoken Zendesk Cisco Systems Avaya Genesys Tom Weiland Amazons AWS Amazons Information Follow Salesforce Architect Ian Varley Architecture Files Salesforce Engineering Medium,6
28,"LONDON, April 10, 2017 /PRNewswire/ -- KEY DYNAMICS: Software ""robotic"" applications recently picked up lot of media attention which acts like excel macros and replicates human employee's mundane tasks/actions is transforming the enterprises internal processes and IT functions. RPA tools has witnessed an evolution in terms of capabilities, value delivery, deployment models and impact on global mindset. Organizations dealing with global pressure to increase their revenues, transform their business models and reduce operational costs are leveraging RPA solutions and services in conjunction with machine learning, cognitive computing, artificial intelligence, natural language processing, speech recognition, vision technology, big data analytics, and many more technologies. Integration of RPA capabilities with these above mentioned technologies broadens the application areas and support companies to become agile in their digital journey. However, this is creating challenge for human workers to be replaced and force them to think hard on re-evaluating their existing skill base and reskill to stay relevant in the market. Industry is changing a rapid pace and organizations cannot stay untouched, else fall behind the competition, and needs to evaluate these emerging technologies and employ them. Executive leaders and business unit leaders, procurement managers, advisors, Investors who have responsibilities to set their organization on Digital transformation and Automation journey. ",http://www.prnewswire.com/news-releases/competitivescope-view-of-robotic-process-automation-rpa-market-300437463.html,"... artificial intelligence, natural language processing, speech recognition, vision technology, big data analytics, and many more technologies.",CompetitiveSCOPE View of Robotic Process Automation (RPA) Market,LONDON April KEY DYNAMICS Software IT RPA RPA RPA Digital Automation,-1
29,"Now is the time when we see the raindrops of artificial intelligence and AR/VR startups filling up the pool of thestartup investments. While tech giants and investors are trying to scoop up the best AI, AR, and VR startups,how do we know who are good and who are not Richard Wang, a partner at DFJ DragonFund suggests looking into companies that are applying AI algorithms to verticals of these companies. Over 550 startups using AI as a core part of their products raised US$ 5 billion in funding in 2016 according to CB Insights, and AR/VR startup investments hit US$ 2 billion in one year, according to Digi-Capitals data published in 2016 Q2. In artificial intelligence, we are interested in companies with AI algorithms applied to verticals or enterprise applications,Richard told TechNode. We are interested in the verticals such as financial, medical, industrial sector. As an investment expert in artificial intelligence, fintech, and smart hardware, Richard Wang was a co-founder of OLEA Network and had 20 years of experience in the semiconductor industry before joining DFJ DragonFund in 2011. Richard Wang, partner of DFJ DragonFund (first on the left), Tim Draper, founding partner of DFJ (fourth from the left) (Image Credit: DFJ Dragon Fund) An easy way to think about AI is when you think of your five senses. Outstanding AI startups have better sensors for Input and Output, Richard says. He mentioned two startups as examples. AiSense is a speech recognition and deep learning company that helps improve enterprise productivity by conducting data fusion of human to human conversation. When two people are having aconversation, AiSense will write down a meeting note, and let you know where, when, with whom, what did they say talk about. WestWelllabs is another AI company that uses aneural network for enterprises. We are looking for a company doing neural network computer chips that will make robots image recognition much faster, more accurate, and localized, meaning it doesnt have to go to the cloud to process the data, Richard says. AR and VR companies withverticals in education, logistics, and manufacturing have higher chances of attracting this fund. Ideal AR glasses should have two things. Its hardware should have dual lenses, should use optical technology and it should be light. Its software should have a clear cutbackendand provide a decentAPI or SDK (software development kit) to bring in third parties, Richard remarks. Chinas AR technology is not so different from that of the US now. As for VR, the hardwarestill has room to be improved. For example, UltraVision  () is AR glasses for B2B companies and SculptrVR provides the platform to let users create their own VR social world. Users are able to create their own virtual world to interact with others, just like movie Matrix. Richard Wang at a Startup Grind event in Shanghai (Image Credit: Startup Grind Shanghai) Headquartered in Silicon Valley, DFJ DragonFund manages USD and RMB funds, with a total investment of more than US$ 300 million in China and nearly 100 investment projects involving TMT, healthcare, and clean energy. There are 4 partners in Shanghai, and 2 partners in Silicon Valley, he told us. When we invest in Chinese companies, we benchmark Silicon Valley companies, and find the counterpart in China. USD funds are focused earlier-stage companies while RMB fund is focused on the later stage, who have revenue of several RMB million. In addition, 80% of the RMB fund goes to seriesA while only 20% goes into the angel round. The DFJ DragonFund is a member of the Draper Venture Network, founded by Tim Draper, with companies and offices in more than 30 cities around the world. Funds are more than US$ 7 billion, invested into more than 600 projects, and put into more than 20 unicorn projects. DFJ Dragon invests in green tech, medical, semiconductor. For our USD fund, we invest in a lot of AI in vertical applications. We have 30, 40 portfolios in VR and AR companies, and self-driving technology dealsin the Bay area. We focus a lot onnew energy car, too, Richard noted at Startup Grind event in Shanghai. There is ahuge opportunity in China, especially in enterprise software, AI, healthcare, such as genetics and hospital management. ",http://technode.com/2017/04/11/ai-vr-ar-startups-go-vertical-richard-wang/,AiSense is a speech recognition and deep learning company that helps improve enterprise productivity by conducting data fusion of human to ...,Richard Wang: AI and AR/VR startups should go vertical,AR/VR AR VR Richard Wang DFJ DragonFund AI AI US CB Insights AR/VR US Digi-Capitals Q2 AI Richard TechNode Richard Wang OLEA Network DFJ DragonFund Richard Wang DFJ DragonFund Tim Draper DFJ DFJ Dragon Fund AI AI Input Output Richard AiSense AiSense WestWelllabs AI Richard AR VR Ideal AR SDK Richard Chinas AR US VR UltraVision B2B SculptrVR VR Matrix Richard Wang Startup Grind Shanghai Image Credit Startup Grind Shanghai Silicon Valley DFJ DragonFund USD RMB US China TMT Shanghai Silicon Valley Silicon Valley China RMB RMB RMB DFJ DragonFund Draper Venture Network Tim Draper US DFJ Dragon USD AI VR AR Bay Richard Startup Grind Shanghai China AI,4
30,"Translator and Skype can even translate for you on the fly. senpai."" The app can now turn your spoken words into Nihongo to help you get around the country. Translator can recognize a bevy of languages, but Japanese is only the 10th language its speech translation feature supports. That's right -- it now reads the resulting Japanese words or phrases out loud to make it possible to hold almost real-time conversations with native speakers. The other nine languages in the list are Arabic, Chinese, English, French, German, Italian, Portuguese, Russian and Spanish. The technology's end-to-end speech translation capability works by using two neural-network based AIs. Its Automatic Speech Recognition AI detects your words, then its natural language processing technology gets rid of all the fillers like ""um"" and ""uh."" After the machine translation AI is done conjuring up a result, the app's speech synthesizer reads it out loud on the fly. Microsoft's Translator app is available for Android , iOS and Amazon Fire devices, though that's not the only way you can access the tech's Japanese speech translation feature. It's now live on the translation solution's website , as well as on Skype's real-time translation tool . ",https://www.engadget.com/2017/04/07/microsoft-translator-spoken-japanese/,The technology's end-to-end speech translation capability works by using two neural-network based AIs. Its Automatic Speech Recognition AI ...,Microsoft Translator turns your words into spoken Japanese,Skype Nihongo Arabic Chinese English French German Italian Portuguese Russian Spanish AIs AI AI Microsoft Translator Android Amazon Fire Skype,-1
31,"Global Speech Recognition Industry 2016 Market Research Report Purchase This Report by calling ResearchnReports.com at +1-888-631-6977. Global Speech Recognition Market had made research and come up with a report that focuses on the major players in the Global Speech Recognition Market throughout the world. The report includes information like company profiles, specification and product picture, production, capacity, contact information, cost and revenue. Likewise, equipment and upstream raw materials as well as downstream demand analysis is also tackled. The report investigates and analyzes the Global Speech Recognition Market and shows a comprehensive evaluation of the evaluation and its specifications. Another aspect that was taken is the cost analysis of the main products dominant in the global ice cream industry considering the profit margin for the manufacturers. Click here to Download Sample PDF illustration: www.researchnreports.com/request_sample.phpid=49467 Through this report, the core driving factors of the Global Speech Recognition Market were identified and the business partners and end-users were also elaborated. The business sector structure, business patterns and challenges affecting the market globally were also included in the extensive analysis for this research report. Various interviews and talks were held with the prominent leaders in the industry in order to obtain reliable and updated information pertaining to the market. The report firstly introduced the Speech Recognition basics: definitions, classifications, applications and industry chain overview; industry policies and plans; product specifications; manufacturing processes; cost structures and so on. Then it analyzed the world's main region market conditions, including the product price, profit, capacity, production, capacity utilization, supply, demand and industry growth rate etc. In the end, the report introduced new project SWOT analysis, investment feasibility analysis, and investment return analysis. For more inquiry before purchase: www.researchnreports.com/enquiry_before_buying.phpid=49467 2 Global Speech Recognition Competition by Manufacturers, Type and Application 3 USA Speech Recognition (Volume, Value and Sales Price) 4 China Speech Recognition (Volume, Value and Sales Price) 5 Europe Speech Recognition (Volume, Value and Sales Price) 6 Japan Speech Recognition (Volume, Value and Sales Price) 7 India Speech Recognition (Volume, Value and Sales Price) 8 Southeast Asia Speech Recognition (Volume, Value and Sales Price) Research N Reports is a new age market research firm where we focus on providing information that can be effectively applied. Today being a consumer driven market, companies require information to deal with the complex and dynamic world of choices. Where relying on a sound board firm for your decisions becomes crucial. Research N Reports specializes in industry analysis, market forecasts and as a result getting quality reports covering all verticals, whether be it gaining perspective on current market conditions or being ahead in the cut throat global competition. Since we excel at business research to help businesses grow, we also offer consulting as an extended arm to our services which only helps us gain more insight into current trends and problems. Consequently we keep evolving as an all-rounder provider of viable information under one roof. ",http://www.openpr.com/news/477053/What-Is-The-Future-Of-Global-Speech-Recognition-Market-In-Global-Industry-Find-Out-Its-Uses-Benefits-etc.html,Global Speech Recognition Industry 2016 Market Research Report Purchase This Report by calling ResearchnReports.com at ...,What Is The Future Of “Global Speech Recognition Market” In Global ...,Market Research Report Purchase Report ResearchnReports.com +1-888-631-6977 Global Market Global Market Likewise Click Download Sample PDF SWOT Global Manufacturers Type Application USA Value Price China Value Price Europe Value Price Japan Value Price India Value Price Southeast Asia Value Price Research N Reports Research N Reports,3
32,"New Philips SpeechExec Pro 10 Turns Voice to Text Business Wire Wednesday, March 15, 2017 Speech Processing Solutions, the global leader in professional dictation and voice solutions, today launched a new version of its SpeechExec Pro dictation and transcription workflow software. New Philips SpeechExec Pro 10 connects authors and transcriptionists for easier, more efficient and professional document creation. The updated software integrates Nuance Dragon Professional speech recognition, combining two market leading professional dictation solutions into one easy-to-use workflow. Within the Philips dictation workflow, the software turns voice into text without the need for typing, making document creation one simple process. This Smart News Release features multimedia. View the full release here: http://www.businesswire.com/news/home/20170315005122/en/ This is the first time we have integrated Nuance Dragon Professional speech recognition with the Philips SpeechExec dictation workflow solution, explains Dr. Thomas Brauner, CEO of Speech Processing Solutions. This combined solution turns speech into a text document instantly  guaranteeing the Philips quality that customers have come to expect and backed by the power of Dragon, resulting in greater documentation productivity and reduced transcription costs for our users, he adds. Nuances Dragon Professional speech recognition solution integrated with Philips SpeechExec Pro 10 is easily set up in just one installation and controlled within the intuitive Philips user interface. Customers can choose one of multiple languages, including English, French, German, Italian, Dutch and Spanish, to turn their voice instantly into text. SpeechExec Pro 10 learns the words and phrases used over time, while also adapting to the way each user speaks, driving higher dictation accuracy and greater productivity,, says Dr. Brauner. And every use of the solution contributes to even more accurate speech recognition results, he adds. In addition to using speech recognition on your desktop computer, dictations and voice files recorded on the go with the Philips dictation recorder app on your smartphone can be automatically run through the speech recognition system. With one click, the text documents are synced to and visible in the recorder app on your smartphone in either .doc or .rtf formats. With optional Philips SpeechLive, users can also take advantage of the Philips transcription service, which works with trained professionals to type documents. This virtual assistant is convenient for lawyers and professionals in many industries transcribing complex documents. Legal professionals who wish to expand support for legal vocabularies can use Philips SpeechExec Pro 10 with Dragon Legal, providing advanced functionalities for increased flexibility. Authors can also use their unique voice profile on more than one workstation. Our users can work proficiently from anywhere  at home or on the go  leveraging their trained vocabulary to always ensure accurate speech recognition results. This improves and simplifies the creation of text documents, saving time and keeping costs low, explains Dr. Thomas Brauner. Philips SpeechExec Pro 10 is one of the few speech recognition solutions in the market that can be tested for free for 30 days. Certified Philips Partners can help throughout the trial period and provide support during the installation and training, and can address any other question users may have. Philips will also provide special upgrade offers for users of older dictation systems who are looking to renew their voice technology set up. For more information, visit: www.philips.com/dictation Speech Processing Solutions is the global leader in professional dictation solutions. The company was founded in 1954 in Austria as a Philips subsidiary, and has been a driving force for innovative speech-to-text solutions for 60 years. The company developed ground-breaking products such as the mobile Philips SpeechAir , the Philips Pocket Memo voice recorder , the Philips SpeechMike Premium USB dictation microphone and the Philips Dictation Recorder app for smartphones, thus meeting its demands for excellence and superior quality. Thanks to the innovation, Philips SpeechLive , dictations and recordings will become faster and easier than ever before with cloud-based workflow services. Speech Processing Solution's perfectly tailored offers and products help professionals save time and resources and maximize efficiency. Connect with Speech Processing Solutions on: ",http://finance.yahoo.com/news/philips-speechexec-pro-10-turns-080200066.html,"The updated software integrates Nuance Dragon Professional speech recognition, combining two market leading professional dictation ...",New Philips SpeechExec Pro 10 Turns Voice to Text,New Philips SpeechExec Pro Turns Voice Text Business Wire Wednesday March Speech Processing Solutions SpeechExec Pro New Philips SpeechExec Pro Nuance Dragon Professional Philips News Release Nuance Dragon Professional Philips SpeechExec Dr. Thomas Brauner CEO Speech Processing Solutions Philips Dragon Dragon Professional Philips SpeechExec Pro Philips Dutch Spanish SpeechExec Pro Dr. Brauner Philips .doc Philips SpeechLive Philips Philips SpeechExec Pro Dragon Legal Dr. Thomas Brauner Philips SpeechExec Pro Certified Philips Partners Philips Speech Processing Solutions Austria Philips Philips SpeechAir Philips Pocket Memo Philips SpeechMike Premium USB Philips Dictation Recorder Philips SpeechLive Speech Processing Solution Speech Processing,5
33,"26 Mar 2017 at 18:02, Richard Chirgwin Getting an AI to understand speech is already a tough nut to crack. A group of Australian researchers wants to take on something much harder: teaching once-deaf babies to talk. Think about what happens when you talk to Siri or Cortana or Google on a phone: the speech recognition system has to distinguish your OK Google (for example) from background noise; it has to react to OK Google rather than OK something else; and it has to parse your speech to act on the command. And you already know how to talk. The Swinburne University team working on an app called GetTalking can't make even that single assumption, because they're trying to solve a different problem. When a baby receives a cochlear implant to take over the work of their malfunctioning inner ear, he or she needs to learn something brand new: how to associate the sounds they can now hear with the sounds their own mouths make. Getting those kids started in the world of conversation is a matter of habilitation  no rehabilitation here, because there isn't a capability to recover. GetTalking is the brainchild of Swinburne senior lecturer Belinda Barnet, and the genesis of the idea was her own experience as mother to a child with a cochlear implant. Children interact well with apps. Can one As she explained to The Register: With my own daughter  she had an implant at 11 months old  I could afford to take a year off to teach her to talk. This involves lots of repetitive exercises. That time and attention, she explained, is the big predictor of success. In the roughly 10 years since it became standard practice to provide implants to babies at or before 12 months of age (fully funded by Australia's national health insurance scheme Medicare since 2011), 80 per cent of recipients achieve speech within the normal range. What defines the 20 per cent that don't get to that point Inability, either because of family income or distance from the city, to spend a year sitting on the carpet with flash-cards. That makes it hard for parents in rural or regional locations, regional, or low-income mothers, Barnet said. The idea for which Barnet and associate professor Rachael McDonald sought funding looks simple: an app to run on something like an iPad that gives the baby a bright visual reward for speaking. However, it does test the boundaries of AI and speech recognition, because of a very difficult starting point: how can an app respond to speech when the baby has never learned to speak It's possible that not everything the GetTalking team needs has to be written from scratch. For example, while their speech recognition might be ground-up; both Barnet and Sterling said the team is looking at how a long-standing project, LENA, could lighten the development load. The LENA Project has its own focus: measuring a child's early language development from birth to 48 months old. Some of its components, however, look tantalising: speech recognition and analysis directed towards GetTalking's target age group. Apple never revealed the price it paid to acquire the team that developed Siri, but rumours of US$150 million don't sound unreasonable  and Siri takes its input from someone who knows how to speak. For all the effort that's gone into speech recognition and AI, we also know it remains so difficult it's been automated for only a couple of per cent of languages. Leon Sterling, a Swinburne computer science researcher, had his interest piqued as a member of the university panel assessing the project, and is helping bring a long experience of AI research to the project. He explained the hidden complexities behind what needs to present itself as a simple app. You've got to get the signal, you have to extract the signals, separate them from the background noise, the parents speaking, et cetera. Most of those problems have precedent, but GetTalking needs yet more machine learning  like trying to measure the child's engagement with the app. You've got to look at the ability to observe, to tag video strings together with audio strings. The team understands that an app can't replace a speech therapist or parent, but only support them  and that adds new complexities like building in the knowledge of how children interact with physiotherapists. You need to understand the developmental stages of children when they're interacting with the app. ",https://www.theregister.co.uk/2017/03/26/speech_recognition_for_kids_with_cochlear_implants/,Think about what happens when you talk to Siri or Cortana or Google on a phone: the speech recognition system has to distinguish your OK ...,Researcher hopes to teach infants with cochlear implants to speak ...,Richard Chirgwin AI Siri Cortana Google Google Google Swinburne University GetTalking GetTalking Swinburne Belinda Barnet Children Australia Medicare Inability Barnet Barnet Rachael McDonald AI GetTalking Barnet Sterling LENA LENA Project GetTalking Apple Siri US Siri AI Leon Sterling Swinburne AI,-1
34,"Global Automotive Voice and Speech Recognition System Industry Research Report from 2016 to 2021: This study is helpful for market players to determine competitive landscape and growth prospects. An extensive analysis of the global industry is provided for the historic period, 20112016 and estimations are made for the forecast period, 20162021.  Market to grow at a CAGR of 4.31% during the period 2016-2020 Big Market Research analysts forecast the Global Automotive Voice and Speech Recognition System Market to grow at a CAGR of 4.31% during the period 2016-2020. The VR systems, which we know today, featuring advanced speech recognition, emerged in the automotive domain in 2004-2005. One of the first vehicles to release it was Honda Acura RL, wherein the system was a part of the standard equipment. Honda also offered the VR system in its Acura MDX and Odyssey models as optional equipment. The Honda VR system was based on the IBM-embedded ViaVoice software, which was used to jointly develop the system by the two companies. The system was designed to access the majority of the in-car navigation system with voice inputs by the user. There were more than 700 commands and millions of city and street names, which were built into the system. However, this was not the first VR system; earlier, Mercedes-Benz had come up with a rudimentary system (used only to operate the on-board telephone) back in 1996. It did not take off during those early years, mostly due to the limited nature of consumer exposure (Mercedes only offered it in the S and CL Classes), as well as because the technology was still in its infancy. The report covers the present scenario and the growth prospects of the global automotive VR system market for 2016-2020. To calculate the market size, Technavio considers the revenue generated from the total consumption of automotive VR system market globally. The report does not include revenue generated from the aftermarket service of the product. Big Market Research report, global automotive VR system market 2016-2020, has been prepared based on an in-depth market analysis with inputs from industry experts. The report covers the market landscape and its growth prospects over the coming years. The report also includes a discussion of the key vendors operating in this market. ",https://www.whatech.com/market-research/transport/283287-learn-details-of-the-global-automotive-voice-and-speech-recognition-system-market,Global Automotive Voice and Speech Recognition System Industry Research Report from 2016 to 2021: This study is helpful for market players ...,Learn details of the global automotive voice and speech recognition ...,Automotive Voice System Industry Research Report CAGR Research Global Automotive Voice System Market CAGR VR Honda Acura RL Honda VR Acura MDX Odyssey Honda VR ViaVoice VR Mercedes-Benz Mercedes S CL Classes VR Technavio VR Market Research VR,3
35,"Speech, gesture and mobile devices are among the tools that AV pros can use to provide more intuitive control system interfaces and experiences. But as Tim Kridel explains, each one has as much peril as promise. How does thing work Its the last question a control system vendor or its reseller wants to hear, but its often the first. Speech and gesture recognition are among a host of new and emerging technologies available to make the control user interface and experience (UI and UX) more intuitive. The catch is that those technologies also have the potential to make the UI and UX even more bewildering. So for vendors and integrators that can get them right, those technologies are new market-differentiation opportunities. For example, when an AV system doesnt prompt users to call support half the time, or it doesnt require a tech in every meeting to run things, the operating expenses are much lower. Those savings could sway a customer to choose that system over rival products, especially for clients whose dozens or hundreds of meeting rooms compound the bottom-line benefit. Speech is a definite trend in control, says Eric Olson, business development manager for Almo Professional A/V, a US-based distributor. But more generally, its intelligent and invisible automation. Customers want a series of events to happen without the need for user input at all. For instance, reserving a conference room for a video call would mean the room system would be powered up, lights adjusted, connection made, etc. The meeting can start immediately without the need to touch a screen or even speak to the system. Speech recognition is a catchall term that covers a wide variety of applications, including voice-controlled virtual assistants such as Apples Siri. Another is voice biometrics, which some banks have added to their interactive voice response (IVR) telephone systems to authenticate callers just by their voice, thus eliminating the hassle of passwords and Pins. In pro AV and elsewhere, speech recognition is an attractive UI option largely because virtually everyone has a voice and already knows how to use it. Indeed, controls use of it isnt new. There were people taking an AMX touchpanel and embedding their own voice control five to seven years ago, says Shaun Robinson, Harman Professional Solutions director of CEG solutions and marketing. Those hacks are now being replaced by vendor-provided solutions, some for the enterprise market and others for the residential space. Two respective examples are Harmans partnership with IBM Watson and Crestron home systems that now can be controlled using Amazons Alexa. Harman/AMX is using artificial intelligence through IBMs Watson to control room settings via voice commands, says Almos Olson. There is a plan to implement this in the hospitality vertical first to make simple changes to ones hotel room like climate control, shading, display functions, lighting, etc. Why the sudden surge of pro AV solutions One reason is because virtual assistants such as Amazon Echo, Google Home and Apples Siri are steadily making more people comfortable with the concept of using their voice to control devices. Its yet another example of how peoples experiences as consumers set their expectations for whats possible and preferable at work. Speech will definitely move into meeting and collaboration places, says Michael Jarl Christensen, Neets CEO and co-owner. But looking back on previous tech trends, its driven by the consumer market being first-mover, and once [the] technology is accepted, then it moves in different solutions into the commercial market. Another reason is why speech-controlled consumer devices are suddenly so common: because speech-recognition technology has become sophisticated enough to provide a consistently good user experience, even when a person has a cold or a thick accent. For example, speech vendors that target the IVR market frequently say their platforms can understand well over 90% of what callers say. That high accuracy makes pro AV vendors more willing to embed speech recognition in their control and other products. Now its to the point that the reliability is there to be able to offer a solution that your brand can stand behind, Robinson says. Were very, very focused on speech recognition and voice control. We see that as a key way for delivering an intuitive experience. In business IVRs, todays speech recognition technology enables callers to use everyday terms instead of only industry jargona capability often referred to as natural language understanding (NLU). In control, NLU could mean the system understands, for example, to lower the shades when a person instead says to drop them.  Meanwhile, the voice biometrics used in IVRs could be adapted to control to recognise and authenticate meeting participants. Pair that with NLU, and one potential scenario is a presenter simply saying, Start my video, and the system knowing which one to pull down from the cloud because its already verified her identity.  A call to customer support typically involves just one person, while a household device such as Amazon Echo deals with maybe three or four family members. A business meeting typically has a much larger number of people, which means the control system needs to be sophisticated enough not only to discern between each presenter, but also understand whose commands to follow at a particular moment.  To meet that requirement, pro AV could leverage speech technologies used in other verticals. For example, some speech-to-text platforms now can distinguish between multiple participants on a conference call for accurate attribution in the transcript. A control vendor could license the underlying technology to help its system identify each meeting participant. Or it could expand its use of voice biometrics for distinguishing between people in a room. Either way, theres no shortage of speech technology vendors looking for licensing and partnership opportunities in pro AV.  All of us have been approached by a lot of those companies to consider their services or technologies, says Rainer Stiehl, Extron Electronics vice president of marketing for Europe. What does that gesture mean Besides a mouth, most meeting participants have hands and arms, too. That installed base is why some control vendors are exploring gesture UIs. However, gesture is trickier to design and use.  If there are many LCD displays that look similar in a conference room, and Im not sure which ones to touch, how do I know which gestures are supported on each Stiehl says. Gesture also is an example of how technologies that are viable in the consumer world arent always practical at work, too. Its one thing to wave that new TV remote to see which movement changes channels, or poke and pinch on a new phone. Its another to do that in the heat of an important presentation. On my personal device, I have plenty of time to experiment, Stiehl says. On a public device, how much time do I have between my meeting start and my presentation to figure out whether I use a gesture to swipe for volume or an up-down button A similar example is tablets. When the iPad debuted in April 2010, many people immediately speculated that it would decimate the market for touchpanels. If many companies provided their employees with iPads, the reasoning went, then they could just as easily control AV gear, lighting and other systems from those devices. We found that tablets became great augmenters to solutions that already had a dedicated touchpanel, Robinson says. We have installations where they use only iPads. Its really a customer choice. But we havent seen a lot of degradation in touchpanel sales, thats for sure. People still value a dedicated device that they know will work. Tablets, as well as smartphones, could play another type of role going forward. In EMEA and the rest of the world, theres a demographic shift underway where millennials and other younger people are starting to outnumber older workers. Different demographics have different comfort levels with technology, which is one reason why some clients still request simple, pushbutton-style control UIs. We see that primarily in the education space, Robinson says. I think it has a lot to do with the workforce demographic, where maybe some people prefer the simplicity of a button push. Almo also sees healthy demand for pushbutton UIs. Its most often because of lower cost, Olson says. In the Mrsk Tower, a university project covered by InAVate in the Jan/Feb 2017 edition of the magazine, Neets pushbutton control panels were deployed in smaller classrooms where the complexity of the equipment didnt warrant a more sophisticated system and teaching staff favoured the straightforward operation. In the more technology rich spaces Crestron control panels were provided. Despite the growing number of mobile devices, we still experience growth in keypad control systems and solutions, Christensen says. Its simply easy and intuitive for users, and thats exactly what we focus on. We believe in making life easy for presenters, whether thats by physical keypad buttons equal to a fixed remote control, touchpanel-based solutions or even automatic detecting display controllers. The demographic shift likely means pushbutton systems will become even less common, although some vendors say they will never entirely disappear from the market. Were getting more requests now [where clients say] Yes, I know it would be cheaper to put a keypad here, but we want a touch interface because our users are demanding and expecting that because they grew up with mobile devices, Robinson says. Are keypads and pushbutton interfaces going away No, theres definitely going to be tons of applications for those. That means until all of the older demographics have retired, many companies will need control systems capable of meeting the needs and wants of all employees. There are at least two ways that tablets and smartphones could strike that balance. Younger workers could download control apps to their mobile devices, thus meeting their preference for tile and touch UIs, while older workers could use pushbutton panels. Or the control system could use technologies such as Bluetooth, Wi-Fi or nearfield communications (NFC) to connect to each employees mobile device as she enters a meeting room so it knows which type of interface she prefers. Then as each participant goes to present, the system then would know to configure the rooms touchpanel with a tile, touch or even gesture UI, or a pushbutton one, albeit with virtual rather than physical keys. For employees who are so low tech that they dont carry a smartphone or tablet, the latter scenario could get their preference from the electronic ID badge they probably already wear. For control vendors and their integrator partners, one challenge with leveraging smartphones and tablets is that those devices are constantly changing. For example, their operating system (OS) versionsand thus capabilitieschange dramatically in just a couple of years, so control systems will have to be updated to support the new OS. But some AV pros believe that these and other challenges wont prevent mobile devices from playing an increasingly bigger role in control. Its likely that this function, which exists today, will become more common and in higher demand, says Almos Olson. Future-proofing any aspect of a system is always a challenge for integrators and can rarely be 100% ensured. However, the risk of later incompatibility can definitely be mitigated by designing around standard protocols like Bluetooth and NFC rather than the proprietary technologies like AirPlay. ",http://www.inavateonthenet.net/features/article/outlook-for-user-interface-and-experience,Speech and gesture recognition are among a host of new and emerging technologies available to make the control user interface and ...,Outlook for user interface and experience,Speech AV Tim Kridel UI UX UI UX AV Speech Eric Olson Almo Professional A/V Customers Apples Siri IVR Pins AV UI AMX Shaun Robinson Harman Professional Solutions CEG Harmans IBM Watson Crestron Amazons Alexa Harman/AMX IBMs Watson Almos Olson AV Amazon Echo Google Home Apples Siri Speech Michael Jarl Christensen Neets CEO ] IVR AV Robinson Were IVRs NLU NLU IVRs NLU Start Amazon Echo AV AV Rainer Stiehl Extron Electronics Europe UIs LCD Im Stiehl Gesture Stiehl A April AV Robinson EMEA UIs Robinson Almo UIs Olson Mrsk Tower InAVate Jan/Feb Neets Crestron Christensen Yes Robinson Are No UIs Bluetooth Wi-Fi NFC UI ID OS OS AV Almos Olson Future-proofing Bluetooth NFC AirPlay,-1
36,"/PRNewswire/ Certified eSupport, Corp., one of North America's premiere technical services organizations for industry leaders in cloud and enterprise speech solutions, has announced the launch of their new virtual training portal, CES University . This customer-focused knowledge center will serve as a professional resource and training hub for dictation, transcription, and speech recognition products. CES University will greatly enhance service to the healthcare, legal, and corporate organizations that utilize the solutions offered by Certified eSupport. To further support its continued growth, Certified eSupport is also expanding its team. To accommodate the addition of both sales and technical staff, the organization has relocated to 2305 Donley Drive, Suite 104, in Austin, TX, as of April 1, this year. The expansion has been a long-time dream for Joshua Stewart, Director of Technical Services for CES, and his team. ""The goal of this online educational endeavor is to provide the unique resources and training needed to implement and use speech solutions successfully,"" Stewart said. Built upon in-house documentation, interactive training, product workshops, and online certification courses, CES University will draw more attention to the benefits of speech solutions, increase customer expertise, and position the organization for further growth. In addition to CES University , the Certified eSupport Web site features the CESU Blog , providing insight on the latest industry news, as well as a Knowledge Base , where users can find more information about the speech recognition, dictation and transcription software and hardware solutions offered, including Nuance's Dragon speech to text software, as well as Philips and Olympus digital voice recorders. Stewart explained that this growth in a virtual environment translated into the company's overall growth, with notable advantages for their users. ""As our business expanded, we found we needed more space for research and development, product integration, and in-house training,"" Stewart said, adding that the new facility allows for a more strategic organizational layout, focused specifically on role development and collaboration. ""CES University will help ensure that anyone using speech solutions is well-equipped and well-informed. To further benefit our customers, we will be submitting our certification programs to several professional organizations, so that they can be counted as continuing education credits,"" he said. ""We're providing an environment where businesses can utilize our resources to boost their adoption rates and maximize their return on investment."" Certified eSupport, Corp. is an organization made up of engineers, certified instructors, trainers, Microsoft Certified Professionals and Network Administrators with a focus in voice processing, speech recognition and enterprise-wide software solutions, specializing in the healthcare, legal and corporate verticals. With an expertise in digital dictation, transcription, speech recognition and enterprise solutions, CES is an industry expert in Dragon Voice Recognition Software , Dragon Medical Practice Edition, Philips or Olympus Dictation or Transcription Solutions. As a complete solutions provider, services include Support, Training, Customization, Consultation and Certification Preparation on several industry leading manufacturers. For more information, visit https://www.certifiedesupport.com /. ",http://www.broadwayworld.com/bwwgeeks/article/Certified-eSupport-Launches-CES-University-Prompting-Expansion-Relocation-20170406,"With an expertise in digital dictation, transcription, speech recognition and enterprise solutions, CES is an industry expert in Dragon Voice ...",Certified eSupport Launches CES University Prompting Expansion ...,Certified Corp. North America CES University CES University Certified Certified Donley Drive Suite Austin TX April Joshua Stewart Director Technical Services CES Stewart CES University CES University Certified Web CESU Blog Knowledge Base Nuance Dragon Philips Olympus Stewart Stewart CES University Certified eSupport Corp. Microsoft Certified Professionals Network Administrators CES Dragon Voice Recognition Software Dragon Medical Practice Edition Philips Olympus Dictation Transcription Solutions Support Training Customization Consultation Certification Preparation,-1
37,"This article was updated on 7th April, 2017. Added new frameworks. The artificial intelligence, personal assistant and chatbot space has been growing rapidly. The idea of having a personal assistant you can beckon with the words Siri, Alexa, Cortana or Ok Google which connects us to the web and the ever growing Internet of Things (IoT) is becoming ever more commonplace. Almost every messenger program and smartphone OS have chatbots or personal assistants available in 2017! While their true level of artificial intelligence is debatable, we are witnessing the start of a world where we all have virtual assistants at our disposal! Luckily for developers who want to get in on the action, there are a range of services available that make it simple to get started with the basics of building your own artificial intelligence, chatbot and/or personal assistant for whatever purpose you can dream up. Connect up your smart home, control a self made media center, deliver all sorts of information via a personal AI assistant there are so many options available thanks to APIs and services. This lead up throughout 2015 and 2016 has made 2017 the year where developers have more options than ever before. Developers really can start building solutions of their own. In this overview, well look at the services that exist which can enable developers to begin connecting their own apps and IoT devices to voice recognition, chatbots and artificial intelligence throughout 2016. Wit.ai is a service which provides a nice combination of both voice recognition and machine learning for developers. It provides the service to convert verbal commands into text and can also be trained up in how to understand those commands. It also has a form of machine learning, where you can train it to understand commands which are said to it which it previously didnt understand, however this isnt an automatic process (its not a totally intelligent being yet!). Early in 2015, they joined Facebook and opened up the entire platform to be free for both public and private instances. Its development has been up and down since then, but the team have big plans for 2017 . Wit.ai has two main elements to it that you set up within your app  intents and entities. An intent is what action an instruction should take (e.g. turn on a light). An entity is a specific object or piece of information that our AI needs to know about to enact that intent (e.g. which light Is it a smart light Should it understand particular colors the light can switch to). Rather than needing to create intents from scratch, Wit.ai also provides access to existing intents from the developer community which is quite neat! Wit.ai also has the concept of roles, where it can learn to differentiate between entities in different contexts (e.g. numbers in different parts of an instruction can refer to different things  like an age, an order, a count). It also has some entity types built in that it can understand, such as temperature, URLs, emails, duration etc. A new feature in Wit.ai is the Story feature, which allows you to define typical conversations in a new way. You can set up the initial question, like Whats the weather in Sydney and then define the steps and subsequent questions that the system should ask. It has the concept of branches which move the conversation in different ways if the system doesnt get all the required information up front (e.g. if the user instead says Whats the weather). There is a Wit.ai API for developers of iOS, Android, Node.js, Raspberry Pi, Ruby, Python, C, Rust and Windows Phone. It even has a JavaScript plugin for front end developers. Api.ai is a chatbot API which provides similar capabilities to Wit.ai, with intents and entities. It provides machine learning capabilities which can sometimes guess if someone uses a slightly different phrase than youve hardcoded into your assistant. They recently were purchased by Google in September 2016 . It is now one of the main ways to build conversational interfaces for Googles Home platform. One key focus of Api.ai that differs from Wit.ai is its Domains. Domains are a whole collection of knowledge and data structures from Api.ai that are ready for use in every Api.ai agent (apps are called agents in Api.ai). Domains can include knowledge of common verbs and content types. As an example, it understands the different types of data that a request of Book restaurant needs, compared to Book hotel. It has a range of real information about encyclopedia-like topics such as history, word definitions, people of significance (e.g. celebrities, writers, characters), movies, stock prices and a lot more. Api.ai is free to use but it is a little bit misleading on their website at the moment  it isnt completely free as of 2016. Most of the Domains now require your account to be upgraded, however the price for this isnt clear (developers will need to contact Api.ais sales team). Api.ai also still has a paid enterprise option which allows for the whole service to be run on a private cloud internally and more from their services team. This is potentially valuable if your usage needs to be totally private. Api.ai has SDKs for Android, iOS, the Apple Watch, Node.js, Cordova, Unity, C++, Xamarin, Python and JavaScript. The Unity integration in particular might open this up to a range of additional platforms not listed above! It also can be integrated with Amazons Echo, Skype, Slack, Facebook Messenger, Microsofts Cortana and much more. If youd like to give Api.ai a try, Ive got a series on getting started with Api.ai here at SitePoint. Just keep in mind that the domains have since required a paid account, so my example I put together doesnt answer every question any longer as I dont have a paid account. If youd rather do more of the programming side of the AI yourself and you are a fan of Raspberry Pi, you could look into Melissa . Melissa is an open source personal assistant written in Python that runs on Raspberry Pi, Windows, OS X and Linux. Its updated quite frequently and has quite a few who speak very highly of it! Melissa has always-on voice control and has a range of sample dialogues out of the box, including things like taking notes, telling your horoscope, getting definitions from Wikipedia, playing music and more. For the Python developer who wants total control  Melissa might just be for you! To find out more and get full details on how it is put together, Tanay Pant, its main developer, has a whole book that covers it in more detail and serves as the detailed documentation for Melissa. I actually spoke with him all about Melissa at the start of the year . Hes done a lot of work on it! One service from a completely different perspective is Clarifai , an artificial intelligence service that can recognize image and video content. It has its own deep learning engine that continuously improves with every use. If you are keen to take your AI prototype to a whole new level, why not give it the ability to see and recognize objects It can do all sorts of things from tagging images, searching for other images that are visually similar and flagging inappropriate images. If you want to take it to the next level, you can even teach the platform entirely new concepts by training it with your own examples. To integrate this into your own applications, Clarifai has both a REST API that could be integrated with your preferred language along with a Python, Java and Node.js API. Their service is free for up to 5000 uses a month. Ive got a guide on using Clarifai here at SitePoint for those whod like to give it a go  How to Make Your Web App Smarter with Image Recognition . If you are wanting to go beyond services which do a lot of the heavy lifting for you and really want to make true artificial intelligence systems from relative scratch, Googles TensorFlow might be the option for you! While its something that will take longer to put together, youll learn a lot more about deep learning and artificial intelligence. TensorFlow is an open source software library for numerical computation using data flow graphs. It would be best for things like training your own image recognition system or learning to do language processing. You could also make conversational AI with TensorFlow that is trained on specific data, such as SpeakEasy AI which was a chatbot built on a neural model trained on millions of comments from Reddit. Theres no limit to the sorts of things you could get a TensorFlow-powered program to do, this developer trained it to write new episodes of hit 90s show, Friends . There are a range of services and APIs out there which can provide artificial intelligence, personal assistants, chatbots and more. You dont need to be a computer science expert to implement some of the core basics in your own apps! Try out a few of the above and see what you can create. If you feel super confident, go straight for TensorFlow and make something seriously mindblowing. If you do put together your own AI prototype using any of the above services, or youve had some experience with the above or a service I did not mention  please share it in the comments or get in touch with me on Twitter ( @thatpatrickguy ). Id love to hear about it! ",https://www.sitepoint.com/simple-ways-to-build-artificial-intelligence/,Wit.ai is a service which provides a nice combination of both voice recognition and machine learning for developers. It provides the service to ...,Five Simple Ways to Build Artificial Intelligence in 2017,April Siri Alexa Cortana Ok Google Internet Things IoT OS AI APIs IoT Wit.ai Facebook Wit.ai AI Wit.ai Wit.ai URLs Wit.ai Story Whats Sydney Whats Wit.ai API Android Node.js Raspberry Pi Ruby Python C Rust Windows Phone JavaScript Api.ai API Wit.ai Google September Googles Home Api.ai Wit.ai Domains Api.ai Api.ai Api.ai Book Book Api.ai Domains Api.ais Api.ai Api.ai Android Apple Watch Node.js Cordova Unity C++ Xamarin Python JavaScript Unity Amazons Echo Skype Slack Facebook Messenger Microsofts Cortana Api.ai Ive Api.ai SitePoint Just AI Raspberry Pi Melissa Melissa Python Raspberry Pi Windows OS X Linux Melissa Wikipedia Python Melissa Tanay Pant Melissa Melissa Hes Clarifai AI Clarifai API Python Java Node.js API Ive Clarifai SitePoint Web App Smarter Image Recognition Googles TensorFlow TensorFlow AI TensorFlow SpeakEasy AI Reddit Friends APIs TensorFlow AI Twitter @ Id,-1
38,"It's not how intelligent the machines are; it's how much control we give them, says Gary Marcus. (Photo by Flickr user normalityrelief, used under a Creative Commons license) As weve reported, NYU Tandon is making a bid for New York City to become the capital city of artificial intelligence .One graduate of its Future Labs incubator, Geometric Intelligence, has already made a big splash in the field: last year, it was acquired by Uber , where its founder, Gary Marcus, launched the ride-hailing companys R&D lab forartificial intelligence. Based upon that accomplishment, you might think Marcus, a professor of psychology at NYU, would be one of the biggest cheerleaders for the potential of AI. Instead, hes consistently thrown cold water on the grandiose notions that are commonly disseminated in the media. IBMs Watson might have beaten Ken Jennings atJeopardy!, but in Marcuss view, the average AI system isnt smarter than a fifth-grader: it cant make abstractions, and it cant converse naturally. Marcusleft Uberlast month, and hes currently in the process of deciding his next steps, he told Technical.ly. Inthe meantime, he has plenty to say about all the hype associated with his industry. We spoke with him about what he thinks it will take to bring about a legitimaterise of the machines. Technical.ly Brooklyn: While others acknowledge that artificial general intelligence is a long way off, the overall sentiment seems to be a lot more optimistic than yours. Do you find yourself feeling like a bit of a contrarian Gary Marcus: Yeah, I mean, theres no question that Im taking a contrarian view. My view is that people are very enthusiastic about something that represents only a small part of what we need to actually accomplish. Obviously, different people are going to emphasize different things. I come from a background in language acquisition, where the core question is really how a two- or three-year-old child can learn to understand the world and learn to talk. I feel like machines just havent made progress on those kinds of things. They have made progress on, for example, speech recognition. But thats not language understanding; thats just transcription. TB: How do you think the field needs to adjust its approach in order to move closer to artificial general intelligence GM: The approach that Im urging on the field is to take the cognitive sciences more seriously. Especially developmental psychology, developmental cognitive science. I think that human children do a lot of things that machines havent been able to do yet. Theyre able to draw inferences from small amounts of data. Theyre able to learn a very complex language. They seem to have a lot of  at least in my view  innate structures that help them get started. The dominant approach in machine learning right now is to find statistical approximations without a lot of prior knowledge, and I dont think its competitive with human children in domains like language and everyday, common-sense reasoning.  Sam Charrington (@samcharrington) April 5, 2017 TB: In your talk at the AI Summit , you pointed out self-driving cars as something we havent made much progress on. Is that something youre optimistic about in the near future GM: I have no doubt that self-driving cars will eventually be safe enough and reliable enough that they actually replace human drivers. I dont think its as close as some people might think. Its certainly not going to happen, say, in the next year. I think it might take a decade. The problem there is that there are lots of edge cases. Its pretty easy to train a neural network to drive straight down a highway in good traffic conditions. But that doesnt mean that we understand how to robustly engineer things for all the unusual edge cases that dont happen all the time, like a truck making a left turn on a highway, which is what happened in the fatal accident with Tesla . [Editors note: Tesla was cleared of fault by the National Highway Traffic Safety Administration .] So I think theres a long way to go to get the reliability to where were really comfortable with it. TB: Along withall the positive potential of AI, theres been alarm about its potential downsides. One in particular isthe potential for AI to amplify bias against certain demographics in applications such as, say, finance. Could the altered, more human-brain-like approach you advocate addressthat GM: I mean, theres no magic bullet there. Every algorithm has bias. I think if you can learn some of the techniques that humans use for generalization, thats going to be really useful in medical discovery, but its not a magic cure-all for all of the things that are troubled in AI. Logically speaking, every system has a bias. Thats just the nature of the game. That includes people, that includes machines. TB: Do you think AI has the potential to mitigate that bias a lot more effectively than humans can GM: Well, you can mitigate the biases that you know about. So I think computers can be useful tools for mitigating bias, but I dont think people should be naive in thinking that it can be magically eliminated. There may be always be biases we dont know about, for example. I dont think theres a magic bullet there. Reason 2 we're not at #AGI . Statistics NOT equal to knowledge. ML is mostly based in collected data sets. @GaryMarcus #FLSummit #ai pic.twitter.com/8JaiyuMoTZ  Adelyn (@adelynzhou) April 5, 2017 TB: In addition to concrete drawbacks like bias, there are plenty of doomsday predictions aboutwhat might happen if and when the machines take over.It seems like from your perspective, thats not likely to happen. GM: I wouldnt say that. I mean, a lot of people think that whatever AI risk there is tied to super-intelligence. And Id say its not really about how intelligent the machines are; its about how much power they have, how much they can directly control things like the energy grid and the stock market and so forth. Theres some risk even if theyre not that intelligent. The analogy I use is, teenagers may not be the most intelligent, and theyre certainly not the most emotionally intelligent, but theyre pretty powerful. So maybe we should be worried about teenage machines at some level: the machines that have very strong cognitive ability but limited ethical abilities, for example. TB: One last question: what problems are you working on right at this moment GM: Im fundamentally interested in a pair of questions right now. One of them is definitely artificial general intelligence and how we can make such strong inferences from little data, and the other is essentially how the brain works. I think that theres a lot of work to be done in both fields, and Im choosing where Im going to fit into that next. April Joyner is a journalist who covers business, tech and finance. As a freelance writer, she has contributed to OZY, NewYorker.com and FastCompany.com. Joyner's writing has also appeared on Business Insider and USAToday.com. ",https://technical.ly/brooklyn/2017/04/10/nyu-gary-marcus-artificial-intelligence-contrarian/,"I feel like machines just haven't made progress on those kinds of things. They have made progress on, for example, speech recognition.",NYU's Gary Marcus is an artificial intelligence contrarian,Gary Marcus Photo Flickr Commons NYU Tandon New York City .One Future Labs Geometric Intelligence Uber Gary Marcus R D Marcus NYU AI IBMs Watson Ken Jennings Marcuss AI Marcusleft Uberlast Technical.ly Inthe Technical.ly Brooklyn Gary Marcus Im GM Im Especially Theyre Theyre Sam Charrington @ April AI Summit GM Tesla National Highway Traffic Safety Administration ] AI AI GM Every AI Logically AI GM Reason AGI Statistics NOT ML GaryMarcus FLSummit Adelyn @ April GM AI Id GM Im Im Im April Joyner OZY NewYorker.com FastCompany.com Joyner Business Insider USAToday.com,-1
39,"Windows 10 has gotten a lot of flak from privacy advocates for the amount of data the operating system collects about you. But while theres no simple way to turn offallof the operating systems data collection, Microsoft does let you disable some features such as location tracking, speech recognition and features related to advertising. Now that the Windows 10 Creators Update is about to begin rolling out , Microsoft is unveiling a new unified privacy settings feature thats baked into the update. Theres a new Privacy Settings screen that will let you toggle the following features: Location  Let Windows and apps request your location and share that data with Microsoft. Speech Recognition  Let Cortana and Windows Store apps recognize your voice and send data to Microsoft to improve speech recognition. Relevant ads  Allow apps to use advertising IDs to show ads based on your usage. Tailored experiences with diagnostic data  Let Microsoft use diagnostic data from your PC to offer tips and recommendations. Theres also one more feature calledDiagnostics. You cant turn this one off, but you can switch betweentwo options: Full  Send data such as browser, app and feature usage, and typing data to Microsoft to help fix problems. Basic Send  less  data. Previously these settings were scattered throughout other areas. The Windows 10 Creators Update puts them all in one place, and by default your settings will be based on whatever options youd selected before updating. If youre setting up Windows 10 for the first time, theres a new screen that offers the same toggles during the setup process. ",https://liliputing.com/2017/04/windows-10-creators-update-puts-privacy-settings-one-place.html,Speech Recognition – Let Cortana and Windows Store apps recognize your voice… and send data to Microsoft to improve speech recognition.,Windows 10 Creators Update puts all your privacy settings in one ...,Microsoft Windows Creators Update Microsoft Privacy Settings Location Let Windows Microsoft Cortana Windows Store Microsoft IDs Let Microsoft Full Send Microsoft Basic Send Windows Creators Update,2
40,"Media releases are provided as is by companies and have not been edited or checked for accuracy. Any queries should be directed to the company itself. Artificial intelligence is pushing the envelope of possibility: NVIDIA Dr Simon See, Director and Chief Solution Architect for the NVIDIA AI Tech Center and Professor at Shanghai Jiaotong University (SJTU) and the King Mongkut'sUniversity of Technology Thonburi (KMUTT), speaks at the IoT Asia keynote, titled Leading Intelligence with Imagination. The 'what now' and 'what next' are more interesting, said Dr Simon See, Director and Chief Solution Architect for the NVIDIA AI Tech Center and Professor at Shanghai Jiaotong University (SJTU) and the King Mongkut'sUniversity of Technology Thonburi (KMUTT) during a keynote at IoTAsia 2017. Over the last few decades we've seen tremendous improvement and advancement in technology, from computers to the Internet and the Internet of Things (IoT), and of course now we have artificial intelligence (AI), he said. Everyone has projections about the large numbers of devices going to be connected... all of these devices are going to be more intelligent and they are going to be connected one way or another. My concern is how they are going to be connected; how they are going to interact with one another and what they are going to achieve. According to Dr See, the technology popularised by science fiction movies such as Iron Man could well become reality soon as achievements in the field of AI accelerate. In Iron Man the J.A.R.V.I.S. AI delivers what hero Tony Stark asks  what we can already do to some extent with Siri, Cortana or Alexa today  but also makes suggestions of its own. We could have machines able to advise me, generate ideas for me, and at the same time provide suggestions to me, Dr See said. Extend that concept to being a lawyer, a nurse, a doctor, an accountant and so on. An (AI) assistant could help you do your work. While the first neural network was invented all the way back in 1943, it could not deliver today's results because the technology wasn't available at that point in time and the data wasn't available (for training) at that point in time to deliver what AI has promised, Dr See said. We see that over the last couple of years there has been an amazing rate of improvement.   We have come very far since 1943, Dr See noted. Cases in point include Alexnet, an AI built to recognise images that made waves in 2012 at the 'Olympics' of computer vision, the ImageNet Large-Scale Visual Recognition Challenge, when it displayed much better accuracy for image recognition than had been possible before. The technology has become more mature. If you go to Pinterest, you can take a picture and then find out where you can buy the object. (Similar technology) is being used in self-driving cars. You want to recognise whether it's a car, a human, a cat, rubbish on the road, or open space so a car can move intelligently on the road without hitting anything, he said. Voice recognition and translation technologies have also seen improvements with AI techniques. Baidu's DeepSpeech 2 speech recognition platform, powered by NVIDIA GPUs, recognises both English and Mandarin accurately, while machine-based simultaneous translation capabilities have been demonstrated at conferences, Dr See said. The next step is natural language processing  we want the context to it, he said. AIs can perform anomaly detection as well, a godsend for use cases such as cancer diagnosis. Hospitals are developing new applications with machine or deep learning to help doctors find medical cures faster. PathAI is dedicated to cancer diagnosis using AI technology, for instance. NVIDIA is involved as well, working with the National Cancer Institute, the US Department of Energy and several US laboratories on the Cancer Distributed Learning Environment (CANDLE) project. AI can accelerate discovery of cancer therapies, predict drug responses of cancer patients, and automate the analysis of treatment effectiveness, Dr See said. Anomaly detection is also useful with machinery, to predict or prevent catastrophic failures. GE has used machine learning to detect combustion anomalies within gas turbines, and used the data to predict the probability of failure. With the advance of neural networks we are able to actually train those networks and detect those anomalies easily, Dr See explained. The field is also moving from passive to generative AI, and the sky is the limit on what can be achieved, and where. A neural network has been trained to take in artistic styles, and are able to generate art in specific styles based on real world photographs, Dr See noted. Generative design creates complex forms that wouldn't be possible otherwise, he said. StackGAN can even search for pictures given a text description, Dr See shared, useful for identifying birds for instance.   The next stage in the evolution of design would be solutions such as the AutodeskDreamcatcher project. Given initial requirements, the AI generates different options satisfying the requirements, allowing designers and manufacturers to pick those which are most relevant for them. You could simulate molecules to bind to a peptide, Dr See suggested. It takes a long time for a human to do this, but it is pretty easy for machines to generate different ideas on how a molecule can fit into a peptide. Ultimately, AI technology, supported by all the connected devices in the Internet of Things (IoT), could become even more helpful. J.A.R.V.I.S. is intuitive and self-learning, Dr See pointed out. It can ask Tony 'what are you trying to do' It has been demonstrated that AIs can learn by themselves, and achieve more than humans, too. In 2013, Google's Deepmind showed how it canlearn how to play an Atari game called Breakout. At the time Google said, We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them. The algorithm plays Atari Breakout. It has never played it before; it just knows rules and objectives, noted Dr See, playing a video that shows the AI making mistakes in the first minutes, but progressing to the expert level and then surpassing human capabilities in a few hours. The techniques are not complicated, Dr See said, noting that a neural network (AI) basically has to be trained in an optimum way to learn. The results can surprise, he noted, referencing Google's AlphaGo go-playing AI which resoundingly beat a world champion last year - a feat thought impossible because go is extremely complicated, with many possible moves. In game number 3, AlphaGo made a move that all experts at that point thought (was) an extremely silly move. After AlphaGo had won the game, experts analysed it, and found that it was a profound move that no expert had (seen before), he said. AlphaGo had become imaginative. Real-world interactions will go a long way towards training AIs to be imaginative. Dr See concluded with food for thought for the audience by running a clip of Nadia, an AI from New Zealand firm Soul Machines which will be trained by real-world conversations from the Australian public. A February 2017 blog post by Louise Glanville, Deputy CEO, National Disability Insurance Agency (NDIA), Australia. explains the Nadia project: The plan is for Nadia to be released in a trial environment on the myplace portal in the next few months. Nadia will start as a 'trainee'. It will take 12 months and a great deal of interactions with NDIS stakeholders for Nadia to become fully operational. The agency will hold information sessions to inform people how they can engage with and use Nadia over the next couple of months. We hope that you will start using Nadia as soon as she is available, and help build her knowledge base, making it easier for all stakeholders to have their questions answered quickly and clearly. The crucial piece of the puzzle is the ecosystem required to make AI a ubiquitous reality. NVIDIA can provide the compute horsepower, Dr See explained. The AI techniques used to train AIs can require a great deal of experimentation and a lot of data, much more than normal computing requires. Graphics processing units (GPUs) from NVIDIA can cut down training times. In AlphaGo's case it took several weeks to train the network, using a few hundred million training steps on 50 GPUs. A lot of compute power is needed. We will need new AI data centres, Dr See said. We know AI frameworks are available. We can develop neural networks easily right now. We're building systems that run those networks very quickly. ",http://www.cso.com.au/mediareleases/29416/artificial-intelligence-is-pushing-the-envelope/,Voice recognition and translation technologies have also seen improvements with AI techniques. Baidu's DeepSpeech 2 speech recognition ...,Artificial intelligence is pushing the envelope of possibility: NVIDIA,Media NVIDIA Dr Simon See Director Chief Solution Architect NVIDIA AI Tech Center Professor Shanghai Jiaotong University SJTU King Mongkut'sUniversity Technology Thonburi KMUTT IoT Asia Imagination 'what Dr Simon See Director Chief Solution Architect NVIDIA AI Tech Center Professor Shanghai Jiaotong University SJTU King Mongkut'sUniversity Technology Thonburi KMUTT IoTAsia Internet Internet Things IoT AI Dr See Iron Man AI Iron Man J.A.R.V.I.S AI Tony Stark Siri Cortana Alexa Dr See AI AI Dr See Dr See Alexnet AI ImageNet Large-Scale Visual Recognition Challenge Pinterest Similar Voice AI Baidu DeepSpeech NVIDIA GPUs English Mandarin Dr See AIs PathAI AI NVIDIA National Cancer Institute US Department Energy US Cancer Distributed Learning Environment CANDLE AI Dr See Anomaly GE Dr See AI Dr See Generative StackGAN Dr See AutodeskDreamcatcher AI Dr See AI Internet Things IoT J.A.R.V.I.S Dr See Tony AIs Google Deepmind Atari Breakout Google Atari Breakout Dr See AI Dr See AI Google AlphaGo AI AlphaGo AlphaGo AlphaGo AIs Dr See Nadia AI New Zealand Soul Machines February Louise Glanville Deputy CEO National Disability Insurance Agency NDIA Australia Nadia Nadia Nadia NDIS Nadia Nadia Nadia AI Dr See AI AIs Graphics GPUs NVIDIA AlphaGo GPUs AI Dr See AI,-1
41,"Scientists at Oxford say they've invented an artificial intelligence system that can lip-read better than humans. The system, which has been trained on thousands of hours of BBC News programmes, has been developed in collaboration with Google's DeepMind AI division. ""Watch, Attend and Spell"", as the system has been called, can now watch silent speech and get about 50% of the words correct. That may not sound too impressive - but when the researchers supplied the same clips to professional lip-readers, they got only 12% of words right. Joon Son Chung, a doctoral student at Oxford University's Department of Engineering, explained to me just how challenging a task this is. ""Words like mat, bat and pat all have similar mouth shapes."" It's context that helps his system - or indeed a professional lip reader - to understand what word is being spoken. ""What the system does,"" explains Joon, ""is to learn things that come together, in this case the mouth shapes and the characters and what the likely upcoming characters are."" The BBC supplied the Oxford researchers with clips from Breakfast, Newsnight, Question Time and other BBC news programmes, with subtitles aligned with the lip movements of the speakers. Then a neural network combining state-of-the-art image and speech recognition set to work to learn how to lip-read. After examining 118,000 sentences in the clips, the system now has 17,500 words stored in its vocabulary. Because it has been trained on the language of news, it is now quite good at understanding that ""Prime"" will often be followed by ""Minister"" and ""European"" by ""Union"", but much less adept at recognising words not spoken by newsreaders. A lot more work needs to be done before the system is put to practical use, but the charity Action on Hearing Loss is enthusiastic about this latest advance. ""AI lip-reading technology would be able to enhance the accuracy and speed of speech to text,"" says Jesal Vishnuram, the charity's technology research manager. ""This would help people with subtitles on TV, and with hearing in noisy surroundings."" Right now the system has limitations - it can only operate on full sentences of recorded video. ""We want to get it to work in real time,"" says Joon Son Chung. ""As it keeps watching TV, it will learn."" And he says getting the system to work live is a lesser challenge than improving its accuracy. He sees all sorts of potential uses for this technology, from helping people to dictate instructions to their smartphones in noisy environments, to dubbing old silent films. In many cases, the AI lip-reading system could be used to improve the performance of other forms of speech recognition. Where the Oxford researchers and the hearing loss charity agree, is on the fact that this is not a case where AI is going to replace humans. Professional lip-readers need not fear for their jobs - but they can look forward to a time when technology helps them become a lot more accurate. ",http://www.bbc.com/news/technology-39298199,"In many cases, the AI lip-reading system could be used to improve the performance of other forms of speech recognition. Where the Oxford ...",Towards a lip-reading computer,Oxford BBC News Google DeepMind AI Watch Attend Spell Joon Son Chung Oxford University Department Engineering Joon BBC Oxford Breakfast Newsnight Question Time BBC Prime Minister Union Action Hearing Loss AI Jesal Vishnuram Joon Son Chung AI Oxford AI,11
42,"The Tensor Processing Unit chips, which were announced in May 2016, run 15 to 30 times faster than contemporary CPUs and GPUs, says Google. Though the company unveiled the existence of the TPU chips back in May of 2016, it only now is revealing more of the technological details of the chips, which were developed by Google to continue and advance the compute-intensive machine learning processes it has been using in its products for more than 15 years. ""The need for TPUs really emerged about six years ago, when we started using computationally expensive deep learning models in more and more places throughout our products,"" Norm Jouppi, a Distinguished Hardware Engineer at Google, wrote in an April 5 post on the Google Cloud Platform Blog. ""The computational expense of using these models had us worried. If we considered a scenario where people use Google voice search for just three minutes a day and we ran deep neural nets for our speech recognition system on the processing units we were using, we would have had to double the number of Google data centers."" That need led to the creation of TPU chips to help increase compute power without requiring an expansion of data centers, wrote Jouppi.  And speed is what the TPU chips bring to the process, he wrote, with performance 15 to 30 times above the compute power of typical CPUs and GPUs on production artificial intelligence workloads, along with vastly increased energy efficiency. The TPU chips offer 30 to 80 times higher energy efficiency compared with standard chips using the TOPS/Watt measure (tera-operations [trillion or 1012operations] of computation per Watt of energy consumed), according to Jouppi. ""TPUs allow us to make predictions very quickly, and enable products that respond in fractions of a second,"" he wrote. ""TPUs are behind every search query; they power accurate vision models that underlie products like Google Image Search, Google Photos and the Google Cloud Vision API; they underpin the groundbreaking quality improvements that Google Translate rolled out last year; and they were instrumental in Google DeepMind's victory over Lee Sedol , the first instance of a computer defeating a world champion in the ancient game of Go."" Google released its expanded details of its TPU development in a presentation at a National Academy of Engineering meeting at the Computer History Museum in Silicon Valley , where it also released a study about the inner workings of the custom TPU chips, wrote Jouppi. More than 70 authors contributed to the report, which details the intricacies, design and thought processes that went into the development of the chips. ""It really does take a village to design, verify, implement and deploy the hardware and software of a system like this,"" he wrote. A TPU is an application-specific integrated circuit (ASIC) that was built specifically for machine learning . It is designed to use the TensorFlow open source software library, which was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization to conduct research on machine learning and deep neural networks, according to Google. TPU chips are tailored to machine learning applications, allowing the chips to be more tolerant of reduced computational precision, which means it requires fewer transistors per operation, according to Google. Due to that inherent efficiency, the chips can squeeze more operations per second into the silicon using more sophisticated and powerful machine learning models to get results more rapidly. Each TPU chip can be installed in a data center rack on a board that fits into a hard disk drive slot. ",http://windowsitpro.com/business-intelligence/google-shares-new-details-about-its-tpu-machine-learning-chips,"... for our speech recognition system on the processing units we were using, we would have had to double the number of Google data centers..",Google Shares New Details About its TPU Machine-Learning Chips,Tensor Processing Unit May CPUs GPUs Google TPU May Google TPUs Norm Jouppi Distinguished Hardware Engineer Google April Google Cloud Platform Blog Google Google TPU Jouppi TPU CPUs GPUs TPU TOPS/Watt [ Watt Jouppi TPUs TPUs Google Image Search Google Photos Google Cloud Vision API Google Translate Google DeepMind Lee Sedol Go Google TPU National Academy Computer History Museum Silicon Valley TPU Jouppi TPU ASIC TensorFlow Google Brain Team Google Machine Intelligence Google TPU Google Due TPU,4
43,"The new research report on Speech & Voice RecognitionMarket offered by DecisionDatabases.com provides Global Industry Analysis, Size, Share, Growth, Trends and Forecast Till 2022. The global speech & voice recognition market research report provides detailed information about the industry based on the revenue (USD MN) for the forecast period. The research study is a descriptive analysis of this market emphasizing the market drivers and restraints that govern the overall market growth. The trends and future prospects for the market are also included in the report which gives an intellectual understanding of the speech & voice recognition industry. Furthermore, the report quantifies the market share held by the major players of the industry and provides an in-depth view of the competitive landscape. This market is classified into different segments with detailed analysis of each with respect to geography for the study period. The report offers a value chain analysis that gives a comprehensive outlook of the speech & voice recognition market. The attractiveness analysis of this market has also been included so as to evaluate the segments that are anticipated to be profitable during the forecast period. The speech & voice recognition market has been segmented based on technology such as speech recognition, and voice recognition. The study incorporates periodic market estimates and forecasts. Each technology has been analyzed based on the market size at regional and country levels. The speech & voice recognition market has been segmented based on vertical such as automotive, enterprise, finance, consumer, government, retail, healthcare, and others. The report provides forecast and estimates for each vertical in terms of market size during the study period. Each vertical has been further analyzed based on regional and country levels. The speech & voice recognition market has been segmented based on application such as artificial intelligence, and non-artificial intelligence. The report provides forecast and estimates for each application in terms of market size during the study period. Each application has been further analyzed based on regional and country levels. Geographically, the speech & voice recognition market has been segmented into regions such as North America, Europe, Asia Pacific, Latin America, and Rest of the World. The study details country-level aspects based on each segment and gives estimates in terms of market size. The report also studies the competitive landscape of the global market with company profiles of players such as Nuance Communications Inc., Microsoft Corporation, Agnitio SL, Raytheon BBN Technologies, Validsoft UK Limited, Sensory, Inc., Biotrust ID B.V., Voicevault Inc., Voicebox Technologies Corp., Lumenvox, LLC, M2SYS LLC, Advanced Voice Recognition Systems, Inc., and Mmodal Inc. A detailed description of each has been included, with information in terms of headquarters, inception, stock listing, upcoming capacities, key mergers & acquisitions, financial overview, and recent developments. This analysis gives a thorough idea about the competitive positioning of market players. The report also gives information of speech & voice recognition markets mergers/acquisitions, partnerships, collaborations, new product launches, new product developments and other industry developments. ",http://www.military-technologies.net/2017/04/04/global-speech-voice-recognition-market-forecast-research-report-2015-2022/,"The new research report on Speech & Voice Recognition Market offered by DecisionDatabases.com provides Global Industry Analysis, Size, ...",Global Speech & Voice Recognition Market | Forecast Research ...,Speech Voice RecognitionMarket DecisionDatabases.com Global Industry Analysis Size Share Growth Trends Forecast Till USD MN North America Europe Asia Pacific Latin America Rest World Nuance Communications Inc. Microsoft Corporation Agnitio SL Raytheon BBN Technologies Validsoft UK Limited Sensory Inc. Biotrust ID B.V. Voicevault Inc. Voicebox Technologies Corp. Lumenvox LLC M2SYS LLC Advanced Voice Recognition Systems Inc. Mmodal Inc. A,3
44,"Russ Wilcox is a jack-of-all-trades in the Boston startup community. In the mid-1990s, Wilcox worked on speech recognition software at PureSpeech. He later co-founded and became CEO of E Ink, an e-reader display maker that took off after it was chosen as the screen for the Amazon Kindle. E Ink was sold in 2009 to a Taiwanese display electronics company for about $480 million, according to Wilcoxs LinkedIn profile . Wilcox hasnt shied away from complex technologies or sectors. After E Ink, he helped start Transatomic Power , a nuclear energy company spun out of MIT, and Piper Therapeutics , a drug developer focused on the immune system. Now, Wilcox will try to propel innovative companies forward as the newest partner at Pillar , an early-stage venture fund based in Boston. The move means he is stepping back from running Piperwhich is still at the pre-clinical research stage, he saysbut he will remain on the biotech startups board. (Hes also on Transatomics board.) Interestingly, joining Pillar reunites Wilcox with Jamie Goldstein, the venture firms founder and managing director. The pair worked together at PureSpeech, which Goldstein co-founded. I sat down with Wilcox on Friday to find out what he has learned from working on companies in a range of industries, why he became an investor, and where he thinks technology is headed. He shared some interesting perspectives about the entrepreneur-investor relationship, the convergence of different technologies, and privacy concerns in an era of always-listening devices, and a tech oligarchy that is getting rich off a lot of our personal data. Here are some of the highlights of our chat: Xconomy: That was a big change for you to go into biotech. What lessons did you learn from that Russ Wilcox: First of all, Ive spent a lot of my career working with scientists, whether it was E Ink or Transatomic or Piper, its all working with scientists [and] people who are innovators or technologists in general. I see [Piper] as a continuation of the practice of trying to understand someones technology, trying to understand what the market needs, and then trying to commercialize a business around that. [Piper licensed technology from Boston Childrens Hospital.Eds.] The word entrepreneur, its French. And entre means between, and preneur means to take, to carry. So literally, your job as an entrepreneur is to be the person who carries between a great idea and a business. And youre the bridge. So being a bridge is what Ive learned to do. Life sciences is very different territory scientifically. There is some overlap in terms of good practice, good experimental design, but it is definitely a new field [for me]. I thought it was exciting and neat to spend a year and a half focused on that, learn a lot about biology, cells, cancer, immunology. So, its been terrific, and I hope its a great success scientifically, but also its been a wonderful experience for me. [What] I see now is room for more companies in Boston to blend tech and biology. But you see this, theres a little bit of a schism between the VCs that are good at health and the VCs that are good at tech. Theres a few that bridge it, like Flare [Capital Partners]Michael Greeley at Flare has got a great portfolio doing digital health. But generally, theres not that much action in the overlap between those two fields, and I dont see why. I think tech could be applied quite well to health in lots of ways. Id look for more ways that that could occur. X: You have a really wide breadth of experience. Not many people have been in software, clean energy, and biotech. What lessons have you learned from working in all those different sectors Has that been challenging RW: I think every business youre in, theres a deep base of specific knowledge youve got to master, whether its how to be a good writer or how to work on deadline or whatever it is. I think thats common to every different job. I think probably theres some people who are more efficient with their time than I have been, who focus deeply and maybe would say I couldve gotten farther if I had picked one. But you dont want to be pigeon-holed in your life, you want to explore. I love to scout things. Im a curious person. I love to learn new things, and I see it as a big advantage personally when I have a chance to dig in on something completely new. Thats exciting for me. If you look at where the field of innovation is these days, its rarely within one discipline. The great chemistry ideas strictly due to chemists, a lot of them happened since the 40s or earlier. How to do electronics, the physics behind electronics was figured out also. But taking chemistry and electronics and [putting] them together to do display technology, thats interdisciplinary. I think that being able to connect across different fields is an important frontier right now for how people are innovating. Part of what makes that accelerate is the Internet and the fact that people can find any fact they want now in seconds. It becomes less important that you know all the facts and more important that you learn the patterns and you learn the rhythms of how to marshal those facts to innovate. I think its less and less important what youve studied in your textbook and more and more important about seeing opportunities and gathering the right people around themhelping them to collaborate, building teams, building cultures. X: How did you know you were ready to cross over to the other side and do investing, be on the other side of the table RW: Its funny, we made this announcement two or three days ago. And I think at least 10 people said to me, Oh, youre going to the dark side, the other side. It is really interesting. It is pervasive right now that people conceive of the investor-entrepreneur relationship as having sides. I would say part of what attracted me to Pillar is that its trying to narrow that gap and get everybody on the same side. I think not just Pillar, but a lot of VCs in Boston would like to get there. What it means is you have to be very aligned with the entrepreneur. So, one thing Pillar does is it comes in early. Its going to get diluted alongside the founder. In some cases, Pillar will take common stock. Were using simple term sheets. Were trying to pay attention and listen to people.  It influences up and down everything that a venture capitalist does if the focus is alignment with the founder. So, part of how I knew is I found that Jamie and [Pillar partner] Sarah [Hodges] really had that mindset, and they were going to be on the side of the entrepreneur. That resonated with me. Jamie co-founded the company with 16 of Bostons superstar CEOs . Just think about a bunch of CEOs around a table saying, What VC firm would we like to be part of Theyd like to be part of one that is hyper-friendly to the entrepreneur. I think that thats a really special thing about Pillar. I think another thing for me is I had been an [entrepreneur in residence] at Harvard Business School at the [Arthur] Rock Center [for Entrepreneurship], which means I would go over to the [Harvard Innovation Lab] and once a month spend an afternoon chatting with students. Ive done that for five years, so Ive probably seen a couple hundred student groups. And I got great joy from trying to help them each take one more step forward to their dreams, whatever their dream is.  I loved that. I loved those people, I loved the energy. But sooner or later they need capital to get started. I didnt love not being able to help them. I think becoming an investor  is going to allow me to be more supportive of startups in Boston, and Im going to really love that. X: With E Ink, you were on the new frontier of displays, and the screen is still the dominant tech interface. But whats going to be the next evolution in how we interact with technology RW: Its a very correct question to ask because if you look at the economics of technology, a lot of it comes down to advertising and persuasion. So, the reason Facebook, Google, Amazon, all those guys have huge valuations is because they influence your spending. In the end, its the ads that are really driving those companies valuations. And the place that you put the ad is in the very last step [to reach] the eyeball of the user. So thats why you see Google investing in its own mobile OS because thats where they get to control what you see on the screen. The interface actually is the leading driver of control for the attention of the user. The battle for the next interface is very high stakes. Single Page Currently on Page: 1 2 Jeff Engel is a senior editor at Xconomy. Email: jengel@xconomy.com Follow @JeffEngelXcon ",http://www.xconomy.com/boston/2017/04/10/pillars-russ-wilcox-talks-vc-data-control-moving-beyond-screens/,"In the mid-1990s, Wilcox worked on speech recognition software at PureSpeech. He later co-founded and became CEO of E Ink, an e-reader ...","Pillar's Russ Wilcox Talks VC, Data Control, Moving Beyond Screens",Russ Wilcox Boston startup Wilcox PureSpeech CEO E Ink Amazon Kindle E Ink Wilcoxs LinkedIn Wilcox E Ink Transatomic Power MIT Piper Therapeutics Wilcox Pillar Boston Piperwhich Hes Transatomics Pillar Wilcox Jamie Goldstein PureSpeech Goldstein Wilcox Friday Russ Wilcox First Ive E Ink Transatomic Piper [ Piper ] Piper Boston Childrens Hospital.Eds So Ive ] Boston VCs VCs Flare [ Capital Partners ] Michael Greeley Flare Id RW [ Internet Pillar VCs Boston Pillar Were Were So Jamie [ Pillar ] Sarah [ Hodges ] Jamie Bostons CEOs Just CEOs VC Theyd Pillar Harvard Business School [ Arthur ] Rock Center [ Entrepreneurship ] [ Harvard Innovation Lab ] Ive Ive Boston Im E Ink RW Facebook Google Amazon OS Single Page Currently Jeff Engel Xconomy @ xconomy.com Follow @ JeffEngelXcon,3
45,"Microsoft is expected to unveil some new devices later this spring, although it's not yet clear whether those will include the anticipated Surface Pro 5. However, new details emerged this week about some features that might appear on the successor to Microsoft's well-received Surface Pro 4 2-in-1. The Surface Pro 5 will retain the same proprietary Surface Connect power connector rather than switching to a USB-C connector, according to a tweet yesterday by technology writer Paul Thurrott, who said the information came from someone who has had a first-hand look at the coming Surface Pro 5. The source also said the Surface Pro 5 will come with new processors: the Kaby Lake processors from Intel. Other than those changes, the Surface refresh isn't expected to feature any ""dramatic"" changes, according to Thurrott. Launched in late 2012, Microsoft's current Surface lineup includes the Surface 3 and Surface Pro 4 (pictured above) 2-in-1s, along with the Surface Book laptop. The Surface Hub, a wall-mounted touchscreen device for workplace presentations and video-conferencing, was also launched last year. More details about the Surface 5 and other new devices are likely to come out during Microsoft's next Build developer conference, scheduled to be held from May 10 to 12 in Seattle. In the meantime, Microsoft is gearing up for its next big update to the Windows 10 operating system. The so-called ""Creators Update"" is set to be released starting Tuesday. Upon announcing the release date last week, Windows and Devices Group corporate vice president Yusuf Mehdi also revealed that Microsoft will be launching its current Surface Book and Surface Studio in more markets around the globe. The Surface Book will be available starting April 20 in Austria, China, Denmark, Finland, France, Germany, Hong Kong, Japan, the Netherlands, Norway, Sweden, Switzerland, and the U.K. The Surface Studio and Surface Dial are also expanding to new markets, including Australia, Canada, and New Zealand. Ahead of the general release of the Creators Update, Microsoft has rolling out successive preview versions for developers. The most recent feature a number of updates for the Surface Book and Surface Pro 4, including a Surface driver fix for an issue affecting screen brightness after a device comes out of sleep, and improved stability for the Surface's Intel Precise Touch Device. Last week, Microsoft said the Creators Update would also be rolling out a number of improvements for the enterprise-focused Surface Hub. ""The update delivers some of the top requested features to make Surface Hub an even more powerful collaboration device,"" Mehdi said in a blog post at that time. Among the operating system improvements coming to the Surface Hub is a new start screen that allows users to sign on with their credentials and access their Office 365 apps and OneDrive content. The Creators Update will also add new management tools to make it easier for businesses to handle large-scale deployments, as well as improvements in Skype for Business and the Whiteboard app. More than 70% of data center incidents are caused by human error. Find out how you can avoid that. Discover the most essential attributes to consider when selecting a colocation provider. By Jef Cozza / Top Tech News. Updated April 03, 2017. Quick, which operating system is the most widely used when it comes to surfing the Internet You would be forgiven for saying Microsoft Windows, since the tech behemoth has dominated the OS landscape since, well, forever. But for the first time ever, Windows has been eclipsed by Google's Android OS, marking a major turning point in the history of Internet usage. Android topped the worldwide OS Internet usage market share for the month of March with 37.93 percent, putting it marginally ahead of Windows' 37.91 percent share for the first time, according to a new report by StatCounter, an independent Web analytics company. The study looked at overall Internet usage across all devices, including desktop, laptop, tablets, and mobile phones. The fact that Android, an operating system built for mobile devices, is now the most common platform individuals use to access the Internet represents a major shift in the way the world connects online, with the desktop finally being overtaken by mobile devices. ""This is a milestone in technology history and the end of an era,"" said Aodhan Cullen, CEO, StatCounter, in a statement. ""It marks the end of Microsoft's leadership worldwide of the OS market which it has held since the 1980s. It also represents a major breakthrough for Android which held just 2.4 percent of global Internet usage share only five years ago."" Cullen said that the main drivers of the transition include the increased use of smartphones to access the Internet, a decline in sales of traditional PCs, and the impact of Asia on the global market. But despite losing the number one spot in overall traffic, Windows still dominates the worldwide operating system desktop market, which includes desktops and laptops, with an 84 percent Internet usage share in March. However, that might be cold comfort for Microsoft if the statistics reflect an underlying shift in the way people use their devices. ""Windows won the desktop war but the battlefield moved on,"" said Cullen. ""It will be difficult for Microsoft to make inroads in mobile but the next paradigm shift might give it the opportunity to regain dominance. That could be in augmented reality, AI, voice or continuum (a product that aims to replace a desktop and smartphone with a single Microsoft powered phone)."" In North America Windows (all versions) maintained its lead across all platforms with a 39.5 percent share of the market in March, followed by iOS with 25.7 percent and Android with 21.2 percent. The story is similar in Europe, where Windows again owns more than twice the share of the Internet market compared to Android, 51.7 percent to 23.6 percent. Microsoft's biggest problem seems to be Asia, where consumers prefer Android to Windows devices 52.2 percent to 29.2 percent. Given the size and continued growth of the Asian IT market, the switch to Android is likely to remain a challenge for Microsoft, even as it maintains its dominance in more developed markets. ",http://www.toptechnews.com/article/index.php?story_id=022000OS1Y2K,... would have overwhelmed existing data center processors running deep neural nets for speech recognition. Keeping up would have required ...,ENTERPRISE HARDWARE Here's What We Know about Microsoft's ...,Microsoft Surface Pro Microsoft Surface Pro Surface Pro Connect Paul Thurrott Surface Pro Surface Pro Kaby Lake Intel Surface Thurrott Microsoft Surface Surface Surface Pro Surface Book Surface Hub Surface Microsoft Build May Seattle Microsoft Windows Update Tuesday Windows Devices Group Yusuf Mehdi Microsoft Surface Book Surface Studio Surface Book April Austria China Denmark Finland France Germany Hong Kong Japan Netherlands Norway Sweden Switzerland U.K Surface Studio Surface Dial Australia Canada New Zealand Ahead Creators Update Microsoft Surface Book Surface Pro Surface Intel Precise Touch Device Microsoft Update Surface Hub Surface Hub Mehdi Surface Hub Office OneDrive Creators Update Skype Business Whiteboard Jef Cozza / Top Tech News April Quick Internet Microsoft Windows OS Windows Google Android OS Internet Android OS Internet March Windows StatCounter Web Internet Android Internet Aodhan Cullen CEO StatCounter Microsoft OS Android Internet Cullen Internet Asia Windows Internet March Microsoft Windows Cullen Microsoft AI Microsoft North America Windows March Android Europe Windows Internet Android Microsoft Asia Android Windows IT Android Microsoft,-1
46,"Microsoft is expected to unveil some new devices later this spring, although it's not yet clear whether those will include the anticipated Surface Pro 5. However, new details emerged this week about some features that might appear on the successor to Microsoft's well-received Surface Pro 4 2-in-1. The Surface Pro 5 will retain the same proprietary Surface Connect power connector rather than switching to a USB-C connector, according to a tweet yesterday by technology writer Paul Thurrott, who said the information came from someone who has had a first-hand look at the coming Surface Pro 5. The source also said the Surface Pro 5 will come with new processors: the Kaby Lake processors from Intel. Other than those changes, the Surface refresh isn't expected to feature any ""dramatic"" changes, according to Thurrott. Launched in late 2012, Microsoft's current Surface lineup includes the Surface 3 and Surface Pro 4 (pictured above) 2-in-1s, along with the Surface Book laptop. The Surface Hub, a wall-mounted touchscreen device for workplace presentations and video-conferencing, was also launched last year. More details about the Surface 5 and other new devices are likely to come out during Microsoft's next Build developer conference, scheduled to be held from May 10 to 12 in Seattle. In the meantime, Microsoft is gearing up for its next big update to the Windows 10 operating system. The so-called ""Creators Update"" is set to be released starting Tuesday. Upon announcing the release date last week, Windows and Devices Group corporate vice president Yusuf Mehdi also revealed that Microsoft will be launching its current Surface Book and Surface Studio in more markets around the globe. The Surface Book will be available starting April 20 in Austria, China, Denmark, Finland, France, Germany, Hong Kong, Japan, the Netherlands, Norway, Sweden, Switzerland, and the U.K. The Surface Studio and Surface Dial are also expanding to new markets, including Australia, Canada, and New Zealand. Ahead of the general release of the Creators Update, Microsoft has rolling out successive preview versions for developers. The most recent feature a number of updates for the Surface Book and Surface Pro 4, including a Surface driver fix for an issue affecting screen brightness after a device comes out of sleep, and improved stability for the Surface's Intel Precise Touch Device. Last week, Microsoft said the Creators Update would also be rolling out a number of improvements for the enterprise-focused Surface Hub. ""The update delivers some of the top requested features to make Surface Hub an even more powerful collaboration device,"" Mehdi said in a blog post at that time. Among the operating system improvements coming to the Surface Hub is a new start screen that allows users to sign on with their credentials and access their Office 365 apps and OneDrive content. The Creators Update will also add new management tools to make it easier for businesses to handle large-scale deployments, as well as improvements in Skype for Business and the Whiteboard app. Affordable car safety systems for ALL cars. See our backup cameras, accident prevention products, blind spot detection, parking sensors, and more, at Make My Car Safe . On sale now! By Jef Cozza / Top Tech News. Updated April 03, 2017. Quick, which operating system is the most widely used when it comes to surfing the Internet You would be forgiven for saying Microsoft Windows, since the tech behemoth has dominated the OS landscape since, well, forever. But for the first time ever, Windows has been eclipsed by Google's Android OS, marking a major turning point in the history of Internet usage. Android topped the worldwide OS Internet usage market share for the month of March with 37.93 percent, putting it marginally ahead of Windows' 37.91 percent share for the first time, according to a new report by StatCounter, an independent Web analytics company. The study looked at overall Internet usage across all devices, including desktop, laptop, tablets, and mobile phones. The fact that Android, an operating system built for mobile devices, is now the most common platform individuals use to access the Internet represents a major shift in the way the world connects online, with the desktop finally being overtaken by mobile devices. ""This is a milestone in technology history and the end of an era,"" said Aodhan Cullen, CEO, StatCounter, in a statement. ""It marks the end of Microsoft's leadership worldwide of the OS market which it has held since the 1980s. It also represents a major breakthrough for Android which held just 2.4 percent of global Internet usage share only five years ago."" Cullen said that the main drivers of the transition include the increased use of smartphones to access the Internet, a decline in sales of traditional PCs, and the impact of Asia on the global market. But despite losing the number one spot in overall traffic, Windows still dominates the worldwide operating system desktop market, which includes desktops and laptops, with an 84 percent Internet usage share in March. However, that might be cold comfort for Microsoft if the statistics reflect an underlying shift in the way people use their devices. ""Windows won the desktop war but the battlefield moved on,"" said Cullen. ""It will be difficult for Microsoft to make inroads in mobile but the next paradigm shift might give it the opportunity to regain dominance. That could be in augmented reality, AI, voice or continuum (a product that aims to replace a desktop and smartphone with a single Microsoft powered phone)."" In North America Windows (all versions) maintained its lead across all platforms with a 39.5 percent share of the market in March, followed by iOS with 25.7 percent and Android with 21.2 percent. The story is similar in Europe, where Windows again owns more than twice the share of the Internet market compared to Android, 51.7 percent to 23.6 percent. Microsoft's biggest problem seems to be Asia, where consumers prefer Android to Windows devices 52.2 percent to 29.2 percent. Given the size and continued growth of the Asian IT market, the switch to Android is likely to remain a challenge for Microsoft, even as it maintains its dominance in more developed markets. ",http://www.toptechnews.com/article/index.php?story_id=022000OS1Y2K,... would have overwhelmed existing data center processors running deep neural nets for speech recognition. Keeping up would have required ...,ENTERPRISE HARDWARE Here's What We Know about Microsoft's ...,Microsoft Surface Pro Microsoft Surface Pro Surface Pro Connect Paul Thurrott Surface Pro Surface Pro Kaby Lake Intel Surface Thurrott Microsoft Surface Surface Surface Pro Surface Book Surface Hub Surface Microsoft Build May Seattle Microsoft Windows Update Tuesday Windows Devices Group Yusuf Mehdi Microsoft Surface Book Surface Studio Surface Book April Austria China Denmark Finland France Germany Hong Kong Japan Netherlands Norway Sweden Switzerland U.K Surface Studio Surface Dial Australia Canada New Zealand Ahead Creators Update Microsoft Surface Book Surface Pro Surface Intel Precise Touch Device Microsoft Update Surface Hub Surface Hub Mehdi Surface Hub Office OneDrive Creators Update Skype Business Whiteboard Make My Car Safe Jef Cozza / Top Tech News April Quick Internet Microsoft Windows OS Windows Google Android OS Internet Android OS Internet March Windows StatCounter Web Internet Android Internet Aodhan Cullen CEO StatCounter Microsoft OS Android Internet Cullen Internet Asia Windows Internet March Microsoft Windows Cullen Microsoft AI Microsoft North America Windows March Android Europe Windows Internet Android Microsoft Asia Android Windows IT Android Microsoft,-1
47,"In a further sign of China's technological advancement, Volkswagen Group China, a subsidiary of German automotive giant Volkswagen Group, has led a US$180 million series D financing round in Mobvoi, a Chinese speech recognition and artificial intelligence company backed by Google. Volkswagen and Mobvoi said they plan to establish a 50-50 joint venture company to develop automotive applications based on Mobvois cutting-edge voice recognition and natural language processing technologies. ""This partnership is a particular example of Volkswagens determination to work with groundbreaking Chinese tech companies like Mobvoi to create new forms of people-oriented mobility technology,"" said Prof. Dr. Heizmann, President and CEO of Volkswagen Group China. ""We are impressed by Mobvois innovative approach to AI technologies, and we are pleased to form this joint venture to explore the next generation of smart mobility."" The joint effort between Mobvoi and Volkswagen is a milestone for both AI and the automotive industry in China. The in-vehicle experience of human-machine interaction is a key focuses of Mobvoi. The joint venture will develop products for a wide range of Volkswagen Group models as well as other brands. Initial products will include Mobvois existing smart rear-view mirror, which provides navigation, POI (points of interest) search, instant messaging, and on-board infotainment via voice input. In addition to Volkswagen Group models, the joint venture partners believe the technologies will be attractive throughout the automotive industry. Founded in 2012 by former Google research scientists Li Zhifei and Mike Lei, Mobvoi develops and applies speech recognition technology and Chinese language processing to smart wearable devices. Its products include smart watch Ticwatch, smart rear-view mirror Ticmirror, and voice driving assistant mobile app Ticauto. Ranked eighth in China Money Network's China AI Top 10 Ranking earlier this year, Mobvoi, also known as Chumenwenwen in Chinese, became a strategic partner to Google's Android Ware in 2015 by providing Chinese language recognition and processing capabilities. Google, Sequoia Capital's angel investment unit, China's angel investor Zhen Fund, SIG Asia Investment and unnamed Chinese technology companies previously invested roughly US$70 million in Mobvoi. With the additional US$180 million from Volkswagen, Mobvoi announced that its total fundraising has increased to US$250 million. ""I feel indescribably proud for the investment from Volkswagen. Five years ago, Li Zhifei left his position as a scientist at Google to return to China and start up his own company. Even though I didn't understand what he was doing, I saw his dream and determination. One U.S. investor withdrew at the last minute, and I stepped up to support him,"" wrote Xu Xiaoping, founder of Zhen Fund, in a Weibo posting today. ",https://www.chinamoneynetwork.com/2017/04/07/volkswagen-invests-180m-in-joint-venture-with-chinese-artificial-intelligence-firm-mobvoi,"Founded in 2012 by former Google research scientists Li Zhifei and Mike Lei, Mobvoi develops and applies speech recognition technology and ...",Volkswagen Invests $180M In Joint Venture With Chinese Artificial ...,China Volkswagen Group China Volkswagen Group US D Mobvoi Google Mobvoi Mobvois Volkswagens Mobvoi Prof. Dr. Heizmann President CEO Volkswagen Group China Mobvois AI Mobvoi Volkswagen AI China Mobvoi Volkswagen Group Mobvois POI Volkswagen Group Google Li Zhifei Mike Lei Mobvoi Ticwatch Ticmirror Ticauto China Money Network China AI Top Mobvoi Chumenwenwen Chinese Google Android Ware Google Sequoia Capital China Zhen Fund SIG Asia Investment Chinese US Mobvoi US Volkswagen Mobvoi US Volkswagen Li Zhifei Google China U.S. Xu Xiaoping Zhen Fund Weibo,4
48,"When you've got both hands on the wheel and both eyes focused on the road, the last thing you want to do is start arguing with your voice-powered assistant about what exactly you were asking it to look up. Unfortunately, that happens a little bit too frequently on my road trips when my failure to use precise phrasing stumps Siri and sends me spiraling into road rage. So imagine how pleasant it was when the folks from Nuance picked me up in a tricked-out Chrysler Pacifica loaded up with the speech recognition company's Dragon Drive automotive assistant, and I found out that Dragon Drive not only understood me perfectly but could handle two requests nearly simultaneously. Dragon Drive uses AI as well as natural language and speech recognition to understand your commands. (Credit: Nuance)To show off Dragon Drive's ability to listen to two commands at once and understand both, Lior Ben-Gigi, a senior product manager at Nuance, told the automotive assistant to bring up a route to the Golden Gate Bridge and notify one of his contacts about our estimated arrival time. Dragon Drive produced a route and then told us a message about our arrival in an hour's time had been sent  pretty impressive stuff if you've ever struggled to make a voice-powered assistant understand just one of your commands. Dragon Drive's trick: it's built on multiple layers that help the automotive assistant figure out and act on just exactly what you're saying. In addition to automatic speech recognition that can hear your dictated command and natural language understanding that can figure out the intent behind your words, Nuance has also added an artificial intelligence layer that's able to place your commands in context. All these things come together to make things easier on the driver, Ben-Gigi explained to me. ""As a driver, I don't have to think too much,"" he said. Dragon Drive, which Nuance developers for automakers to include in their in-car infotainment systems, can also tap into the sensors and diagnostic information of the car itself. In another demo, we plotted a course between my home in Northern California and San Diego; the Dragon Drive system calculated a route, but also threw in stops at gas stations when it calculated we would need to refuel the Pacifica. ""It can understand the distance [we're driving], that we have a certain amount of fuel, and it can calculate a detour that doesn't take a lot of time,"" said Ben-Gigi, pointing out that the gas stations selected by the driving assistant were conveniently located right of the freeway. What I particularly appreciate about Dragon Drive is that it didn't seem to require a series of specific commands to recognize what you were saying. That's a byproduct of the system's prowess with natural language. Say ""I'm cold,"" explained Nuance senior director of product marketing and strategy Eric Montague, and the automotive assistant would recognize that it needed to turn on the car's heating system. The Pacifica minivan we used in the demo had been specially modified by the Nuance team. Whereas most cars with a Dragon Drive-powered assistant contain just two microphones, this car had six  all the better to show off the voice biometrics features Nuance has developed. After registering your voice with the Dragon Drive system much in the same way you would with Siri or Google Assistant  you basically say ""Hello, Dragon"" a few times so that the assistant can learn the sound of your voice  you're able to control the voice commands of the car. There's a Guest mode, which allows someone like a passenger or a valet to operate limited commands, but info you might not want other people to access  your contacts, say  remains accessible only if Dragon Drive recognizes your voice. Dragon Drive uses voice recognition and seat location in its Name That Tune-style game. (Credit: Nuance)All those biometric microphones in a car can also let you and your fellow passenges pass the time on a road trip with a friendly game. To showcase the system's ability to recognize different voices, Nuance created a version of Name That Tune that could play through the Pacifica's infotainment system. As a music clip played, my fellow passengers and I could shout out ""got it,"" and the voice-recognition system would figure out which of us had buzzed in first. The system would then only listen to that contestant's guess, ignoring any interruptions from the other players. ","http://www.tomsguide.com/us/nuance-dragon-drive-automotive-assistant,news-24451.html",Dragon Drive uses AI as well as natural language and speech recognition to understand your commands. (Credit: Nuance)To show off Dragon ...,I Rode Shotgun with Nuance's AI-Powered Auto Assistant,Siri Nuance Chrysler Pacifica Dragon Drive Dragon Drive Dragon Drive AI Dragon Drive Lior Ben-Gigi Nuance Golden Gate Bridge Dragon Drive Dragon Drive Nuance Ben-Gigi Dragon Drive Nuance Northern California San Diego Dragon Drive Pacifica Ben-Gigi Dragon Drive Say Nuance Eric Montague Pacifica Nuance Dragon Drive-powered Nuance Dragon Drive Siri Google Assistant Hello Dragon Guest Dragon Drive Dragon Drive Nuance Tune Pacifica,1
49,"Nils Lenke koordiniert bei Nuance verschiedene Forschungsinitiativen zur Entwicklung eines breiten Spektrums an Technologien und deren Verwendung in den Mrkten Mobile, Automotive, Healthcare sowie Enterprise. Seine Hauptaufgaben umfassen die Forschung und Entwicklung von Software fr Knstliche Intelligenz (KI), Maschinelles Lernen, Wissensreprsentation, Spracherkennung, und natrlich-sprachliche Systeme. Er hlt acht Patente fr Erfindungen von Spracherkennungssystemen und spricht sechs Sprachen. Jahrzehntelange kommunizierten Menschen vorzugsweise mithilfe von Fernbedienungen, Tastatur und Maus. Nicht unbedingt miteinander, sondern mit den unzhligen technischen Systemen, die sie umgeben. Sprache dagegen war den menschlichen Kommunikationspartnern vorbehalten. Das ndert sich nun: Im Smartphone und Rechner sitzen Cortana und Siri, auf dem Nachtisch steht Alexa. Digitale Assistenten gehorchen aufs Wort - und steuern zentrale Aspekte der Heim-Automation wie Beleuchtung, Heizung oder Surround-Anlage mithilfe automatischer Spracherkennungssysteme. In absehbarer Zeit werden auch selbstfahrende Autos und Maschinen aller Art genau zuhren, was der Nutzer will. Damit Digitale Assistenten aufs Wort gehorchen, mssen sie dieses erst einmal verstehen. Ob diese Systeme dann auch tun, was man ihnen sagt, hngt allerdings davon ab, wie gut sie ihren Menschen verstehen. Dafr sorgt Automatic Speech Recognition (ASR). Doch hier steckt der Teufel im Detail - denn nicht jeder Nutzer von Spracherkennungssystemen spricht gestochenes Hochdeutsch oder Oxford-English. So verwenden laut einer Umfrage des Bayerischen Lehrer- und Lehrerinnen-Verbandes (BLLV) an bayerischen Schulen immerhin rund 33 Prozent der Schler einen Dialekt. Weitere 31 Prozent greifen sowohl auf ihren Heimatdialekt als auch auf das Hochdeutsche zurck. So ist es nicht verwunderlich, dass sich ein Groteil der (potenziellen) Nutzer von Spracherkennung die Frage stellt, ob ein solches System auch mit Dialekten umgehen kann. Dies gilt nicht nur fr Dialekte in Deutschland wie Hessisch oder Donaubayerisch. Auch die Schotten, Schweizer und Kanadier sprechen nicht nur eine Sprache - und wollen, dass ihre technischen Helfer sie dennoch verstehen. brigens verluft der Graben zwischen den Sprachen und Dialekten nicht zwischen Volksgruppen, sondern oft in ein und demselben Sprecher. Im Familien- und Freundeskreis sprechen viele Menschen ihren Dialekt - mit Kollegen reden sie im Jargon, etwa als Juristen oder Entwickler - doch mit fremden Personen oder bei offiziellen Anlssen nutzen sie weiterhin Hochdeutsch. Diese Entwicklung wird durch nach Angaben von Sprachwissenschaftlern durch Schulen und Universitten gefrdert. Aus Sicht der Automatic Speech Recognition bilden Dialekte zum Glck keine berwindbare Hrde. Dafr danken knnen wir Techniken aus dem Machine Learning, speziell Deep-Learning-Verfahren in Form von neuronalen Netzen. Diese Technologie ist in der Lage, mit der Vielzahl unterschiedlicher Sprachformen sowie der Schriftsprache in einem Kulturraum umzugehen. Die Voraussetzung ist, dass die Daten, mit denen ASR-Systeme ""gefttert"" werden, alle Sprachvariationen abdecken. In Grobritannien allein sind 20 Dialekt-Regionen vorhanden. Die Modelle, die einer ASR-Lsung zugrunde liegen, mssen fr alle Dialekte die Art und Weise bercksichtigen, wie Phoneme ausgesprochen werden. Ein Phonem ist der kleinste lautliche Bestandteil eines Wortes, anhand dem sich Begriffe unterscheiden lassen, etwa ""Bach"" statt ""Buch"". Damit nicht genug: Eine Spracherkennungs-Software muss auerdem spezielle Begriffe bercksichtigen, die in einem Dialekt vorkommen. So verkaufen Bcker in Sdbayern ""Semmeln"", aber keine ""Brtchen"" wie ihre Kollegen in Nord- und Ostdeutschland, auch keine ""Weck"" wie Backstuben in Schwaben und Franken. Weiterhin gilt es die Variation in der Aussprache eines Wortes zu bercksichtigen. Der Londoner Flughafen Heathrow lsst sich beispielsweise auf mehr als 50 unterschiedliche Arten aussprechen, vor allem da auch nicht-Muttersprachler nach ihm fragen werden. Eine ""intelligente"" ASR-Lsung erkennt alle Formen. Der Nutzer einer Spracherkennungssoftware sollte auerdem die Option haben, zwischen mehreren Spielarten einer Sprache zu whlen. Das gilt nicht nur innerhalb eines Landes, sondern auch zwischen Sprachrumen, in denen sich Eigenheiten herausgebildet haben. So bestehen teilweise deutliche Unterschiede zwischen dem Flmischen (Belgien) und der niederlndischen Sprache. Auch das Franzsisch in Frankreich und dem frankophonen Teil Kanadas hat sich im Lauf der 400 Jahre, die seit Grndung der Stadt Qubec vergangen sind, unterschiedlich entwickelt. Fr jeden Nutzer eines ASR-Programmes stehen somit unterschiedliche Sprachpakete zur Verfgung. Dabei kann es bei Sprachen mit einem breiten Spektrum an Dialekten im Verborgenen dann auch noch spezielle ""Dialektpakete"" geben. Mithilfe eines""Classifiers"", einem anderen Verfahren des maschinellen Lernens, identifiziert die Software automatisch welches Paket fr einen bestimmten User und dessen Dialekt das optimale ist. Gleichzeitig nimmt die Genauigkeit der ASR-Software durch die Fortschritte des Maschinellen Lernens generell stark zu, was sich dann auch in Fortschritten bei den einzelnen Dialekten niederschlgt. Den Erfolg dieses Optimierungsprozesses in der Praxis belegen Tests des Sprachspezialisten Nuance. So stieg im Vergleich zur Vorgngerversion die Erkennungsquote der Spracherkennungssoftware Dragon bei Nutzern, die ein spanisch gefrbtes Englisch sprachen, um 22,5 Prozent. Bei Sprechern aus Indien waren es 17,4 Prozent und bei Probanden aus Sdostasien an die 17,4 Prozent. Ebenso wie fr Menschen gilt dabei auch fr Lsungen aus dem Bereich automatische Spracherkennung, dass sie lebenslang hinzulernen mssen, in dem sie die uerungen des Nutzers auswerten, um ihre Funktionen weiter zu verfeinern. Dazu war es notwendig die Trainingsprozeduren fr Neuronale Netze, die allgemein sehr ""leistungshungrig"" ist, soweit zu optimieren, dass sie auf dem Laptop oder Computer des Nutzers ablaufen kann. Nicht zuletzt durch diese Selbstoptimierungsfunktionen ist die Genauigkeit erheblich gestiegen. Sie betrgt derzeit bei erfahrenen Nutzern einer solchermaen angepassetn Diktiersoftware bis zu 99 Prozent. Mittlerweile greifen Tausende von Apps auf Spracherkennungssoftware zurck - in IoT-Systemen (Internet of Things), Smartphones, Fahrzeugen und intelligenter Kleidung (Wearables). Die automatische Spracherkennung ist daher ein wichtiger Bestandteil der Digitalisierung unseres Alltags und der Geschftswelt. Bedenken, dass diese Technologie liebgewonnene Eigenheiten wie Dialekte zugunsten einer uniformen Hochsprache ""glatthobelt"", sind allerdings nicht angebracht. Vielmehr trgt Automatic Speech Recognition in Verbindung mit maschinellem Lernen und Deep Learning dazu bei, dass viele Dinge einfacher funktionieren - und dass die Welt ebenso bunt bleibt wie wir sie gerne hren. (mb) ","http://www.computerwoche.de/a/computer-host-mi-wie-der-computer-bayerisch-lernt,3330445",Aus Sicht der Automatic Speech Recognition bilden Dialekte zum Glück keine überwindbare Hürde. Dafür danken können wir Techniken aus ...,"""Computer, host mi?"" - Wie der Computer bayerisch lernt",Nils Lenke koordiniert Nuance Forschungsinitiativen zur Entwicklung Spektrums Technologien Verwendung Mrkten Mobile Automotive Healthcare Enterprise Seine Hauptaufgaben Forschung Entwicklung von Software Knstliche Intelligenz KI Maschinelles Lernen Wissensreprsentation Spracherkennung Systeme Er Patente Erfindungen von Spracherkennungssystemen Sprachen Jahrzehntelange Menschen Fernbedienungen Tastatur Maus Nicht Systemen Sprache Kommunikationspartnern Das Im Smartphone Rechner Cortana Siri Nachtisch Alexa Digitale Assistenten Wort Aspekte Heim-Automation Beleuchtung Heizung Surround-Anlage Spracherkennungssysteme Zeit Autos Maschinen Art Nutzer Digitale Assistenten Wort Ob Systeme Menschen Dafr Automatic ASR Doch Teufel Detail Nutzer von Spracherkennungssystemen Hochdeutsch Oxford-English So Umfrage Bayerischen Lehrer- Lehrerinnen-Verbandes BLLV Schulen Prozent Schler Dialekt Prozent Heimatdialekt Hochdeutsche Groteil Nutzer von Spracherkennung Frage System Dialekten Dialekte Deutschland Hessisch Donaubayerisch Auch Schotten Schweizer Kanadier Sprache Helfer Graben zwischen Sprachen Dialekten zwischen Volksgruppen Sprecher Im Familien- Freundeskreis Menschen Dialekt Kollegen Jargon Juristen Entwickler Personen Anlssen Hochdeutsch Diese Entwicklung Angaben von Sprachwissenschaftlern Schulen Universitten Aus Sicht Automatic Dialekte zum Glck Hrde Dafr Techniken Machine Learning Deep-Learning-Verfahren Form von Netzen Diese Technologie Lage Vielzahl Sprachformen Schriftsprache Kulturraum Die Voraussetzung Daten ASR-Systeme Sprachvariationen Grobritannien Dialekt-Regionen Die Modelle ASR-Lsung zugrunde Dialekte Art Weise Phoneme Ein Phonem Bestandteil Wortes Begriffe Bach Damit Spracherkennungs-Software Begriffe Dialekt Bcker Sdbayern Semmeln Brtchen Kollegen Nord- Ostdeutschland Weck Backstuben Schwaben Franken Weiterhin Variation Aussprache Wortes zu Der Londoner Flughafen Heathrow Arten ASR-Lsung Formen Der Nutzer Spracherkennungssoftware Option zwischen mehreren Spielarten Sprache zu Das Landes Sprachrumen Eigenheiten Unterschiede zwischen Flmischen Belgien Sprache Auch Franzsisch Frankreich Teil Kanadas Lauf Jahre Grndung Stadt Qubec Fr Nutzer ASR-Programmes Sprachpakete Verfgung Dabei Sprachen Spektrum Dialekten Verborgenen Dialektpakete Mithilfe Verfahren Lernens Software Paket User Dialekt Gleichzeitig Genauigkeit ASR-Software Fortschritte Maschinellen Lernens Fortschritten Dialekten Den Erfolg Optimierungsprozesses Praxis Sprachspezialisten Nuance So Vergleich zur Vorgngerversion Erkennungsquote Spracherkennungssoftware Dragon Nutzern Englisch Prozent Bei Sprechern Indien Prozent Probanden Sdostasien Prozent Ebenso Menschen Lsungen Bereich Spracherkennung Nutzers Funktionen Dazu Trainingsprozeduren Neuronale Netze Laptop Computer Nutzers Nicht Selbstoptimierungsfunktionen Genauigkeit Sie Nutzern Diktiersoftware Prozent Mittlerweile Tausende von Apps Spracherkennungssoftware zurck IoT-Systemen Internet Things Smartphones Fahrzeugen Kleidung Wearables Die Spracherkennung Bestandteil Digitalisierung Alltags Geschftswelt Bedenken Technologie Eigenheiten Dialekte zugunsten Hochsprache Vielmehr Automatic Verbindung Lernen Deep Learning Dinge Welt,-1
50,"Results: Tap vs Swipe vs Voice: How do you type Results: Tap vs Swipe vs Voice: How do you type Last week, we asked you what input method do you use for your typing needs  tapping, swiping, voice or a combination of the above. And the results are now in. It appears that the majority of you prefer the classic Tap method, since it racked in 43.46% of your votes. Combined Tap + Swipe followed in a close second with 20.04%. The top three is completed by the Swipe option, which was voted for by 20.04% of you. Unsurprisingly, Voice and Others have only managed to get 1.84% and 1.53% respectively. This only makes sense, considering the speech recognition is often slightly inaccurate, and a lot of people would rather silently type, than say their private messages out loud for the random bystanders to hear. ",http://www.phonearena.com/news/Results-Tap-vs-Swipe-vs-Voice-How-do-you-type_id92933,"This only makes sense, considering the speech recognition is often slightly inaccurate, and a lot of people would rather silently type, than say ...",Results: Tap vs Swipe vs Voice: How do you type?,Tap Swipe Voice Tap Swipe Voice Tap Tap + Swipe Swipe Voice,-1
51,"Global Speech Recognition Industry 2016 Market Research Report includes market share, market research report, market trade, market prices, market geography trend and market forecast Global Speech Recognition Market had made research and come up with a report that focuses on the major players in the Global Speech Recognition Market throughout the world. The report includes information like company profiles, specification and product picture, production, capacity, contact information, cost and revenue. Likewise, equipment and upstream raw materials as well as downstream demand analysis is also tackled. The report investigates and analyzes the Global Speech Recognition Market and shows a comprehensive evaluation of the evaluation and its specifications. Another aspect that was taken is the cost analysis of the main products dominant in the global ice cream industry considering the profit margin for the manufacturers. Click here to Download Sample PDF illustration: www.researchnreports.com/request_sample.phpid=49467 Through this report, the core driving factors of the Global Speech Recognition Market were identified and the business partners and end-users were also elaborated. The business sector structure, business patterns and challenges affecting the market globally were also included in the extensive analysis for this research report. Various interviews and talks were held with the prominent leaders in the industry in order to obtain reliable and updated information pertaining to the market. The report firstly introduced the Speech Recognition basics: definitions, classifications, applications and industry chain overview; industry policies and plans; product specifications; manufacturing processes; cost structures and so on. Then it analyzed the world's main region market conditions, including the product price, profit, capacity, production, capacity utilization, supply, demand and industry growth rate. In the end, the report introduced new project SWOT analysis, investment feasibility analysis, and investment return analysis. 2 Global Speech Recognition Competition by Manufacturers, Type and Application 3 USA Speech Recognition (Volume, Value and Sales Price) 4 China Speech Recognition (Volume, Value and Sales Price) 5 Europe Speech Recognition (Volume, Value and Sales Price) 6 Japan Speech Recognition (Volume, Value and Sales Price) 7 India Speech Recognition (Volume, Value and Sales Price) 8 Southeast Asia Speech Recognition (Volume, Value and Sales Price) Company profile: Research N Reports is a new age market research firm where we focus on providing information that can be effectively applied. Today being a consumer driven market, companies require information to deal with the complex and dynamic world of choices. Where relying on a sound board firm for your decisions becomes crucial. &nbsp; Research N Reports specializes in industry analysis, market forecasts and as a result getting quality reports covering all verticals, whether be it gaining perspective on ... ",https://www.whatech.com/market-research/consumer/277753-new-study-what-is-the-future-of-global-speech-recognition-market-in-global-industry,Global Speech Recognition Market had made research and come up with a report that focuses on the major players in the Global Speech ...,New study: What is the future of “global speech recognition market ...,Market Research Report Global Market Global Market Likewise Click Download Sample PDF SWOT Manufacturers Type Application USA Value Price China Value Price Europe Value Price Japan Value Price India Value Price Southeast Asia Value Price Company Research N Reports Research N Reports,3
52,"Computer engineers have dreamed of a machine that would translate speech into something that a vacuum tube or transistor could understand. Now at last, some promising hardware is being developed.... It is still a long way from the kind of science fiction computer that can understand sentences or long speeches.  Science News ,March 4, 1967 That 1967 device knew the words one through nine. Earlier speech recognition devices sliced a word into segments and analyzed them for absolute loudness. But this machine, developed by Genung L. Clapper at IBM, identified the volume of a pitch segment compared with its neighbors to account for the variability of human speech. Todays speech recognition goes much further, dividing words into distinct units of sound and syntax. The software decodes speech by applying pattern recognition and a statistical method called the hidden Markov model to the sounds. We rely on speech recognition to open an app to order groceries or to send a text to ask someone at home if we need more milk. Hello, Siri. B.H. Juang, L.R. Rabiner. Speech Recognition, Automatic: History . Elsevier Encyclopedia of Language and Linguistics, Second Edition, 2006. K. Leino. Breakthroughs in automatic speech recognition technology . Anatomy of Breakthroughs. July 13, 2015. ",https://www.sciencenews.org/article/speech-recognition-has-come-long-way-50-years?mode=topic&context=96,"Earlier speech recognition devices sliced a word into segments and analyzed them for absolute loudness. But this machine, developed by ...",Speech recognition has come a long way in 50 years,Computer Science News March Genung L. Clapper IBM Markov Hello Siri B.H Juang L.R Rabiner Elsevier Encyclopedia Language Linguistics Second Edition K. Leino Breakthroughs Anatomy Breakthroughs July,-1
53,"Speech and Voice Recognition Market Is Expected To Propel The Demand In Different Sectors Due To Advancement in Technology Till 2024: Grand View Research, Inc According to the new research report of Grand View Research: Global Speech and Voice Recognition Market is expected to reach USD 127.58 billion by 2024. Speech and Voice Recognition is also a core technology leveraged in the semi-autonomous and autonomous vehicles. Global Speech and Voice Recognition Market is expected to reach USD 127.58 billion by 2024, according to a new study conducted by Grand View Research, Inc. The increasing number of benefits offered by the speech and voice recognition technology, including the easy accessibility, better productivity, and flexibility are anticipated to drive the demand for speech recognition solutions over the next eight years. The increased demand for high comfort and convenience is also expected to propel the demand. The speech and voice recognition technology exhibits vast potential in various application areas, especially in the home automation system, due to advantages such as convenience and lower energy consumption. speech recognition is also a core technology leveraged in the semi-autonomous and autonomous vehicles. The autonomous car industry is expected to witness an explosive growth, which is anticipated to subsequently drive the market growth over the forecast period. However, the high price of AI-based voice control system is anticipated to pose a challenge to market expansion. Additionally, the inability to accurately recognize speech in the regional accent is a key challenge to the market. Full research report on global Speech & Voice Recognition Market: http://www.grandviewresearch.com/industry-analysis/voice-recognition-industry Further key findings from the study suggest: Enhanced device dominated the market in 2015with over 50% of the total market share. This is attributed to the increasing usage of voice recognition solutions in home automation and consumer electronics application areas. The software segment is anticipated to grow significantly over the forecast period as several prominent players have open sourced their software development kit code. The growing trend toward the development of the artificial intelligence-based system is expected to be the key factor driving industry growth over the foreseeable future. Leveraging deep learning algorithm in voice and speech solutions for better search results is expected to be the key factor for the growth of AI-based application segment. The deployment of speech recognition solutions in consumer and home security & automation verticals is anticipated to take the industry ahead at a substantial pace. This can be attributed to the changing lifestyle in several countries including the U.S., Germany, and the UK. The growing adoption of smart electronics in India, China, Japan, and Brazil may also catalyze industry growth in the consumer vertical. North America dominated the market (in terms of revenue) representing more than 30% of the total share in 2014. Additionally, North America and Asia Pacific are anticipated to witness a considerable growth due to the presence of several U.S. and China-based players, including Apple, Inc., Facebook, Inc., Baidu, Inc., Amazon.com, Inc., and Alphabet, Inc., working toward the development of this technology. Furthermore, the presence of several notable consumer electronics manufacturers, such as Samsung Electronics, Co., Ltd., with a strong distribution channel in the North American and Asia Pacific regions are expected to be the key factors for industry growth in this region over the forecast period. The prominent industry participants include Nuance Communications, Apple, Inc., Baidu, Inc., Alphabet, Inc., and Amazon.com, Inc. These players are focusing on integrating the artificial intelligence technology in order to build superior products to increase their customer base. View more reports of this category by Grand View Research at: http://www.grandviewresearch.com/industry/emerging-and-next-generation-technologies Grand View Research has segmented the Global Speech & Voice Recognition Market on the basis of components, application, vertical and region: Speech & Voice Recognition Component Outlook (USD Billion; 2013 - 2024) ",http://www.digitaljournal.com/pr/3285148,According to the new research report of Grand View Research: Global Speech and Voice Recognition Market is expected to reach USD 127.58 billion by 2024.,Speech and Voice Recognition Market Is Expected To Propel The ...,Voice Recognition Market Is Demand Different Due Advancement Technology Till Grand View Research Inc Grand View Research Global Speech Voice Recognition Market USD Voice Recognition Global Speech Voice Recognition Market USD Grand View Research Inc Full Speech Voice Recognition Market Further Enhanced U.S. Germany UK India China Japan Brazil North America Additionally North America Asia Pacific U.S. Apple Inc. Facebook Inc. Baidu Inc. Amazon.com Inc. Alphabet Inc. Furthermore Samsung Electronics Co. Ltd. Asia Pacific Nuance Communications Apple Inc. Baidu Inc. Alphabet Inc. Amazon.com Inc View Grand View Research Grand View Research Global Speech Voice Recognition Market Speech Voice Recognition Component Outlook USD Billion,3
54,"The company has reached a 5.5 percent word error rate that's nearly on par with humans. The tech world has spent years trying to create speech recognition software that listens as well as humans. Now, IBM says it's achieved a 5.5 percent word error rate, down from its previous record of 6.9 percent -- an industry milestone that could eventually lead to improvements in voice assistants like Siri and Alexa. Microsoft claimed to reach a 5.9 percent word error rate last October using neural language models resembling associative word clouds. At the time, the company believed 5.9 percent was equivalent to human parity. But, IBM says it's not popping the champagne yet. ""As part of our process in reaching today's milestone, we determined human parity is actually lower than what anyone has yet achieved  at 5.1 percent,"" George Saon, IBM principal research scientist, wrote in a blog post this week. IBM reached the 5.5 percent milestone by combining so-called Long Short-Term Memory, an artificial neural network, and WaveNet language models with three strong acoustic models. It was then measured using the ""SWITCHBOARD"" corpus, a collection of telephone conversations that's been used as a benchmark for speech recognition software for decades. SWITCHBOARD is not the industry standard for measuring human parity, however, which makes breakthroughs harder to achieve. ""The ability to recognize speech as well as humans do is a continuing challenge, since human speech, especially during spontaneous conversation, is extremely complex,"" said Julia Hirschberg, a professor and Chair at the Department of Computer Science at Columbia University, in a statement to IBM. ""It's also difficult to define human performance, since humans also vary in their ability to understand the speech of others."" ",https://www.engadget.com/2017/03/10/ibm-speech-recognition-accuracy-record/,"The tech world has spent years trying to create speech recognition software that listens as well as humans. Now, IBM says it's achieved a 5.5 ...",IBM inches toward human-like accuracy for speech recognition,IBM Siri Alexa Microsoft October IBM George Saon IBM IBM Long Short-Term Memory WaveNet SWITCHBOARD SWITCHBOARD Julia Hirschberg Chair Department Computer Science Columbia University IBM,0
55,"The company has reached a 5.5 percent word error rate that's nearly on par with humans. The tech world has spent years trying to create speech recognition software that listens as well as humans. Now, IBM says it's achieved a 5.5 percent word error rate, down from its previous record of 6.9 percent -- an industry milestone that could eventually lead to improvements in voice assistants like Siri and Alexa. Microsoft claimed to reach a 5.9 percent word error rate last October using neural language models resembling associative word clouds. At the time, the company believed 5.9 percent was equivalent to human parity. But, IBM says it's not popping the champagne yet. ""As part of our process in reaching today's milestone, we determined human parity is actually lower than what anyone has yet achieved  at 5.1 percent,"" George Saon, IBM principal research scientist, wrote in a blog post this week. IBM reached the 5.5 percent milestone by combining so-called Long Short-Term Memory, an artificial neural network, and WaveNet language models with three strong acoustic models. It was then measured using the ""SWITCHBOARD"" corpus, a collection of telephone conversations that's been used as a benchmark for speech recognition software for decades. SWITCHBOARD is not the industry standard for measuring human parity, however, which makes breakthroughs harder to achieve. ""The ability to recognize speech as well as humans do is a continuing challenge, since human speech, especially during spontaneous conversation, is extremely complex,"" said Julia Hirschberg, a professor and Chair at the Department of Computer Science at Columbia University, in a statement to IBM. ""It's also difficult to define human performance, since humans also vary in their ability to understand the speech of others."" ",https://www.engadget.com/2017/03/10/ibm-speech-recognition-accuracy-record/,"The tech world has spent years trying to create speech recognition software that listens as well as humans. Now, IBM says it's achieved a 5.5 ...",IBM inches toward human-like accuracy for speech recognition,IBM Siri Alexa Microsoft October IBM George Saon IBM IBM Long Short-Term Memory WaveNet SWITCHBOARD SWITCHBOARD Julia Hirschberg Chair Department Computer Science Columbia University IBM,0
56,"Language barriers are slowly becoming a thing of the past thanks to artificial intelligence. Googles newest machine translation system converts speech directly into the text of another language, greatly speeding the process by removing the intermediate transcription step. Googles offering isnt the first real time speech-to-text options. Skype, for example, rolled out a live translation feature in 2014. The difference, though, is that Skypes and others translate from a transcribed version of the audio. Errors in speech recognition could result in incorrect transcription and, therefore, translation. Google's latest take on speech-to-text translation could bridge international communication. Googles deep-learning research team, Google Brain, is essentially cutting out the middle step which has the potential to lead to quicker, more accurate translations. The system was developed by analyzing hundreds of hours of Spanish audio along with the corresponding English text. By using several layers of neural networks, or computer algorithms that mirror the human brain, wavelengths of the spoken Spanish were linked to the corresponding chunks of written English. Its the computer equivalent of your ears hearing Spanish while your brain understands the words as English. After a learning period, Googles system produced a better-quality English translation of Spanish speech than one that transcribed the speech into written Spanish first. It was evaluated using the BLEU score, which is designed to judge machine translations based on how close they are to that by a professional human. The system could be particularly useful for translating speech in languages that are spoken by very few people, says Sharon Goldwater at the University of Edinburgh in the UK. International disaster relief teams, for instance, could use it to quickly put together a translation system to communicate with people they are trying to assist. When an earthquake hit Haiti in 2010, says Goldwater, there was no translation software available for Haitian Creole. Not only could this system come in handy for international disaster relief teams, but it could also be used to translate rare languages that are seldom written down. For example, Goldwater is currently using similar methods to translate Arapaho, a Native American language spoken only by about 1,000 people of the Arapaho tribe. She is also working on translating Ainu, which is spoken by a small portion of the Japanese population. The new approach isnt ready for prime time, but with additional training on bigger data sets, it could set a new standard for machine translation. Tell us what you think on Twitter #NOVAnext, Facebook , or email . ",http://www.pbs.org/wgbh/nova/next/tech/ai-translation-could-speed-rare-language-research-and-save-lives/,"Google's newest machine translation system converts speech directly ... Errors in speech recognition could result in incorrect transcription and, ...",AI Translation Could Speed Rare Language Research—and Save ...,Googles Googles Skype Skypes Google Googles Google Brain English English English Googles BLEU Sharon Goldwater University Edinburgh UK International Haiti Goldwater Creole Goldwater Arapaho Arapaho Ainu Twitter NOVAnext Facebook,4
57,"Get started Bring yourself up to speed with our introductory content. Artificial intelligence apps that learn from experience are behind the wheel of self-driving cars and are poised to do more. Know the speed limits and how to get started quickly. Share this item with your network: Developing applications for the cloud is yesterday's news. The new vista is artificial intelligence, the ability of applications or devices with embedded software -- think self-driving cars -- to learn from their experiences and continually improve. The challenge for developers is that AI goes far beyond cranking out lines of code to encompass other disciplines, including advanced mathematics, analytics and deep learning. In other words, if you're building applications for AI, there are many things to know before approaching the starting line. In this expert handbook, we explore the issues and trends in cloud development and provide tips on how developers can pick the right platform. You forgot to provide an Email Address. This email address doesnt appear to be valid. This email address is already registered. Please login . You have exceeded the maximum character limit. Please provide a Corporate E-mail Address. By submitting my Email address I confirm that I have read and accepted the Terms of Use and Declaration of Consent. By submitting your personal information, you agree that TechTarget and its partners may contact you regarding relevant content, products and special offers. You also agree that your personal information may be transferred and processed in the United States, and that you have read and agree to the Terms of Use and the Privacy Policy . Understand what AI is -- and isn't. AI is not a digital counterpart to broad-based human intelligence. Applications for AI, like all applications, are task-oriented -- designed to do one thing and do it well. A cloud-based AI app can examine medical test results and MRI images to arrive at a diagnosis or recommended treatment regimen more quickly and accurately than a physician, but it's not going to replace the surgeon who's about to perform heart surgery. Set limits. AI isn't likely to beget battling bots intent on enslaving humankind. Nevertheless, best practices for building applications for AI urge setting limits on its decision-making capabilities. People need to remain the ultimate arbiter. While sifting through hundreds of rsums to find an ideal candidate is a good fit for an AI app , making the final choice and extending a job offer are best left to humans -- for now. Leverage existing technology. Artificial intelligence as a service is alive, well and maturing quickly. These platforms offer proven technologies and expertise to help execute your AI project. Amazon, Google, Microsoft, IBM and Salesforce offer natural language understanding, automatic speech recognition, visual search and image recognition, text-to-speech, and machine learning. ",http://searchcloudapplications.techtarget.com/feature/Building-applications-for-AI-demands-real-intelligence-from-developers,"Amazon, Google, Microsoft, IBM and Salesforce offer natural language understanding, automatic speech recognition, visual search and image ...",Building applications for AI demands real intelligence from developers,Get Bring AI AI Email Address Please Please Address Email Use Declaration Consent TechTarget United States Use Policy AI AI AI AI MRI Set AI Nevertheless AI AI AI Amazon Google Microsoft IBM Salesforce,-1
58,"businessman hand touch virtual graph,chart, diagram UBS Assumes The Charles Schwab Corporation (SCHW)s stock to Buy with the price target of $50. This rating was issued on 3/13/17. The stock recently closed its previous session at $39.9 by showing a percentage change of -0.4% from its previous day closing price of $40.06. Price Target is basically a projection of future price of a companys stock by the expert analysis of investment analysts or investment firms. There may be various price targets for a stock. These analysts and investment firms use various valuation methods to decide a price target for a stock. Several investment firms issued their expert ratings on The Charles Schwab Corporation (SCHW) in which SunTrust Robinson Humphrey Upgrades The Charles Schwab Corporation (SCHW) to Buy by settling a price target of $44 on 12/19/16. Keefe Bruyette & Woods Initiates Coverage On the stock to Outperform on 12/08/16 by stationing a price target of $46. Citigroup Upgrades the companys stock to Buy on 11/22/16 with no specific Price Target. SunTrust Robinson Humphrey Maintains The Charles Schwab Corporation (SCHW) to Hold with no specific Price Target. The Charles Schwab Corporation (SCHW) currently has a consensus Price Target of $45.72. While some analysts have a High Price target for the stock of $50 and a Low Price Target of $40.5. Several sell side analysts reviewed their recommendations on The Charles Schwab Corporation (SCHW) where 7 analyst have rated the stock as Strong Buy, 9 analysts said its a Buy, 3 rated the stock as Hold, 0 analysts reported Underperform and 0 analysts gave their recommendations as Sell. (Current Month Yahoo Finance Ratings) Zacks Investment Research also rated the stock with a value of 1.54. This scale runs from 1 to 5 where 1 represents Strong Buy and 5 represents Sell. The Charles Schwab Corporation (SCHW) has a market capitalization of 53.27 Billion. The stock traded with the volume of 9.3 Million shares in the last trading session. The stock touched its high share price of $43.65 on 03/17/17 and the stock also touched its Lowest price in the last 52-weeks of trading on 07/06/16 as $23.68. The company has a 1 Year high price target of $46.13. The stock is currently trading with a distance of 20-Day Simple Moving Average (SMA20) of -3.03%. The Moving Average SMA50 is -3.17% while SMA200 is 13.8%. The Charles Schwab Corporation (SCHW) is currently showing its ROA (Return on Assets) of 0.8%. The Return on Investment (ROI) is at 1% while its Return on Equity (ROE) value stands at 13.4%. The stock currently shows its YTD (Year to Date) performance of 1.29 percent while its Weekly performance value is -2.23%. The Monthly and Yearly performances are -5.56 percent and 51.3 percent respectively. The Relative Volume value measured for The Charles Schwab Corporation (SCHW) is 1.2. The Average Volume (3 months) is 7.81 Million. The stock currently has its Annual Dividend of $0.32 and an annual Dividend Yield of 0.8 Percent. SCHW has P/E (Price to Earnings ttm) value of 30.5, Forward P/E of 20.49, P/C (Price to cash per share) of 4.95 and Price to Free Cash Flow (P/FCF) value of 29.26. The stock is showing its Operating Margin of 39.2 percent. Charles Schwab Corp. provides a full-service investing experience to customers through a clicks and mortar multi-dimensional, five channel offering of the Internet, branch offices, speech recognition, touch-tone telephone, e-mail and wireless technologies, multilingual and international services, and direct access to professionals day or night. ",https://hugopress.com/2017/04/10/most-active-stock-update-the-charles-schwab-corporation-schw/,"... five channel offering of the Internet, branch offices, speech recognition, touch-tone telephone, e-mail and wireless technologies, multilingual ...",Most Active Stock Update: The Charles Schwab Corporation (SCHW),UBS Charles Schwab Corporation SCHW Price Target Charles Schwab Corporation SCHW SunTrust Robinson Humphrey Upgrades Charles Schwab Corporation SCHW Keefe Bruyette Woods Coverage Citigroup Price Target SunTrust Robinson Humphrey Charles Schwab Corporation SCHW Price Target Charles Schwab Corporation SCHW Price Target Price Price Charles Schwab Corporation SCHW Strong Buy Buy Hold Underperform Sell Month Yahoo Finance Ratings Zacks Investment Research Strong Buy Sell Charles Schwab Corporation SCHW Billion Million Lowest Simple Moving Average SMA20 Average SMA50 SMA200 Charles Schwab Corporation SCHW ROA Return Assets Return Investment ROI Return Equity ROE YTD Date Weekly Charles Schwab Corporation SCHW Annual Dividend Dividend Yield Percent SCHW P/E Price Earnings Forward P/E P/C Price Price Cash Flow P/FCF Charles Schwab Corp. Internet,6
59,"The technology research report  Intelligent Virtual Assistant (IVA) Market Size By Technology (Speech Recognition, Text-To-Speech, Voice Recognition), By Service (Customer Service, Marketing Assistant), By Application (Automotive, BFSI, Retail, IT & Telecom, Healthcare, Education), By End-Use (SMBs, Large Enterprises, Individual Users), Industry Analysis Report, Regional Outlook (U.S., Canada, Germany, UK, France, Italy, Spain, China, India, Japan, South Korea, Brazil, Mexico), Application Potential, Price Trends, Competitive Market Share & Forecast, 2016  2024; by Global Market Insights, Inc. says Intelligent Virtual Assistant Market size was USD 800 million in 2015; fuelled by growing global mobile device adoption. Growing focus on customer engagement to enhance user experience is expected to drive the intelligent virtual assistant market size in the future. Virtual assistants are deployed in mobile devices, enterprise websites, and social media that enable them to have constant communication with clients. For instance, companies install them to provide brand or product information to clients. In addition, it helps in promotions by asking new users to sign in to the companys loyalty account. Customer engagement solutions emphasize on providing enhanced direct experience and help companies to increase revenue and improve customer satisfaction and retention. Growing focus on streamlining business activities to lower the overall operating cost will also propel the intelligent virtual assistant market size. IVAs have application across several business processes such as interviewing, employee training and advertising that allow enterprises to minimize the cost. With the help of smartphones and other mobile devices, tasks such as checking in with staff, location based reminders, searching through emails, and scheduling meetings makes the work life of the employee more efficient. Thus, the increasing penetration of mobile devices is expected to provide a fillip to the intelligent virtual assistant market share. Key insights from the report include: Speech recognition is expected to witness significant growth with a CAGR of over 37% from 2016 to 2024 owing to the high demand for speech recognition technology across the medical and automotive applications. Moreover, increasing focus of technology giants including Microsoft and IBM to minimize the error rate is expected to significantly contributes towards high intelligent virtual assistant market share. The ability to covert a persons voice into a recognizable data pattern is projected to propel the demand for voice recognition over the coming years. IVA market share as service assistant is anticipated to witness high adoption over the future as it is capable of assisting businesses on customer requirements and work flow balance, thereby, delivering immediate productivity. Intelligent virtual assistant market share as customer assistants is projected to grow considerably at nearly 35% CAGR over the forecast timeline. The technology provides advantages such as enhance support, low operating cost, high customer satisfaction, personalized service to customers, multiple language & device support. Furthermore, it proves to be a key differentiator that increases consumer loyalty, income and sales. Intelligent virtual assistant market size is expected to witness significant demand across the retail industry owing to the features they offer such as responding efficiently to consumers queries and issues in a cost-effective manner. IVA offer personalized and expert service to customers irrespective of time, geography and channel without any sale support. This increases consumer satisfaction and lowers cost, which is expected to impel demand over the next few years. Enterprises are witnessing high adoption rate as it provides user friendly self-service feature which is much faster and convenient as compared to talking to a live agent. High IVA market adoption rate is further accredited to increasing focus on customer satisfaction and cost reduction. For instance, Nuances Nina is the first virtual integrated assistant in the UK insurance sector. Dominos Pizzas Dom, ING Banks Inge, and JetStar Airlines Jess are virtual assistants designed to deliver a convincing, multi-channel, automatic customer service experience for the enterprise and consumer segment. S. intelligent virtual assistant market size contributed significantly to the overall revenue in 2015, with CAGR forecast to exceed the global average. Companies contributing to the intelligent virtual assistant market share include IBM Corporation, Nuance, Clara Labs, InteliWISE, eGain Communications, Creative Virtual, CX Company, 24/7 Customer Inc., Artificial Solutions and Anboto among others. ",http://www.satprnews.com/2017/03/22/intelligent-virtual-assistant-market/,"The technology research report Intelligent Virtual Assistant (IVA) Market Size By Technology (Speech Recognition, Text-To-Speech, Voice ...",Intelligent Virtual Assistant Market in speech recognition to grow at ...,Intelligent Virtual Assistant IVA Market Size Technology Text-To-Speech Voice Recognition Service Customer Service Marketing Assistant Application Automotive BFSI Retail IT Telecom Healthcare Education End-Use SMBs Large Enterprises Individual Users Industry Analysis Report Regional Outlook U.S. Canada Germany UK France Italy Spain China India Japan South Korea Brazil Mexico Application Potential Price Trends Competitive Market Share Forecast Global Market Insights Inc. Intelligent Virtual Assistant Market USD Customer IVAs Key CAGR Microsoft IBM IVA Intelligent CAGR IVA IVA Nuances Nina UK Dominos Pizzas Dom ING Banks Inge JetStar Airlines Jess S. CAGR IBM Corporation Nuance Clara Labs InteliWISE Communications Creative Virtual CX Company Customer Inc. Artificial Solutions Anboto,3
60,"IBM has broken an industry record with its speech recognition technology, reaching a word error rate of 5.5 percent. Its a considerable accomplishment given that the company attained a word error rate of 6.9 percent only half a year ago. More importantly, it has broken the record of a rival, with Microsoft researchers having reached an error rate of 5.9 percent last October. At the time, the Microsoft researchers claimed they had reached human parity with respect to word recognition. But IBM has since endeavored to take that away from them, teaming with Appen to reassess that industry benchmark. The companies now say that human parity would actually be a word error rate of 5.1 percent. That, of course, is currently beyond anyones reach, but IBM is closer to it than ever. And the new benchmark could further spur a healthy sense of competition as multiple companies race to meet it, given the increasingly important role that voice and speech recognition play in consumer electronics and the emerging Internet of Things. ",http://findbiometrics.com/ibm-speech-recognition-record-403096/,IBM Sets New Speech Recognition Record It's a considerable accomplishment given that the company attained a word error rate of 6.9 percent ...,IBM Sets New Speech Recognition Record,IBM Microsoft October Microsoft IBM Appen IBM Internet Things,-1
61,"Philips SpeechMike Premium Air: Revolutionary new dictation microphone revealed Business Wire Tuesday, April 4, 2017 Speech Processing Solutions , the worlds number one in professional dictation, has just announced they will be releasing a new cordless dictation microphone. The Philips SpeechMike Premium Air allows healthcare, legal and business professionals to dictate their documents, saving them valuable time and resources. Professionals can focus on core responsibilities of their roles, such as taking care of their patients, thereby significantly increasing their overall effectiveness. This Smart News Release features multimedia. View the full release here: http://www.businesswire.com/news/home/20170404005036/en/ The brand-new device will be the first Li-ion battery operated SpeechMike Premium ever launched by Philips. As the device does not require a cable to transmit voice files to the computer, users can comfortably pace around their office whilst recording their thoughts. The new device enables users to benefit from all the advantages of the industry leader, SpeechMike Premium, without the constraints of a cable explains Dr Thomas Brauner, CEO of Speech Processing Solutions. State of the art technology: Lossless speech transfer The lossless speech technology ensures the users voice is safely transmitted to the computer in highest audio quality and without any interruptions. Dr Brauner continues: Our cutting edge technology ensures every word is captured from the moment you press record. You can even move up to 10 meters away from your workstation. If you do move away too far, you will be gently alarmed to move closer to your station again. In this way, no important data will ever be lost. The free-floating microphone is decoupled from the devices housing, which guarantees that almost no background, touch, click, air or structure-borne noise is recorded. The Philips SpeechMike Premium Air comes with a premium studio-quality microphone with a built-in noise reduction filter, which helps users achieve exceptional speech recognition results. The device can be used with the latest Philips SpeechExec Pro software with integrated speech recognition or with the Philips cloud-based dictation workflow solution SpeechLive, which amongst many features, offers a professional transcriptionist service. The included Philips docking station provides high-speed wireless charging under three hours and instant pairing between your device and the docking station. The charged device can easily be used for up to 23 hours. This is especially perfect for heavy users in hospitals, legal practices and other roles which carry a high administrative burden adds Dr Brauner. With an optional foot control connected, the docking station even enables hands-free recording. Philips SpeechMike Premium Air will be available from Summer 2017. For more information on Philips dictation solutions, please visit: www.philips.com/dictation Speech Processing Solutions is the global leader in professional dictation solutions. The company was founded in 1954 in Austria as a Philips subsidiary, and has been a driving force for innovative speech-to-text solutions for 60 years. The company developed ground-breaking products such as the mobile Philips SpeechAir , the Philips Pocket Memo voice recorder , the Philips SpeechMike Premium USB dictation microphone and the Philips Dictation Recorder app for smartphones, thus meeting its demands for excellence and superior quality. Thanks to the recent innovation, Philips SpeechLive , dictations and recordings will become faster and easier than ever before with cloud-based workflow services. Speech Processing Solution's perfectly tailored offers and products help professionals save time and resources and maximize efficiency. Connect with Speech Processing Solutions on: ",http://finance.yahoo.com/news/philips-speechmike-premium-air-revolutionary-070200357.html,The device can be used with the latest Philips SpeechExec Pro software with integrated speech recognition or with the Philips' cloud-based ...,Philips SpeechMike Premium Air: Revolutionary new dictation ...,SpeechMike Premium Air Business Wire Tuesday April Speech Processing Solutions Philips SpeechMike Premium Air News Release SpeechMike Premium Philips SpeechMike Premium Dr Thomas Brauner CEO Speech Processing Solutions State Lossless Dr Brauner Philips SpeechMike Premium Air Philips SpeechExec Pro Philips SpeechLive Philips Dr Brauner Philips SpeechMike Premium Air Philips Speech Processing Solutions Austria Philips Philips SpeechAir Philips Pocket Memo Philips SpeechMike Premium USB Philips Dictation Recorder Philips SpeechLive Speech Processing Solution Speech Processing,5
62,"Answer by Sunit Sivasankaran , Research Engineer, on Quora : Why isn't speech recognition software more accurate This is an excellent question to start off an automatic speech recognition (ASR) interview. I would slightly rephrase the question as ""Why is speech recognition hard"" The reasons are plenty and here is my take on the topic: An ASR is just like any other machine learning (ML) problem, where the objective is to classify a sound wave into one of the basic units of speech (also called a ""class"" in ML terminology), such as a word. The problem with human speech is the huge amount of variation that occurs while pronouncing a word. For example, below are two recordings of the word ""Yes"" spoken by the same person ( wave source: AN4 dataset [1] ). It can easily be seen that the signals differ and the same can be verified by analyzing it in frequency or time-frequency domain. Comparison of two different recording of the word ""Yes"" in the time domain . There are several reasons for this variation, namely stress on the vocal chords, environmental conditions, and microphone conditions, to mention a few. To capture this variation, ML algorithms such as the hidden Markov model (HMM)[2] along with Gaussian mixture models are used. More recently, deep neural networks (DNN) have been shown to perform better. One way to do ASR is to train ML models for each word. During the training phase, the speech signal is broken down into a set of features (such as Mel frequency cepstral coefficients, or MFCC for short) which are then used to build the model. These models are called acoustic models (AM). When a speech signal has to be ""recognized"" (testing phase), features are again extracted, and are compared against each word model. The signal is assigned to represent the word, which has the highest probability value. This way of doing ASR works pretty well for small vocabularies. When the number of words increases, we end up comparing with a very large set of models, which is computationally not feasible. There is another problem of finding enough data to train these models. The word model fails for large vocabulary continuous speech recognition tasks due to the high complexity involved in decoding as well the need for the high amounts of training data. To overcome this problem, we divide words into smaller units called phones. In the English language (and many Indian languages), there are approximately fifty phones that can be combined to make up any word. For example the word ""Hello"" can be broken in to ""HH, AH, L, OW"". You can look up the CMU pronunciation dictionary [6] for phonetic expansion of English words. The problem of ASR boils down to recognizing the phone sequence instead of a word. This requires building ML models for every phone. These models are called Monophone models. If you can do a good job of recognizing the phones, you have solved a big part of the ASR problem. Unfortunately, recognizing phones is not an easy task. If we plot the Fourier spectrum of a phone utterance, distinct peaks are visible, as can be seen in this plot : Formant Frequencies [5]. The peak frequencies F1 and F2 are key indicators of a phone. A scatter plot of the vowels with respect to F1 and F2 is shown here . As can be seen, the spread is large and very often overlaps with one another. Variation of dominant frequencies between vowels. No clear boundaries can be drawn to differentiate the vowels [3]. This overlap makes it hard for a ML algorithm to distinguish between phones. Another problem with monophones is that they are often influenced by the neighboring phones. This figure shows the time domain as well as the time-frequency domain (STFT) representation of a speech utterance ""Heel"". Time domain and STFT representation of the word ""Heel"" [ Page on upenn.edu ]. The word heel can phonetically be expanded as ""HH IY L"". The influence of the phone ""HH"" on ""IY"" can clearly be seen in the figure. Triphone models, also called context dependent models, are proposed as solutions to model this context dependency. Here models are built for every possible variation of the triphone, with the hope that they capture enough contextual variations. The possible count of such triphones would be in the order of 50^3, which is a very high number. Building such large numbers of models is, again, not feasible. Fortunately, not all triphones occur in the English language (other languages also). After a few smart tricks, we can reduce the number of classification units to be in the range of 5,000-10,000. Even with good phoneme recognition, it is still hard to recognize speech. This is because the word boundaries are not defined beforehand. This causes problems while differentiating phonetically similar sentences. A classic example for such sentences are ""Let's wreck a nice beach"" and ""Let's recognize speech"". These sentences are phonetically very similar and the acoustic model can easily confuse between them. Language models (LM) are used in ASR to solve this particular problem. Another factor which bugs an ASR system is an accent. Just like humans, machines too have a hard time understanding the same language with different accents. See this video for an example. This is because the classification boundaries previously learnt by a system for a particular accent do not stay constant for other accents. This is the reason why ASR systems often ask for your location/speaking style (English-Indian, English-US, English-UK, for example) during the configuration process. The complexities described so far are part of natural speech. Even with such large complexities, recognizing speech in noiseless environments is generally considered a solved problem. It is the external influence such as noise and echos which are bigger culprits. Noise and echoes are unavoidable interference while recording audio. Echoes happen due to the reflections of speech energy from surfaces such as walls, mirrors, and tables. This is not much of a problem when a speaker is speaking close to the microphone. But when spoken from a distance (making a purchase through Amazon Echo, for example), multiple copies of the same signal are reflected and combined at different time delays and intensities. This will result in the stretching of phones across time and will end up corrupting the neighboring speech information. This phenomenon is called as smearing. The process of removing the smear is called dereverberation, which is a commonly used technique to address the reverberation problem. An other problem of note in ASR is during the decoding stage. Here, the LM and AM are combined to form a huge network. Recognition is basically a search problem in such a big space. The bigger the space, the bigger the problem. Realtime recognition involves scanning the network using Viterbi algorithms to obtain the transcription of the speech signal. ",http://www.huffingtonpost.com/quora/why-is-speech-recognition_b_13873978.html,Why isn't speech recognition software more accurate? This is an excellent question to start off an automatic speech recognition (ASR) interview ...,Why Is Speech Recognition Technology So Difficult to Perfect?,Sunit Sivasankaran Research Engineer ASR ASR ML ML AN4 Comparison ML Markov HMM DNN ASR ML Mel MFCC AM ASR Hello HH AH L OW CMU English ASR ML Monophone ASR Fourier F1 F2 F1 F2 ML STFT Heel Time STFT Heel HH IY L HH IY Building Let Let LM ASR ASR Just ASR English-UK Echoes Amazon Echo ASR LM AM Realtime Viterbi,-1
63,"This option sends Microsoft data critical for the operation, updating, and improvement of the Windows 10 operating system. The Full telemetry option expands the amount of data Windows 10 collects about the user to include content consumption habits, search history, download URLs, and more. Below is a summary of diagnostic categories and data types collected by Windows 10 at the Full telemetry level: Device, Connectivity, and Configuration  Hardware specifications, local network information, and peripheral specifications. Product and Service Usage  App interaction data, in-app content searches, favourites, and image/video editing app data. Product and Service Performance  Crash dumps, device responsiveness, and disk footprint. Software Setup and Inventory  Installed application data and Windows update information. Content Consumption  Video performance, music streaming URLs, photo resolutions, time spent reading content. Microsoft Browser and Search  Auto-completed text, URLs, search terms, and page titles. Inking Typing and Speech Utterance  Speech recognition success rate and pen gesture recording. Licensing and Purchasing  Purchase history, store history, and OS licence type. Windows 10 collects the data above in addition to that listed under the Basic telemetry level. Users can switch between Full and Basic diagnostics in the Windows 10 privacy settings section, as shown below. This article first appeared on MyBroadband and is republished with permission. ",https://mygaming.co.za/news/features/116703-here-is-all-the-data-windows-10-collects-about-you.html,Inking Typing and Speech Utterance – Speech recognition success rate and pen gesture recording. Licensing and Purchasing – Purchase ...,Here is all the data Windows 10 collects about you,Microsoft Windows Full Windows URLs Below Windows Full Device Connectivity Configuration Hardware Product Service Usage App Product Service Performance Crash Software Setup Inventory Installed Windows Consumption Video URLs Microsoft Browser Search Auto-completed URLs Speech Utterance Purchase OS Basic Full Basic Windows MyBroadband,2
64,"Weve reached human parity, said Xuedong Huang, the companys chief speech scientist. This is a historic achievement. After decades of testing, the milestone comes on the heels of last months close but no cigar score of 6.3 WER and figures to have wide-reaching implications as the battle for digital assistant supremacy heats up. Cortana , Xbox , and Windows could see the biggest initial impact. To achieve these levels of accuracy, researchers employed deep neural networks to store significant amounts of data  called training sets  that helped systems recognize patterns from human input. Sounds and images were both used to train the network to utilize its stored data more efficiently. Researchers want to be clear that parity is far from perfection. In this case, it just means its as good as humans, and were far from flawless. Moving forward, the team hopes to achieve even higher levels of accuracy as well as ensure that speech recognition works better in real-world situations, such as noisy restaurants, crowded streets, and in powerful winds.In the future, the team dreams of a system that will not just recognize speech, but truly understand it. Were still a ways off, but the future consists of a world where we no longer have to understand computers, they have to understand us. ",https://thenextweb.com/microsoft/2016/10/18/microsofts-speech-recognition-is-now-just-as-accurate-as-humans/,"Moving forward, the team hopes to achieve even higher levels of accuracy as well as ensure that speech recognition works better in real-world ...",Microsoft's speech recognition is now just as accurate as humans,Weve Xuedong Huang WER Cortana Xbox Windows Researchers Were,-1
65,"Zion Market Research published new report on ""Microphone Market: Global Industry Analysis, Size, Share, Growth, Trends, and Forecasts 20162024"" in its database. Sarasota, FL -- ( SBWIRE ) -- 04/10/2017 --  Global Microphone Market: Overview Microphones are utilized in various applications such as hearing aids, telephones, public events, public address systems for recorded audio engineering and concert halls live, motion picture production, two-way radios, sound recording, radio and television broadcasting, megaphones, in computers for speech recognition, recording voice, VoIP. The global microphone market is likely to have a huge elevation in coming future. Increasing demand from the industry is the main reason that has driven the growth of the global market. Besides, use of advanced technology in the production of the microscope is also boosting the growth of the global microphone market. Furthermore, rapid urbanization and industrialization are also driving the growth of the global market. In addition to this, growing awareness about the advantages of microphones is also lowering the growth of the global microphone market. Increased use of microphones in various applications is also one of the main reasons that are towering the global market. On the other hand, high cost for production of microphones is the main reason hampering the growth of global microphone market. Lack of job opportunities is one of the reasons that are decreasing the growth of the global market. Moreover, high cost of transportation is also one of the significant reasons that are restraining the growth of the global market. Similarly, high cost for raw materials is also one of the significant reasons that are restraining the growth of the global market. Last but not the least, high labor charge is also helping in the decrement of the global market. Based on the type, the global microphone market is mainly classified as MEMS based microphones, electrets, and other microphones. On the basis of applications, the global market is divided into commercial security & surveillance, automotive, industrial, consumer electronics, sensing applications, medical, and others. Based on the geographical area, the global market for the microphone is segmented into Europe, Latin America, North America, and the Middle East and Africa. Regionally, the global market for the microphone is significantly divided into Asia Pacific, the Middle East and Africa, and North America. Increasing demand from the industry is boosting the global market in the North America. Rising awareness about the advantages of microphones is towering the global market in Asia-Pacific countries such as Japan, India, and China. Rapid urbanization and industrialization will boost the global market in Germany and France. The Rest of the World will witness a slow growth in the global market in the coming future. Key players in the global microphone market are Robert Bosch GmbH, AAC Technologies Holdings Inc., Goertek, BSE, Cirrus Logic Inc., InvenSense, Inc., Hosiden Corporation, STMicroelectronics N.V., Knowles Corporation, and Shandong Gettop Acoustic Co. Ltd. Zion Market Research is an obligated company. We create futuristic, cutting edge, informative reports ranging from industry reports, company reports to country reports. We provide our clients not only with market statistics unveiled by avowed private publishers and public organizations but also with vogue and newest industry reports along with pre-eminent and niche company profiles. Our database of market research reports comprises a wide variety of reports from cardinal industries. Our database is been updated constantly in order to fulfill our clients with prompt and direct online access to our database. Keeping in mind the client's needs, we have included expert insights on global industries, products, and market trends in this database. Last but not the least, we make it our duty to ensure the success of clients connected to usafter allif you do well, a little of the light shines on us. For more information on this press release visit: http://www.sbwire.com/press-releases/microphone-market-size-share-analysis-and-global-forecast-2016-2024-792586.htm ",http://www.digitaljournal.com/pr/3301736,"... radios, sound recording, radio and television broadcasting, megaphones, in computers for speech recognition, recording voice, VoIP.","Microphone Market Size, Share, Analysis and Global Forecast, 2016 ...",Market Research Analysis Size Share Growth Trends Sarasota FL SBWIRE Overview Microphones VoIP Besides Furthermore Lack MEMS Europe Latin America North America Middle East Africa Asia Pacific Middle East Africa North America North America Japan India China Rapid Germany France Rest World Key Robert Bosch GmbH AAC Technologies Holdings Inc. Goertek BSE Cirrus Logic Inc. InvenSense Inc. Hosiden Corporation STMicroelectronics N.V. Knowles Corporation Shandong Gettop Acoustic Co. Ltd. Zion Market Research,3
66,"BigHand, a software development company providing speech, workflow and document creation tools to law firms, today announced an integration between their digital dictation software version 5.0.5 and template management solution, BigHand Create version 7.5. The integration will reduce the number of clicks and simplify the process of producing legal documents for both attorneys and support staff, enabling them to dictate, proofread and generate finished documents directly within the BigHand system. BigHand is best known for offering voice solutions, including digital dictation, speech recognition and mobility apps, and has expanded their offering to include workflow and document creation tools that enable firms to streamline even more processes. BigHand's modules work independently or in conjunction with one another, to give clients the benefit of a multi-faceted solution that is highly configurable to suit their needs. The decision to further integrate the voice and template management tools was the next logical step. Eric Wangler, President, BigHand North America, comments, ""The integration between BigHand Voice and BigHand Create will make it much simpler for users to create documents from dictations. The process of typing up a dictation or proofreading text transcribed by speech recognition is the same as before, but the steps required to transform that text into a great looking, finished document are simpler and more user-friendly. With this integration, users will be able to launch Microsoft Word templates directly within BigHand, whether it be a pleading, a letter, or any other document, and the text is automatically carried over. Any editing, including with the BigHand Hyperstyles formatting tools, can be completed within Microsoft Word."" For a firm already using BigHand Digital Dictation and Speech Recognition, the integration will automate the majority of the back-end document production process. Wangler continues, ""This is a really exciting prospect for law firms producing hundreds of documents a day. The time involved in creating so many documents and the pressure to be highly productive means people often take shortcuts, like starting a new document by working directly over an old one. This can cause complex document corruptions, which can take even longer to fix. By automating a large portion of the administrative work, and lifting the burden from their staff, firms can ensure all documents are produced to a consistently high quality, while freeing up administrative time for higher-value tasks."" For more information on the integration between BigHand Voice and BigHand Create, please call 951 506 5641 or email inquiry(at)bighand(dot)com. For the original version on PRWeb visit: http://www.prweb.com/releases/2017/03/prweb14194339.htm ",https://www.benzinga.com/pressreleases/17/03/p9234390/bighand-announces-voice-and-template-management-integration-to-revoluti,"BigHand is best known for offering voice solutions, including digital dictation, speech recognition and mobility apps, and has expanded their ...",BigHand Announces Voice and Template Management Integration ...,BigHand BigHand Create BigHand BigHand BigHand Eric Wangler President BigHand North America BigHand Voice BigHand Create Microsoft Word BigHand BigHand Hyperstyles Microsoft Word BigHand Digital Dictation Wangler BigHand Voice BigHand Create PRWeb,-1
67,"Imagine Clippy dishing out sarcastic headlines. Who wouldn't want that 17 Mar 2017 at 08:32, Tim Anderson Hands on Microsoft has invested big in its Cognitive Services for programmable artificial intelligence, along with a Bot Framework for using them via a conversational user interface. How easy is it to get started Cognitive Services , the AI piece, was announced at the companys Build developer conference in April 2015. The initial release had just four services: face recognition, speech recognition, visual content recognition, and language understanding. That has now been extended to over 20 APIs. Note that Cognitive Services, which are pre-baked specialist APIs, are distinct from Azure Machine Learning, which lets you do generalized predictive analytics based on your own data. A year or so later, at the March 2016 Build event, Microsoft announced the Bot Framework , for building a conversational user interface (still in preview). This links naturally to Cognitive Services since a bot needs some sort of language parsing service. Both services are included (along with a bunch of other stuff) in Microsofts overall Machine Learning and AI offering, called Cortana Intelligence Suite. At the recent QCON software development conference in London, Microsofts exhibition stand was focused entirely on Cognitive Services, and it gave a couple of presentations (albeit in the sponsored track) on the subject, though not without glitches. I dont know why it hasnt picked up Seattle as a place, said the presenter. Note that both the Bot Framework and the Language Understanding Intelligent Service (LUIS) are still in preview. The main use cases for bots are for sales and customer service. Actions like booking travel or appointments, searching for hotels, and reporting faults are suitable. In most cases interacting with a human is preferable, but also more expensive. Another argument is that the popularity of messaging services means that it pays to have an integrated presence there. How hard is it to build a bot on Microsofts platform I sat down to build a Reg bot. In this case the main service is to offer content, so to keep things simple I decided the bot should simply search the site for material in response to a query. There are several moving parts: LUIS: Your bot has to send text to LUIS for interpretation, which means you have to create and publish a LUIS app. Bot Framework: Microsofts cloud service provides the channels your bot uses to communicate. There are currently 11 channels, including Skype, Facebook Messenger, web page widgets, Direct Line (a REST API direct to your bot), Slack, Microsoft Teams, and SMS via Twilio. Bing Search API: The bot has to know how to search the Register site; using a search API is the quickest way. Hosting: A bot is itself a web service, and you have to host your bot somewhere. The tools lead you towards Microsoft Azure, but anywhere that can host an ASP.NET Web API application should do. Your LUIS app also has to be hosted, only on Azure. RegBot uses a free Cognitive Services account and the lowest paid-for web app hosting service. It makes sense to start with LUIS rather than running up Visual Studio immediately. LUIS is a service that accepts a string of text and parses it into an Intent, along with one or more Entities. You can think of an Intent as a verb and an Entity as a noun. RegBot currently has two Intents, DoSearch and Help, and one Entity, TechSubject. You set up your LUIS app by typing example text strings that match your Intent and tagging them with their Entities. So Tell me about malware becomes Intent: DoSearch TechSubject: Malware. You can test your LUIS app on the page. Once you have done the LUIS bit you can get going with the code. I found and installed a Visual Studio Bot Application template, started a new project, restored Nuget packages (which download libraries from Microsofts repository), and got an error: The name GlobalConfiguration does not exist. A quick search told me to add the WebHost package. That is how development is today; you mix up various pre-built pieces and hope they get along. Unfortunately, the Bot Application template is not pre-configured for LUIS. My approach was to find another bit of sample code which includes LUIS support and borrow a few pieces from it. I also downloaded the Bot Framework Emulator, which lets you test your bot locally. I messed around with various App IDs and secret keys to hook up my bot to the LUIS app. A key feature of the Bot Framework is that it keeps track of your conversation by means of a context object, so that your app is able to interact with the user. RegBot does not need much interaction, but to test this I wrote code that asks the user how many results they want to see. It does this with one line of code: AfterDoSearch: This is the name of the method which gets called when the user responds. Each type of interaction therefore needs a separate method. This wrapping means state management is taken care of for you  a substantial benefit. Getting the Bing Search API working took more time than expected. It turns out that a News Search works better than a Web Search, since it has a useful Description field. I also spent time working out how the JSON response was structured; either I missed it, or Microsoft could do with some more basic samples. I got it working, connected RegBot to Skype, and successfully tested my bot. Publishing it to the wide world involves a few more steps, so not yet. Afew thoughts though. This can work well for simple, well-defined use cases. LUIS is a bit of a black box, but has the advantage that you can see when it goes wrong and try to fix it. Once your app is up and running, it is easy to modify. The ability to code your own bot with a few hours of work is impressive. That said, none of it is very sophisticated. Throw anything more than a short, simple sentence at LUIS, and it will quickly get confused or give up. It would not be difficult to add speech recognition and text-to-speech via yet more Cognitive Services, though in the case of RegBot its not much use unless it also read web content back to you. I came into this project as a bot sceptic. It is clever, but not clever enough to be useful other than in a few niche cases, or to hand over to a human after collecting some basic information. You can imagine eyes lighting up at the thought of replacing call center staff with a few lines of code. Now it is simple to run up a prototype showing why, most of the time, this is probably not a good idea. More positively, the bot concept is a newish way to interact with users: one that is amenable to voice and therefore handy for in-car use or other scenarios where typing is difficult. It does not feel ready yet, particularly in the case of the difficult AI piece, but give it time.  ",https://www.theregister.co.uk/2017/03/17/microsofts_bot_framework_and_cognitive_services/,"The initial release had just four services: face recognition, speech recognition, visual content recognition, and language understanding.",Hell freezes over: We wrote an El Reg chatbot using Microsoft's AI,Imagine Clippy Who Tim Anderson Hands Microsoft Cognitive Services Bot Framework Cognitive Services AI Build April APIs Cognitive Services APIs Azure Machine Learning March Build Microsoft Bot Framework Cognitive Services Microsofts Machine Learning AI Cortana Intelligence Suite QCON London Microsofts Cognitive Services Seattle Bot Framework Language Understanding Intelligent Service LUIS How Microsofts Reg LUIS LUIS Bot Framework Microsofts Skype Facebook Messenger Direct Line API Slack Microsoft Teams SMS Twilio Bing Search API Register API Microsoft Azure ASP.NET Web API LUIS Azure RegBot Cognitive Services LUIS Studio LUIS Intent Intent Entity RegBot DoSearch Help TechSubject LUIS Intent Malware LUIS LUIS Studio Bot Application Nuget Microsofts GlobalConfiguration WebHost Bot Application LUIS LUIS Bot Framework Emulator App IDs LUIS Bot Framework RegBot Bing Search API News Search Web Search Description JSON Microsoft RegBot Skype Afew LUIS LUIS Services RegBot AI,-1
68,"The tidal wave of fake news spreading across the web has brought mounting pressure on Google and Facebook to face up to their responsibilities as platforms for false information. But now the two web giants are under pressure over another kind of fakery - fake advertising. On this week's Tech Tent we hear about the advertising industry's mounting anger over a problem that is damaging its credibility with its clients. When advertising began to move online, there was the promise of much better targeting and much more accurate measurement of how well a marketing message performed. Instead all sorts of issues, from bots that generate phony views of ads to the placing of advertisements next to unsuitable content, have shaken confidence in the industry. A media industry conference in London this week heard a prediction that out of $80bn (65bn) of digital ad spending in 2017, over $16bn would be eaten up by fraud. Johnny Hornby, founder of a media agency called The&Partnership, tells Tech Tent that the time has come for a radical response. He says a tool called pre-bid verification, which checks sites to make sure they are not fraudulent and allows agencies to make sure they are not placing ads where they will generate phony clicks, has been shown to wipe out most fraud. But he says both Google and Facebook have refused to let this software on their platforms. Hornby is calling for the industry to unite and threaten a boycott if the two web giants, which account for two-thirds of online advertising, don't shape up. Google told us it did not allow third-party software on to its services but argued that it was a leader in combating ad fraud: ""Last year, we took down almost seven million bad ads for intentionally attempting to trick our detection systems,"" a spokesman said. Of course, its online advertising and the revenue it can produce is also driving the creation of fake news sites. But the digital media commentator Prof Jeff Jarvis tells us that journalists must share the blame with the likes of Google and Facebook for this phenomenon. Indeed he thinks the old media, which treated its audience as one giant indistinguishable mass, can learn from the web giants how to relate to people on an individual level. But he does believe the platforms have a duty to show their users more about the sources of information and to try to understand how they are being manipulated. Two other big tech trends on this week's programme - driverless cars and speech recognition. Prof Paul Newman is the founder of Oxbotica, which is creating operating systems for autonomous vehicles. He talks to us about the contrast between the rapid advance in the technology behind driverless cars and the slow progress of the infrastructure - physical, social, and legal - which will be needed to support them. And then we talk about how quickly we've moved in getting machines to understand the human voice, with Dan Faulkner of Nuance, a company that has been a pioneer in speech recognition. He tells us that now machines can understand what we say and respond to our instructions, it's time to move to the next stage - ""enabling machines to actually drive a conversation and take it in a direction which makes sense to them"". In other words, one day soon your smartphone could start asking you questions or perhaps giving you instructions. Are you ready for Siri or Alexa to tell you to empty the dishwasher and put the dinner on I'm not sure I am ",http://www.bbc.com/news/technology-39309088,"Two other big tech trends on this week's programme - driverless cars and speech recognition. Prof Paul Newman is the founder of Oxbotica, ...","Tech Tent: Fake ads, fake news and real voice tech",Google Facebook Tech Tent London Johnny Hornby Partnership Tech Tent Google Facebook Hornby Google Prof Jeff Jarvis Google Facebook Prof Paul Newman Oxbotica Dan Faulkner Nuance Siri Alexa,-1
69,"Essential Retail takes a look at the development of voice technology and how consumers are increasingly using devices like Amazon's Alexa and Google Home to shop with their voices. Published: 08:00:00 on the 7th Apr 2017 Author: Caroline Baldwin Fifty years ago, audiences watched Captain Kirk ask the Starship Enterprise computer for information using only his voice. While voice technology was seen as something that belonged in sci-fi, it was the ultimate aim for ease of communication. And just like drones, robotics and artificial intelligence, today we are seeing future technologies become a reality and voice-activated intelligent assistants are entering consumers' homes in the form of Apple's Siri, Amazon's Alexa and Google Home  which launched in the UK only this week. Voice is not completely new, consumers may remember using their smartphones to ask Siri for the weather or to tell Microsoft's Cortana a joke, but the results were often laughable. Even now 45% of people only use voice ""because it's fun"", according to a report released this week by JWT and Mindshare. The JWT and Mindshare 'Speak Easy' report states 87% of regular voice users believe the technology simplifies their lives, but only when it works properly. Duncan Anderson, European CTO at IBM Watson , says every decimal point matters: ""5.1% and 5.9% is everything  it's the difference between people thinking it's great and people thinking it's rubbish. A few percentage points is a massive achievement, it sounds little but it's everything."" Today's consumers want to interact with technology and information in all parts of their lives, but traditional screen interfaces are not always appropriate  in the car, while watching TV or using a wearable device  this is where experts think voice can step in to become the natural way to communicate with our surroundings. Not to mention the benefits of not constantly craning our necks to look down at our iPhones: 44% of smartphone users believe voice technology will help people interact more with each other, never mind technology. But it's exactly that lack of screen with home voice devices like Amazon's Alexa and Google Home, which deters consumers from using the products for shopping. While Amazon would like to see its users ask Alexa to order every item that comes to mind, only 18% of regular voice users have bought a product using their vocal chords, without looking at the product online first. While 24% of consumers have bought a product using their voice after seeing it elsewhere. One voice shopper in the 'Speak Easy' report said: ""I don't trust it that much and I always log in on my phone to check if the right item has been ordered."" But Jeremy Pounder, futures director at Mindshare, tells Essential Retail the intelligence of digital assistants will continue to improve, but in the meantime, voice is ideal for repeat purchases, like 'Alexa, order my toothpaste' or 'Siri, place my usual order from Deliveroo'. Moving forward, IBM's Anderson believes home devices will soon be manufactured with a screen. And a quick Google search reveals rumours that the next Amazon Echo device will feature a touchscreen which would help convert voice users into voice shoppers. ""Voice is an isolated channel, but it's evolving to be part of an ecosystem,"" he explains. Anderson describes how the first step for the big voice players in the market was to develop cheaper devices  without touchscreens  to prove an appetite for the technology, which there certainly seems to be with Comscore now forecasting 50% of all searches will be conducted through voice by 2020. A screen interface would also solve the problem of traditional advertising models struggling to monetise voice technology. Only last month, Google Home was left red faced after it informed users simply wanting to know the weather forecast or receive a traffic update about the new live-action film, Beauty and the Beast. Google Home users were outraged and Google quickly removed the ad. Results from the Speak Easy report unsurprisingly found voice advertisements jarring to the conversation consumers are having with their digital assistant. Joseph Evans, senior researcher at Enders Analysis , points to a big shift away from paid media to owned media. Instead the voice apps  such as the skills in Amazon's Alexa ecosystem  are primed to be sponsored by brands, such as a cleaning skill in the US , giving consumers step-by-step stain removal instructions, which is sponsored by Tide. He says: ""If an advertisement interrupts the workflow of doing something, that's where this is no place for ads."" Amazon finally brings its Echo devices and cloud-based intelligent service, Alexa, to the UK. Google launches its voice-activated intelligent assistant in the UK. Kantar Retail's Alvaro Morilla shares with Essential Retail his experience of living with Amazons... ",http://www.essentialretail.com/features/article/58e5fa7c536f7-voice-technology-the-new-frontier,"But today, advancements in voice recognition and natural language processing have improved dramatically and speech recognition error rates ...",Voice technology: the new frontier,Retail Amazon Alexa Google Home Apr Caroline Baldwin Fifty Captain Kirk Starship Enterprise Apple Siri Amazon Alexa Google Home UK Voice Siri Microsoft Cortana JWT Mindshare JWT Mindshare 'Speak Easy Duncan Anderson CTO IBM Watson Amazon Alexa Google Home Amazon Alexa Easy Jeremy Pounder Mindshare Retail Deliveroo IBM Anderson Google Amazon Echo Voice Anderson Comscore Google Home Beauty Beast Google Home Google Speak Easy Joseph Evans Enders Analysis Amazon Alexa US Tide Amazon Echo Alexa UK Google UK Kantar Retail Alvaro Morilla Essential Retail,4
70,"A new study by the Hackett Group shows that in 2017, HR will continue overhauling delivery of its services, but with flat or decreased budgets. Most organizations will continue upgrading their capabilities, per the report, but some of the most critical issues wont be addressed during the year. The Hackett Group says most HR professionals arent up to the task of aligning talent strategies with business needs, handling talent skills shortages and executing organizational change. Addressing skills shortages, retaining key staff and strategizing wont get the attention they need. The study also found that most HR executives recognize the impact technology will have on their profession but don't think their organizations have the resources or strategies to make upgrades. HR executives might be operating with fewer resources, as the Hackett Group found, but their priorities and understanding of talent and technology strategies are clear, and their determination to make changes when possible is evident. It's going to take robust people analytics to demonstrate to leadership why such changes are important. Half of the HR executives in a recent IBM survey recognize how artificial intelligence  including machine learning, computer vision, speech recognition and natural language processing will impact HR functions and strategies. HR executives also understand that while robotics wont replace humans , it can make tasks more efficient and cost-effective. Meanwhile, industry leaders are already initiating strategies aimed at closing the skills gap . A collaborative study by LinkedIn and Whiteboard Advisors showed that 71% of the survey respondents believe partnering with an outside trainer can best close the skills gap. Most of the respondents (60%) think employers will move from basing hiring on candidates educational backgrounds to the skills theyve acquired, more than half of those polled (57%) will add more educational benefits, such as debt reduction programs , than other types of benefits to their plan offerings. ",http://www.hrdive.com/news/hackett-group-hr-may-not-have-resources-to-address-critical-issues-in-2017/439758/,"... machine learning, computer vision, speech recognition and natural language processing — will impact HR functions and strategies.",Hackett Group: HR may not have resources to address critical issues ...,Hackett Group HR Hackett Group HR HR HR Hackett Group HR IBM HR HR LinkedIn Whiteboard Advisors,-1
71,"Amazon is the public cloud leader right now, but each company has its strengths. Democratising access to powerful AI software is the latest battleground, and could decide which tech giant emerges as the ultimate winner in a cloud infrastructure market worth US$25bil (RM110.87bil) this year, according to researcher IDC.  Bloomberg Back in October, Deschutes Brewery Incs Brian Faivre was fermenting a batch of Obsidian Stout in a massive tank. Something was amiss; the beer wasnt fermenting at the usual temperature. Luckily, a software system triggered a warning and he fixed the problem. We would have had to dump an entire batch, the brewmaster said. When beer is your bottom line, that's a calamity. The software that spotted the temperature anomaly is from Microsoft Corp and it's a new type that uses a powerful form of artificial intelligence called machine learning. What makes it potentially revolutionary is that Deschutes rented the tool over the internet from Microsoft's cloud-computing service.  Day to day, Deschutes uses the system to decide when to stop one part of the brewing process and begin another, saving time while producing better beer, the company says. The Bend, Oregon-based brewer is among a growing number of enterprises using new combinations of AI tools and cloud services from Microsoft, Amazon.com Inc and Alphabet Inc's Google. C-SPAN is using Amazon image-recognition to automatically identify who is in the government TV programmes it broadcasts. Insurance company USAA is planning to use similar technology from Google to assess damage from car accidents and floods without sending in human insurance adjusters. The American Heart Association is using Amazon voice recognition to power a chat bot registering people for a charity walk in June. AI software used to require thousands of processors and lots of power, so only the largest technology companies and research universities could afford to use it. An early Google system cost more than US$1mil (RM4.43mil)and used about 1,000 computers. Deschutes has no time for such technical feats. It invests mostly in brewing tanks, not datacentres. Only when Microsoft, Amazon and Google began offering AI software over the Internet in recent years did these ideas seem plausible.  Amazon is the public cloud leader right now, but each company has its strengths. Democratising access to powerful AI software is the latest battleground, and could decide which tech giant emerges as the ultimate winner in a cloud infrastructure market worth US$25bil (RM110.87bil) this year, according to researcher IDC. There's a new generation of applications that require a lot more intense data science and machine learning. There is a race for who is going to provide the tools for that, said Diego Oppenheimer, chief executive officer of Algorithmia Inc, a startup that runs a marketplace for algorithms that do some of the same things as Microsoft, Amazon and Google's technology. If the tools become widespread, they could transform work as more automation lets companies get more done with the same human work force.  C-SPAN, which runs three TV stations and five web channels, previously used a combination of closed-caption transcripts and manpower to determine when a new speaker started talking and who it was. It was so time-consuming, the network only tagged about half of the events it broadcast. C-SPAN began toying with Amazon's image-recognition cloud service the same day it launched, said Alan Cloutier, technical manager for the network's archives. Now the network is using it to match all speakers against a database it maintains of 99,000 government officials. C-SPAN plans to enter all the data into a system that will let users search its website for things like Bernie Sander's healthcare speeches or all times Devin Nunes mentions Russia.  As companies try to better analyse, optimise and predict everything from sales cycles to product development, they are trying AI techniques like deep learning, a type of machine learning that's produced impressive results in recent years. IDC expects spending on such cognitive systems and AI to grow 55% a year for the next five years. The cloud-based portion of that should grow even faster, IDC analyst David Schubmehl said.  In the fullness of time deep learning will be one of the most popular workloads on EC2, said Matt Wood, Amazon Web Services' general manager for deep learning and AI, referring to its flagship cloud service, Elastic Compute Cloud. Pinterest Inc. uses Amazon's image-recognition service to let users take a picture of an item  say a friend's shoes  and see similar footwear. Schools in India and Tacoma, Washington, are using Microsoft's Azure Machine Learning to predict which students may drop out, and farmers in India are using it to figure out when to plant peanut crops, based on monsoon data. Johnson & Johnson is using Google's Jobs machine-learning algorithm to comb through candidates' skills, preferences, seniority and location to match job seekers to the right roles. Google is late to the public cloud business and is using its AI experience and massive computational resources to catch up. A new ""Advanced Solutions Lab lets outside companies participate in training sessions with machine-learning experts that Google runs for its own staff. USAA was first to participate, tapping Google engineers to help construct software for the financial-services company. Heather Cox, USAA's chief technology officer, plans a multi-year deal with Google. You can build software that is cognitive  that can sense emotion and understand your intent, recognise speech or whats in an image  and we provide all of that in the cloud so customers can use it as part of their software, said Microsoft vice president Joseph Sirosh. Amazon, in November, introduced similar tools. Rekognition tells users what's in an image, Polly converts text to human-like speech and Lex  based on the company's popular Alexa service  uses speech and text recognition for building conversational bots. It plans more this year. Chris Nicholson, CEO of AI company Skymind Inc., isnt sure how large the market really is for AI in the cloud. The massive data sets some companies want to use are still mostly stored in house and it's expensive and time-consuming to move them to the cloud. Its easier to bring the AI algorithms to the data than the other way round, he said.  Amazon's Wood disagrees, noting healthy demand for the company's Snowball appliance for transferring large amounts of information to its data centers. Interest was so high that in November Amazon introduced an 18-wheeler truck called Snowmobile that can move 100 petabytes of data. Microsoft's Sirosh said the cloud can be powerful for companies that don't want to invest in the processing power to crunch the data needed for AI-based apps.  Take Norwegian power company eSmart Systems AS, which developed drones that photograph power lines. The company wrote its own algorithm to scan the images for locations that need repair. But it rents the massive computing power needed to run the software from Microsoft's Azure cloud service, CEO Knut Johansen said.  As the market grows and competition intensifies, each vendor will play to their strengths.  Google has the most credibility based on tools they have; Microsoft is the one that will actually be able to convince the enterprises to do it; and Amazon has the advantage in that most corporate data in the cloud is in AWS, said Algorithmia's Oppenheimer. It's anybody's game.  Bloomberg ",http://www.thestar.com.my/tech/tech-news/2017/04/10/beer-bots-and-broadcasts-companies-start-using-ai-in-the-cloud/,C-SPAN is using Amazon image-recognition to automatically identify who ... uses speech and text recognition for building conversational bots.,"Beer, bots and broadcasts: Companies start using AI in the Cloud",Amazon AI US RM110.87bil IDC Bloomberg Back October Brewery Incs Brian Faivre Stout Something Microsoft Corp Deschutes Microsoft Day Bend AI Microsoft Amazon.com Inc Alphabet Inc Google C-SPAN Amazon Insurance USAA Google Heart Association Amazon June AI Google US RM4.43mil Deschutes Microsoft Amazon Google AI Internet Amazon AI US RM110.87bil IDC Diego Oppenheimer Algorithmia Inc Microsoft Amazon Google C-SPAN C-SPAN Amazon Alan Cloutier Bernie Sander Devin Nunes Russia AI IDC AI IDC David Schubmehl EC2 Matt Wood Amazon Web Services AI Elastic Compute Cloud Pinterest Inc. Amazon Schools India Tacoma Washington Microsoft Azure Machine Learning India Johnson Johnson Google Jobs Google AI Solutions Lab Google USAA Google Heather Cox USAA Google Microsoft Joseph Sirosh Amazon November Rekognition user Polly Lex Alexa Chris Nicholson CEO AI Skymind Inc. AI AI Amazon Wood Snowball Interest November Amazon Snowmobile Microsoft Sirosh Systems AS Microsoft Azure CEO Knut Johansen Google Microsoft Amazon AWS Algorithmia Oppenheimer Bloomberg,-1
72,"Microsoft's new speech recognition technology is able to transcribe conversational speech as well as (or even better than) humans. The technology scored a word error rate (WER) of 5.9%, which was lower than the 6.3% WER reported just last month. A study published last Monday,heralded as an historic achievement by Microsoft , details a new speech recognition technology thats able to transcribe conversational speech as well as humans  or at least, as best as professional human transcriptionists (which is better than most humans). The technology scoreda word error rate (WER) of 5.9%, which was lower than the 6.3% WER reported just last month. [I]ts the lowest ever recorded against the industry standard Switchboard speech recognition task, Microsoft reports . The rate is the same as (or even lower than)the human professional transcriptionists who transcribed the same conversation. Weve reached human parity, says Xuedong Huang, Microsofts chief speech scientist. The new technology uses neural language models that allow for more efficient generalization by grouping similar words together. The achievement comes decades after speech pattern recognition was first studied in the 1970s. With Googles DeepMind making waves in speech and image recognition (and speaking like humans do ), the technology is Microsofts timely contribution to the fast-paced artificial intelligence (AI) research and development. The achievement was unlocked using the Computational Network Toolkit , Microsofts homegrown system for deep learning. The applications for the new technology are bound to improve user experience for Microsofts personal voice assistant for Windows and Xbox One. This will make Cortana more powerful, making a truly intelligent assistant possible, says an excited Harry Shum, the executive vice president heading the Microsoft Artificial Intelligence and Research group. Of course, it will also develop better speech-to-text transcription software. Microsoft clarifies, however, that parity does not mean perfection. The computer did not recognize every word clearly, which is something not even humans could do perfectly (nor can Siri or other existing voice assistants). Impressive as it is, there remains room for improvement. The next goal: making computers understand human conversation. The next frontier is to move from recognition to understanding, says Geoffrey Zweig , Speech & Dialog research group manager. ",https://futurism.com/microsofts-speech-recognition-tech-is-officially-as-accurate-as-humans/,Microsoft's new speech recognition technology is able to transcribe conversational speech as well as (or even better than) humans.,Microsoft's Speech Recognition Tech Is Officially as Accurate as ...,Microsoft WER WER Monday Microsoft WER WER Switchboard Microsoft Weve Xuedong Huang Microsofts Googles DeepMind Microsofts AI Computational Network Toolkit Microsofts Microsofts Windows Xbox One Cortana Harry Shum Microsoft Artificial Intelligence Research Microsoft Siri Geoffrey Zweig Speech Dialog,2
73,"Robots Delight  A Lyrical Exposition on Learning by Imitation from Human-Human Interaction is a video submission that wonBest Video at the 2017 ACM/IEEE International Conference on Human-Robot Interaction (HRI 2017). The team also provides an in-depth explanation of the techniques and robotics in the video. Select video clips (c) 2016, IEEE. Reused, with permission. Although social robots are growing in popularity and technical feasibility, it is still unclear how we can effectively program social behaviors. There are many difficulties in programming social robots  we need to design hundreds or thousands of dialogue rules, anticipate situations the robot will face, handle common recognition errors, and program the robot to respond to many variations of human speech and behavior. Perhaps most challenging is that we often do not understand the reasoning behind our own behaviorand so it is hard to program such implicit knowledge into robots. In this video, we present two studies exploring learning-by-imitation from human-human interaction. In these studies, we developed techniques for learning typical actions and execution logic directly from people interacting naturally with each other. We propose that this is more scalable and robust than developing interaction logic by hand, and it requires much less effort. In the first study, we asked participants to role-play a shopkeeper and a customer in a camera shop scenario, and we recorded their motion and speech in 178 interactions. By extracting typical motion and speech actions using unsupervised clustering algorithms, we created a set of robot behaviors and trained a machine learning classifier to predict which of those actions a human shopkeeper would have performed in any given situation. Using this classifier, we programmed a Robovie robot to imitate the movement and speech behavior of the shopkeeper, e.g. greeting the customer, answering questions about camera features, and introducing different cameras. Experiment results showed that our techniques enabled the robot to perform correct behaviors 84.8% of the time, which was particularly interesting since speech recognition was only 76.8% accurate. This illustrates the robustness of the system to sensor noise, which is one advantage of using noisy, real-world data for training. In the second study, we used a similar technique to train the android ERICA to imitate peoples behavior in a travel agent scenario. In this case, the challenge was to model the topic of the interaction so that ERICA could learn to answer ambiguous questions like how much does this package cost. We did this by observing that utterances in the same topic tend to occur together in interactions, so we calculated co-occurrence metrics similar to those used in product recommendation systems for online shopping sites. Using these metrics, we were able to cluster the customer and shopkeeper actions into topics, and these topics were used to improve ERICAs predictor in order to answer ambiguous questions. In both of these studies, we illustrated a completely hands-off approach to developing robot interaction logic  the robots learned only from example data of people interacting with each other, and no designer or programmer was needed! We think scalable, data-driven techniques like these promise to be powerful tools for developing even richer, more humanlike interaction logic for robots in the future. The extended abstract for this video can be found here. For the full details of the Robovie study, please see our IEEE Transactions on Robotics paper. Phoebe Liu, Dylan F. Glas, Takayuki Kanda, and Hiroshi Ishiguro, Data-Driven HRI: Learning Social Behaviors by Example from Human-Human Interaction , in IEEE Transactions on Robotics, Vol. 32, No. 4, pp. 988-1008, 2016. ",http://robohub.org/robots-delight-japanese-robots-rap-about-their-artificial-intelligence/,"By extracting typical motion and speech actions using unsupervised ... the time, which was particularly interesting since speech recognition was ...",Robot's Delight: Japanese robots rap about their artificial intelligence,Robots Delight A Lyrical Exposition Learning Imitation Human-Human Interaction ACM/IEEE International Conference Human-Robot Interaction HRI Select IEEE Robovie ERICA ERICA ERICAs Robovie IEEE Transactions Robotics Phoebe Liu Dylan F. Glas Takayuki Kanda Hiroshi Ishiguro Data-Driven HRI Learning Social Example Human-Human Interaction IEEE Transactions Robotics Vol No,5
74,"Google has launched its voice-activated Wi-Fi speaker , Google Home, in the UK. The device, around the size of a pint glass, can respond to questions, play music and control smart home technology, all by using speech recognition. The device is a challenger to the Echo, Amazons smart speaker, which went on sale in the UK last year. The Echo has been seen as a pioneer but Google, which sees itself as a leader in voice recognition and artificial intelligence, hopes its software will give it an edge. The Google Home , which will cost 129 and go on sale next Thursday, April 6, runs Google Assistant, the same virtual assistant software that features on the Google Pixel phone, and is activated by speaking the words OK Google, followed by a command. As well as answering questions from the Google search engine, such as giving restaurant recommendations, traffic information and reading out a daily schedule, the device will be able to deliver news briefings from The Telegraph and other news outlets. Tech companies are scrambling to lead in voice recognition, which is seen as a new, easier way of interacting with technology. Voice assistants such as Apples Siri have seen limited success but Amazons Echo, which was the first smart speaker to feature a virtual assistant, has been a surprise hit. The Google Home will be able to connect to Googles Chromecast internet TV device. For example, asking it to play a show on Netflix will mean it starts playing on the TV. The company also launched its Wi-Fi router system, which can use multiple stations to create a more reliable internet network in the home. By using more than one Wi-Fi point, it can build a mesh network that eliminates internet blackspots in the home. Google Wi-Fi will also come with an app that lets users pause internet connections in their home, for example to encourage children to switch off at dinner time. The Google Wi-Fi system will go on sale on April 6 and cost 129 for a single station or 229 for a two-pack. ",http://www.telegraph.co.uk/technology/2017/03/28/google-home-ai-speaker-launched-uk/,"... around the size of a pint glass, can respond to questions, play music and control smart home technology, all by using speech recognition.",Google Home: AI speaker to launch in the UK next week,Google Google Home UK Echo Amazons UK Echo Google Google Home Thursday April Google Assistant Google Pixel OK Google Google Telegraph Voice Apples Siri Amazons Echo Google Home Googles Chromecast Netflix Wi-Fi Google Wi-Fi Google Wi-Fi April,4
75,"Human-level speech recognition has been a long time coming. One by one, the skills that separate us from machines are falling into the machines column. First there was chess, then Jeopardy!, then Go, then object recognition, face recognition, and video gaming in general. You could be forgiven for thinking that humans are becoming obsolete. But try any voice recognition software and your faith in humanity will be quickly restored. Thoughgood and getting better, these systems are by no means perfect. Are you ordering ice cream or saying I scream Probably both, if its a machine you are talking to. So it ought to be reassuring to know that ordinary conversational speech recognition is something machines still struggle atthat humans are still masters of their own language. That view may have to change. Quickly. Today, Geoff Zweig and buddies at Microsoft Research in Redmond, Washington, say theyve cracked this kind of speech recognition and that their machine-learning algorithms now outperform humans for the first time in recognizing ordinary conversational speech. Speech recognition research has a long history. In the 1950s, early computers could recognize up to 10 words spoken clearly by a single speaker. In the 1980s, researchers built machines that could transcribe simple speech with a vocabulary of 1,000 words. In the 1990s they progressed to recordings of a person reading the Wall Street Journal, and then onto broadcast news speech. These scenarios are all increasingly ambitious. But they are also simpler than ordinary speech because of various constraints. The vocabulary in the Wall Street Journal is limited to business and finance, and the sentences are well structured and grammatically correct, which is not necessarily true ofordinary speech. Broadcast news speech is less formal but still high structured and clearly pronounced. All of these examples have eventually been conquered by machines. But the most difficult tasktranscribing ordinary conversational speechhas steadfastly resisted the onslaught. Ordinary speech is significantly more difficult because of the vocabulary size and also because of the noises other than words that people make when they speak. Humans use a range of noises to manage turn-taking in conversation, a type of communication that linguists call a backchannel. For example, uh-huh is used to acknowledge the speaker and signal that he or she should keep talking. But uh is a hesitation indicating that the speaker has more to say, a warning that there is more to come. In turn management, uh plays the opposite role to uh-huh. Humans have little difficulty parsing these sounds and understanding their role in a conversation. But machines have always struggled with them. In 2000, the National Institute of Standards and Technology released a data set to help researchers tackle this problem. The data consisted of recordings of ordinary conversations on the telephone. Some of these were conversations between individuals on an assigned topic. The rest were conversations between friends and relatives on any topic. Most of the data was to help train a machine-learning algorithm to recognize speech. The rest was a test that the machines had to transcribe. The measure of performance was the number of words that the machine got wrong, and the ultimate goal was to do the task better than humans. So how good are humans The general consensus is that when it comes to transcription, humans have an error rate of about 4 percent. In other words, they incorrectly transcribe fourwords in every hundred. In the past, machines have got nowhere near this benchmark. Now Microsoft says it has finally matched human performance, albeit with an important caveat. The Microsoft researchers began by reassessing human performance in transcription tasks. They did this by sending the telephone recordings in the NIST data set to a professional transcription service and measuring the error rate. To their surprise, they found that this service had an error rate of 5.9 percent for the conversations between individuals on an assigned topic and 11.3 percent for the conversations between friends and family members. Thats much higher than had been thought. Next, Zweig and co optimized their own deep-learning systems based on convolutional neural networks with varying number of layers, each of which processes a different aspect of speech. They then used the training data set to teach the machine to understand ordinary speech and let it loose on the test data set. The results: overall, Microsofts speech recognition system has a similar error rate to humans, but the kinds of errors it makes are rather different. The most common error that Microsoft machine makes is to confuse the backchannel sounds uh and uh-huh. By contrast, humans rarely make that mistake and instead tend to confuse words like a and the or uh and a. There is no reason in principle why a machine cannot be trained to recognize backchannel sounds. Zweig and co think the difficulty the machine has with these is probably to do with the way these noises are labeled in the training data set. The relatively poor performance of the automatic system here might simply be due to confusions in the training data annotations, they say. Overall, however, the machine matches the human error rate of 5.9 percent for the conversations on an assigned topic but outperforms humans in the task of transcribing friend and family conversations with an error rate of 11.1 percent. For the first time, we report automatic recognition performance on par with human performance on this task, say Zweig and co. Thats interesting work. Microsoft may have moved the goalposts in recording this victory for its machines, but the writing is clearly on the wall. Machines are becoming better than humans at speech recognition. This will have significant implications for the way we interact with machines, not least when it comes to ordering ice cream. Weren't able to make it to EmTech Digital ",https://www.technologyreview.com/s/602714/first-computer-to-match-humans-in-conversational-speech-recognition/,"Speech recognition research has a long history. In the 1950s, early computers could recognize up to 10 words spoken clearly by a single ...",First Computer to Match Humans in Conversational Speech ...,Human-level First Jeopardy Go Quickly Geoff Zweig Microsoft Research Redmond Washington Wall Street Journal Wall Street Journal Broadcast National Institute Standards Technology Microsoft Microsoft NIST Zweig Microsofts Microsoft Zweig Zweig Thats Microsoft EmTech Digital,2
76,"Thai mobile operator Advanced Info Service (AIS) has launched a campaign dubbed 'No.1 Service with heart for digital lifestyle' under the concept of 'AIS Live 360'. The operator has introduced the 'My AIS App', 'Ask Aunjai', 'IVR Speech Recognition', and 'Omni Channel'. AIS says its 'Ask Aunjai' service is an intelligent virtual agent who can provide customer service 24/7. AIS-IVR Speech Recognition directs the customer straight to the service they need by using voice commands, while Omni Channel appears on all AIS contact points for faster services. We hope you've enjoyed your free articles. Sign up below to get access to the rest of this article and all the telecom news you need. Register free and gain access to even more articles from Telecompaper. Register here Subscribe and get unlimited access to Telecompapers full coverage, with a customised choice of news, commentary, research and alerts. ",https://www.telecompaper.com/news/ais-expands-customer-service-offering--1189654,"The operator has introduced the 'My AIS App', 'Ask Aunjai', 'IVR Speech Recognition', and 'Omni Channel'. AIS says its 'Ask Aunjai' service is ...",AIS expands customer service offering,Thai Advanced Info Service AIS Service 'AIS Live 'My AIS App Aunjai 'IVR Channel AIS Aunjai Omni Channel AIS Sign Register Telecompaper Register Subscribe Telecompapers,-1
77,"Bob Ross as you've never seen him before: Psychedelic clip shows painter through the eyes of Google's DeepDream neural network But it later become used to create bizarre, nightmare-like psychedelic and abstract art ",http://www.dailymail.co.uk/sciencetech/article-4392070/Video-runs-Bob-Ross-Google-s-neural-network.html,"'Artificial Neural Networks have spurred remarkable recent progress in image classification and speech recognition,' wrote Alexander ...",Video runs Bob Ross through Google's DeepDream,Bob Ross Psychedelic Google DeepDream,-1
78,"50% of HR leaders foresee cognitive technology's impact, says IBM A new IBM survey found that half of HR executives already recognize cognitive technologys potential impact on their profession. The report showed that cognitive technology will significantly affect key HR functions , from hiring and onboarding to employee engagement and talent management. Amajority of CEOs in the survey (65%) envision cognitive technology driving significant value for HR functions. IBM lists several areas in which this technology will have the biggest impact, including: making ""information-rich decisions,"" interacting with users, handling large volumes of ""unstructured data"" and creating personalized output for individual workers. Studies such as IBMs might counter the argument that AI, robotics and other technological developments will replace HR departments , rather than give them tools to improve their performance. Most CEOs appear ready for cognitive technology, compared with just half of HR folks, but both groups need to partner on technological initiatives . Employee buy-in is good news, although resistance to changes with certain functions is to be expected. It's hard not to associate Big Blue with its own iconic cognitive tech application, Watson. Previous reports have detailed Watson's ability to assist HR professionals in areas like recruiting as well as its ability to monitor and improve employee health . Workers' especially millennials'  pre-exposure to cognitive technologies such as artificial intelligence, speech recognition and natural language processing through mobile devices and robotics might make for a more natural transition into cognitive-driven activities. Cognitive tech can be applied to a broad range of workplace activities. IBMs Susan Steele, executive partner in Global Talent and Engagement, said that cognitive tech will enhance decision-making, an age-old critical skill across all disciplines and workplace functions. ",http://www.hrdive.com/news/50-of-hr-leaders-foresee-cognitive-technologys-impact-says-ibm/439543/,"Workers' — especially millennials' — pre-exposure to cognitive technologies such as artificial intelligence, speech recognition and natural ...","50% of HR leaders foresee cognitive technology's impact, says IBM",HR IBM A IBM HR HR Amajority CEOs HR IBM IBMs AI HR HR Employee Big Blue Watson Watson HR Workers IBMs Susan Steele Global Talent Engagement,0
79,"A team of Swinburne researchers is developing an app to teach infants with cochlear implants how to speak. GetTalking is designed for infants who are born deaf and receive cochlear implants. These infants must learn how to associate the sounds they can newly hear with the sounds they are able to make with their mouths. The project was started by Swinburnes Dr Belinda Barnet after her own experience raising her hearing impaired daughter. ""With my own daughter  she had an implant at 11 months old  I could afford to take a year off to teach her to talk. This involved lots of repetitive exercises and time. Now that she can talk I'd like to help other families who may not have that time,"" Dr Barnet says. Being developed to run on iPads or other tablets, GetTalking gives infants a bright visual reward for speaking. When a child has not heard any sound, they don't understand that a noise has an effect on the environment. So the first thing has to be a visual reward for an articulation, Dr Barnet says. After the app recognises any kind of speech from the infant it then has to recognise what word the child was trying to say, rewarding them for speaking words and approximations of words. As understanding a baby that doesnt know how to form words is exceptionally complicated, complex speech recognition software and artificial intelligence is needed. That's quite difficult. The speech needs to be cross-matched with thousands of articulations from normally-speaking babies, Dr Barnet says. Swinburnes BabyLab is supporting GetTalking in this area with a large collection of speech samples. GetTalking responds to the infants word-approximation by re-articulating the correct word and showing a picture of what they are saying. Swinburne Department Chair of Health and Medical Sciences, Associate Professor Rachael McDonald, became involved with GetTalking as Dr Barnet was seeking someone with experience in occupational therapy and child development. I had just started at Swinburne and was very excited about the prospect of developing real technology that had the opportunity to really change peoples lives, Associate Professor McDonald says. Dr Barnet and I gathered a team of experts to work on this project, in order to not only develop the app but to ensure that it is engaging for children and families and that it has an evidence base, so that it can be used in clinical settings. Associate Professor McDonald hopes that the app will improve the lives of children and families who need to do repetitive speech activities. Rather than replacing, we are augmenting the experience, which will improve adherence to activities that often children and families struggle to do.  ",http://www.swinburne.edu.au/news/latest-news/2017/03/app-will-teach-infants-with-cochlear-implants-to-speak.php,"As understanding a baby that doesn't know how to form words is exceptionally complicated, complex speech recognition software and artificial ...",App will teach infants with cochlear implants to speak,Swinburne GetTalking Swinburnes Dr Belinda Barnet Dr Barnet Being Dr Barnet Dr Barnet Swinburnes BabyLab Swinburne Department Chair Health Medical Sciences Associate Professor Rachael McDonald GetTalking Dr Barnet Swinburne Associate Professor McDonald Dr Barnet Associate Professor McDonald,6
80,"As the uses and potential of machine learning grows, the V4 countries are uniquely qualified to expand in this specialised area of the market The moment of the next revolution in the history of mankind is fast approaching  the difference between this and all previous revolutions, both social and economic, is that this time we wont be in it alone. We will be accompanied by complex computer systems carrying out activities, which, due to our cognitive abilities, were previously reserved only for human beings. The era of artificial intelligence is here. Probably the most famous examples of the application of artificial intelligence, though rather simplistic, were the chess duels between grandmaster Garry Kasparov and IBM computer Deep Blue in 1996 and 1997. These duels were supposed to show that a machine can not only perform certain actions similar to a human being, but also outperform its human counterpart. The first match in 1996 ended in victory for Kasparov. The second, which took place a year later, forced the grandmaster to recognise the superiority of the modernised computer. Its been almost twenty years since the aforementioned historic duel, and artificial intelligence, which seemed so abstract back then, has stealthily penetrated our daily lives; it has changed our relationship with our surroundings, continually transforming our environment into a more and more digitalised world. To better understand its level of penetration in our society, it is worthwhile to examine what is currently the most developing segment of artificial intelligence  machine learning. As the name suggests, machine learning consists of creating computer systems which can learn first in a simpler form with human assistance, next in a more advanced form (i.e. with less human assistance) ultimately reaching instructional autonomy. The solutions of the first kind are already available on a mass scale  for example, in online stores, which learn the behaviours of their customers and suggest other products (e.g. in in the form of an advertising banner) which could potentially be of interest in them. More complex machine learning systems include speech recognition, available in personal assistants such as Google Now and Siri on mobile operating platforms like Android or iOS, or facial recognition on photos through online services such as Flickr, Google Photos or Facebook. Image analysis is especially interesting due to the fact that it uses machine learning that can be defined as deep learning, which is structurally inspired by human brain neural connections. Deep learning is currently considered as the segment of artificial intelligence with the greatest potential, and it can be applied to automatic translation (interpretation) of texts, colourisation of black and white photography and videos or digital graphological analysis. Unfortunately, most of the above solutions require constant collaboration between large teams of scientists and programmers which improve the algorithms, in order to increase their models efficiency in achieving a specified objective; this might include creating customer behavioural patterns on an e-commerce website, a correct diagnosis of a voice command or an accurate selection of a friends face on a photo. As in the entire science of artificial intelligence, having well-defined goals in machine learning is paramount, while the biggest barriers are the lack of high quality data and the still limited computing capabilities of processors. In the more advanced area of machine learning  which is now being developed by major players in the world of technology, such as Google (DeepMind and Google Brain) or Facebook (Facebook AI Research)  more complex models of AI are being created. These systems, when properly programmed, teach themselves how to reach a certain goal. The best example of such an application of machine learning can be found in a London-based company DeepMind (bought in 2014 by Google), which creates its artificial intelligence systems so that they learn how to play computer games from the 1980s. When playing a simple game which involves shooting-down of alien creatures from a spaceship, the system developed by DeepMind has one goal: survive in the virtual space as long as possible. The task for the system is, in this case, to learn to survive by repeatedly attempting the game through the method of trial and error. It is this ability to teach and improve its own operations (although, for now, with human assistance) which will cause the speed of technological development in the future to gain even greater momentum. Imagine a normative, contemporary system  which is supported by a large team of programmers working on enormous databases and spending hundreds of hours to correct the AI model  being replaced by a system which learns to correct itself in a shorter time with greater efficiency. The potential for the development of the AI sector is immense, and its global value can reach $36 billion by 2025. However, it is true that the greater part of this market will be taken by the largest technological companies such as Google, Facebook or Microsoft, which, for many years, systematically invested in the development of artificial intelligence, and the results of their work can already be witnessed in the services that they offer. Fortunately, there are smaller segments of the machine learning market  usually focused on specific, individual solutions  which are being developed in the CEE region (most notably in Poland) by technological start-ups. Examples of such companies are Craftinity, Voicelab, Growbots or Deep.BI. Craftinity specialized in analysing and learning of hierarchical databases using the deep learning method with which they help detect systemic anomalies and provide semantic analysis for their customers. Voicelab deals with speech recognition and speech analysis. The founders believe that in the future they can create a system of voice recognition that will work without the need of an Internet connection, which is today required by Google Now and Siri. Growbots developed an innovative system for data analysis which helps retailers search for customers. Thanks to their system, e-mail sales campaigns for their clients are automated and the sales people or product owners can focus on getting in touch with the client who has a higher probability of being interested in the product. Deep.BI, on the other hand, measures raw data about online publications, such as readers behaviour and content performance, analyses them in real time which allows for a better understanding of the needs of customers by content creators and publishers, at the same time enabling them to increase their audience. The biggest problem faced by companies currently operating in the segment of artificial intelligence is funding, which in this part of the world is considerably smaller than, for example, in the UK and the United States, where private equity funds are driving the development of experimental technology. A great opportunity for the sector of machine learning in Poland is a government run program Start in Poland, which, through a venture capital fund PFR Ventures, will offer 2.8 billion PLN for the development to innovative start-ups. Alternatively, Horizon 2020, a programme run by the European Commission has earmarked 38 million for research on big data, which includes machine learning. Unfortunately, even the aforementioned amounts, pale in comparison with the money that is pumped into research on artificial intelligence in other places. What is also worth mentioning is the huge programming potential which has been dormant in the CEE region and could be used in the development of artificial intelligence technologies. The most notable potential is in the fact that young representatives from countries comprising the Visegrad Group have been, for many years, achieving enormous successes at the International Olympiad in Informatics (IOI); in the historic hall of fame, three countries of the V4 group are in Top 20 (Poland has a high second place in that ranking). Unfortunately, these young programming talents are usually fished out by the great players from the West. The best example of this is the recent employment of Filip Wolski, the most successful Polish Informatics Olympian, by California based Open.AI, which is financed by none other than Elon Musk. Interestingly, one of the founders of Open.AI, which aims to maximise the potential of open artificial intelligence is also a Pole, Wojciech Zaremba. The sector of artificial intelligence has a very bright future and is therefore worth being supported at both the research and business levels. With so many dynamic, technology-dependent changes in store for humanity in the near future, there is ample space within the machine learning sector which could be allocated not only by people from the CEE region working for foreign companies, but also for companies from the CEE region. These companies can utilise the dormant programming potential and pervasive hunger of this region to build an economy similar to those in the Silicon Valley. It is worth retaining those talents here; after all, there is no better resource in business than the human resource and, in particular, one that creates artificial intelligence. Jerzy Brodzikowskiis Community Development Manager at TechHub Warsaw, where he helps over 40 technological startups develop to their maximum potential. He specializes in new technologies and their application in professional life, startup development, public relations, social media, and project management. ",http://visegradinsight.eu/the-yet-untapped-potential-of-ai/,"More complex machine learning systems include speech recognition, available in personal assistants such as Google Now and Siri on mobile ...",The Yet Untapped Potential of AI,V4 Garry Kasparov IBM Deep Blue Kasparov Google Siri Android Flickr Google Photos Facebook Deep Google DeepMind Google Brain Facebook Facebook AI Research AI DeepMind Google DeepMind AI AI Google Facebook Microsoft CEE Poland Craftinity Voicelab Growbots Deep.BI Craftinity Internet Google Siri Growbots Deep.BI UK United States Poland Start Poland PFR Ventures PLN Horizon Commission CEE Visegrad Group International Olympiad Informatics IOI V4 Poland West Filip Wolski Informatics Olympian California Open.AI Elon Musk Interestingly Open.AI Pole Wojciech Zaremba CEE CEE Silicon Valley Jerzy Brodzikowskiis Community Development Manager TechHub Warsaw,4
81,"This should be a moment of triumph for University of Toronto computer science professor Geoffrey Hinton. The British-born 69-year-old is known as the godfather of the strain of artificial intelligence called neural networks or neural nets, which involves setting up computer systems to mimic the human brain, allowing them to learn. It is, some experts say, going to radically transform our lives  already is, actually  the way electricity did in the 20th century. For years, Prof. Hinton worked not just in relative obscurity but on the losing side of a decades-long battle within the academic cloister of computer science. His neural nets were considered weak-minded nonsense by better-funded adherents of more conventional methods of creating artificial intelligence, which involve more hands-on programming. Academic journals used to reject papers on neural nets out of hand, Prof. Hinton says. But in just the past five years or so, after a series of stunning breakthroughs by his graduate students, neural nets are all the rage and he is being hailed as a guru of a new era of computing. Neural nets are already powering most of the voice recognition software in your cellphone. They can recognize pictures, down to different breeds of dog, almost as accurately as humans can. And the worlds biggest tech companies are now throwing millions of dollars into neural net research, hiring many of Prof. Hintons former students, who now run or conduct AI research at Apple, Twitter, Google and Facebook. He says the possibilities for AI range from driverless cars to smartphones that can diagnose skin cancer better than any dermatologist. The new wave of technology is expected to disrupt industries and make those who develop it and control it a lot of money. Prof. Hinton splits his time between U of T and Googles Toronto office, where he is an engineering fellow and will help direct a new AI lab. He has just been named the chief scientific adviser of the newly announced Vector Institute, which will fund research into artificial intelligence and is aimed at turning Toronto into a global AI hub. Despite all these recent victories, Prof. Hinton sounds as if he is still waging a rearguard action. In an interview in his narrow, spartan office at Google  there are no chairs, and a wall-sized whiteboard behind him is covered in equations  he wastes almost no time on small talk before launching into a blow-by-blow account of the battle thats still raging between advocates of neural nets and those who back more traditional forms of AI. His own university, he says, dragged its feet on hiring a new neural-net professor, despite receiving $1-million from Google to do just that. Now, many people are jumping on the neural-net bandwagon, he says, as research funding flows more freely. Now that neural nets work, industry and government have started calling neural nets AI. And the people in AI who spent all their life mocking neural nets and saying theyd never do anything are now happy to call them AI and try and get some of the money, Prof. Hinton said. (It soon became apparent that Prof. Hinton enjoys tweaking noses. During the interview, he made a couple of snarky asides about U.S. President Donald Trump before halfheartedly apologizing to his Google PR handler with a grin.) The traditional concept of AI relies on logic and rules to program computers to think. In the 1960s, when much of this work was more theoretical and not yet available in the palm of your hand, the neural-net alternative was destroyed and discredited, Prof. Hinton says. The traditional model was accepted as an article of faith. But breakthroughs in the past few years, made possible partly by dramatic increases in computing power, have changed all that. In 2009, two of Prof. Hintons graduate students won a speech-recognition competition, besting more established methods by using a neural net that was then upgraded and incorporated into Googles Android phones. In 2012, two of his other students handily won an image-recognition competition. That technology, which involves training a system by using a database of one million images, can recognize and describe an image with a 5-per-cent error rate  about the same as humans. To explain just how neural nets work, Prof. Hinton uses the example of a translation program. Using a neural net as a translator involves feeding a computer network a mountain of words and word fragments, he explains. The system figures out the meaning of a sentence, then feeds that into another neural net to spit out the sentence in another language, without the use of programming or linguistic rules. It even learns the difference between the active voice and the passive  by itself. Nobody ever told it this thing about actives and passives. Just like your little kid, you dont say: Look, Johnny, theres actives and theres passives.  No, after a while, they just get it, Prof. Hinton said. And the point about these neural nets is they just get it. He credits two factors for his decision to come to U of T in 1987, after bouncing between a handful of universities in the United States. One was funding for his brand of AI from the Canadian Institute for Advanced Research. The other was more political: I didnt want to take money from the U.S. military. And most of the AI funding in the States came from the military. Born in Wimbledon and raised in Bristol, England, Prof. Hintons mother was a math teacher and his father was an entomologist with a fondness for beetles. His great-great-grandfather was 19th-century logician George Boole, the inventor of Boolean algebra, a foundation of modern computing. Prof. Hinton attended what he described as a second-tier private school (called a public school in Britain): I wasnt particularly good at math at school. I liked physics. And soccer. He went to the University of Cambridge for physics and chemistry but only lasted a month, dropping out and switching to architecture, where he said he only lasted a day. He re-enrolled in physics and physiology but found the math in physics too tough and so switched to philosophy, cramming two years into one. That was a very useful year, because I developed very strong antibodies against philosophy, Prof. Hinton said. I wanted to understand how the mind worked. To that end, he switched to psychology, only to decide that psychologists didnt have a clue. He spent a year as a carpenter before heading to graduate school at the University of Edinburgh in 1973 to study artificial intelligence under Christopher Longuet-Higgins, whose students included Nobel Prize winners John Polanyi, the U of T chemist, and theoretical physicist Peter Higgs. Even then Prof. Hinton was convinced that the discredited neural-net concept was the way forward. But his supervisor had recently converted to the traditional AI camp. I had a stormy graduate career, where every week we would have a shouting match, Prof. Hinton said. I kept doing deals where I would say, Okay let me do neural nets for another six months and I will prove to you they work. At the end of the six months, I would say, Yeah, but I am almost there, give me another six months. And since then I have been saying, Give me another five years, and people have been saying, You have been doing it these five years, this never worked. And finally, it worked. He denies ever doubting that neural nets would one day be proven superior: I never had doubts, because the brains got to work somehow. The brain sure as hell doesnt work by somebody programming in rules. Confronted with the typical Are robots going to take over the world question, he agrees that limits must be placed on AI. He recently signed a petition asking the United Nations to ban artificially intelligent lethal weapons, a cause championed by the aptly named Campaign to Stop Killer Robots: I think thats the scariest bit. And thats not the distant future  Thats now. The more benign future for AI, he predicts, will see neural nets used in doctors offices to diagnose diseases or skin cancers. They will also be refined into personal assistants that not only remind you of a lunch appointment but use common sense to observe your behaviour and decide to interrupt you if you have forgotten your appointment. Big banks, cable companies and many others are looking to use AI to analyze things such as sales data and to better interact with their customers, says Steve Irvine, who left Facebook to return to Toronto and launch a startup called Integrate.ai to help firms do just that. I dont think he can get enough praise, Mr. Irvine said of Prof. Hinton. Because hes been in AI in the dark days, when he looked like a mad scientist and people never thought this was going to happen.  Now all these things that were talked about for 20, 30 years are happening. I think its a nice reward for him  and now it is this worldwide hysteria, and hes the godfather. It was definitely not an overnight success. Follow Jeff Gray on Twitter: @jeffreybgray ",http://www.theglobeandmail.com/news/toronto/u-of-t-professor-geoffrey-hinton-hailed-as-guru-of-new-era-of-computing/article34639148/,"In 2009, two of Prof. Hinton's graduate students won a speech-recognition competition, besting more established methods by using a neural net ...",U of T professor Geoffrey Hinton hailed as guru of new computing era,University Toronto Geoffrey Hinton Prof. Hinton Prof. Hinton Prof. Hintons AI Apple Twitter Google Facebook AI Prof. Hinton U T Googles Toronto AI Vector Institute Toronto AI Prof. Hinton Google AI Google AI AI AI Prof. Hinton Prof. Hinton U.S. President Donald Trump Google PR AI Prof. Hinton Prof. Hintons Googles Android Prof. Hinton Just Look Johnny Prof. Hinton U T United States AI Canadian Institute Advanced Research U.S. AI States Wimbledon Bristol England Prof. Hintons George Boole Boolean Prof. Hinton Britain University Cambridge Prof. Hinton University Edinburgh Christopher Longuet-Higgins Nobel Prize John Polanyi U T Peter Higgs Prof. Hinton AI Prof. Hinton Okay Are AI United Nations Campaign Stop Killer Robots Thats AI AI Steve Irvine Facebook Toronto Integrate.ai Mr. Irvine Prof. Hinton AI Follow Jeff Gray Twitter,-1
82,"This AI device will be your personal assistant for the car Yahoo Finance Friday, March 31, 2017 Meet Chris. The first AI device to keep your digital life connected while youre behind the wheel. The no-touch gadget is designed for drivers so they can keep their eyes on the road. Chris listens and talks to you using artificial intelligence and understands speech recognition and gesture control. Now when you get a call, voicemail, Facebook, Whats App, or email message, you can safely answer without taking your eyes off the road. The company that makes it, German Autolabs, says you can also control music and navigation. Chris has already surpassed its Kickstarter campaign goal, but is still taking backers if youre looking for a trusty driving companion. More: ",http://finance.yahoo.com/news/this-ai-device-will-be-your-personal-assistant-for-the-car-153814060.html,"Chris listens and talks to you using artificial intelligence and understands speech recognition and gesture control. Now when you get a call, ...",This AI device will be your personal assistant for the car,AI Yahoo Finance Friday March Meet Chris AI Chris Facebook Whats App Autolabs Chris Kickstarter,-1
83,"Microsoft has long loved the sound of the human voice. PHOTO: Breakingpics Microsoft is beefing up Skype for Business in Office 365 with advanced call features, meeting room solutions and call analytics. The enhancements and partner solutions are aimed at making Skype for Business ""the backbone for enterprise voice and video meetings in Office 365,"" Ron Markezich, corporate vice president for Office 365 Marketing, noted in a blog post . Markezich announced the changes during a keynote he delivered yesterday at Enterprise Connect , UBM's conference for enterprise IP telephony, converged networks and unified communications. The conference runs through Thursday at the Gaylord Palms Convention Center in Orlando. Microsoft also announced it's bringing its intelligent personal assistant, Cortana, to the lock screen of Android devices. The move should make the service more accessible and more competitive with Googles voice assistant Microsoft has long loved the sound of the human voice. In fact, last October, it announced its speech recognition is as accurate as humans. Lauded as a historic achievement by Microsoft , the study detailed a new speech recognition technology thats able to transcribe conversational speech as well as professional human transcriptionists. With Skype now firmly embedded within Office 365, more customers ""are turning to Skype for Business in Office 365 for all of their conferencing and calling needs,"" Markezich wrote. Through integration with Outlook for email, SharePoint for intelligent content management, Yammer for networking across the organization, and Microsoft Teams for high-velocity, chat-based teamwork  Skype for Business is the backbone for enterprise voice and video meetings in Office 365, he wrote. Microsoft wants Skype for Business to become the core voice-based communication app in the enterprise  ultimately using it as a substitute for desk phones. Markezich said there were more than a billion meetings on the Skype network last year, and said the use of Skype for Business Online has doubled year-over-year. The latest upgrades ""advance our goal of putting communication at the heart of productivity with Skype,"" Markezich wrote. The improvements include two advanced calling features, Auto Attendant and Call Queue. Microsoft is also offering new meeting room solutions from its partners, including Polycom RealConnect for Office 365, which enables customers to connect existing video conferencing devices to Skype for Business Online meetings; and Crestron SR for Skype Room Systems, which integrates with the Crestron control and AV systems. It's also revealed a preview of the new Skype for Business Call Analytics dashboard, which provides IT admins with greater visibility to identify and address call issues, and availability of Enghouse Interactives TouchPoint Attendant, an attendant console for Skype for Business Online. Microsoft is working to differentiate itself in the competitive enterprise collaboration space. Google recently boosted its collaboration play with two new enterprise-focused services  Hangouts Meet and Hangouts Chat to make both more enterprise friendly, Cisco is also at Enterprise Connect to grow interest in Cisco Spark, an app-centric, cloud-based service that provides a complete collaboration suite for teams to create, meet, message, call, whiteboard and share anytime from anywhere. Amazon last month unveiled Chime , a communications platform thats ready to battle it out with the likes of Skype, Hangouts, GoToMeeting and WebEx. The competition extends to the virtual personal assistant space. The Cortana lock screen integration, which was rolled out earlier this month, is now widely available, Microsoft said. During the beta trial, users had to go into the Cortana settings menu and turn on the lock screen integration, which enabled them to open it with a swipe. Now users can access Cortana from the lock screen, which means you can talk to Cortana even when your phone is locked. ",http://www.cmswire.com/digital-workplace/microsoft-adds-new-features-to-skype-for-business/,"In fact, last October, it announced its speech recognition is as accurate as humans. Lauded as a historic achievement by Microsoft, the study ...",Microsoft Adds New Features to Skype for Business,Microsoft Microsoft Skype Business Office Skype Business Office Ron Markezich Office Marketing Markezich Enterprise Connect UBM IP Thursday Gaylord Palms Convention Center Orlando Microsoft Cortana Android Googles Microsoft October Microsoft Skype Office Skype Business Office Markezich Outlook SharePoint Yammer Microsoft Teams Skype Business Office Microsoft Skype Business Markezich Skype Skype Business Online Skype Markezich Auto Attendant Call Queue Microsoft Polycom RealConnect Office Skype Business Online Crestron SR Skype Room Systems Crestron AV Skype Business Call Analytics IT Enghouse Interactives TouchPoint Attendant Skype Business Online Microsoft Google Hangouts Meet Hangouts Chat Cisco Enterprise Connect Cisco Spark Amazon Chime Skype Hangouts GoToMeeting WebEx Cortana Microsoft Cortana Cortana Cortana,2
84,"By Serdar Yegulalp , Senior Writer, InfoWorld | Feb 7, 2017 Microsoft's AI APIs add content moderation, speech recognition New APIs for Microsoft's Cognitive Services cloud allow speech-to-text and vice versa, as well as provide tools to automatically moderate images, video, and text Your message has been sent. If you want your apps to understand what someones saying or know if your user-content rules are being broken, Microsoft has you covered. Microsoft is expanding its portfolio of Cognitive Services in-the-cloud APIs that provide out-of-the-box versions of useful algorithmsto include two new services that go into general availability next month: the Content Moderator and Bing Speech APIs. [ Jump into Microsofts drag-and-drop machine learning studio: Get started with Azure Machine Learning . | The InfoWorld review roundup: AWS, Microsoft, Databricks, Google, HPE, and IBM machine learning in the cloud . ] Bing Speech converts audio into text and vice versa. Its also able to apply contextual understanding to that speech or text. The Speech APIs demo page lets you try a limited sample of both text-to-speech and speech-to-text for yourself. Both processes show their limits pretty quickly, though. Text-to-speech still sounds somewhat robotic; theres always the sense that the speaker is emphasizing the wrong syllables. And speech-to-text still seems best suited for processing short command phrases rather than for performing transcriptions of longer texts.Googles speech recognition API appears to be more accurate, although Microsoft offers competitive features likereal-time streaming of results (as per Googles Voice Typing function). Intent Recognition, or the ability to return structured data about the captured speech rather than flat text, is another feature of the Speech API that Microsoft is touting as an improvement over existing speech recognition systems. This enables apps to easily parse the intent of the speaker, and subsequently drive further action, according to Microsoft, which calls the feature Language Understanding Intelligent Service (LUIS). Microsofts demo of LUIS includes the ability to parse command examples like turn off all the lights or switch all lights to green (for those of you with fancy multicolored LED bulbs). With the Content Moderator API, Microsoft provides tools to help automate one of the more tedious and time-consuming jobs in creating services that accept user-submitted content.Content Moderator can check images, text, and video for offensive and unwanted content that creates risks for businesses. Image moderation can check for adult or racy content, and can extract text from images by way of OCRfor example, to determine if meme-type images have offensive content. Both image and video moderation return simple is/is not checks for adult material, as well as confidence scores for more precise evaluation. Text moderation can check for profanity in more than 100 languages, as well as malware/phishing URLs. It can also return details about the original and corrected texts if needed. The process isnt intended to be entirely automatic; Microsoft provides both a tool and an API to allow individuals and teams to moderate submitted content and apply custom tags and workflows to data. But the underlying moderation APIs are meant to zero in on the content that needs at least some human oversight and to filter out the things that dont. Another aspect of the APIwhich might prove even more useful than checking for objectionable contentis flagging submissions that have personally identifiable information. Images, for instance, can be run through a face-detection algorithm; anything that has an identifiable face in it can be flagged. Future versions of the service could provide a line of defense against doxxing , or having personal information broadcast maliciously by third parties. Microsoft says Cognitive Services provides ready-to-use APIs and data models so that companies dont have to build their own data sets or trained models. Equally important in time will be that it provides convenient ways for organizations to build extensions to these services without writing from scratch applicationsthat back-end into Microsofts APIs. The workflow/tagging mechanism in Content Moderator provides a hint at this; these systems could be customized for a specific environment by feedback from nontechnical users instead of developers alone. ",http://www.infoworld.com/article/3166788/artificial-intelligence/microsoft-adds-content-moderation-speech-recognition-to-its-ai-apis.html,"Google's speech recognition API appears to be more accurate, although Microsoft offers competitive features like real-time streaming of results ...","Microsoft's AI APIs add content moderation, speech recognition",Serdar Yegulalp Senior Writer InfoWorld | Feb Microsoft AI APIs New APIs Microsoft Cognitive Services Microsoft Microsoft Cognitive Services APIs Content Moderator Bing Speech APIs Jump Microsofts Get Azure Machine Learning InfoWorld AWS Microsoft Databricks Google HPE IBM Bing Speech Speech APIs API Microsoft Googles Voice Typing Intent Recognition Speech API Microsoft Microsoft Language Understanding Intelligent Service LUIS Microsofts LUIS LED Content Moderator API Microsoft Moderator OCRfor URLs Microsoft API APIs APIwhich Microsoft Cognitive Services APIs Microsofts APIs Content Moderator,2
85,"Talk2Me: AI & ASR for Real-Time Speech Translation. Applications Technology (AppTek) is excited to announce the release of the latest version of its Talk2Me applications. Leveraging AppTeks proprietary Automatic Speech Recognition (ASR) technology, Talk2Me allows for instant bi-directional speech-to-speech translation in 14 languages.  Currently available for free on the App Store are apps for English and Spanish as well as English and Arabic. The updated version uses AppTeks latest real-time streaming API infrastructure for faster and more accurate translations. Advances in our deep neural networks modeling and API infrastructures enable real-time conversation between speakers of different languages, commented Adam Sutherland, CEO of AppTek. We will be expanding beyond Spanish and Arabic soon. Currently available on the App Store, updates to the current applications for Android are in the pipeline for release. The Android platform is important to us both domestically and internationally. We intend to support all major mobile platforms going forward, stated Jintao Jiang, AppTeks Chief Scientist. As a pioneer in automatic speech recognition, machine learning and artificial intelligence; AppTek partners with its customers to provide solutions focused on closed captioning and subtitling, call center content discovery and mobile intelligent voice agents. In todays hyper-connected and data-rich marketplace, enterprises are seeking to drive revenue, save costs and increase productivity. Customers rely on AppTek to solve for these issues by delivering the markets robust speech technology solutions focused on mining for business insights, ensuring compliance and delivering value across the enterprise. For more information, visit http://www.apptek.com . Share article on social media or email: ",http://www.prweb.com/releases/2017/03/prweb14143985.htm,"Leveraging AppTek's proprietary Automatic Speech Recognition (ASR) technology, Talk2Me allows for instant bi-directional speech-to-speech ...",AppTek Announces Updated Talk2Me® Bi-directional Speech-to ...,AI ASR Real-Time Speech Translation Applications Technology AppTek Talk2Me AppTeks Automatic ASR Talk2Me Currently App Store English Spanish English Arabic AppTeks API API Adam Sutherland CEO AppTek Arabic Currently App Store Android Android Jintao Jiang AppTeks Chief Scientist AppTek Customers AppTek Share,-1
86,"The Chinese search giant lost the star leader of its AI lab last week, but the technology remains an essential long-term focus. When Andrew Ng, one of the worlds leading thinkers on artificial intelligence, announced he would be stepping down from his position as chief scientist at Chinese search giant Baidu, the companys stock dropped nearly 3 percent in just a few hours. It was a reflection not only of Ngs prominence and fame, but also of the importance investors have placed on the search giants focus on AI. The technology has become a key element of the companys strategy , and Ngs departure comes at a time when Baidu is determined to double down on its AI efforts. Baidu has quickly reaffirmed its commitment to AI, naming Wang Haifeng, an expert in natural-language processing, as Ngs replacement. CEO Robin Li has recently written numerous opinion pieces in Chinese newspapers about the importance of developing AI expertise. The disruption in traditional business models, industrial chain, and value chain brought about by artificial intelligence will cause fundamental changes in the global economy, Li wrotein Peoples Daily, a government flagship newspaper, in early March. And also in March, Chinas National Development and Reform Commission approved Baidu as the leader of the new National Engineering Lab of Deep Learning Technology and Application, reflecting Chinas top-level government dedication to AI development. In the lab, Baidu will collaborate with top Chinese universities including Tsinghua, exploring a variety of different areas of AI research including visual perception, speech recognition, and human-machine interaction. Shengjin Wang, a professor at Tsinghua University who studies computer vision and image recognition, and who also contributes to the research done at the national lab, says Baidu has been a leading player in AI, and there is reason to believe that it will be able to maintain its lead. Why are so many AI stars who first made their reputations in academia leaving their big corporate jobs Tell us what you think. Personally I think its a pity, Wang says about Ngs departure. But he stresses that Ng has built a very solid AI team with over 1,300 researchers at Baidu already, giving the company an advantage in human resources. Baidu also has a competitive edge because of the large amounts of data it has collected from its search engine business, he says. Additionally, he thinks the newly formed Intelligent Driving Group, which focuses on automated driving research, shows the company is heading in a good direction. He predicts Baidu will be able to adjust its AI resources and strategy after Ngs departure. Some of that adjustment is already under way. A couple of days ago, Baidu launched its first overseas campus recruitment campaign, seeking AI talent for its Beijing headquarters in top American universities such as Carnegie Mellon and Columbia. A Baidu spokesperson says the company particularly hopes to attract Chinese students in American universities. Wang says that China can provide an interesting challenge for young researchers with its cities difficult-to-navigate road conditions and many different dialects, forcing Chinese AI researchers to find innovative ways to produce useful applications. ",https://www.technologyreview.com/s/604014/baidus-plan-for-artificial-intelligence-without-andrew-ng/,"... Chinese universities including Tsinghua, exploring a variety of different areas of AI research including visual perception, speech recognition, ...",Baidu's Plan for Artificial Intelligence without Andrew Ng,AI Andrew Ng Baidu Ngs AI Ngs Baidu AI Baidu AI Wang Haifeng Ngs CEO Robin Li AI Li Peoples Daily March March Chinas National Development Reform Commission Baidu National Engineering Lab Deep Learning Technology Application Chinas AI Baidu Tsinghua AI Shengjin Wang Tsinghua University Baidu AI AI Wang Ngs Ng AI Baidu Baidu Intelligent Driving Group Baidu AI Ngs Baidu AI Beijing Carnegie Mellon Columbia A Baidu Wang China AI,3
87,"Give those fingers of yours a well-earned rest. Baidu's new voice-to-text keyboard app for Android is more accurate, anyway. Are you tired of typing your messages like some kind of 20th century doofus If so, you'll be glad to know an Android keyboard designed from the ground up to work around your voice is being released today. The app comes courtesy of Baidu, often referred to as the Google of China. The company teamed up with Stanford University for a study that showed its speech recognition technology was three times faster than typing in English -- and with an error rate 20.4 percent lower. To get this result, Baidu's Deep Speech 2 tech competed against 32 texters aged 19 to 32. Half of those were Chinese, with the study finding Baidu's speech recognition was 2.8 times faster than texters typing in Mandarin, and had an error rate 63.4 percent lower. ""We knew speech recognition is pretty good, so we expected it to be faster,"" said Sherry Ruan, a study co-author and computer science Ph.D student. ""We were actually quite surprised to find that it was almost three times faster than typing on a keyboard."" Voice recognition in keyboards isn't new. Google Keyboard has offered voice input for years. But Baidu's app, as seen in the video above, promises to offer a more realized version of the feature. The company said an early user rapped "" Rapper's Delight "" and the app was able to transcribe it perfectly, so there's that. Whether it'll deliver for you, or whether you want to be talking messages into your phone on the train, we'll soon find out. ",https://www.cnet.com/news/this-keyboard-is-designed-entirely-for-speech-recognition/,The company teamed up with Stanford University for a study that showed its speech recognition technology was three times faster than typing ...,This keyboard is designed entirely for speech recognition,Baidu Android Are Android Baidu Google China Stanford University English Baidu Deep Speech Baidu Mandarin Sherry Ruan Ph.D Voice Google Keyboard Baidu Rapper Delight,7
88,"Conversational interfaces a la Alexa, Siri, and Cortana top the list of tech trends companies want to incorporate in their software Your message has been sent. Conversational user interfaces like Amazon's Alexa, Apple's Siri, and Microsoft's Cortana have caught the attention of developers, with businesses now looking to incorporate the technology into customer-facing apps, consultant firm ThoughtWorks said in assessing recent technology trends. ThoughtWorks' recently released Technology Radar report noted that conversational UI and natural language processing is a key emerging trend. Conversational UI refers to designs like intelligent chatbots, which can learn and improve over time, and voice recognition, which has had a dramatically improved error rate recently. ""The explosion of interest in the marketplace and mainstream media leads to a corresponding rise in developer interest,"" ThoughtWorks said. The industry has reached a tipping point, with machine speech recognition as accurate as humans are now, said Mike Mason, ThoughtWorks head of technology. Multimodal interaction, meanwhile, is ""the future of conversational UI,"" said Bharani Subramaniam, market tech principal for ThoughtWorks. ""We're trying to push boundaries beyond voice and understanding the intent through hand movements, gestures, and facial expressions."" ThoughtWorks also cited what it calls ""intelligence as a service,"" which encompasses capabilities like voice processing, natural language understanding, image recognition, and deep learning. ""Capabilities that would have consumed costly resources a few years ago now appear as open source or SaaS platforms,"" the report said. Developer experience is ""the new differentiator,"" according to ThoughtWorks. A rapid rise in developer-facing tools and products combined with a scarcity of engineering talent is driving this focus. Steps companies can take include treating internal infrastructure (private clouds) as a product that needs to be compelling enough to compete with external offerings, focusing on self-service, understanding the developer ergonomics of the APIs produced, and committing to ongoing research of developers using the services. The report mentioned pervasive Python as a trend as well. ""Its ease of use as a general programming language, combined with its strong foundation in mathematical and scientific computing, has historically led to its grassroots adoption by the academic and research communities,"" ThoughtWorks said. ""More recently, industry trends around AI commoditization and applications, combined with the maturity of Python 3 , have helped bring new communities into the Python fold."" ThoughtWorks saw a rise in platforms as companies are examining how they can use platforms that can provide for self-provisioning and self-configuring via APIs, enabling developers to get features out into production faster. The term ""platforms"" can mean many things, Mason said. ""A lot of people mean 'technical delivery platform for hosting applications,' which today usually means public or private cloud, AWS, Cloud Foundry, OpenShift, Bluemix, etc."" But it could also mean a business platform where business capabilities are exposed as services. Released twice a year, Thoughtworks' report looks at business strategy and software development trends; it is based on an assessment by the company's advisory board and factors in what the company sees at client sites. ",http://www.infoworld.com/article/3187024/application-development/enterprises-want-to-add-alexa-style-interfaces-to-apps.html,"The industry has reached a tipping point, with machine speech recognition as accurate as humans are now, said Mike Mason, ThoughtWorks ...",Enterprises want to add Alexa-style interfaces to apps,Alexa Siri Cortana Conversational Amazon Alexa Apple Siri Microsoft Cortana ThoughtWorks ThoughtWorks Technology Radar UI Conversational UI ThoughtWorks Mike Mason ThoughtWorks Multimodal UI Bharani Subramaniam ThoughtWorks SaaS Developer ThoughtWorks A APIs Python ThoughtWorks AI Python Python APIs Mason AWS Cloud Foundry OpenShift Bluemix Thoughtworks,-1
89,"According to a new market research report ""Affective Computing Market by Technology (Touch-Based and Touchless), Software (Speech Recognition, Gesture Recognition, Facial Feature Extraction, Analytics Software, & Enterprise Software), Hardware, Vertical, and Region - Forecast to 2021"", published by MarketsandMarkets, the global market to increase from USD 12.20 Billion in 2016 to USD 53.98 Billion by 2021, at a Compound Annual Growth Rate (CAGR) of 34.7%. Early buyers will receive 10% customization on this report. The Affective Computing Market is growing rapidly, owing to the increasing need for data archiving tools to organize the data generated from varied end-use sectors. The research study for the global Affective Computing Market encompasses the analysis of the market on the basis of software, which is further segmented into speech recognition, gesture recognition, facial feature extraction, analytics software, and enterprise software. The deployment of facial feature extraction software is mainly driven by the increasing demand expression recognition technique, used by various end-use sectors, especially, healthcare & life sciences and media & entertainment. The affective computing end-users are segmented into academia & research, media & entertainment, government & defense, healthcare & life sciences, Information Technology (IT) & telecom, retail & e-commerce, automotive, and Banking, Financial Services, & Insurance (BFSI). The healthcare and life sciences sector holds large scale application areas for affective computing technology, which include facial expression recognition for the specially-abled children (autism & dyslexia) and detection of psychological disorders, thereby holding the largest market share among other end-use verticals studied for the market analysis. The research study encompasses regional market analysis for North America, Europe, Asia-Pacific (APAC), and the Rest of World (RoW), along with some of the major countries in specific regions. North America is expected to hold the largest share in the Affective Computing Market in 2016, which may then overshadowed by the APAC region, by the end of forecast period. The rapid developments in infrastructure and higher adoption of digital technologies are the two major drivers that increase the demand for the Affective Computing Market. Furthermore, the U.S is the most technologically-advanced region, with the presence of different business verticals in this region, such as BFSI, healthcare, and retail & e-commerce. The prominent players in the Affective Computing Market are Google Inc. (California, U.S.), IBM Corporation (New York, U.S.), Microsoft Corporation (Washington DC, U.S.), Saffron Technology (North Carolina, U.S.), Softkinteic System S.A. (Brussels, Belgium), Affectiva (Waltham, U.S.), Elliptic Labs (Oslo, Norway), Eyesight Technologies Ltd. (Israel), Pyreos Ltd. (Edinburgh, U.K.), Cognitec Systems GmbH (Germany), Beyond Verbal Communication Ltd. (Tel Aviv, Israel), Numenta (California, U.S.), GestureTek (Canada), and SightCorp (Amsterdam, the Netherlands). Emotion Detection and Recognition Market by Technology (Bio-Sensor, NLP, Machine Learning), Software Tool (Facial Expression, Voice Recognition), Service, Application Area, End User, and Region - Global Forecast to 2021 ",http://www.prnewswire.com/news-releases/affective-computing-market-worth-5398-billion-usd-by-2021-616910674.html,"... to a new market research report Affective Computing Market by Technology (Touch-Based and Touchless), Software (Speech Recognition, ...",Affective Computing Market Worth 53.98 Billion USD by 2021,Technology Touchless Software Gesture Recognition Facial Feature Extraction Analytics Software Enterprise Software Hardware Vertical Region Billion Compound Annual Growth Rate CAGR Affective Market Information Technology IT Banking Financial Services Insurance BFSI North America Europe Asia-Pacific APAC Rest World RoW North America APAC Affective Computing Market Furthermore U.S BFSI Google Inc. California U.S. IBM Corporation New York U.S. Microsoft Corporation Washington DC U.S. Saffron Technology North Carolina U.S. Softkinteic System S.A. Brussels Belgium Affectiva Waltham U.S. Labs Oslo Norway Eyesight Technologies Ltd. Israel Pyreos Ltd. Edinburgh U.K. Cognitec Systems GmbH Germany Beyond Verbal Communication Ltd. Tel Aviv Israel Numenta California U.S. GestureTek Canada SightCorp Amsterdam Netherlands Emotion Detection Recognition Market Technology Bio-Sensor NLP Machine Learning Software Tool Facial Expression Voice Recognition Service Application Area End User Region Global Forecast,3
90,"25 Oct 2016 at 15:00, Katyanna Quach Microsoft has released a catalogue of AI software under Microsoft Cognitive Toolkit on GitHub today. The new toolkit is an updated version of the Computational Network Toolkit, which was developed by a team of computer scientists interested in speech recognition and natural language processing. It has since expanded into other areas. The 22 APIs cover computer vision, emotion recognition, web search and text analysis, and have been updated to be compatible with C++ and Python. Microsoft's AI researchers have already used the toolkit to build an automated system capable of recognising recorded speech on the NIST 2000 Switchboard at a word error rate of 5.9 per cent. Heralded as a ""major breakthrough"" in speech recognition, the system performs slightly better than the level needed to be a professional transcriptionist. Microsoft are keen to integrate the system into its AI assistant Cortana. The sudden surge of capabilities in AI has actually been brewing away for over 20 years, Xuedong Huang, Microsofts Chief Speech Scientist and a developer of the Microsoft Cognitive Toolkit (MCT), told The Register. A combination of large datasets, better computer infrastructure and deep learning  something Huang calls ""the three pillars"" of AI  has led to sudden and significant advances in the field. In the past it could take up to two months to train a speech recognition model on a single GPU, but using the toolkit it only takes days. Huang credits Microsoft's improving ""breakthroughs"" in speech recognition to fast training times. In September, Huang's team announced they had achieved the lowest error rate of computer speech recognition at 6.3 per cent. But a month later, the error rate has decreased to 5.9 per cent and the system has reached ""human parity"". More than 20 years ago, the error rate was higher than 60 per cent. Now that AI is rapidly improving, it's important to 'democratise' the technology, Huang told The Register. He hopes that developers will seize the opportunity to use the toolkit for research or to create new products with novel applications.  ",https://www.theregister.co.uk/2016/10/25/microsofts_cognitive_toolkit_is_now_on_github/,"Heralded as a major breakthrough in speech recognition, the system performs slightly better than the level needed to be a professional ...",Microsoft's Cognitive Toolkit on GitHub in all its speech-recognising ...,Katyanna Quach Microsoft AI Microsoft Cognitive Toolkit GitHub Computational Network Toolkit APIs C++ Python Microsoft AI NIST Switchboard Microsoft AI Cortana AI Xuedong Huang Microsofts Chief Speech Scientist Microsoft Cognitive Toolkit MCT Register Huang AI GPU Huang Microsoft September Huang AI Huang Register,2
91,"This week we take a look at how Aumina and Akeira, Uniphores speech recognition software products, are gaining traction in the Indian market. In America, a startup has figured out how to supply meat without hurting animals and, closer home, Margo Networks new technology has lured Zee Entertainment. Heres what been buzzing on Startup Street. Forget Siri, Akeira can give you sound financial advise and then help you carry out your transactions. Who is she, you ask Akeira is a sophisticated speech recognition software developed by Uniphore. It is meant for large enterprises such as banks and insurance companies. The software works out of the companys data base, advising clients on important financial investments and the products available in the market. Uniphore is currently in the midst of striking a deal with an insurance company that will bring Akeiras expertise to Indian users, said the firms chief executive officer and co-founder Umesh Sachdev in a conversation with BloombergQuint. While the deal is expected to conclude soon, Sachdev refused to disclose the name of the company that he is in talks with. For a software like Akeira, which will operate only out of the insurance companys database, to succeed the data bank needs to be equally sophisticated. And that is where Aumina comes in. Already deployed in India across companies like State Bank of India, Axis Bank and ITC, Aumina analyses conversations and converts them into data on a real time basis. For example, if a client is in conversation with an employee at the insurance companys call center, Aumina will analyse the conversation and give the employee his options. Aumina tracks words, voice modulation and a number of other parameters to compute whether the call was a successful one or not. Moreover, it sends alerts even as the conversation is going on if it spots a sales opportunity. Speech recognition is a $50-60 billion market in India, said Sachdev while adding that he believes the software sisters can capture a big chunk. No Slaughter Was Involved In The Making Of This Nugget! San Fransisco based Memphis Meats announced a breakthrough in 'clean poultry', creating poultry meat through cell replication. ""It is thrilling to introduce the first chicken and duck that didnt require raising animals,"" said co-founder and managing director Uma Valeti. But how does this process work The startup starts with meat cells that have the ability to self renew, which are then fed with rich nutrients, vitamins, minerals and plant. The growth process takes about two-three weeks, Valeti explained in a mail to BloombergQuint. This way, the company creates ""real meat"" without adding to the problems for the environment, animal welfare and human health, he added. As for it's target audience, Valeti said, ""Some folks who are otherwise vegetarian might eat it, but we are firmly focused on people who already love meat."" The startup will commercially launch its products in the next five years and is hoping to grab a ""large portion"" of the meat market. Zee Entertainment will acquire 80 percent stake in Mumbai based startup - Margo Networks for Rs 75 crore. Founded in August 2016, Margo has developed a technology which has the potential to boost digital consumption of content. The Zee group acquired a stake in the company ""considering the strong synergies of the technology developed by Margo with the current business of the company (Zee),"" it said in an exchange filing. The deal is expected to be concluded within 30 days. ",https://www.bloombergquint.com/technology/2017/03/19/startup-street-forget-siri-akeira-can-give-you-sound-financial-advice,"This week we take a look at how Aumina and Akeira, Uniphore's speech recognition software products, are gaining traction in the Indian market ...","Startup Street: Forget Siri, Akeira Can Give You Sound Financial ...",Aumina Akeira Uniphores America Margo Networks Zee Entertainment Startup Street Forget Siri Akeira Akeira Uniphore Uniphore Akeiras Umesh Sachdev BloombergQuint Sachdev Akeira Aumina India State Bank India Axis Bank ITC Aumina Aumina Aumina India Sachdev Slaughter Was Making San Fransisco Memphis Meats Uma Valeti Valeti BloombergQuint Valeti Zee Entertainment Mumbai Margo Networks Rs August Margo Zee Margo Zee,-1
92,"I have been seeing some innovative development in microphone and speech recognition. Consumers want to speak commands to their automobiles, mobile devices, and wearables, but ambient noise can get those messages wrong. Automatic speech recognition (ASR) and natural language processing (NLP) for systems like Siri, Google Now, Alexa, Cortana, etc., work pretty well in a quiet home, but our real-world environment surrounds us with a great deal of noise. Most system designs developed to mitigate ambient noise will analyze the speech and noise and try to enhance the speech and suppress the noise. Seems reasonable. But this technique distorts the voice signal; this is a physics approach which suppresses noise signals and boosts voice signals  but this inherently introduces distortions that speech engines cannot process. Lets take a look at where most people use their phones and wearables (Figure 1). Why do we want to use voice commands @KCPB , a Venture capital firm tells us (Figure 2). Figure 2 Hands- and vision-free interaction is the main reason consumers want to use voice in the home, car, or just on-the-go. (Image courtesy of @KCPB) Speech recognition is presently at about 95% accuracy, but experts, like Andrew Ng, Chief Scientist at Baidu, say that going to 99% would be a game-changer. Accuracy is a primary goal and the secondary goal is latency (Who wants to wait 10 seconds to get a response from your system) for explosive use of voice recognition by consumers. In 1970, Machine speech recognition was only 10s of words. Fast-forward to 2016 and 7-8 million words were recognizable with 90% accuracy in a low noise environment, according to Google. Kopins Whisper Voice Interface IC takes a different approach to noise with artificial intelligence (AI). They sample the acoustic environment 16,000 times per second, perform a dynamic analysis of noise and voice activity, and use their Voice Extraction Filter to extract voice without distortion once the parameters are tuned for the device and the application. Figure 3 Voice recognition accuracy in a real world noise environment is demonstrated here. The chart compares the performance of smart glasses with the Whisper chip against the ASR and noise cancellation technologies found in two popular devices: a leading Bluetooth earphone and a leading smartphone. (Image courtesy of Kopin) As can be seen in Figure 3, the Whisper chips performance remains consistent as noise levels increase, the earphones performance begins degrading at 75 decibels (the amount of noise associated with a car interior or dishwasher) while the smartphones ASR performance starts to drop at approximately 85 decibels (or the amount of noise associated with restaurant). Figure 4 Logistically the Whisper Chip is simple to implement, and is situated between the microphones and speech engine. It also works with the leading operating systems, processors, and speech recognition engines. I really like this Whisper Voice IC which is one of the best ways I have seen for voice recognition applications so far in the industry. The Voice Extraction technique lends to just about zero distortion to the voice signal. The adaptive voice detection architecture allows for listening which adjusts to the environmental noise level. The Whisper Voice Chip can be tuned for mid- and far-field applications (up to 5 meters distance from microphone). Some of the audio processing capabilities of this solution are: ",http://www.edn.com/electronics-products/electronic-product-reviews/other/4457645/A-soft-whisper-heard-clearly--Getting-automatic-speech-recognition-right,I have been seeing some innovative development in microphone and speech recognition. Consumers want to speak commands to their ...,A soft whisper heard clearly: Getting automatic speech recognition ...,Automatic ASR NLP Siri Google Alexa Cortana Figure KCPB Venture Figure @ KCPB Andrew Ng Chief Scientist Baidu Accuracy Machine Google Kopins Whisper Voice Interface IC AI Voice Extraction Filter Voice Whisper ASR Bluetooth Kopin Figure Whisper ASR Logistically Whisper Chip Whisper Voice IC Voice Extraction Whisper Voice Chip,7
93,"A team of MIT researchers have developed a speech recognition chip that they claim uses only 0.2 to 10 milliwatts of power for every full watt that a standard phone-based speech recognition system would use, reports MIT News . The technology is based on a neural network structure that only activates more complex sections of processing nodes when human speech is detected. Its a technology that could have important impacts in the emerging Internet of Things, where voice interaction technology is emerging as an important  and perhaps the dominant  user interface. While the system currently requires a sizable onboard memory circuit for its intermediate computations, as MIT News reports, it could find an increasingly wide range of applications if it is refined to perform on a smaller design scale  a project that could very well interest the MIT researchers partners at the Taiwan Semiconductor Manufacturing Companys University Shuttle Program, which developed the chip prototype. MIT isnt uniquein exploring this kind of cutting-edge speech recognition technology. Sensory, Inc. offers low-power speech recognition through its TrulyHandsfree solution , and major companies like Apple, Google, and Microsoft have been investing heavily in developing speech recognition technologies in their own platforms, recognizing its growing importance in products and services for consumers. But MITs researchers are clearly doing their part to push the state of the art further, which should inspire other organizations to stay competitive and result in more powerful technologies for end users down the line. ",http://findbiometrics.com/mit-low-power-speech-recognition-402152/,A team of MIT researchers have developed a speech recognition chip that they claim uses only 0.2 to 10 milliwatts of power for every full watt ...,MIT Researchers Advance Low-Power Speech Recognition,MIT MIT News Internet Things MIT News MIT Taiwan Semiconductor Manufacturing Companys University Shuttle Program MIT Sensory Inc. TrulyHandsfree Apple Google Microsoft MITs,3
94,"Windows 10 Creators Update Review: Microsoft makes it fun Windows has a feature it doesnt like to talk about. While the OS lets you scrawl notes with a stylus, log in with you face (or secure the Web )via Windows Hello , and even order Cortana to set a reminder, what its not so eager for you to do, apparently, is use its speech recognition engine to issue commands or take voice dictation. The reason for its silence may go back 10 years, to when Microsoft product managerShanen Boettcher demonstrated voice dictation  inside Windows Vistaand flubbed it. The technology kept a low profile after that, and today, few users know you can dictate a document within Windows. If there were ever a time for Windows to try again, though, it would seem to be now, when advances in computers and artificial intelligence provide a much better foundation for the technology. [ Further reading: Our best Windows 10 tricks, tips and tweaks ] ""This is such a great question,""saidHarry Shum, the executive vice president overseeing Microsofts speech-recognition research, as well as Cortana and Bing, when asked about dictation's future within Microsoft Office. ""There is really no reason why it is not playing a much more prominent role yet."" We decided to give it another chance: We delved into Windows voice dictation features to see how they compared to more recent speech-based technologies. Ask Word 2016 about dictation, and its like the app has never even heard the term. Word displays a similar response for speech recognition. Some of us still think about voice dictation in the same wayDoonesbury lampooned the Apple Newton, turning I am writing a test sentence into Siam fighting atomic sentry. And youd be forgiven for thinking so, too: Windows Speech Recognition is powered by the Microsoft Speech Recognizer 8.0, which has remained literally unchanged since Vista. Shum called it a grandpa technology. Whathaschanged, however, is the hardware: Listening for and interpreting speech requires far less processing power than a decade ago. The quality of integrated array mics within PCs like the Surface Book mean that dedicated headsets arent necessarily required to achieve superior accuracy. Voice dictation for the masses is here, right When I tested Windows speech capabilities, however, I experienced firsthand the merciless perfection thats required for the system to be usable. This story has 1,028 words in it, including subheadings. If you used voice dictation software to write it, a 95.0% accuracy rate would mean youd have to correct more than fifty mistakes. That gets old fast. In my tests, based on a methodology I developed for another speech recognition product Im testing, Windows produced an accuracy rate of 93.6%, Thats pretty bad on paper, and somewhat behind the dedicated software Im trying. Windows also had an odd habit of interjecting the word comma when I was dictating the punctuation mark. The speech community seems split on whether relatively minor mistakes like this are significant. That, of course, was just the baseline.As anyone whos used dictation software can tell you, the key to accuracy is training. Over time, a voice dictation program learns your accent, whether you pronounce the a in apricot like bad or ape, and how to filter out our unconscious verbal tics. Ive seen Microsoft employees claim that, properly trained, Windows speech recognition was 99% accurate. Ten mistakes or so per 1,000 words isnt bad at all. Very few of us, though, probably want to spend the time training the software. Windows Speech Recognition requires up to 10 minutes to run through a few practice sentences, and it feels like a lifetime. Cortana and Siri dont require any of the same setup time, as theyve already been trained on millions of voice samples. Theres something to be said for instant gratification. Training speech within Windows is a lengthy process. The setup time associated with Nuances Dragon software is far shorter, perhaps a minute or so. But modern digital assistants recognize your words instantly. What makes Cortana (which you can use on your PC or phone) so much better than Windows own ancient voice dictation systems is her link to the massive computational power of the Microsoft cloud. Microsoft can crunch and correlate your voice input together with whatever other data Microsoft knows about you, generating the intelligence that is the soul of Cortana. Given Cortanas proven skills, youd think speech would have taken center stage at Microsofts Ignite show last week. But Ignite containedexactly zero sessions on voice dictation and apparently just one on speech recognition. Meanwhile,CEO Satya Nadellas keynote address painted speech recognition as a critical component of Microsofts future. Take Skype Translator, for example. Microsofts Star Trek-like universal translator depends upon three different strands of research, according to Nadella: speech recognition, speech synthesis, and machine translation. So you take those three technologies, apply deep reinforced learning and neural nets and the Skype data and magic happens, he said. Even inside of Word or Outlook when youre writing a document we now dont have simple thesaurus-based spell correction, Nadella added, adding that Office can now even compensate for dyslexia. We have complete computational linguistic understanding of what youre building. Or what youre writing. But not what youre saying, apparently. Microsoft chief executive Satya Nadella stands next to NFL star Deion Sanders at Microsofts Ignite conference. Has Microsoft fumbled its dictation opportunity During the same speech, Nadella bragged that Microsofts speech algorithms achieved a word error rate of 6.9 percent using the NIST Switchboard test . That sounds bad: thats accuracy of about 93.1 percent. But the Switchboard test uses sample rates of just 8KHz, about the quality of a telephone conversation in the year 2000. Windows Media Audio 10, the codec within OneNote, can capture audio at up to 48KHz, providing much more accurate samples. I think its pretty obvious that the pieces of the puzzle are there, technically. If theres any obstacle, it might be organizational:As of Thursday, Microsofts Office apps were spun out into their own group , away from Cortana and Bing. Shum, however, said that intelligence is still part and parcel of Microsofts offerings. Rest assured that we are infusing AI technology into all Microsoft products, he said. We see value in conversations across a range of devices and experiences,"" Microsoft said in a statement. ""Were just at the beginning of what we believe is possible and certainly see lots of opportunity to connect Cortana and conversations into a number of productivity scenarios. Today, Cortana integrates with Office 365 for glance-able information about upcoming meetings, along with flight and package tracking, and Bing is also providing intelligent insights directly in Office. We will continue to invest heavily here. If Microsoft truly believes in productivity, though, the future of speech recognition within your PC probably isnt using Skype to book a hotel in Bangladesh. Its writing about the experiencebut with your voice rather than your fingers. Updated at 2:17 PM with a statement from Microsoft. To comment on this article and other PCWorld content, visit our Facebook page or our Twitter feed. ",http://www.pcworld.com/article/3124761/windows/the-windows-weakness-no-one-mentions-speech-recognition.html,"This is such a great question, said Harry Shum, the executive vice president overseeing Microsoft's speech-recognition research, as well as ...",The Windows weakness no one mentions: Speech recognition,Update Review Microsoft Windows OS Web Windows Hello Cortana Microsoft Boettcher Windows Vistaand Windows Windows Shum Microsofts Cortana Bing Microsoft Office Windows Ask Word Word Apple Newton Siam Windows Microsoft Speech Recognizer Vista Shum Whathaschanged Surface Book Voice Windows Im Windows Thats Im Windows Ive Microsoft Windows Very Cortana Siri dont Windows Nuances Dragon Cortana Windows Microsoft Microsoft Microsoft Cortana Given Cortanas Microsofts Ignite Ignite CEO Satya Nadellas Microsofts Skype Translator Microsofts Star Trek-like Nadella Skype Even Word Outlook Nadella Office Microsoft Satya Nadella NFL Deion Sanders Microsofts Ignite Has Microsoft Nadella Microsofts NIST Switchboard Switchboard Media Audio OneNote Thursday Microsofts Office Cortana Bing Shum Microsofts Rest AI Microsoft Microsoft Cortana Cortana Office Bing Office Microsoft Skype Bangladesh PM Microsoft PCWorld Facebook Twitter,2
95,"Kerry Costello, the town's clerk, presents information on the town's phone system during their committee of the whole on Monday March 27, 2017. Smiths Falls town council has commissioned Bell Canada to solve the telephone issues in Smiths Falls eight business locations, including the arena and the Heritage House Museum. Our current phone system has been wonky, said Kerry Costello, the director of corporate services/clerk, as she opened up discussion about telephone systems. Police and fire stations will not be switching telephone systems because they are a necessary service, and not included in the proposal. The other business locations included in the proposal are: According to Costello, Bell had the cheapest and best proposal, one of the few fitting inside the budget. The telephone system has a budget of $60,000, having been set last year. Since the system had not been changed last year, the budget moved over to this year. Im glad to see were replacing our phone system. Its archaic and embarrassing, said Coun. John Maloney. The current phone system was purchased in 2010 and used a speech recognition system, although the speech recognition has not worked properly. Im glad to see were replacing our phone system. Its archaic and embarrassing. Coun. John Maloney ",http://www.insideottawavalley.com/news-story/7212557-ring-ring-ring-new-telephone-service-on-the-way/,"The current phone system was purchased in 2010 and used a speech recognition system, although the speech recognition has not worked ...","Ring ring ring, new telephone service on the way",Kerry Costello Monday March Smiths Falls Bell Canada Smiths Falls Heritage House Museum Kerry Costello Police Costello Bell Im Coun John Maloney Im Coun John Maloney,2
96,"Microsoft has leapfrogged IBM to claim a significant test result in the quest for machines to understand speech better than humans. The techniques Microsoft Research used to achieve a new world-best error rate will eventually enhance the Cortana Windows 10 personal assistant. Microsoft claims to have achieved the world's lowest error rate for speech recognition, as the company jostles with Amazon, Apple, Google, and IBM to develop products that understand speech as well as humans can. According to Microsoft , its speech scientists at Microsoft Research have achieved a word error rate (WER) of just 6.3 percent under an industry-standard evaluation, using techniques that will eventually enhance Cortana. The previous lowest error rate was 6.9 percent, achieved by IBM's Watson team , which beat their own record of eight percent set last year. Both Microsoft and IBM presented papers detailing their work on speech recognition at the Interspeech conference in San Francisco this week, where papers were also presented by Google's speech researchers. As Microsoft notes, 20 years ago the lowest error rate in speech recognition was 43 percent and that was achieved by IBM in 1995. By 2004, IBM had cut its error rate to 15.2 percent. However, these days with more research funds being funnelled into deep neural networks, tech giants are boasting error rates of well below 10 percent, but not quite at a level that exceeds human-level accuracy, which IBM estimates to be at about four percent. Google CEO Sundar Pichai last year boasted its deep neural networks helped it achieve an error rate of eight percent in speech recognition systems that power voice Search and Android. More recently, Apple's senior director of Siri, Alex Acero, a former Microsoft Research member, said error rates for speech recognition have been ""cut by a factor of two in all languages"", with greater gains in some languages, again thanks to its work on deep neural networks. Acero's statement was more cautious than a claim by Apple vice president of software engineering Craig Federighi which suggested that Siri had an error rate of just five percent under industry-standard tests. Microsoft's speech recognition systems were assessed against the NIST 2000 Switchboard task, an evaluation that started in 2000 to test conversational speech recognition over the telephone. Back then, it evaluated technology from SRI, the company acquired by Apple in 2010 as the basis of Siri, Dragon software, IBM, and BBN Technologies, acquired by Raytheon in 2009. Like its rivals, Microsoft has made artificial intelligence a key plank in its strategy for human-computer interaction with voice-based platforms such as Cortana set to play a key role in enabling computing in wearables, mobile, the home, vehicles, and the enterprise. ",http://www.zdnet.com/article/microsofts-newest-milestone-worlds-lowest-error-rate-in-speech-recognition/,"Microsoft claims to have achieved the world's lowest error rate for speech recognition, as the company jostles with Amazon, Apple, Google, and ...",Microsoft's newest milestone? World's lowest error rate in speech ...,Microsoft IBM Microsoft Research Cortana Windows Microsoft Amazon Apple Google IBM Microsoft Microsoft Research WER Cortana IBM Watson Microsoft IBM Interspeech San Francisco Google Microsoft IBM IBM IBM Google CEO Sundar Pichai Search Android Apple Siri Alex Acero Microsoft Research Acero Apple Craig Federighi Siri Microsoft NIST Switchboard Back SRI Apple Siri Dragon IBM BBN Technologies Raytheon Microsoft Cortana,-1
97,"According to the new market research reportIntelligent Virtual Assistant (IVA) Market, by Technology (Speech Recognition, Text-to- Speech Recognition), Applications (Messenger Bots, Websites, Contact Centers), End-users (Small Enterprises, Medium Enterprises, Large Enterprises, Individual Users)  Global Revenue, Trends, Growth, Share, Size and Forecast to 2022 published by Scalar Market Research, the global intelligent transportation systems market is expected to grow at a healthy rate during the forecast period. Early Buyers Can Get 10% Free Customization On This Report. The objective of this report is to describe the market trends and revenue forecasts for the intelligent virtual assistant (IVA) market for the next five years. The report focuses on defining and describing the key influencing factors for the growth of the market. It also offers an in-depth analysis of the market size (revenue), market share, major market segments, different geographic regions, key market players, and premium industry trends. The report tracks the major market events including product launches, technological developments, mergers & acquisitions, and the innovative business strategies opted by key market players. Along with strategically analyzing the key micro markets, the report also focuses on industry-specific drivers, restraints, opportunities and challenges in the intelligent virtual assistant (IVA) market. The scope of this report covers the intelligent virtual assistant (IVA) market by its major segments, which include the technologies, applications, end-users, and the major geographic regions. The key reasons for the growth of the intelligent virtual assistant (IVA) market include enhanced customer services, the rise in adoption of smartphones, and the cost-effectiveness of these systems. However, the lack of awareness about this technology is the major factor that may hamper the growth of the market in the coming years. The addition of natural language processing is expected to be the key growth opportunity factor for the intelligent virtual assistant (IVA) market during the forecast period. Scalar Market Research Inc. aspires to assist organizations from around the world to achieve their business goal with premium market research reports and consulting services. Our real-time industry tracking with the help of advanced analytics offers a crystal clear view of all the activities in niche markets. Our team, with thorough global understanding, works relentlessly to gather the necessary market insights, including customer analysis, competitions and global forecast. Find out more about our services at: www.scalarmarketresearch.com 8770 W Bryn Mawr Ave., ",http://www.military-technologies.net/2017/04/10/intelligent-virtual-assistant-iva-market-forecast-to-2022-scalar-market-research/,"According to the new market research reportIntelligent Virtual Assistant (IVA) Market, by Technology (Speech Recognition, Text-to- Speech ...",Intelligent Virtual Assistant (IVA) Market Forecast to 2022 – Scalar ...,Virtual Assistant IVA Market Technology Text-to- Applications Messenger Bots Websites Contact Centers End-users Small Enterprises Medium Enterprises Large Enterprises Individual Users Global Revenue Trends Growth Share Size Forecast Market Research Report IVA IVA IVA IVA IVA Market Research Inc. W Bryn Mawr Ave.,-1
98,"Thad Starner was the technical lead on a project that eventually led to Google Glass. Now he's building wearables for dogs, among other things. We caught up at SXSW. I was in Austin, Texas, last month for SXSW Interactive, where I had the chance to sit down with a number of tech industry execs for my video series Fast Forward, including Chris Becherer , VP of Product at Pandora. In this edition of Fast Forward, we're talking with Thad Starner , Professor of Computing at Georgia Tech. Thad was the technical lead on a project that eventually led the world to Google Glass , which he was wearing during our chat. But he's been in the wearable space for much longer than that, and his work focuses on the very nature of the human-machine interface. His most recent work involves building mind-reading wearables for dogs. Check out our chat in the video and transcript below. Dan Costa: It's a busy SXSW. There is a lot of talk of augmented reality. There's a lot of talk of virtual reality. Wearables are still a very hot topic. When most people think of wearables, they think of either Google Glass or a Fitbit that you wear on your wrist to track your steps. When you think of wearables, what do you think Thad Starner: Well, for me it's any computer, any body-worn device that helps you while you're doing some other, primary task. [Referencing his Google Glass] It's any body-worn computer that is designed to be secondary. It helps you for some other primary task, like answering questions during an interview. My notes are on my iPad mini. I have to keep looking down to get the latest notes and to get your title correct. You've got that right in [Google Glass] itself. Yes. All I have to do is glance up, grab a couple of words and go right back to the conversation. My old systems, I don't know if you looked at some of my older pictures, but my older systems had the display right here [gesturing directly in front of his left eye]. As a matter of fact, there was one from 1997 where it was even in the lens. That was made by a company called Micro Optical at the time. That actually allows you to be pretty facile with having an intelligent assistant that can help you from second to second. It really changes your confidence and changes your ability to do things on the fly that you might normally stumble over. You were one of the original crew. It seems like you guys were almost in competition to see who could build the coolest, most sophisticated, most functional wearable system. I started making my system back in 1990, but I failed at over and over again. It wasn't until '93 that I finally had a system I could wear in my daily life. At the time, I was working for a company called BBN. They helped make the first routers for the internet. After I put the device on, I realized in a month that it wasn't just good research. It was a good lifestyle. That's what these devices are about. It creates a killer lifestyle for you. Of course, back then I had a cell phone connection for the internet that was measured in feet, not in bit rate. In other words, how many feet could I walk before the analog cell phone call dropped and I had to redial the connection Eventually, I went back to the MIT Media Lab and started recruiting more and more students who were interested in this. We had a little convoy of people who made the devices for their particular needs and we figured out how to live as a community of cyborgs. That changed what we thought about the technology. I'll give you an example: Suppose you come in as a VIP to visiting the lab. We'll have one person talk with you about what your interests are. While that's happening, there will be messages going back and forth between the eyeballs of the other members of the group trying to plan your day. ""Oh, he's very interested in augmented senses. So you take him for lunch. I need to go to the doctor's appointment at three, so I'll get him at two."" So, you have these people typing on their keyboards sending messages to each other while the conversation's going on trying to schedule a day based on what your interests are. It kind of gets to the stage of telepathy. A lot of times it sounds very much like Slack , where we've got real-time communication and different segments taking shape. But instead of making it terminal based and making it connected to a computer, where you have to be in front of your laptop or phone, it's wherever you are. You can do two things at once. You can have that conversation and schedule lunch. Well, no, you don't want to do that. Your IQ drops by 40 points when you try to do that. What you want to do is make the computer support the conversation. For example, we have a system called a Remembrance Agent that has a little noise-canceling microphone, listens in to my side of the conversation and everything I say gets transcribed and put into a text buffer. And then the computer is continuously looking up previous emails, previous papers, previous conversations that might relate to what we're talking about. It pulls up little reminders in my display of the details I might want to include in this conversation. So it's supporting the conversation, providing me information. I had a student who was talking to me about speech recognitionhow you want to use speech recognition with wearables. And I said, well if you really want to do that, you shouldand I pulled up my notes while we're having the conversationI say to him, you should really look at Steve Whittaker's talk on Filochat from 1995. I believe he had some great details about how up to 93 percent of people's day is spent in opportunistic communication. I provided him with all of these details. My student just looked at me like ""how do you know all this"" It's all right here [points to Google Glass]. I knew enough to look for Steve Whittaker and then the computer provided me with all of the details. Having that kind of support in real-time is really kind of awesome. We've also done it before where I was giving some testimony at the National Academy of Sciences in Washington, D.C. about augmenting humans. I showed them what I was doing. I had my students, my grad students, back at Georgia Techso I was in D.C., my students at Georgia Tech were listening in and also I showed everybody I had a live feedback to Georgia Tech. I put up both my presentation notes and my students' notes [on the head-up display Google Glass] that they were suggesting in real-time. So as I was testifying, they said, ""Oh, you should talk about this,"" and they'd throw me ""or you should talk about this"" and throwing stuff up, providing other information. The panel that was interviewing me, they started asking questions of the students, not of me. I was just kind of the face man for all of this knowledge of this consortium of people back at the lab who could quickly answer questions because they were experts in the field. But they could look up information a whole lot faster than I could. They simply used me to talk through. These intellectual collectives, this ability to go to experts, being able to help you out at a moment's notice, is quite a powerful thing. On the other hand, if you try to say ""Read email"" while having a conversation [on a different topic], you sound like an idiot. Humans cannot multi-task very well. It seems like the nature of being smart, the nature of knowledge, the nature of intelligence has changed. You used to have to memorize all of these things to have a conversation with it without referring to your notes, and that's not the paradigm anymore. You just need to know what questions to ask and be connected to a system that can deliver those answers to you. There is a certain amount of information you need to internalize so you can look it up quickly and be able to think in those areas, but for a lot of other things, you just want the world's knowledge at your fingertips and have it brought up delivered when you need it. This idea of just-in-time information retrieval is very powerful. That's what I think is going to happen within the next five or 10 years. We're going to have these intelligent assistants that can help us on a second-by-second basis. That's one of the great things about wearable computers. What I think defines wearable computers is that it's something where the computer becomes part of us. You probably have used binoculars before right It's something you pick up. You look at something in the distance. You adjust the focus. They're kind of finicky, right You have to hold them just right and adjust the focus every time you change from one subject to another. You put them down, up and down. You don't think of your binoculars as part of you. But your eyeglasses, you put them on, and you use them all day. You see through them. You don't think about them. You just use them. So, what we have right now with most of these devices, most of these computers, our smartphones, our tablets; they're not part of us. They're other. As soon as you get the devices close to the body, suddenly the technology gets out of the way. That's the paradox of wearable computers. By bringing the technology close to the body, it gets it out of the way. I think that's where we're going to go in the future. We're going to see more and more ways for technology to assist us, make us more powerful, more independent, more confident and more informed on a split-second by split-second basis. I want to see these intelligent assistants go from something that you're talking to; to something that's part of you. Something that's assisting you from one moment to the next very proactively. I think that concept of experiencing the world through the technology as opposed to just using the technology, is a crucial difference. That's the experience I've been working towards. A lot of that is simply the speed of access. Larry Page said to me, ""It's all about reducing the time between intention and action."" I was stunned. I've been trying to articulate the importance of the speed of access of a device. Here he did it in one phrase. The time between intention and action is crucial. One of the demos I do [when I'm giving a talk] is to have everybody [in the audience] complete two tasks. I have them raise their hand after they complete each task. So, I'll say something like, ""Okay, what time is it"" I'll raise my hand because my time is on the front of my screen right now, and my audience also completes the task very quickly. Then, the second task I have them do is to answer a question like ""where is Mount Pinatubo"" I can raise my hand because asking the question got me the answer on my [Google Glass] screen right now. It's in the Philippines, and as a matter of fact, I have a little map of where it is right now [showing] on my screen. It's so hard for my audience to get to that information quickly that they don't even bother doing it. I'm talking about technical audiences like Xerox Park or Georgia Tech or Stanford. So here I am, in an audience full of nerds, doing an experiment with them and most of them don't even bother trying to get to that information because they know it's going to take too long. What if we can change it from something that takes too long to something that's as easy as checking the time. Suddenly, you make people a whole lot more powerful. If you can reduce the time of intention to action to within two seconds or less, suddenly people will do it without thinking about it. It's part of them. Once it gets beyond two seconds, the Before we started recording, we were talking about patron saints. And we were trying to figure out what Saint Stanislaus was the patron saint of. I've got an internet-connected tablet in my hand. I wanted the information. But I did not bother opening up Google, typing it in, and searching for it because I didn't care that much. But just by having that conversation, it popped up on your screen. I mean I have it here. I just asked. This is more explicit. I said simply asked, ""Saint Stanislaus was the patron saint of""the answer is Polandand then [the answer] popped up a couple of seconds later. It's quite powerful. It reduces the sort of reluctance to try and find information about it in the world. It's great for me as a professor because it makes me seem more intelligent than I am. This is something that happened when I was a student at MIT. I was taking a class from Justine Cassell, and it was about . During a review before the final exam, she asked, ""What was the importance of Deixis"" I had all my class notes on my computer, and I could access it very quickly with this [Twiddler] keyboard. It was before the speech recognition got very good. What I would do is construct my sentences so that I could find the answer in time to finish a sentence. So I raised my hand to answer the question. She called on me. I said, ""At the beginning of class, we said the importance of deixis was uh, uh, uh, um,"" because I made a mistake. I dropped my keyboard. I got into the wrong mode of E-Macs [my word processor]. I said, ""I'll have to get back to you. I got in the wrong mode."" The entire class broke up laughing. They had spent all this time with me and had not realized I was doing this on a daily basis. A professor who was taking the class with me leaned over and said, ""Now I want one. You do this all the time, don't you You can pull up information so quickly. It seems like its part of you."" Yes, that's exactly the reason I do that. It was amazing that these professors and students who I had been working with for years just never realized how much assistance it the device gives me. The first time I gave a professional talk, one of the senior students came up and said to me, ""Thad that was the best talk I've ever heard you give, but why were you wearing your computer"" She had no idea that while most people have 3-by-5 index cards, I have it up here [in front of me]. Having that sort of assistance made me a better speaker. It was just mind-blowing to people at the time that you could use your machine this way. So, let's make a small pivot. We've talked a lot about human wearables. You're working with dogs now. Explain the work you're doing. The project is called FIDO, Facilitating Interactions for Dogs with Occupations . Yes, it's an acronym, but it's a cute one. The idea is there are all these working dogs out there. Dogs that are allergy alert dogs or medical alert dogs, search and rescue, bomb detection, inspection for TSA for contraband or agriculture. These dogs do a lot of jobs, but they have a very hard time communicating what they're sensing and what they're thinking to their handlers. We had one of our trainers [at SXSW] a couple of years ago to do a [live] demonstration. She's a diabetic and one of these people who can just faint from lack of blood sugar. She has a dog that's trained to react to that alert situation. The dog goes and finds the nearest person, grabs them, and tries to bring them to her because she's going to pass out soon. Before she's about to faint, it goes and gets help , and get help. This happened at the security gate at the Atlanta Airport, and the dog did what it was trained to dowent up on a TSA security officer. The security officer just thought the dog was being friendly, but our trainer said, ""I'm going to faint in a second. I've got to sit down."" You can imagine it would be a whole lot better if the dog could speak. So, we've made a vest where the dog goes and communicates alerts by pulling the vest. When the sensor is activated and [a voice command] says, ""Please follow me. My owner needs your assistance."" Of course, the first time you test this out, people think they're being pranked, or they just don't believe it. Did this dog just talk to me It's an audio recording of a human voice that actually says that sentence Yes. We have to train the dog to do it twice because the first time the humans don't believe it. No, we just have it repeat the same thing and it seems to work. We're still testing it out. We need to test it out on some unsuspecting people at Georgia Tech and see what they say. If it happens to you, you know it's part of our testing. We'll have one of our assistant dogs in training coming here to do that demonstration. That dog's got to learn new tricks by 2 p.m. this afternoon As a matter of fact, we initially thought we were going to have troubles with bringing the dog to SXSW and so we're going to be doing a lot of training today to be able to show off what we want. The idea that these dogs have all these perceptions that they can articulate to us is kind of new. We had some press on our FIDO vest, and we got a call from the Georgia Tech police asking if the vests could be used for dogs who do bomb sniffing. We were surprised to learn that they have bomb-sniffing dogs, [but they] explained that the dogs are used to scan the sports venues before people come in."" We're like, ""Oh. Tell us about bomb sniffing."" We're interested It turns out the dogs are trained for things like gun powder and C4 but also things like peroxide bombs, which...smell very different. Peroxide bombs are also very unstable. If you get one of those, you don't want the dog or handler anywhere near it. Gunpowder or C4 are very stable and it's not a problem. What the Georgia Tech police wanted to do was have the dogs have their vest with two little pull toys. If the dog smells them and it shows the dog's location on the officer's display where the dog is. Then they know that there's a bomb here but it's a stable bomb and they can just command the dog to stay there and just bark at the bomb until they can find it quickly. If the dog pulls on the other side of the vest, it's a peroxide bomb. We know it's unstable and then you actually recall the dog to get it out of the way and then you send a different type of bomb disposal unit. The same thing with search and rescue...and these TSA agriculture dogs for what they're smelling in people's luggage. You can and even do this for inspection of farmers' fields. There are some things like sweet potato rot where you can't tell from the outside that there's a problem. It's a new thing in North Carolina that is just ravaging their crops. The dogs can smell it. You can send out the dogs and have them so they can search the fields until they find it. If they find it, they alert. You see where they are. You go there. You can see where they're located, dig up the roots, confirm that it's actually rotting and then you get rid of that part of the field and start again. It turns out that once we start utilizing these dogs for doing these things, more people come out of the woodwork, new applications are discovered. Dogs that sniff truffles, for example. That's what I was thinking. That's one of my favorites. We have a hobbyist who wants to play with us now on that one. Can I borrow the dog for the weekend The thing I like about it is that you're giving humans access to the dog's perceptions, which are in many ways superior to humans, and just using the technology to allow them to communicate those perceptions. Their sense of smell is 100 times better than anything we do electronically. They have very good sense of hearing. Their eyesight is not as good as ours but they perceive differently. If we just allow them to actually communicate better, we might be able to learn a whole lot more about what they're capable of doing and how they might do their jobs better. Of course, the dogs think this is great fun. This is all play for them. Let's move on to haptic feedback and haptic learning and some systems you've done there. Is it true you can teach someone how to play the piano without actually playing It's pretty crazy. We call this thing passive haptic passive feedback learning. It's one of the super powers that wearable computers can give you. The idea that it can teach you a complex mechanical task or complex manual task without your attention. It is kind of surprising. We've done this with piano several times. Suppose you want to learn Beethoven's ""Ode to Joy."" It's one of the simpler ones we use for our demonstrations. We have you put on a glove, and the glove has little vibration motors on each finger. You put on your Bluetooth headset, and it just plays the song over and over again as you and I are talking or you're reading email, or you're driving, whatever. You ignore it and it just plays the song over and over again. As each note is played, it taps the finger that belongs to that note. After 20 minutes, you have the first stanza of ""Ode to Joy."" The next 20 minutes, you have the second one. In about an hour and 30 minutes of just this tapping, we got to break it up in the right chunks, but after about an hour and thirty minutes, you have the whole song. If you put your fingers down, then you put your hand down on the piano, we mark where the first note is because you wouldn't know, and your hand becomes possessed and it just plays the song it. It's one of these things that if you focus on it too much, you can't do it but if I distract you, you can. You play the song and, I imagine, you play the sounds back in your head to try and hit the same notes. We did this with Chad Myers on CNN once. He has no musical talent. We put it on his hand while he was tracking a hurricane in the Gulf, then brought him out live on CNN. You could see him have this stage fright. This guy's in front of 20 million people every day, yet gets stage fright as he puts his hand on the piano. You could see he didn't even know what the song was. He just starts tapping it. He doesn't know what it was going to be but as he goes on he recognizes the tune. Suddenly his rhythm gets better and he then repeats it. He gets better and better at it as he goes. You could see this ""oh my word"" revelation on his face. Having done it to myself a couple of times, I will say it is a very odd thing. out how far we can go with it. We started with just a single note, single hand melodies. Then, my student, Caitlyn, has figured out how to do chords. Now our standard piece is Mozart's Turkish March. For people who are musicians, you know that two hands are very different, very different rhythms on them. It's a complex enough piece that learning it is is non-trivial. We're taking complete novices, throwing the gloves on their hands and after a few sessions, they're playing this complex piece. I'm not saying they're great musicians but I'm saying they know the note sequence. We got interested in in what else we might be able to teach. One of the things we started looking at was typing. Since we figured out how to do chords, we tried to do braille. Braille involves these six fingers [showing his thumb, index, and middle fingers on both hands], so it's the dots, or these eight fingers if you do the extended form of it. Most people don't bother learning braille as they get older as they lose their vision. It's also a real crisis with students. About 40 percent of them never learn braille at least not sufficiently enough to communicate with using it. Learning braille for a student who is blind is one of the big indicators of future educational progress as well as economic success. These braille teachers who teach braille are itinerant, moving. They just go from school to school because there isn't enough demand to have a permanent teacher at a particular school. We started looking at making gloves that could teach people braille passively. The answer was yes. In four hours, I can give you an earbud and these gloves. While you're doing and you can do whatever you want, and it just repeats back to you, ""The quick brown fox jumps over the lazy dog."" It does each letter at a time and you feel the letter on your fingers. After about four hours of this, I say, ""What's the letter G,"" and you know it's that [showing the pattern on his fingers]. What's the letter A,"" you know it's Not only can you type it but you can feel it. You run your finger over the and know how to type the letter. You go, ""Okay, now how would I type that Okay, so that's the G."" To our surprise, it just works. What do you think is happening in the brain to enable that kind of connection Because it's not conscious and it's not just intellectual either It's something else between the mind and the body. There's a lot I really don't know. But there's some speculation by a colleague of mine who believes that what's happening is that the sense of touch is a much earlier evolutionary human sense that humans got evolutionarily. Almost all animals have a sense of touch. That seems to fit with the idea that touch is part of a very low-functioning part of our brain, very low lizard brain type stuff. Maybe it's the exposure that's really conditioning us to recognize these patterns. Whereas you try to learn something like language when you sleep, it doesn't work really well because audio is a very much higher brain thing. That's why we didn't expect the text entry stuff experiment to work. Music is very low brain stuff, whereas systems of language are thought of as higher brain function. Right. The fact that learning text entry worked was a surprise to us. We're not quite sure how far up we can go. Can we teach sign language Can we teach dancing Can we help experts become more efficient One of the things we want to do is get some baseball pitchers and record their arm movements and see if we can play that back to them in their downtime and increase their consistency. I have no idea if any of this stuff is going to work. One thing we did try, though, was Morse Code. Braille is just a key sequence. Morse Code is rhythm. We found that we can use the induction transducer on glass. To tap the side of your head. Now, this normally works by vibrating your skull and sending the sound directly to your cochlea, bypassing your eardrum. This is why I can wear this and get little alerts and not have anything blocking my ears, but if your hit with 15 hertz it feels like a tap. We started with ""quick brown fox jumps over the lazy dog"" style training, but now we're just tapping the side of your head in Morse Code and believe it or not in 4 hours againit seems like the magic time for the alphabetmost of our participants went down to 0 error...when we ask them to type S. We can show them dashes and dots on paper and they can write down the letters for it. We can tap the side of their head again and they can know what message we're sending. So four hours and we're having them do this during a video game. So we're making sure we distract them with a video game. Video games turn out to be the most efficient distraction. We've known that for a very long time. That's true. We're trying graduate entrance exams. Give them reading comprehension, or math problems. It turns out the only one that's distracting is video games. It's an interesting thing. We were just talking to Poppy Crum earlier today about audio and mixing senses. There's a lot of AR at this show; there's a lot of virtual reality. Some of the most interesting things are happening not just in one sense, but in a collection of senses. So, in this conversation we've talked about the visual space, we've talked about the audio space and also the physical. The tactile sensation. When you put all those things together it seems really like really interesting things start to happen. That's the whole thing. It's not just the synthesis or the senses. It's also the fact that it's something you can wear with you throughout your day. Once you have that sort of proximity to the body, you can do all sorts of things with the body we didn't think about before. Passive tactile tapping learning is one, and another thing that came up was passive tactile tapping rehabilitation. This was also quite a surprise to us. We had an open house demonstrating our piano playing thing and one of our colleagues at the Shepherd Centerwhich does a lot of work on spinal cord injuryshe came by and said, ""Hey Thad I want this for my patients."" I couldn't understand why she would want people with tetraplegia to practice piano. She said, ""No you don't understand, Tetraplegia just means all four limbs are affected. My patients with partial spinal cord injury still have some sensations and some dexterity. I bet if we put your gloves on them and trained them to play piano, that they'll actually regain some sensation; improve their sensation and dexterity."" She was right. We taught about eight different songs over eight weeks. Participants would have the gloves on two hours a day, five days a week and then have piano lessons three times a week. At the end of the eight weeks, the participants who had the gloves did much better. They had significant improvements in their hand sensation and dexterity compared to the people who just had normal piano lessons. So, this idea that we can do rehabilitation, that we can trick your brain into devoting more and more resources to explain these signals coming out from your hand is just astonishing. I don't know if you've ever had a broken leg or had a parent that had a hip surgery, but rehabilitation takes a long time. It's very boring, and it's hard to make it fun. It's hard to get compliance. If I can just put on a glove and have rehab happen, and by the way, we're going to give you this super power that you're going to learn to play piano by the end of it, that's kind of motivating for a lot of people. That's what we're currently doing. We're doing it right now. We're trying to see if this might help with recovery of hand sensation after stroke. If that turns out to work, then there are a million people a year we might help. That's very speculative. We have no idea if it's going to work or not, but we'll see. All right. Let's get to some questions I ask all my guests. What technology trends do you see right now that concern you the most What keeps you up at night Very little keeps me up at night. I sleep quite well. I have three jobs. I don't have time to sleep. A lot of people's worries about technology is what you see in the press. Reality's a whole lot more boring than what a lot of commentators will lead you to believe or the movies will. I guess the thing I worry most about is the echo chamber that social media can cause. People can believe pseudo-science. People can be led to believe things that just aren't true. People make commentaries on my work, and I'm like, that's wrong. I do a lot of work with the deaf community. It turns out that teaching your kid sign language will help them acquire language skills faster. For the longest time people believed that if you teach your kids sign language, it will delay the onset of normal speech. That's precisely wrong. Teaching sign language from birth seems to help improve everyone's language skills. Most kids can't really form the words coherently until they're 12 months old. They can start forming the signs, you know things like milk or food. They can do that at six months. Having children be able to express themselves is what gives them memory. You learn short-term memory from learning how to communicate. So having the kids be able to sign six months earlier than they can speak may be giving them an advantage on memory and language in general. The original research was 20 years old but they're pretty sure about it now. So it wasn't until recently that even the medical community has said, ""Oh you should teach not just deaf kids but all kids sign language."" It's a pretty powerful thing. We have a game called Pop Sign that allows hearing parents of deaf children to learn sign language, or any parents to learn sign language, by playing on this addictive game site or their mobile phones. So, coming back to your original question I see the other situations where people who get into echo chambers especially with rare knowledge. It's easy to get people going down the wrong path. On the optimistic side, what are you most encouraged by and most hopeful for Well, I like the stuff that Elon Musk is doing about trying to convert us over to solar power . I'm the kind of person waiting for those roof tiles to be available so I can convert my house over. In my own space, the wearable computing, the intelligent assistant is taking off. Seeing people's lives changed by the availability of this device is not just because it is improving their personal lives or their professional lives, but literally saving lives on the operating room table, just because of the information we can deliver just in time to the surgeon. So this idea that we're going to have these intelligent assistants that can help us on a second-by-second basis. Finally, we are getting to the stage where this stuff these things are becoming useful. What gadget, a service or a device that you use every day, changed your life What do you love the most You cannot pick Google Glass. I guess I'll allow that. The system I had before Glass was a Micro Optical SVB6 display and the Twiddler keyboard and a shoulder pack computer. The hardware wasn't the game changer, it was the ability to take notes during conversations and refer to these notes that changed my life. If I sit across the table from a Nobel Prize winner at lunch, I want to be able to remember that conversation. I want to know just the five words that will spur my natural memory. It turns out that having this keyboard so I can type underneath the table and take notes on that conversation has made me a much more effective human being and more socially graceful, too. One of the things that my students were shocked to see is that before I see somebody, I pull up my Rolodex on them. It has the last time we talked together and what we talked about. It really helps me reload my brain about our conversations. Suppose I'm visiting an old friend. I have notes like ""Hey yeah his daughter's kid was going off to college. She was studying geology; I wanted to put her in contact with my wife. I should ask him if that happened."" It really enables you, not just in your professional career, but also being more social as well. Yeah, we talked a lot about these devices making us smarter and making us seem smarter. That extension of memory where you're suddenly connected to more of your life because you can remember more of it because you can access more of it and get those reminders in real time, that's an interesting application. In the early days of wearable computing, we did three things. We did augmented/alternate reality, intellectual collectives, and augmented memory. The intellectual collective is a kind of social media. Augmented reality is all the rage these days. Most people don't realize it was called Artificial Reality before we coined the term Alternate Augmented Reality. Actually, I can tell you why we changed the name. Why did you do that In my early days of studying this artificial reality, there was a guy name Myron Krueger, who wrote a wonderful book called Artificial Reality 2 . I highly recommend that to anyone who wants to understand Augmented Reality. Anyways, I think it was Timothy Leary who started conducting press interviews where he talked about Virtual Reality and Artificial Reality being like LSD drug trips. At the time, I was a student writing a [US military] grant proposal, and I didn't want them to look up the term Artificial Reality and see it affiliated with LSD. So I thought I needed to find another term to use that wouldn't pull up something on the web that might get me in trouble, so we made this conscious decision to call it Augmented Reality. Fortunately for these other guys at Boeing were also looking at using augmented reality for assembling this stuff for doing electrical line harnesses and making airplanes, and they came up with the same term. So I think I had the first written version of it, and they had the first publication on it. The name Augmented Reality stuck. Did you get the grant I did. It's why I'm here today. Thank you to the NDSEG , the National Defense Science and Engineering Graduate Fellowship, I do believe. If it weren't for you guys, I wouldn't be here. I wouldn't be a professor. That grant allowed my professor to take on an additional grad student and allow me to make wearable computers happen. So thank you very much. ",http://www.pcmag.com/article/352856/fast-forward-thad-starner-talks-google-glass-helping-dogs,"I had a student who was talking to me about speech recognition—how you want to use speech recognition with wearables. And I said, well if ...","Fast Forward: Thad Starner Talks Google Glass, Helping Dogs Talk",Thad Starner Google Glass SXSW Austin Texas SXSW Interactive Fast Forward Chris Becherer VP Product Pandora Fast Forward Thad Starner Professor Computing Georgia Tech Thad Google Glass Dan Costa SXSW Google Glass Thad Starner Well Google Glass [ Google Glass ] Micro Optical BBN Eventually MIT Media Lab Suppose VIP Slack Well IQ Remembrance Agent Steve Whittaker Filochat Google Glass ] Steve Whittaker National Academy Sciences Washington D.C. Georgia Techso D.C. Georgia Tech Georgia Tech Google Glass Larry Page ] Okay Mount Pinatubo Google Glass ] Philippines Xerox Park Georgia Tech Stanford Saint Stanislaus Google Saint Stanislaus Polandand MIT Justine Cassell Deixis Twiddler ] E-Macs [ ] ] FIDO Facilitating Interactions Dogs Occupations TSA SXSW [ Atlanta Airport TSA ] Please Yes Georgia Tech SXSW FIDO Georgia Tech C4 Peroxide Gunpowder C4 Georgia Tech TSA North Carolina Suppose Beethoven Ode Joy Bluetooth Ode Joy Chad Myers CNN Gulf CNN Caitlyn Mozart March Braille G A Okay Okay G. Almost Music Right Can Morse Code Braille Morse Code Morse Code S. Video Poppy Crum AR Passive Shepherd Centerwhich Hey Thad Tetraplegia Reality Pop Sign Well Elon Musk Google Glass Glass Micro Optical SVB6 Twiddler Nobel Prize Suppose Hey Artificial Reality Alternate Augmented Reality Actually Myron Krueger Artificial Reality Augmented Reality Anyways Timothy Leary Virtual Reality Artificial Reality LSD US Artificial Reality LSD So Augmented Reality Boeing Augmented Reality Did Thank NDSEG National Defense Science Engineering Graduate Fellowship,1
99,"Are Speech Recognition Solutions Worthwhile in your Contact Center Strategy Speech recognition technology is often seen as a means to cut costs, improve customer satisfaction, and increase productivity in the call center. But are these solutions worth it Contact centers across Latin America are looking to transform their customer service solutions by implementing new channels , such as chat, social media, email, video, or updated voice services, which are still hugely relevant in the region. Within that scope, speech recognition technology is often seenas a means to cut costs, improve customer satisfaction, and increase productivity in the call center. But are these solutions worth it Speech recognition is performed by a machine or program that can identify words or phrases in spoken language and convert them into a machine-readable format. In laymans terms, the software detects what youre saying and automatically transcribes your words into text. Beyond interactive voice response (IVR) systems, some of the more popular call center speech recognition applications are call routing, speech-to-text, voice dialing, and voice search. Some companies are experiencingimproved service quality due to a combination ofspeech recognition andIVR systems, since the automated system isavailable when human agents are not. Moreoutboundcontact centers areusing IVR/speech recognition systems to improve their effectiveness, especially to serve clients in the collections and sales segments. Octavio Maupome: Many contact centers in the region are hiring a lot of people to listen to 3-4% of all calls, which is very costly. Originating from Spain, Verbio is a speech technology provider serving contact centers in Ecuador, Brazil, Colombia, and Mexico. Verbios core speech recognition engine uses a database of words and phrases to create transcripts of calls. This text is then analyzed to determine what the customer has been saying or how the agent has dealt with certain situations. Therefore, the software can measure whether the agents are off-script, adhering to regulations, or not performing at a high enough standard. When trying to track first contact resolution, companies can analyze certain phrases like this is the second time I called to get this fixed or something similar, said Octavio Maupome, Country Manager for Mexico, Cenam, and the Caribbean at Verbio Technologies. This business intelligence can vastly improve KPIs and customer experiences, to a point that human analysis of this kind of data becomes almost obsolete. Another provider in the region, Nuance, uses artificial intelligence in its Natural Language Understanding solutions in order to extract meaning from a customer request. It is also used in some of the companys Biometrics solutions in order to learn and generalize patterns based on a set of observations. Perhaps the most intriguing speech solutions for Latin American contact centers right now are speech analytics, as they help to understand the trends in what their customers are asking, increasing the chances of success in their campaigns. Through business intelligence, contact centers can understand things like when the best time to sell something is, when collections are easier, or which agents are performing best, said Maupome. At the moment, many contact centers in the region are hiring a lot of people to listen to 3-4% of all calls, which is very costly. With speech analytics solutions, 100% of the calls can be reported on, without the need for human interaction. This is why some contact centers and BPO firms are showing more interest. Even though the quality assurance and KPI benefits might seem obvious, it can be difficult to implement such solutions. You need the right level of processing power from your IT infrastructure, specific professional services, and each region requires certain licenses to operate them. Many contact centers are of a mind that if its not broken, dont fix it, so are not taking these solutions on as rapidly as the providers might hope. Regional accents as well as non-native speakers are the reality of the environments in which our solutions are deployed world-wide, said Marco Petroni, Senior Director, Enterprise Professional Services at Nuance. Over the course of 20 years of deployments, Nuance has ensured that our underlying speech recognition technologies are robust to a variety of conditions, not only accents, not just for the U.S. and Latin America, but for all the languages that are supported. Sign up for our Nearshore Americas newsletter: For Verbio, the process of detecting accents in Latin America has been easier due to the companys Spanish origin, which has helped develop a base of vocal samples in European Spanish. The company uses that base to produce specific products for each market, due to the level of variation in dialect, accent, and vocabulary. Even in Mexico, the people from the north of the country speak differently to the central states, so we have to bring in individuals from all locations to record and add their unique dialects and accents to the mix, said Maupome. The complications go even deeper when you consider people of different age groups. People in the younger marketwhile ultimately disinterested in utilizing voice for customer servicehave a completely different way of speaking than the more mature customers. According to Petroni, the main challenge in developing self-service voice solutions for Latin American contact centers is being able to articulate to the clients the business value of a potential solution. Generally speaking, organizations or contact centers have either little or unreliable data regarding call volumes and call dispositions, he said. As a result, while there could be a very compelling case to be made regarding the business value and ROI of a potential solution, one is generally hard pressed to translate proof points from other deployed solutions world-wide to the potential benefits for a specific organization or contact center. Ultimately, speech recognition solutions are best-suited to large-scale contact centers that want to reduce their spending on human resources, particularly in call analysis. The costs of an agent transaction versus an automatic transaction are certainly higher, but companies need to consider the cost of maintaining sufficient IT infrastructure and service fees before taking the plunge into full voice automation. ",http://www.nearshoreamericas.com/speech-recognition-strategy/,"Speech recognition technology is often seen as a means to cut costs, improve customer satisfaction, and increase productivity in the call center.",Are Speech Recognition Solutions Worthwhile in your Contact ...,Are Solutions Worthwhile Contact Center Strategy Contact Latin America Beyond IVR Moreoutboundcontact IVR/ Maupome Spain Verbio Ecuador Brazil Colombia Mexico Verbios Octavio Maupome Country Manager Mexico Cenam Caribbean Verbio Technologies KPIs Nuance Natural Language Understanding Biometrics Maupome KPI IT Marco Petroni Senior Director Enterprise Professional Services Nuance Nuance U.S. Latin America Sign Nearshore Americas Verbio Latin America Spanish Mexico Maupome Petroni ROI IT,3
100,"Google's reCAPTCHA anti-robot widget has been found to be susceptible to a robot attack that leverages its own online services. A security researcher has managed to defeat one of Google's security measures by using another one of its services. According to a security researcher who goes by the moniker East-EE, there is a logic vulnerability within Google's reCAPTCHA field that enabled them to bypass the security using the company's own speech recognition service. This is not the first time that reCAPTCHA has been defeated by security researchers. In April 2016, researchers managed to defeat the image reCAPTCHA 70 percent of the time using deep-learning technology. A proof-of-concept script of the hack has been posted on GitHub using thePython programming language which enables an attacker to automatically bypass reCAPTCHA fields used to protect websites from spam and bot traffic. The researcher said in a blog post that the flaw, dubbed ReBreakCaptcha, works in three stages. The first step is to get reCAPTCHA to present the hacker with an audio challenge. reCAPTCHA always presents a user with three possible challenges  an image, text or audio clip. If presented with an image or text challenge, by clicking on the headphone icon or choosing the Reload Challenge' button, reCAPTCHA will generate an Audio challenge which can be easily bypassed, they said. The second stage is recognition: a hacker would then have to convert the audio from reCAPTCHA to a wav file and send this to Google's Speech Recognition API. There is a great Python library named SpeechRecognition' forperforming speech recognition, with support for several engines and APIs, online and offline, said East-EE. We will use this library implementation of Google Speech Recognition API. We will send the wav' audio file and the Speech Recognition will send us back the result in a string (e.g. 25143'). This result will be the solution to our audio challenge. Lastly, for the hack to be successful, the speech recognition result would need to be verified to bypass reCAPTCHA. This involves copying and pasting the output string from Stage 2 into the textbox, and click Verify' on the reCAPTCHA widget. That's right, we nowsemi-automatically used Google's Services to bypass another service of its own, they said. They added that a lot of people encounter a harder version of the audio challenge. Therefore, I have committed a workaround to the GitHub Repo that should overcome this situation, though at a lower success rate compared to the original easier audio challenges, said East-EE. It is still not fully clear how this harder version is triggered, but the number one reason suspected is when your IP is suspicious to Google. The researcher did not state whether Google has been alerted to the issue. SC Media UK asked Google for a comment, but the company had not replied at the time of writing. Bogdan Botezatu, senior e-threat analyst at Bitdefender, told SC that while image-based CAPTCHA was problematic for hackers, this new technique of piping audio messages to a sound recognition system can be easily used to get the CAPTCHA challenge and defeat the security mechanism. Most CAPTCHA security mechanisms have an accessibility method that allows visually-impaired computer users to solve the challenges. It is a matter of time until the next popular CAPTCHA system will be defeated by exploiting the accessibility features, he said. David Kennerley, director of threat research at Webroot, told SC that this was quite a simple, innovative way to bypass reCAPTCHA. I'm sure the irony of using Google to defeat Google will not be lost on people. While success rates may vary, for the limited amount of effort and time needed to pull off this hack, I'd imagine there's still pay-off even if it doesn't work every time, he said. Lee Munson, security researcher at Comparitech.com, told SC that ditching reCAPTCHA altogether may be a drastic move, given the alternatives are arguably no better, but businesses should keep an eye on server logs and be ready to block individual or ranges of IP addresses or put affected pages into a lockdown state. ",https://www.scmagazineuk.com/ear-ear-hacker-could-defeat-google-recaptcha-with-speech-recognition/article/642286/,"There is a great Python library named 'SpeechRecognition' for performing speech recognition, with support for several engines and APIs, ...","Ear, ear: Hacker could defeat Google reCAPTCHA with speech ...",Google Google East-EE Google April GitHub ReBreakCaptcha Reload Challenge Audio Google API Python SpeechRecognition APIs East-EE Google API Stage Verify Google Services Therefore GitHub Repo East-EE IP Google Google SC Media UK Google Bogdan Botezatu Bitdefender SC CAPTCHA CAPTCHA CAPTCHA David Kennerley Webroot SC Google Google Lee Munson Comparitech.com SC IP,4
101,"2016 was a great year for advancements in AI technologies and machine learning. The AI market is also flourishing. Despite all the hype and media attention, numerous startups and internet giants are all racing to develop this technology. There have been massive increases in investment and adoptions of it by companies. A study by Narrative Science found that last year alone 38% of businesses had already adopted AI. This uptake is expected to grow to 62% by 2018. Another study by Forrester Research predicts a 300% boom in investment in AI in 2017 compared to last year. The AI market is expected to grow to $47 billion by 2020 from $8 billion today. Forrester research recently published a TechRadar report on AI which has a detailed analysis of 13 technologies businesses should consider adopting. Dont get too worried, their main task is to support rather than replace human workers. Well at least for the mean time! So lets get stuck in. This list is not in any particular order, dont worry you wont be made redundant just yet! Natural language generation produces text from computer data. IT is currently used in customer services, report generation and summarizing business intelligence insights. This technology is currently provided by the likes of Attivio, Automated Insights, Cambridge Semantics, Digital Reasoning, Lucidworks, Narrative Science, SAS and Yseop. Speech recognition, as youd expect, enables the computer to transcribe and transform human speech into understandable formats for computer applications. It is most notably used in interactive voice response systems and mobile applications such as Siri, Cortana and Alexa to name a few. Example suppliers includeNICE, Nuance Communications, OpenText and Verint Systems. Forrester refers to virtual agents as the current darling of the media. This encompasses simple chatbots to advanced systems that can talk almost naturally with humans. They are currently used in customer services as well as smart home device managers. These are becoming ever more sophisticated, so dont feel too bad if you are beginning to genuinely like Alexa for example. For those who have watched Her , we may not be too far away! This technology is currently in development from companies such asAmazon, Apple, Artificial Solutions, Assist AI, Creative Virtual, Google, IBM, IPsoft, Microsoft andSatisfi. Machine learning platforms provide algorithms, APIs, development and training toolkits and data. They also provide computing power to design, train and deploy models, applications, processes and other machines. MLPs are currently used in a large array of business applications that are primarily concerned with prediction or classification. This technology is currently in development and provided by companies such as Amazon, Fractal Analytics, Google, H2O.ai, Microsoft, SAS, Skytree. AI-optimized hardware is types of graphics processing units and appliances designed and architected to efficiently run AI-orientated computational tasks. They are primarily employed for use of deep learning applications. Example suppliers include Alluviate, Cray, Google, IBM, Intel and Nvidia. Decision management engines insert rules and logic into AI systems. They are used mainly for the initial setup and training as well as ongoing maintenance and tuning of such systems. This is a mature application of AI and used widely in avariety of business applications. They assist in or actually perform automated decision-making processes. Example suppliers include Advanced Systems Concepts, Informatica, Maana, Pegasystems an UiPath. Deep learning platforms are a special type of machine learning that consists of artificial neural networks and multiple abstraction layers. They are currently mainly used in pattern recognition and classification that involve very large data sets. Sample vendors includeDeep Instinct, Ersatz Labs, Fluid AI, MathWorks, Peltarion, Saffron Technology and Sentient Technologies. Biometrics enable natural interactions between humans and machines. These include, but are not limited to, image and touch recognition, speech and body language recognition. They are currently used mainly for market research by businesses. They are currently developed and supplied by companies like3VR, Affectiva, Agnitio, FaceFirst, Sensory, Synqera, Tahzoo. Robotic process automation use scripts and other methods to automate human action to improve business process efficiency. They are currently employed where it is too expensive or indeed inefficient for humans to perform a similar duty. Perhaps future applications will include personal security Ok, I just wanted to segue a Terminator image into the article! Example suppliers include Advanced Systems Concepts, Automation Anywhere, Blue Prism, UiPath, WorkFusion. NLP or natural language processing supports and uses text analytics by understanding sentence structure and meaning. It is also able to understand sentiment and intent through statistical and machine learning methods. They are currently used in fraud detection and security, a wide range of automated assistants as well as applications for mining unstructured data. Example suppliers includeBasis Technology, Coveo, Expert System, Indico, Knime, Lexalytics, Linguamatics, Mindbreeze, Sinequa, Stratifyd, Synapsify. Businesses are certainly benefiting from abilities of AI technologies today. Forresters survey from 2016 (linked below but its pay to play) does show that there are still some obstacles to their wider adoption. A large proportion of companies do not currently see the need for them. They may also seem too expensive when compared to the more traditional solution of human labor. Other obstacles include a lack of understanding of these technologies, perceived hype around them or lack of in-house expertise to maximize their use. Forrest concludes that once businessesovercome these obstacles, they stand to gain massively from AI technologies. Its widespread adoption will accelerate transformation in customer-facing applications as well as enable developing an interconnected web of enterprise intelligence. Whatever the future of AI is, its certain that its replacement of traditionally human operated tasks will explode. It has been predicted for many years and its a genuine concern that many jobs will actually become redundant once a computer or machine can perform them. In most cases, AI technologies will be more efficient, more competent and will not need to take breaks. But, as with any technological revolution, where traditional jobs are automated new jobs will be conceived, or will they But thats a question for another time ",http://interestingengineering.com/top-business-applications-ai-technologies-today/,"Speech recognition, as you'd expect, enables the computer to transcribe and transform human speech into understandable formats for ...",Top Business Applications of AI Technologies in Use Today,AI AI Narrative Science AI Forrester Research AI AI Forrester TechRadar AI Dont IT Attivio Automated Insights Cambridge Semantics Digital Reasoning Lucidworks Narrative Science SAS Yseop Siri Cortana Alexa Nuance Communications OpenText Verint Systems Forrester Alexa Her Apple Artificial Solutions Assist AI Creative Virtual Google IBM IPsoft Microsoft Machine APIs MLPs Amazon Fractal Analytics Google H2O.ai Microsoft SAS Skytree AI-optimized AI-orientated Alluviate Cray Google IBM Intel Nvidia Decision AI AI Advanced Systems Concepts Informatica Maana Pegasystems UiPath Deep Instinct Ersatz Labs Fluid AI MathWorks Peltarion Saffron Technology Sentient Technologies Biometrics Affectiva Agnitio FaceFirst Sensory Synqera Tahzoo Ok Terminator Advanced Systems Concepts Automation Anywhere Blue Prism UiPath WorkFusion NLP Technology Coveo Expert System Indico Knime Lexalytics Linguamatics Mindbreeze Sinequa Stratifyd Synapsify AI Forrest AI AI AI,7
102,"A researcher has discovered what he calls a ""logic vulnerability"" that allowed him to create a Python script that is fully capable of bypassing Google's reCAPTCHA fields using another Google service, the Speech Recognition API. The researcher, who goes online only by the name of East-EE, released proof-of-concept code on GitHub . East-EE has named this attack ReBreakCaptcha , and he says he discovered this vulnerability in 2016. Today, when he went public with his research, he said the vulnerability was still unpatched. The researcher was not clear if he reported the bug to Google. Bleeping Computer has reached out to the researcher to inquire if Google was, at least, aware of the issue. The proof-of-concept code the researcher released allows attackers to automate the process of bypassing reCAPTCHA fields, currently used on millions of sites to keep out spam bots. East-EE says his attack only works against Google reCAPTCHA v2, the current version of the reCAPTCHA service. More exactly, the attack uses audio challenges, a secondary checking system available only when pushing a button at the bottom of the normal reCAPTCHA popup. In case people are using older browsers with no audio playback support, Google allows users to download the audio challenge. The researcher says that he was able to take this audio file and feed it back into one of Google's own service, the Speech Recognition API. The API returned back a written version of the audio challenge, which the researcher took and fed back into the reCAPTCHA field. Because all of these steps can be automated, it is safe to expect that attackers will use East-EE's ReBreakCaptcha proof-of-concept code to create browser extensions or web-based services that provide reCAPTCHA-bypassing services. This is not the first time a researcher managed to bypass Google's reCAPTCHA system. Back in April 2016 , a trio of researchers found a way to bypass both Google and Facebook's CAPTCHA solutions, with a 70.78% success rate for Google, and 83.5% for Facebook. As reported in December 2016 , Google is working on reCAPTCHA v3, which needs minimal user interaction and is currently referred to as Invisible reCAPTCHA . ",https://www.bleepingcomputer.com/news/security/researcher-breaks-recaptcha-using-googles-speech-recognition-api/,"The researcher says that he was able to take this audio file and feed it back into one of Google's own service, the Speech Recognition API.",Researcher Breaks reCAPTCHA Using Google's Speech ...,Python Google Google API East-EE GitHub East-EE ReBreakCaptcha Google Bleeping Computer Google East-EE Google Google Google API API East-EE ReBreakCaptcha Google April Google Facebook CAPTCHA Google Facebook December Google,4
103,"Apple Mac Pro design was a bad bet, admits firm Apple has acknowledged that the design of its standalone desktop computer, the Mac Pro, restricted its ability to update the model. The firm has announced a limited refresh of the machine, the first time it has upgraded its innards since 2013. But it said it would not be able to release a ""new model"" until an unspecified point after 2017. The tech firm had faced criticism that it was not addressing the needs of its professional users. It released a new line-up of MacBook-Pro-branded laptops in October, but was attacked for not including more powerful graphics cards in them and for removing several of the ports found in earlier models. One expert said the latest announcement would reassure graphic designers, video editors, scientists and other power users that Apple was not abandoning them to focus on iPads and iPhones, which outsell the Macs. Apple's engineering chiefs told a small group of journalists - which did not include the BBC - that the company had been constrained as a consequence of how it had expected graphics chip technology to develop. The firm had believed it would become more common to use multiple graphics processing units (GPUs) to carry out complex tasks. The Mac Pro was designed to use two in parallel. Instead, component-makers have focused on designing parts that maximise the output of a single GPU and give out more heat as a result. This caused problems for the trash-can-like design of the machine. It did away with the multiple heat sinks and fans found in earlier Mac Pros, and instead used a single triangular piece of aluminium to conduct heat away from its processors, helping it to run more quietly as a result. ""The way the system is architected, it just doesn't lend itself to significant reconfiguration for somebody who might want a different combination of GPUs,"" Techcrunch quoted Apple's hardware chief John Ternus as saying. ""We realised we had to take a step back and completely re-architect what we're doing and build something that enables us to do these quick, regular updates and keep it current and keep it state of the art, and also allow a little more in terms of adaptability to the different needs of the different pro customers."" The firm's software engineering chief Craig Federighi added: ""I think we designed ourselves into a bit of a thermal corner... being able to put larger single GPUs required a different system architecture and more thermal capacity than that system was designed to accommodate."" The refreshed models add more powerful CPU (central processing units) and dual-GPUs than before. But otherwise little has changed, including a lack of USB-C or Thunderbolt 3 ports common to Apple's other high-end systems, which offer data speed benefits. ""I think it was simply untenable for Apple to continue to remain silent on the Mac Pro front,"" commented the tech blogger John Gruber, who was also invited to the meeting. ""No matter how disappointing you consider today's speed bump updates to the line-up, they're certainly better than no updates at all. ""[But] if they had released these speed bumps without any comment about the future of the Mac Pro, people would have reasonably concluded that Apple had lost its... mind."" This is the second time in a week that Apple has been in the news because of GPUs. On Tuesday, Imagination Technologies - the British designer of GPUs for its smartphones and tablets - revealed that Apple planned to go it alone and design its GPUs in-house. What's the difference between a CPU and a GPU In basic terms, a CPU calculates single sets of instructions more quickly (or two sets of instructions simultaneously if a dual-core chip is involved), while GPUs specialise in carrying out many calculations at the same time. This makes GPUs better suited for ""parallelisable"" tasks - jobs that can be broken down into several parts and run simultaneously because the outcome of any one calculation does not determine the input of another. As the name suggests, powering graphics is one example, but others include speech recognition and pattern matching. Things that happened since Apple last updated the Mac Pro: Image caption A revamped Mac Pro was unveiled in June 2013 and put on sale six months later, but had not been updated since Apple released the last version of its top-end desktop computer way back on 19 December 2013, but opted not to update it at its latest event. The ""trash can"" was targeted at video editors, 3D graphics artists, engineers and other professionals who wanted a powerful workstation. But three years is a very long time to go without a refresh, and a lot has happened in both tech and beyond in the interim. To give an idea of how long it's been, all the events below have occurred since the computer went on sale: video game Flappy Bird was released, pulled and released again ",http://www.bbc.com/news/technology-39494785,"... the input of another. As the name suggests, powering graphics is one example, but others include speech recognition and pattern matching.","Apple Mac Pro design was a bad bet, admits firm",Apple Mac Pro Apple Mac Pro MacBook-Pro-branded October Apple Macs Apple BBC GPUs Mac Pro GPU Mac Pros GPUs Techcrunch Apple John Ternus Craig Federighi GPUs CPU USB-C Thunderbolt Apple Apple Mac Pro John Gruber Mac Pro Apple Apple GPUs Tuesday Imagination Technologies GPUs Apple GPUs CPU GPU CPU GPUs GPUs Apple Mac Pro A Mac Pro June Apple December Flappy Bird,4
104,"The Mobile Internet Is Over. Baidu Goes All In on AI The Chinese company has more than 1,300 people working on tech like deep learning. On Dec. 6, 2016, thousands of translators filed into office buildings across mainland China to pore over brochures, letters, and technical manuals, all in foreign languages, painstakingly rendering their texts in Chinese characters. This marathon carried on for 15 hours a day for an entire month. Clients that supplied the material received professional-grade Chinese versions of the originals at a bargain price. But Baidu Inc., the Beijing-based company that organized the mass translation, got something potentially more valuable: millions of English-Mandarin word pairs with which to train its online translation engine. China is infamous for its knockoffs, whether luxury handbags or web startups. But the countrys leadership seems to understand that when it comes to artificial intelligence, cheap imitations just wont donot when its rivals include Alphabet, Facebook, IBM, and Microsoft. In February the National Development and Reform Commission appointed Baiduoften described as the Google of Chinato lead a new AI lab, signaling that Beijing believes the company has the makings of a national champion in this sphere. Of the more than 20 billion yuan ($2.9 billion) Baidu has spent on research and development over the past two and a half years, most has been on AI, according to comments co-founder and Chief Executive Officer Robin Li made at the labs launch last month. But Chinas national interest isnt his main motivation: Baidus revenue growth fell to about 6 percent last year, from an average of more than 30 percent over the prior three years. The search ad business, which contributed the lions share of its 70.5 billion yuan in sales in the fiscal year ended on Dec. 31, is under siege from local rivals. A September report from EMarketer Inc. noted that Alibaba Group Holding Ltd. had overtaken Baidu to become the leader in Chinas digital ad market. Baidu hopes AI can help it reclaim share in search, as well as ensure success in newer ventures. Baidu showed off Little Fish, a voice-activated robot, at CES. Thats key as the 17-year-old companys attempts at diversification have produced mixed results. The number of daily visitors to its group buying site, Nuomi, dropped 59 percent in the 12 months through February 2017; its Waimai food delivery service lags in third place, according to Natalie Wu, an analyst with China International Capital Corp. The Netflix-like streaming video service iQiyi.com is hugely popular, but it will take 12 billion yuan to keep it stocked with content this year, estimates analyst Ella Ji with China Renaissance Securities (Hong Kong ) Ltd. Those faltering efforts mean Baidus push into AI is taking on greater importance. The era of mobile internet has ended, said Li in a March 10 interview. Were going to aggressively invest in AI, and I think its going to benefit a lot of people and transform industry after industry. In January the company named former Microsoft Corp. executive Qi Lu as its chief operating officer, with a mandate to reshape the company around such technologies as deep learning, augmented reality, and image recognition. He joins Chief Scientist Andrew Ng, a Stanford academic who worked on Alphabet Inc.s deep learning group before decamping to Baidu in 2014. Under Ngs watch, the companys AI team, which is scattered across research labs in Beijing, Shenzhen, Shanghai, and Sunnyvale, Calif., has grown to 1,300 and is expected to increase by several hundred more hires this year. A ton of stuff is invented in China, and a ton of stuff is invented in the U.S., says Ng, whos based in Silicon Valley. By having people in both countries, we see the latest trends. On the day in May 2014 that the Sunnyvale research center opened, Ng and his top lieutenant, Adam Coates, sat down in front of a blank whiteboard to identify their first project. After drawing up a list of possibilities (and challenges), they settled on speech recognition as a foundation on which they could build a series of other offerings. By mid-2015, the 50-person team had a product called Deep Speech that could decipher much of what was said in English. Rather than picking apart phrases word by word, the software parsed through vast reams of language data and then extrapolated patterns, a process known as deep learning. The system could transcribe speech more accurately than traditional engines that rely on vocabulary lists and phonetic dictionaries, Ng says, because it took into account a words context to determine its meaning. One thing that consistently tripped it up, though, were words and names that had over time crept into the English lexicon from other languages. If you want to say Play music by Tchaikovsky, the software would return answers like Play music and try cough ski,  says Coates, whom Ng recruited from Stanford. We literally dubbed it the Tchaikovsky Problem. Instead of simply adding Tchaikovsky to the systems vocabulary list, Baidus programmers had to help Deep Speech teach itself to understand the word. That involved pumping in even more data to help the system put things in context. Shiqi Zhao, the Beijing-based associate director of Baidus natural language processing department, recalls that as a computer science student at Chinas Harbin Institute of Technology he had only 2 million word pairs of English-to-Chinese terms to play with while working on computer-based translation; Baidu has about 100 million. However, thats still far fewer than Alphabets 500 million, according to a 2016 article in Science magazine that featured one of the U.S. companys research scientists, Quoc V. Le. To help close the gap, Baidu has resorted to an age-old tactic: throw lots of people at the problem. The company now facilitates manual translations year-round and stages marathon events such as the one in December at regular intervals, in which it offers clients prizes such as smartphones and water purifiers. The data collected help enhance the performance of its Baidu Translate engine as well as further the development of Deep Speech. The software created by the Sunnyvale team had its commercial debut in July 2016 with the release of TalkType, a keyboard app with a talk-to-text feature. The technology has since been incorporated into other products, including a Siri-like personal assistant named DuMi in China and DuEr everywhere else. (DuMi is a fusion of du from Baidu and mi, which means secretary in Mandarin; DuEr sounds like doer.) The machine learning Baidu has inculcated into Deep Speech is helping it animate other products with intelligence. For instance, its the secret sauce in Xiaoyu Zaijia (Little Fish), a voice-controlled robot,  la Amazon Echo, that Baidu showed off at the CES show in Las Vegas in January. Baidus portfolio of web properties gives it access to one of the largest and most detailed sets of consumer data ever produced in China, whichin theory at leastshould give it an edge in building AI-infused products and services for the mainland. Thanks to Nuomi and Waimai, the company knows what Chinese households buy and eat, while Ctrip.com, the worlds second-largest online travel agent, reveals where they want to holiday. Every month 665 million smartphone users surf its mobile portal and apps, while 341 million use Baidu Maps to reach their destination. Its a mistake to think of AI as a productit underpins and enables product, says HSBC Holdings Plc analyst Chi Tsang. Think of all the use cases. The most important business stories of the day. The new AI products arent contributing much to Baidus bottom line yet. But the companys nascent expertise in this area could help it achieve dominance in segments where its already present and propel it into new ones, such as cloud computing and self-driving cars. In the next three to five years all those areas have the potential to become another Baidu, company President Zhang Ya-Qin says, referring to Baidus $60.2 billion market capitalization. Right now its time to make some bets. The bottom line: Baidus 1,300-person AI team is writing software to improve everything from translation to food ordering. Before it's here, it's on the Bloomberg Terminal. ",https://www.bloomberg.com/news/articles/2017-03-16/the-mobile-internet-is-over-baidu-goes-all-in-on-ai,"After drawing up a list of possibilities (and challenges), they settled on speech recognition as a foundation on which they could build a series of ...",The Mobile Internet Is Over. Baidu Goes All In on AI,Mobile Internet Over Baidu Goes All AI Dec. China Baidu Inc. English-Mandarin China Alphabet Facebook IBM Microsoft February National Development Reform Commission Baiduoften Google Chinato AI Beijing Baidu AI Chief Executive Officer Robin Li Chinas Baidus Dec. September EMarketer Inc. Alibaba Group Holding Ltd. Baidu Chinas Baidu AI Baidu Fish CES Thats Nuomi February Waimai Natalie Wu China International Capital Corp Ella Ji China Renaissance Securities Hong Kong Ltd. Baidus AI Li March AI January Microsoft Corp. Qi Lu Scientist Andrew Ng Stanford Alphabet Inc.s Baidu Ngs AI Beijing Shenzhen Shanghai Sunnyvale Calif. A China U.S. Ng Silicon Valley May Sunnyvale Ng Adam Coates Deep Speech English Rather Ng Play Tchaikovsky Play Coates Ng Stanford Tchaikovsky Problem Tchaikovsky Baidus Deep Speech Shiqi Zhao Baidus Chinas Harbin Institute Technology Baidu Science U.S. Quoc V. Le Baidu December Baidu Translate Deep Speech Sunnyvale July TalkType DuMi China DuEr DuMi Baidu Mandarin DuEr Baidu Deep Speech Xiaoyu Zaijia Little Fish Amazon Echo Baidu CES Las Vegas January Baidus China Nuomi Waimai Ctrip.com Every Baidu Maps AI HSBC Holdings Plc Chi Tsang Think AI Baidus Baidu President Zhang Ya-Qin Baidus Baidus AI Bloomberg Terminal,3
105,"Smartphone speech recognition software gets a bad rap. Most users find the nascent technology to be frustratingly slow, and there are entire blogs dedicated to documenting examples of its biggest and sometimes hilarious  mistakes. But results from a new experiment suggest a different reality: Speech recognition can be used to compose text messages faster and more accurately than humans can type on mobile phone screens. Go to the web site to view the video. Stanford computer science researchers compared speech recognition software with humans for speed and accuracy. Speech recognition is something thats been promised to us for decades, but it has never worked very well, said James Landay , a professor of computer science at Stanford and co-author of the new study. But we were noticing that in the past two to three years, speech recognition was actually improving a lot, benefiting from big data and deep learning to train its neural networks to produce faster, more accurate results. So we decided to formally test it against humans. The research team, which included computer scientists from Stanford, Baidu Inc. and the University of Washington, devised an experiment that pitted Baidus Deep Speech 2 cloud-based speech recognition software against 32 texters, ages 19 to 32, working the built-in keyboard on an Apple iPhone. They grew up texting, so were putting speech recognition up against people who are really good at this task, Landay said. The subjects took turns typing or speaking about 100 phrases sourced from a standard library of everyday phrases used in text-based research phrases such as physics and chemistry are hard, have a good weekend and go out for some pizza and beer  while the testing app recorded their times and accuracy rates. Half the subjects performed the task in English using the QWERTY keyboard; the other half conducted the test in their native Mandarin using iOS Pinyin keyboard. The results were clear no matter the language. For English, speech recognition was three times faster than typing, and the error rate was 20.4 percent lower. In Mandarin Chinese, speech was 2.8 times faster, with anerror rate 63.4 percent lower than typing. We knew speech recognition is pretty good, so we expected it to be faster, but we were actually quite surprised to find that it was almost three times faster than typing on a keyboard, said co-author Sherry Ruan , a computer science PhD student at Stanford who helped run the experiments. Although the researchers used Baidus speech recognition software, they suspect that other high-accuracy speech engines perform at a similar level. Now that the team members have quantified that speech recognition actually works well, they hope it will encourage engineers to design user interfaces that take better advantage of the technology. We should put speech in more applications than just typing an email or text message, Landay said. You could imagine an interface where you use speech to start and then it switches to a graphical interface that you can touch and control with your finger. The study, titled Speech Is 3x Faster than Typing for English and Mandarin Text Entry on Mobile Devices, is published online at arxiv.org. Co-authors included Jacob Wobbrock of the University of Washington and Kenny Liou and Andrew Ng of Baidu; Ng is also an adjunct professor of computer science at Stanford. More information can be found at http://hci.stanford.edu/research/ . ",http://news.stanford.edu/2016/08/24/stanford-study-speech-recognition-faster-texting/,"Smartphone speech recognition software is not only three times faster than human typists, it's also more accurate. The researchers hope the ...",Smartphone speech recognition can write text messages three times ...,Smartphone Stanford James Landay Stanford Stanford Baidu Inc. University Washington Baidus Deep Speech Apple Landay English QWERTY Mandarin Pinyin English Mandarin Chinese Sherry Ruan PhD Stanford Baidus Landay Speech Faster English Mandarin Text Entry Mobile Devices Jacob Wobbrock University Washington Kenny Liou Andrew Ng Baidu Ng Stanford,3
106,"Watch, Attend and Spell was created usingan approach known as machine learning. The researcherscreated an algorithm a neural network that could learn over time. They trained the algorithm by showing it thousands of hours of TV news footage from BBC. The advantage of TV news is that it's relatively high-quality videoand it includes lots of different faces and speaking styles. Plus, the TV shows they used to train the algorithm were already captioned by professionals. So they could match the mouth movements to transcriptions of what had been said on-screen. Researchers trained the algorithm to watch mouth movements to identify words, such as these one-second clips that contain the word about. (Photo courtesy of Joon Son Chung) After the researchers trained their algorithm on these thousands of hours of TV, they put it to the test in the real world to see how it would perform on video without captions. In other words, they wanted to see if their software could take what it had learned, and lip-read faces and mouths that it hadn't necessarily seen before. It was able to getabout 50 per cent of the words right. So,the computer's performance is pretty impressive. What privacy concerns do lip-reading computers raise When I first heard about this research, my mind immediately turned to that scene in 2001:A Space Odyssey, where they reveal that the HAL 9000 computer can lip-read. I thought about all the cameras in the world around us that are constantly capturing video, such as smartphone cameras or security cameras. If it's possible to figure out what someone is saying using only an image of their mouth, the possibilities for surveillance and eavesdropping seem pretty creepy. I asked Chung about this, and he told me that the system doesn't pose a serious privacy risk right now. That's partly because most security cameras aren't high-quality enough to make this type of lip-reading work. He also pointed out the software's 50 per cent accuracy rate. ""Yes, it's true, it can lip-read better than a human, but it still gets half the words wrong when used without the audio. So it's not really useful for privacy intrusive scenarios,"" Chung said. Even if you got a clear, high-resolution video feed of someone, you couldn't know for certain exactly what they were saying. Where might we see lip-reading computers in everyday life Like I said off the top, the researchers had accessibility in mind when designing this system. In particular, they thought about applications that could help people who are deaf or hard-of-hearing. This technology also has the potential to significantly improve general-purpose speech recognition as well. I don't know about you, but I'm often frustrated when I use voice-based services like Siri, or Google Now or Alexa.Sometimes they work well for me, but other times, these voice assistants get things very wrong. Technology like Watch, Attend and Spell could improve voice-based services such as Siri, or Google Now or Alexa. (iStockphoto) The researchers at Oxford believe that by combining voice recognition with lip-reading technology, that could dramatically improve the accuracy of these virtual assistants. And there's another thing to consider: we tend to think of understanding speech as an auditory skill.But humans also pick up on visual cues to understand what's being said. In that way, when we combine speech recognition technology with lip-reading technology, we're building computer systems that mirror how humans perceive speech. And if that can help Siri understand me a little better, that's a bonus. ",http://www.cbc.ca/news/technology/lip-reading-program-helps-accessibility-1.4034565,"In that way, when we combine speech recognition technology with lip-reading technology, we're building computer systems that mirror how ...",Lip-reading program more accurate than humans could help ...,Watch Attend Spell BBC Photo Joon Son Chung Space Odyssey HAL Chung Chung Siri Google Alexa.Sometimes Technology Watch Attend Spell Siri Google Alexa Oxford Siri,-1
107,"AI research has a long history of repurposing old ideas that have gone out of style. Now researchers at Elon Musks open source AI project have revisited neuroevolution, a field that has been around since the 1980s, and achieved state-of-the-art results. The group, led by OpenAIs research director Ilya Sutskever, has been exploring the use of a subset of algorithms from this field, called evolution strategies, which are aimed at solving optimization problems. Despite the name, the approach is only loosely linked to biological evolution, the researchers say in a blog post announcing their results. On an abstract level, it relies on allowing successful individuals to pass on their characteristics to future generations. The researchers have taken these algorithms and reworked them to work better with deep neural networks and run on large-scale distributed computing systems. To validate their effectiveness, they then set them to work on a series of challenges seen as benchmarks for reinforcement learning, the technique behind many of Google DeepMinds most impressive feats, including beating a champion Go player last year. One of these challenges is to train the algorithm to play a variety of computer games developed by Atari. DeepMind made the news in 2013 when it showed it could use Deep Q-Learninga combination of reinforcement learning and convolutional neural networksto successfully tackle seven such games. The other is to get an algorithm to learn how to control a virtual humanoid walker in a physics engine. To do this, the algorithm starts with a random policythe set of rules that govern how the system should behave to get a high score in an Atari game, for example. It then creates several hundred copies of the policywith some random variationand these are testedon the game. These policies are then mixed back together again, but with greater weight given to the policies that got the highest score in the game. The process repeats until the system comes up with a policy that can play the game well. ""In one hour training on the Atari challenge, the algorithm reached a level of mastery that took a [DeepMind]reinforcement-learning system...a whole day to learn."" In one hour training on the Atari challenge, the algorithm reached a level of mastery that took a reinforcement-learning system published by DeepMind last year a whole day to learn. On the walking problem the system took 10 minutes, compared to 10 hours for Googles approach. One of the keys to this dramatic performance was the fact that the approach is highly parallelizable. To solve the walking simulation, they spread computations over 1,440 CPU cores, while in the Atari challenge they used 720. This is possible because it requires limited communication between the various worker algorithms testing the candidate policies. Scaling reinforcement algorithms like the one from DeepMind in the same way is challenging because there needs to be much more communication, the researchers say. The approach also doesnt require backpropagation, a common technique in neural network-based approaches, including deep reinforcement learning. This effectively compares the networks output with the desired output and then feeds the resulting information back into the network to help optimize it. The researchers say this makes the code shorter and the algorithm between two and three times faster in practice. They also suggest it will be particularly suited to longer challenges and situations where actions have long-lasting effects that may not become apparent until many steps down the line. The approach does have its limitations, though. These kinds of algorithms are usually compared based on their data efficiencythe number of iterations required to achieve a specific score in a game, for example. On this metric, the OpenAI approach does worse than reinforcement learning approaches, although this is offset by the fact that it is highly parallelizable and so can carry out iterations more quickly. For supervised learning problems like image classification and speech recognition, which currently have the most real-world applications, the approach can also be as much as 1,000 times slower than other approaches that use backpropagation. Nevertheless, the work demonstrates promising new applications for out-of-style evolutionary approaches, and OpenAI is not the only group investigating them. Google has been experimenting on using similar strategies to devise better image recognition algorithms . Whether this represents the next evolution in AI we will have to wait and see. ",https://singularityhub.com/2017/04/06/openai-just-beat-the-hell-out-of-deepmind-with-an-algorithm-from-the-80s/,"For supervised learning problems like image classification and speech recognition, which currently have the most real-world applications, the ...",OpenAI Just Beat Google DeepMind at Atari With an Algorithm From ...,AI Elon Musks AI OpenAIs Ilya Sutskever Google DeepMinds Go Atari DeepMind Atari Atari DeepMind ] Atari DeepMind Googles CPU Atari DeepMind OpenAI Nevertheless OpenAI Google AI,4
108,"Global IVR System Market - Scope, Drivers, and Forecasts from Technavio Technavio has published a new report on the global IVR system market from 2017-2021. (Photo: Business Wire) LONDON--( BUSINESS WIRE )-- Technavio market research analysts forecast the global        interactive voice response (IVR) system market to grow at a CAGR        of close to 13% during the forecast period, according to their latest        report. The market study covers the present scenario and growth prospects of the        global IVR        system market for 2017-2021. The report also lists on-premises and cloud-based IVR systems as the two major segments based on        deployment models, of which the cloud-based IVR systems accounted for        75% of the market share in 2016. Technavios sample reports are free of charge and contain multiple        sections of the report including the market size and forecast, drivers,        challenges, trends, and more. Technavio ICT analysts highlight the following three market drivers that are        contributing to the growth of the global IVR system market: One of the major drivers for the global IVR system market is the        increase in the number of calls by customers. There is a tremendous        increase in the number of calls due to which agents are not able to        receive the calls. Thus, organizations manage the increased number of        calls and provide increased customer interaction with the help of IVR        solutions. The use of dual-tone and speech technology reduces the        hold time and drop off time for each call. This has increased the number        of outbound calls, which results in automation of the routine tasks and        helps in improving the efficiency of the agent. In the retail industry, speech technology is mostly used for        receiving the calls, which when carried by IVR systems reduces the time,        especially when there is a seasonal demand for some products, says        Amrita Choudhury, a lead analyst at Technavio for enterprise        application research. Organizations have increasingly started adopting open standards. They        have realized the importance of developing solutions and technologies        that are interoperable with the products. Voice extensible markup        language (VXML) offers a shift in the application development because speech        recognition on VXML standard helps the programmer to develop        reusable grammar leveraged by different applications. The ability to        reuse the codes makes the standard more preferred and reduces the        development time of the project. The applications built on this standard        are portable and offer customers with the flexibility to integrate with        other vendor solutions. IVR systems that are based on this standard offer the benefits of        application portability and investment protection. Most of the IVR        system providers offer support for emerging and accepted standards        including VXML, extensible multimodal annotation markup language, Speech        Recognition Grammar Specification , Semantic Interpretation for        Speech Recognition, and Natural Language Semantics Markup Language. Customer relationship is crucial for effective revenue generation in any        enterprise. Enterprises are striving to develop a wide customer base        because of the increase in competition and to obtain a large market        share. They have started implementing enterprise solutions such as        enterprise resource planning (ERP) and CRM .        However, it is difficult to maintain the same level of personal contact        with all the customers. The lack of a centralized system, which provides relevant and        consistent data and engages in continuous communication with customers        results in loss of customers and exposes the business to risks, says        Amrita. IVR systems enable companies to engage with their customers. For        instance, enterprises deploy IVR systems to provide product or brand        information to customers and increase the brand awareness. IVR systems        help in promotions, thereby encouraging customers for signing up to the        company's account. Thus, the effective deployment of IVR systems enables        personalized customer interaction. ",http://www.businesswire.com/news/home/20170313005553/en/Global-IVR-System-Market---Scope-Drivers,Voice extensible markup language (VXML) offers a shift in the application development because speech recognition on VXML standard helps ...,"Global IVR System Market - Scope, Drivers, and Forecasts from ...",Global IVR System Market Drivers Technavio Technavio IVR Photo Business Wire LONDON BUSINESS WIRE IVR CAGR IVR IVR IVR Technavio ICT IVR IVR IVR IVR Amrita Choudhury Technavio Voice VXML VXML IVR IVR VXML Speech Recognition Grammar Specification Semantic Interpretation Natural Language Semantics Markup Language Customer ERP CRM Amrita IVR IVR IVR IVR,-1
109,"Like other major hyperscale web companies, Chinas Tencent, which operates a massive network of ad, social, business, and media platforms, is increasingly reliant on two trends to keep pace. The first is not surprisingefficient, scalable cloud computing to serve internal and user demand. The second is more recent and includes a wide breadth of deep learning applications, including the companys own internally developed Mariana platform , which powers many user-facing services. When the company introduced its deep learning platform back in 2014 (at a time when companies like Baidu, Google, and others were expanding their GPU counts for speech and image recognition applications) they noted their main challenges were in providing adequate compute power and parallelism for fast model training. For example, Marianas creators explain, the acoustic model of automatic speech recognition for Chinese and English in Tencent WeChat adopts a deep neural network with more than 50 million parameters, more than 15,000 senones (tied triphone model represented by one output node in a DNN output layer) and tens of billions of samples, so it would take years to train this model by a single CPU server or off-the-shelf GPU. In 2014, when the company first rolled out details about Mariana, they said they were using an untold number of servers packed with 4-6 GPUs with CPU hosts. They were able to get a speedup of 4.6X by using 6 GPUs per server compared to one GPU and put in extensive work to push multi-GPU scaling to new heightssomething that is still a pressing issue in research and at established deep learning shops like Baidu , Yahoo/Flickr , and others. The 2014 paper makes reference to the Nvidia Tesla series K20 GPU (versus the K40, which was also available then) but as Tencent spokesfolks tell The Next Platform, they were also among the first to use the machine learning-focused M4 GPUS as well. This morning the company provided new details about expanding GPU use across the two elements that power many of their services. Tencent will be integrating the latest Pascal GPUs into its diverse workflows on both the deep learning and cloud frontsusing the former to power richer deep learning training and the latter to offer a broader set of capabilities to its cloud customers (more insight on Chinas cloud market can be found here for a diversion). China will now have a cloud outfitted with the most robust GPUs available, including the P100 and P40, which target the data movement bottlenecks via NVlink and come with CUDA-driven deep learning tools and librariessomething that Nvidia has invested in significantly over the last couple of years with reach across nearly all the main deep learning frameworks. Since GPUs are still king of the hill when it comes to deep learning training, the addition of Tencent to Nvidias public reference list of large-scale deep learning customers keep pushing the idea that GPUs and deep learning are set to go hand in hand for the foreseeable future, despite the tsunami of new chip architectures aimed at the deep learning processor market. On the cloudy front, this is an interesting development because not even the worlds cloud giant, AWS, has moved to offer Pascal-generation GPUs via its cloud. The company made the announcement of the top of the line compute workhorse K80 , a favorite among the supercomputing set, Tencent is blazing some serious trails by being out front of the Pascal cloud wave. As part of the companies collaboration, Tencent Cloud intends to offer customers a wide range of cloud products based on NVIDIAs AI computing platforms. This will include GPU cloud servers incorporating NVIDIA Tesla P100, P40 and M40 GPU accelerators and NVIDIA deep learning software. Tencent Cloud launched GPU servers based on NVIDIA Tesla M40 GPUs and NVIDIA deep learning software in December. The M40 and M4 machine learning focused GPUs will also be made available to Tencent AI cloud customers, which means Chinese deep learning developers will be able to have a lower cost way to enter the larger world market to deliver new frameworks, software, and tools that can be trained and executed in faster, more efficient ways than with CPU alone or older generation Tesla hardware. IBM announced last year that Tencent was integrating OpenPower servers for Tencent featuring GPUs and its own Power architecture, but the Tencent did not disclose the server maker for its cloud build out. One can imagine that Chinese hardware makers, including Inspur and others would be more likely choices for large-scale web builds, but the trick of scaling 8 GPUs per box with the proprietary NVLink interconnect is the domain of IBMs OpenPower initiative. Tencent will put its multi-GPU scaling and NVlink to the test for cloud users with servers holding 8 GPUs. As Tencent VP of cloud, Sam XIe says, Tencent Cloud GPU offerings with Nvidias deep learning platform will help companies in China rapidly integrate AI capabilities into their products and services. Our customers will gain greater computing flexibility and power, giving them a powerful competitive edge. Although we tend to hear quite a bit about Google, Facebook, Baidu, and others when it comes to GPU-driven deep learning at scale, Tecent is certainly a rising powerhouse in China. The company, like Baidu with its Silicon Valley AI lab, has its own artificial intelligence laboratory , which focuses on the extensive application of AI technology and basic research, combined with product data and user behavior learning for multiple products. Here they are studying new algorithms for machine learning at scale with many new outreach programs to capture a new generation of Chinese AI developers. ",https://www.nextplatform.com/2017/03/24/rapid-gpu-evolution-chinese-web-giant-tencent/,"For example, Mariana's creators explain, the acoustic model of automatic speech recognition for Chinese and English in Tencent WeChat ...",Rapid GPU Evolution at Chinese Web Giant Tencent,Chinas Tencent Mariana Baidu Google Marianas Chinese English Tencent WeChat DNN CPU GPU Mariana GPUs CPU GPUs GPU Baidu Yahoo/Flickr Nvidia Tesla K20 GPU K40 Tencent Platform M4 GPUS GPU Tencent GPUs Chinas China GPUs P100 P40 NVlink CUDA-driven Nvidia GPUs Tencent Nvidias GPUs AWS Pascal-generation GPUs K80 Tencent Pascal Tencent Cloud NVIDIAs AI GPU NVIDIA Tesla P100 P40 M40 GPU NVIDIA Tencent Cloud GPU NVIDIA Tesla M40 GPUs NVIDIA December M40 M4 GPUs Tencent AI CPU Tesla IBM Tencent Tencent GPUs Power Tencent Inspur GPUs NVLink IBMs OpenPower NVlink GPUs Tencent VP Sam XIe Tencent Cloud GPU Nvidias China AI Google Facebook Baidu Tecent China Baidu Silicon Valley AI AI AI,-1
110,"How long does it take to be so over an ugly divorce that you can joke about it For Ashton Kutcher , apparently, it takes about six years, one marriage, and two children. Thats how long its been since his very public split from Demi Moore, from whom he separated in 2011; the pair divorced in 2013. At the time Kutcher had his name splashed across every gossip magazine as an adulterer, as he put it while accepting an award for character on Saturday. The actor returned to Iowa, his home state, to receive the Robert D. Ray Pillar of Character Award at Drake University. Kutcher was undoubtably worthy of the recognitionhes a co-founder of the Native Fund, which helps Iowans affected by disasters. Still, he pulled a defensive move, getting ahead of any reason one might say he doesnt deserve it. In the opening to his speech, Kutcher mostly spoke about past mistakes, like getting pulled over while on shrooms, being briefly incarcerated for breaking into his high school when he was 18, and, quite pointedly, his divorce from Moore. Rumors that Kutcher cheated dogged the couple in 2011, culminating in an Us Weekly cover featuring the alleged other woman. Moore said in a statement after announcing the split, It is with great sadness and a heavy heart that I have decided to end my six-year marriage to Ashton. As a woman, a mother, and a wife there are certain values and vows that I hold sacred, and it is in this spirit that I have chosen to move forward with my life. She eventually changed her Twitter handle from MrsKutcher to JustDemi. During his marriage to Moore, Kutcher was able to reframe his MTVs Punkd persona into something more serious . In 2009, they launched the Thorn foundation to combat child trafficking and sexual exploitation (both actors remain on the board of directors). Then post-divorce, besides continuing his stint on Two and a Half Men, he moved away from Hollywood to San Francisco, becoming a frequently recurring name in the tech sector for a series of smart investments . Time helps add some distance to ones mistakes, but so does rebranding. Kutcher is at a safe enough distance from the tabloid headlines that his alleged infidelity can be a useful beat to a story about his character. In his speech on Saturday, Kutcher continued, Character comes from those magazines that tear you apart for something you may or may not have done, and you gotta go out and perform tomorrow with everyone looking at you like you might be an adulterer. There was a point for dredging all this up half a decade later, and it was this lesson: I had the great fortune of getting a divorce because I felt the impact of it and I felt how much loss is in there and how much love is in there and that its not neat or clean or messy. And I understood, finally, my parents divorce in a whole different way. He also managed to tip his hat to  wife Mila Kunis, despite their typically private approach to marriage and parenting. Im telling you, this morning, I woke up and [Kunis] kicked my ass on character, he said. I thought I was awesome because I got up early and helped with the kids before she woke up and I let her sleep a little bit and then shes like, Well, now youre gonna act tired I do it every day. But it was a character moment, right Because shes right! ",http://www.vanityfair.com/style/2017/04/ashton-kutcher-demi-moore-divorce-speech,"Kutcher was undoubtably worthy of the recognition—he's a ... In the opening to his speech, Kutcher mostly spoke about past mistakes, like ...",Ashton Kutcher Somehow Turns Demi Moore Divorce Drama Into ...,Ashton Kutcher Demi Moore Kutcher Saturday Iowa Robert D. Ray Pillar Character Award Drake University Kutcher Native Fund Iowans Kutcher Moore Kutcher Us Weekly Moore Ashton Twitter MrsKutcher JustDemi Moore Kutcher MTVs Punkd Thorn Half Men Hollywood San Francisco Time Kutcher Saturday Kutcher Character Mila Kunis Im [ Kunis ],12
111,"Many of the technological advances predicted in Star Treks fictional universe have become reality , such as the mobile communicator and hand-held tablet computers. Others, such as tractor beams and warp drives , are still a work in progress. But what of the Holodeck  The Holodeck first appeared in The Practical Joker , a 1974 episode of the Star Trek animated series. It was depicted as a recreation room containing a simulated, alternative version of reality. It featured heavily in The Next Generation series and in the 1996 film First Contact . Anyone entering the Holodeck could interact with solid props and characters in any scenario based on whatever parameters they programmed. These programs are not unlike the narrative-driven, cinematic videogames we have today, such as Grand Theft Auto , Red Dead Redemption or The Witcher . The Holodeck was a narrative device that allowed Star Treks writers to experiment with philosophical questions in settings not available in a typical sci-fi context. From Holodeck reality to just an empty room with Counselor Deanna Troi (Marina Sirtis, left) and William Riker (Jonathan Frakes, right). Star Trek/Screenshot/Memory Alpha , CC BY-NC It inspired several generations of computer scientists who spearheaded research in artificial intelligence, computer graphics and human-computer interaction. The convergence of these research areas has given rise to other forms of reality on the path to the construction of a real Holodeck. In virtual reality ( VR ) we are fully immersed in a synthetic, virtual version of reality, experienced through dedicated VR headsets such as the Oculus Rift or the HTC Vive . A typical example of VR is an immersive war game that puts a user in charge of a Roman army as Caesar, battling Vercingetorixs Gaul troops at Alesia. But VR has a major drawback for some applications. Being isolated from the real world, its not easy to engage in social interaction or physical movement in a way that feels natural to most people. Augmented reality ( AR ) blends synthetic, virtual objects with the view of our physical reality. In AR, we can interact with virtual humans inhabiting our physical space or we can work with our children, for example, to build virtual LEGO houses on real tables in our own living rooms. Headsets are available that allow us to create AR in our office or lounge rooms, such as the Microsoft Hololens or the Meta . But AR headsets still suffer from several technical limitations, such as a reduced field of view. The software that lets the virtual and real worlds interact believably and naturally still needs work. Real-world Holodeck programs would also need the technology to sense human actions. This would provide useful information that the virtual personas inhabiting the Holodeck programs would use to anticipate our human intentions. Progress here has been fast and constant, with great improvements in speech recognition and language translation, such as Apples Siri , Googles Assistant and Microsofts Cortana . We now take almost for granted the ability to search for information with speech or to command our mobiles to schedule meetings and appointments. Other devices, originally conceived for entertainment applications, can track human gestures or even their full body posture. For example, Microsoft Kinect can track a human body, and the technology is now included in the Hololens as its gesture-recognition component. Lots of other sensing devices are now commonplace in mobile devices, such as accelerometers, gyroscopes, magnetometers, and temperature and pressure sensors. The general trend is towards giving humans the ability to communicate using a combination of their body and their voice via hands-free or wearable user interfaces. Attacking the Borg on the Holodeck in Star Trek: First Contact with Lily (Alfre Woodard, left) and Captain Picard(Patrick Stewart, right). Star Trek: First Contact/Paramount/Screenshot These advances, while noteworthy, do not necessarily show strong progress towards general forms of artificial intelligence (AGI) exhibited by humans. It has been argued that defining or providing general human intelligence may prove a very elusive problem for a long time, or indeed forever. Fortunately, a restricted version of a Holodeck program may only require a slightly weaker, not fully general form of intelligence. This was exemplified by androids in the popular TV series reboot of Westworld . The good news is that this may shorten the time needed to realise the hypothetical Holodeck programs. The bad news is that such a feat is still beyond us at this stage, although recent progress in machine learning will likely help us close the gap faster. The question is then whether we shall ever be able to reach the level of sophistication in AR and AI needed to build a Holodeck And if so, when Making predictions on such matters is not trivial, but I am inclined to think that current advances in VR and AR technologies will provide us with the required sophisticated headsets within the next five to ten years. The question then is also whether we shall ever be able to achieve AR using alternative forms of projections that remove the need for a headset altogether. This may be possible, eventually, but it would be irrelevant if headsets could be miniaturised and potentially implanted into human eyes, similar to what was suggested in other sci-fi classics such as Neuromancer or Snow Crash , and recently advocated by transhumanists . The recent predictions about breakthroughs in general artificial intelligence by experts seem to converge around a date around 2040. This would put the sort of AI required for Holodeck characters somewhat earlier than that. So I believe that one day humans will be able to experience some form of Holodeck similar to what was envisaged in Star Trek. To paraphrase Star Teeks infamous Borg alien race, I will say that resistance to this technological progress is futile and it will be assimilated, one day. ",http://theconversation.com/star-treks-holodeck-from-science-fiction-to-a-new-reality-74839,"Progress here has been fast and constant, with great improvements in speech recognition and language translation, such as Apple's Siri, ...",Star Trek's Holodeck: from science fiction to a new reality,Star Treks Holodeck Holodeck Practical Joker Star Trek Generation First Contact Anyone Holodeck Grand Theft Auto Red Dead Redemption Witcher Holodeck Star Treks Holodeck Counselor Deanna Troi Marina Sirtis William Riker Jonathan Frakes Star Trek/Screenshot/Memory Alpha CC BY-NC Holodeck VR VR Oculus Rift HTC Vive A VR Roman Caesar Vercingetorixs Gaul Alesia VR AR AR LEGO AR Microsoft Hololens Meta AR Real-world Holodeck Holodeck Apples Siri Googles Assistant Microsofts Cortana Microsoft Kinect Hololens Lots Borg Holodeck Star Trek First Contact Lily Alfre Woodard Captain Picard Patrick Stewart Star Trek First Contact/Paramount/Screenshot AGI Holodeck Westworld Holodeck AR AI Holodeck VR AR AR Neuromancer Snow Crash AI Holodeck Holodeck Star Trek Star Teeks Borg,-1
112,"Get today's popular DigitalTrends articles in your inbox: If youre fed up with chatbots mishearing you, Microsoft is making machineears a little more attentive. Researchers from the tech giant have achieved an impressively low error rate for speech-recognition software  just 6.3 percent , according to a paper published last week. The companyhopes this milestone will help refine and personalize its AIassistant, Cortana, and features like Skype Translator. The newest error rate of Microsofts conversational speech-recognition system is regarded as the lowest in the industry, according to Xuedong Huang, Microsofts chief speech scientist. IBM meanwhile recently announced an error rate of 6.6 percent, bettering its 6.9 percenterror rate from April and the 8 percent milestonethat the company achieved lastyear. Two decades ago, the lowest error rate of a published system was more than 43 percent, Microsoft notesin a blog post. In artificial intelligence development, researchers often model machines of off humans by equipping the systems with the abilities to speak, see, and hear. Although Microsofts achievement is just 0.3 percent below IBMs, incremental advancements like these bring machines closer to human-like capabilities. In speech recognition, the human error rate is around 4percent, according to IBM . This new milestone benefited from a wide range of new technologies developed by the AI community from many different organizations over the past 20 years, Microsofts Huang said. A few of thesetechnologies include biologically inspired systemscalled neural networks, a trainingtechnique known as deep learning, and the adoption of graphic processing units (GPUs) to process algorithms. Over the past two years, neural networks and deep learning have enabled AI researchers to develop and train systems in advanced speech recognition, image recognition, and natural language processing. Just last year, Microsoft created image-recognitionsoftware that outperformed humans. Although initially designed for computer graphics, GPUs are now regularly used to process sophisticatedalgorithms. Cortana can process up to 10 times more data using GPUs than previous methods, according to Microsoft. With steady advances like these,repeating yourquestion toa chatbot may be a thing of the past. ",http://www.digitaltrends.com/computing/microsfot-speech-recognition/,"Researchers from the tech giant have achieved an impressively low error rate for speech-recognition software — just 6.3 percent, according to ...",Microsoft hits another milestone in speech-recognition software ...,Get DigitalTrends Microsoft AIassistant Cortana Skype Translator Microsofts Xuedong Huang Microsofts IBM April Microsoft Microsofts IBMs IBM AI Microsofts Huang A GPUs AI Just Microsoft GPUs Cortana GPUs Microsoft,-1
113,"Companies leverage machine learning and ASR to increase media asset value. Applications Technology (AppTek) and JB&A Associates today announce their partnership effective immediately. AppTeks proprietary Automatic Speech Recognition (ASR) technology powers a media optimization platform that increases the value of media assets for movie studios, news broadcasters and content producers in multiple languages. JB&A will bring AppTeks media optimization platform to channel partners across North America. JB&A is the perfect partner for AppTek. Their deep understanding of media workflows and impressive customer list means that more media companies will have the opportunity to extract the most value from their audio and video assets, stated Adam Sutherland, AppTek CEO. Integrating ASR into media ingest and edit workflows allows for the automated creation of meta-data and closed captions, commented JB&A CEO, Jeff Burgess. Depending on where in the process you leverage ASR, we can even ensure that your b-roll has the meta-data necessary to make it searchable and discoverable. AppTek has ASR models for 14 different languages including English, Spanish, French, Chinese, Russian and Arabic. The Media Asset Optimization platform leverages machine learning to improve results and produces XML which can be easily integrated with media companies existing meta-data taxonomies. As a pioneer in automatic speech recognition, machine learning and artificial intelligence; AppTek partners with its customers to provide solutions focused on closed captioning and subtitling, call center content discovery and mobile intelligent voice agents. In todays hyper-connected and data-rich marketplace, enterprises are seeking to drive revenue, save costs and increase productivity. Customers rely on AppTek to solve for these issues by delivering the markets robust speech technology solutions focused on mining for business insights, ensuring compliance and delivering value across the enterprise. For more information, visit http://www.apptek.com . Founded in 1996, JB&A is a leader in the field of Digital Media and Video Technology. They are dedicated to bringing the most innovative and complete solutions to market including: Media Management, Broadcast, IP & Streaming, Digital Display & Collaboration, and Connectivity & Image Resolution. JB&A is staffed by industry experts, and provides support in every step of the pre and post sales process. JB&A is a unique mix of Consultant, Channel Partner, Solutions Provider and Distributor with an ecosystem of certified, tested and proven products and workflow solutions. For more information, please visit jbanda.com. Share article on social media or email: ",http://www.prweb.com/releases/2017/04/prweb14213607.htm,AppTek's proprietary Automatic Speech Recognition (ASR) technology powers a media optimization platform that increases the value of media ...,AppTek and JB&A Bring New Media Asset Optimization Solutions to ...,ASR Applications Technology AppTek JB A Associates AppTeks Automatic ASR JB A AppTeks North America JB A AppTek Adam Sutherland AppTek CEO ASR JB A CEO Jeff Burgess AppTek ASR English Spanish French Chinese Russian Arabic Media Asset Optimization XML AppTek Customers AppTek JB A Digital Media Video Technology Media Management Broadcast IP Streaming Digital Display Collaboration Connectivity Image Resolution JB A JB A Consultant Channel Partner Solutions Provider Distributor Share,-1
114,"Tencent Joins Rush Into AI to Keep Lead in Social Media, Gaming Company to mine data from semi-public content on Wechat, QQ The days when Chinese internet companies could simply rely on the countrys sheer population are over. Thats why Tencent Holdings Ltd.s Ma Huateng is betting on the future of artificial intelligence. Tencent has assembled more than 250 people for its AI Lab, a fledgling unit intended to work with its most profitable divisions from gaming to social media.The company aims to teach machines how to better battle human players and strike up meaningful conversations, said Zhang Tong, the newly appointed director of the research unit. In Tokyo over the past weekend, Tencent demonstrated an early result of that collaboration, pitting its Jueyi against fellow computer players of the classical game Go in an annual competition. Jueyi -- which means fine art -- won against defending champion DeepZenGo. Chinas largest internet companies are investing billions in AI research, hoping to shed a reputation for being fast imitators and break new ground in a blossoming field. With AI set to transform everything from mobile apps to cars, companies like Tencent and pioneer ways to build smarter software and products.Ma, Tencents billionaire founder and chairman, has warned that companies that fail to create technology will lose out in future. Tencent used to be a product-driven company. Now we want to transform into a technology-driven company, Zhang said in an interview.He wouldnt say how much Tencent was investing but affirmed the company was in it for the long haul. Weve reaped the benefits of a large population, now we need to use technology and AI. Best known for messaging service WeChat, Tencents business encompasses news, entertainment and online games such as League of Legends and Clash of Clans.Its become intertwined with the lives of hundreds of millions of Chinese who use WeChat and QQ to order food, play games and hail taxis.While it employs AI in areas such as news recommendations, infusing the technology intoother services could have broad impact. For now, Tencents demonstrating its ability in gaming, revealingits own version of DeepMinds Alpha Go in Japan. Jueyi won all 11 of its matches in a field of about 30 entrants, beating the eventual runner-up -- Japans DeepZenGo -- twice along the way. The company will use the techniques its learned to teach its games to put up a better fight -- addressing, among other things, a longstanding complaint of expert players. While Zhang didnt provide names, he didnt rule out titles like League of Legends or Dungeon Fighter. Zhang, 45, whose AI career includes stints at International Business Machines Corp. and Baidu , said one of the biggest attractions for him was Tencents trove of data, hoovered up especially from its social media apps. Tencent amasses data predominantly from semi-public content on QQ and WeChat and social media postings on sites like Weibo, Chinas Twitter-equivalent.It places strict limits on what data staff can access, said Zhang. For instance, the company doesnt use personal conversations on WeChat, which has more than 889 million users.The company will use certain mechanisms to wipe names from conversations so user identities will be protected,Zhang added without elaborating. Facebook Inc. He turned to the rest of Silicon Valley and Chinas top universities for talent. Now that the staff is in place, one of their immediate goals is to bolster speech-recognition: helping machines comprehend and converse with humans. The team also works on content generation, including creating automated news stories, photos and music. The company is building a platform that will provide tools for small businesses and startups that want to develop their own AI technology. Tencents looking for ways to keep users glued to WeChat. On Wednesday, it signaled its intention to keep spending on areas from payments to content to increase social media engagement. Ma Huateng said the company could explore AI technology for driverless cars and online health care in the future. Exclusive insights on technology around the world. In many of those areas, Tencent will be competing with a pair of powerful local rivals: Baidu and Alibaba Group Holding Ltd. are also in the race to develop AI use cases. They too can harness a vast database of information. Baidu, the countrys largest search engine, already employs 1,300 people in its artificial intelligence business and this year hired former Microsoft AI-architect Qi Lu to helm its operations. Another thing all three have in common: they want to rank among the foremost companies in the field of AI, despite competition from names like We want to be on par with the best technology companies in the world, Zhang said. We dont just want to import, but also create innovation. Before it's here, it's on the Bloomberg Terminal. ",https://www.bloomberg.com/news/articles/2017-03-23/tencent-joins-rush-into-ai-to-keep-lead-in-social-media-gaming,"Now that the staff is in place, one of their immediate goals is to bolster speech-recognition: helping machines comprehend and converse with ...","Tencent Joins Rush Into AI to Keep Lead in Social Media, Gaming",Rush Into AI Lead Social Media Wechat QQ Tencent Holdings Ltd.s Ma Huateng AI Lab Zhang Tong Tokyo Tencent Jueyi Go Jueyi DeepZenGo Chinas AI AI Tencent Tencents Zhang Weve AI Best WeChat Tencents League Legends Clash Clans.Its Chinese QQ AI DeepMinds Alpha Go Japan Jueyi Japans DeepZenGo Zhang League Legends Dungeon Fighter Zhang AI International Business Machines Corp. Baidu Tencents QQ WeChat Weibo Chinas Twitter-equivalent.It Zhang WeChat Zhang Facebook Inc Silicon Valley Chinas AI WeChat Wednesday Ma Huateng AI Tencent Baidu Alibaba Group Holding Ltd. AI Baidu Microsoft AI-architect Qi Lu AI Zhang Bloomberg Terminal,-1
115,"Above: Google senior fellow Jeff Dean, second from left, appears during a session onstage at the 2016 Google I/O developer conference in Mountain View, California. Speaking at the AI Frontiers Conference in Santa Clara, California today, Google senior fellow Jeff Dean provided an update on the accuracy of the companys speech recognition software. Googles word error rate  how frequently Google transcribes a word incorrectly  has fallen by more than 30 percent, Dean said, according to a tweet from Mashables Karissa Bell. A Google spokesperson confirmed this statistic in an email to VentureBeat. Per Bells tweet, Dean attributed the improvement to the addition of neural nets, which are systems that Google and other companies use as part of deep learning. People train neural nets on lots of data, such as snippets of speech, and then get them to make inferences about new data. Google first put neural nets into production for speech recognition in 2012 , with the launch of Android Jelly Bean. Google doesnt often talk about its momentum in this important area, which affects an increasing number of Google products, from the Google Home smart speaker to the Gboard virtual keyboard for Android and iOS. But in 2015, Google chief executive Sundar Pichai said that the company had an eight percent word error rate. In August, Alex Acero, senior director of Siri at Apple, told Backchannels Steven Levy that Siris error rate has been cut by a factor of two in all the languages, more than a factor of two in many cases. And in September, Microsoft said that researchers had achieved a word error rate of 6.3 percent on a benchmark. Dean also said the Smart Reply feature that first appeared in the Inbox by Gmail app  and has subsequently appeared in other Google products  got its start as an April Fools joke in 2009, as Bell noted in a tweet today. Its time to learn how AI can transform your business at the 2017 VB Summit, June 5th  6th in Berkeley. Meet AI ecosystem leaders who are shaping a new digital economy. Request an Invite - Save 50%! ",https://venturebeat.com/2017/01/11/google-has-slashed-its-speech-recognition-word-error-by-more-than-30-since-2012/,"Google first put neural nets into production for speech recognition in 2012, with the launch of Android Jelly Bean. Google doesn't often talk ...",Google has slashed its speech recognition word error rate by more ...,Google Jeff Dean Google I/O Mountain View California AI Frontiers Conference Santa Clara California Google Jeff Dean Googles Google Dean Mashables Karissa Bell A Google VentureBeat Per Bells Dean Google Google Android Jelly Bean Google Google Google Home Gboard Android Google Sundar Pichai August Alex Acero Siri Apple Backchannels Steven Levy Siris September Microsoft Dean Smart Reply Inbox Gmail Google April Fools Bell AI VB Summit June Berkeley Meet AI Request Invite,4
116,"MIT researchers have built a low-power chip specialised for automatic speech recognition. Researchers at MITs Microsystems Technology Laboratories have built a low-power chip specialized for automatic speech recognition. With power savings of 90 to 99 percent, it could make voice control practical for relatively simple electronic devices. Automatic speech recognition is now on the verge of becoming peoples chief means of interacting with their principal computing devices. In anticipation of the age of voice-controlled electronics, MIT researchers have built a low-power chip specialised for automatic speech recognition. Whereas a cellphone running speech-recognition software might require about 1 watt of power, the new chip requires between 0.2 and 10 milliwatts, depending on the number of words it has to recognise. In a real-world application, that probably translates to a power savings of 90 to 99 per cent, which could make voice control practical for relatively simple electronic devices. That includes power-constrained devices that have to harvest energy from their environments to go months between battery charges. Such devices form the technological backbone of whats called the Internet of Things, or IoT, which refers to the idea that vehicles, appliances, civil-engineering structures, manufacturing equipment and even livestock will soon have sensors that report information directly to networked servers, aiding with maintenance and coordination of tasks. ""Speech input will become a natural interface for many wearable applications and intelligent devices, says Anantha Chandrakasan, the Vannevar Bush Professor of Electrical Engineering and Computer Science at MIT, whose group developed the new chip. The miniaturisation of these devices will require a different interface than touch or keyboard. It will be critical to embed the speech functionality locally to save system energy consumption compared to performing this operation in the cloud."" I dont think that we really developed this technology for a particular application, adds Michael Price, who led the design of the chip as an MIT graduate student in electrical engineering and computer science and now works for chipmaker Analog Devices. We have tried to put the infrastructure in place to provide better trade-offs to a system designer than they would have had with previous technology, whether it was software or hardware acceleration. Price, Chandrakasan, and Jim Glass, a senior research scientist at MITs Computer Science and Artificial Intelligence Laboratory, described the new chip in a paper Price presented last week at the International Solid-State Circuits Conference. Today, the best-performing speech recognisers are, like many other state-of-the-art artificial-intelligence systems, based on neural networks, virtual networks of simple information processors roughly modeled on the human brain. Much of the new chips circuitry is concerned with implementing speech-recognition networks as efficiently as possible. But even the most power-efficient speech recognition system would quickly drain a devices battery if it ran without interruption. So the chip also includes a simpler voice activity detection circuit that monitors ambient noise to determine whether it might be speech. If the answer is yes, the chip fires up the larger, more complex speech-recognition circuit. In fact, for experimental purposes, the researchers chip had three different voice-activity-detection circuits, with different degrees of complexity and, consequently, different power demands. Which circuit is most power efficient depends on context, but in tests simulating a wide range of conditions, the most complex of the three circuits led to the greatest power savings for the system as a whole. Even though it consumed almost three times as much power as the simplest circuit, it generated far fewer false positives; the simpler circuits often chewed through their energy savings by spuriously activating the rest of the chip. A typical neural network consists of thousands of processing nodes capable of only simple computations but densely connected to each other. In the type of network commonly used for voice recognition, the nodes are arranged into layers. Voice data are fed into the bottom layer of the network, whose nodes process and pass them to the nodes of the next layer, whose nodes process and pass them to the next layer, and so on. The output of the top layer indicates the probability that the voice data represents a particular speech sound. A voice-recognition network is too big to fit in a chips onboard memory, which is a problem because going off-chip for data is much more energy intensive than retrieving it from local stores. So the MIT researchers design concentrates on minimizing the amount of data that the chip has to retrieve from off-chip memory. A node in the middle of a neural network might receive data from a dozen other nodes and transmit data to another dozen. Each of those two dozen connections has an associated weight, a number that indicates how prominently data sent across it should factor into the receiving nodes computations. The first step in minimizing the new chips memory bandwidth is to compress the weights associated with each node. The data are decompressed only after theyre brought on-chip. The chip also exploits the fact that, with speech recognition, wave upon wave of data must pass through the network. The incoming audio signal is split up into 10-millisecond increments, each of which must be evaluated separately. The MIT researchers chip brings in a single node of the neural network at a time, but it passes the data from 32 consecutive 10-millisecond increments through it. If a node has a dozen outputs, then the 32 passes result in 384 output values, which the chip stores locally. Each of those must be coupled with 11 other values when fed to the next layer of nodes, and so on. So the chip ends up requiring a sizable onboard memory circuit for its intermediate computations. But it fetches only one compressed node from off-chip memory at a time, keeping its power requirements low. For the next generation of mobile and wearable devices, it is crucial to enable speech recognition at ultralow power consumption, says Marian Verhelst, a professor of microelectronics at the Catholic University of Leuven in . This is because there is a clear trend toward smaller-form-factor devices, such as watches, earbuds, or glasses, requiring a user interface which can no longer rely on touch screen. Speech offers a very natural way to interface with such devices. The research was funded through the Qmulus Project, a joint venture between MIT and Quanta Computer, and the chip was prototyped through the Taiwan Semiconductor Manufacturing Companys University Shuttle Program. ",http://www.deccanchronicle.com/technology/in-other-news/150217/low-power-chip-to-bring-speech-recognition-in-all-devices.html,Researchers at MIT's Microsystems Technology Laboratories have built a low-power chip specialized for automatic speech recognition.,Low-power chip to bring speech recognition in all devices,MIT MITs Microsystems Technology Laboratories Automatic MIT Internet Things IoT Speech Anantha Chandrakasan Vannevar Bush Professor Electrical Engineering Computer Science MIT Michael Price MIT Analog Devices Price Chandrakasan Jim Glass MITs Computer Science Artificial Intelligence Laboratory Price International Solid-State Circuits Conference Which Voice MIT MIT Marian Verhelst Catholic University Leuven Speech Qmulus Project MIT Quanta Computer Taiwan Semiconductor Manufacturing Companys University Shuttle Program,3
117,"Weve started talking to our phones again but in a very different way, as speech-activated functions and speech-to-text services get more accurate and useful Software firm Kingsoft to focus on AI, after 271m yuan loss One big mobile phone trend of the past 15 years has been the slow decline of calls in favour first of SMS, then of messaging. Since then its been all about text over talk, but the creeping use of more and more emojis as a shorthand is quickly transforming into using speech recognition as a way to save time on text input. We may not be chatting to each other as much as we used to, but the new trend is set; were talking to our phones again. From virtual assistants such as Siri, Alexa and OK Google to apps including Dragon Anywhere, Swype, Swiftkey and Baidus new TalkType, speech-activated functions and speech-to-text services are growing. Sundar Pichai, chief executive of Google, claims that 20 per cent of Google searches on smartphones are now entered by voice. Sending messages, creating appointments, getting directions and updating social media  all can now be done using the spoken word, and with ever-increasing accuracy. The voice has emerged as one of the most natural and intuitive ways to interact with devices, applications and intelligent systems, lessening our reliance on the mouse, keyboard and touch screen, says John West at Nuance Communications, whose speech recognition engine powers its Dragon suite of apps and software. In September, SwiftKey Keyboard announced a new version powered by neural networks. That means it can spot similar sentence structures, and capture the relationship and similarity between words. Its only available for US and UK English language models for now, but more languages are promised. Another virtual keyboard typical of the trend is Baidus TalkType app for Android smartphones, which is operated entirely by voice and can help users input text three times faster than typing. TalkType does include an alphanumeric keyboard that supports swipe, emojis, and even GIF sharing. TalkType is the first full-function Android keyboard that is voice-first, not voice-also, says Bijit Halder, head of Baidus Silicon Valley AI Lab product team. Unlike conventional keyboard designs, where voice is targeted for occasional use and delegated to a small icon, TalkType is designed for voice as the primary input mode. The accuracy of these voice recognition apps is sometimes stunning, but despite the use of AI there are still some teething problems. People, places and product names are rarely recognised, similar-sounding words are confused, and punctuation has to be spoken. It means forming entire sentences in your head before speaking, which takes some practice. While using Dragon Dictation, its not unusual for the cursor to head back into the document thats being dictated to make changes, or delete entire sections. Was it something I said Almost certainly; there are a range of spoken commands for completing various editing actions as well as merely having words spelled out onscreen. The learning curve is steep, but theres no doubt that Dragon Dictation  and all other speech recognition software  is getting close to 95 per cent accuracy. A few years ago it was barely at 70 per cent. So while BlackBerry may not be doing away with its keyboard just yet, it probably wont be long before it does. What then for productivity pros who, for now, prefer finger typing to swiping or speaking If thats you, there are some choices. From Amazon Echo to Google Home, how smart speakers can help out at home As well as BlackBerry with its Priv and Passport handsets, theres an optional keyboard cover for Samsungs Galaxy S7, which is surprisingly easy to use. You can also find a keyboard on LGs Xpression 2, but dont kid yourself: phones with keyboards, once common, are becoming increasingly rare. Though the Microsoft SurfacePro 4 tablet has its own keyboard case and the iPad has several options, there are also a host of highly portable wireless Bluetooth keyboards that can link to a phone or tablet. For example, the excellent Cervantes Mobile Jorno keyboard has a trifold design, as does the Zagg Pocket Keyboard, while the LG Rolly Keyboard KBB-700  which also acts as a stand for a phone  can be folded into something pocket-sized. For bigger devices, Windows 10s personal assistant feature has speech recognition, and Apple Macs now have Siri, but these are mostly for dictating short messages and giving commands. If you want to fully embrace the speech-to-text age to produce lengthy text documents purely by speaking, its all about Dragon Professional Individual software. After setting-up a voice profile (effectively teaching the software about your voice simply by reading to it for a few minutes), its possible to speak text into any application, from Microsoft Word to Gmail, and even web browsers. The latest version uses so-called deep learning, a powerful pattern recognition technique inspired by the way the human brain learns and interprets sensory input. It also uses deep neural nets  another phrase common in artificial intelligence  to continuously learn from the users speech patterns. Speaking to a computer while alone in an office is easy to get used to. So is conversing with Siri or Google on a phone in your own home, or in the car. However, talking to technology in public and in shared spaces is a much bigger social jump. Who wants to dictate a message to a phone while on a train, walking down the street, or in a busy restaurant, even if it does save a few seconds Long-time users of hands-free kits who brazenly chat while they walk may disagree, but many of us remain suspicious of such habits. As Amazon rolls out Echo, some worry about the internet listening to our conversations However, there is by now plenty of mobile technology to suit all kinds of applications and attitudes. According to analysts at Gartner, the average adult in Hong Kong will use three to four personal devices by 2018. So rather than all being similarly sized but otherwise identical touch screen tablets all optimised for speech, theres room for some diversity. Either way, the sight of commuters with their heads buried in a smartphone could soon get a spoken word soundtrack; chat is coming back, but not as we knew it. This article appeared in the South China Morning Post print edition as: ",http://www.scmp.com/lifestyle/article/2040614/how-speech-recognition-software-changing-way-we-communicate,"One big mobile phone trend of the past 15 years has been the slow decline of calls in favour first of SMS, then of messaging. Since then it's ...",How speech recognition software is changing the way we ...,Weve Software Kingsoft AI SMS Siri Alexa OK Google Dragon Anywhere Swype Swiftkey Baidus TalkType Sundar Pichai Google Google John West Nuance Communications Dragon September SwiftKey Keyboard US UK English Baidus TalkType Android GIF Android Bijit Halder Baidus Silicon Valley AI Lab TalkType AI Dragon Dictation Almost Dragon Dictation BlackBerry Amazon Echo Google Home As BlackBerry Priv Passport Samsungs Galaxy S7 LGs Xpression Microsoft SurfacePro Bluetooth Cervantes Mobile Jorno Zagg Pocket Keyboard LG Rolly Keyboard KBB-700 Apple Macs Siri Dragon Professional Individual Microsoft Word Gmail So Siri Google Amazon Echo Gartner Hong Kong South China Morning Post,4
118,"To receive (free) news headlines by email, please register online In the UK, video data mining and analysis specialist LivingLens has raised 1.3m in a round of funding which it will use to accelerate development of its enterprise video analytics tech platform, and to further expand its teams and open operations in the US. Launched by Curiosity Insight founder Carl Wong (pictured) and retail sector specialist David Woods, the London and Liverpool-based firm offers access to an app which allows consumers to capture, edit and share videos. Resulting content can then be analysed using software that intelligently searches every spoken word captured 'in any video, in any language'. Last year, LivingLens added machine speech recognition capabilities, to offer a near real-time 'speech to text' service. The new funding round comes from government-backed investment fund Angel CoFund, as well as further angels including a number of marketers and former city financiers. LivingLens plans to use the investment to ramp up its sales, marketing and customer success teams, and expand to the US. Wong says the new funds will enable the firm to enhance customer support, accelerate the development of new capabilities and 'ultimately, scale the business'. He adds: 'We are now working with some of the world's largest brands and agencies to help them to get closer to their consumers and drive more value out of consumer video as a valuable source of insight, and expect 2017 to be a really strong year for our business'. ",http://www.mrweb.com/drno/news24125.htm,"In the UK, video data mining and analysis specialist LivingLens has raised £1.3m in a round of funding which it will use to accelerate ...",LivingLens Raises Funds for US and Platform Expansion,UK LivingLens US Curiosity Insight Carl Wong David Woods London LivingLens Angel CoFund US Wong,-1
119,"How do we train speech recognition devices to understand such unique regional languages Dialects are an intrinsic part of who we are, and can represent home to many of us in a world becoming more global in nature. Many wonder though  how do we train speech recognition devices to understand such unique regional languages So what is a dialect Its a tricky question to answer that can get you into all sorts of political trouble in some areas of the world! In the past, central authorities were often sceptical of communities that claimed to have their own (regional) language, preferring rather to speak of a mere dialect. In the reverse, smaller countries with a big neighbour often insisted they spoke its own language, not just a dialect of the neighbours language. Luckily, linguistic variation today is often seen as a precious treasure of cultural heritage, but in many places linguist Max Weinreichs summary that a language is a dialect with an army and navy, is still valid. Avoiding those issues, I will use dialect in a pragmatic way, to encompass regional languages and accents. Looking back over the more than 20 years I have spoken to customers and others about Automatic Speech Recognition (ASR), the most frequently asked question definitely was, Do your systems speak dialect X, where X may have been Bavarian, Scottish English, Swiss German, Canadian French  and many other examples. After many centuries of authorities trying to discourage the use of dialects, today many people are actually proud of their ability to speak a dialect. Recall, for instance, when during a trip to an exotic place you recognised somebody coming from the place you were born in just by listening to how they speak  its a welcome feeling. Even governments exploit this today: the German state of Baden-Wrttemberg (which prides itself on being the birth place of many inventors and scientists, like Karl Benz, Johann Kepler, Albert Einstein and, coincidentally, is also the home of Nuances Ulm office) coined the amusing quip: We can do everything. Except [speak] High German.  Obviously, the quip is not quite true, in that most speakers of dialects also speak the standard form of their language and apply what linguists call code switching. Depending on the social setting, speakers switch between standard language (in a formal setting) to dialect (at home or with friends) and back. Dialect, similarly, can be a tool with which you can signal to somebody they are welcome in your home or that they will remain a stranger, as they dont speak your dialect. The same mechanism may be at work in those numerous radio spots or YouTube videos where people make fun of ASR, which supposedly does not speak or recognise a dialect; hence the video of Scots in a lift. The second reason why people may have doubts about ASR working well with dialect may also be related to the long history of dialects not being an acceptable language to use in school (at least in some countries). Clearly dialects deviate from the rules of the standard language, as codified in the grammar book, so that somehow encouraged the myth that dialects do not have any rules, are irregular and therefore difficult to capture in a machine. But from a linguistic viewpoint, that is really just a myth: granted, dialects sometimes dont have a written form, but for linguists spoken language is more important anyway with written language only being a secondary derivation. And in the spoken form, dialects are as regular as any other language; they are neither worse nor more difficult, nor better or easier than standard languages. Machine Learning, especially Deep Learning based on Neural Nets, can deal with the variety of having several dialects and a standard form in one population. As long as you make sure all dialects are reflected in your training data (and we make sure it is; in the UK for example, we use more than 20 defined dialect regions) the resulting models will reflect all the ways of pronouncing the phonemes (or sounds) of a language. We make sure to include words that are special to a dialect (again, using the UK as an example, different areas refer to a bread roll as a cob, a barm cake or a bun) and where pronunciation differences go beyond isolated phonemes, we reflect that in the pronunciation dictionary. For instance, our UK English language pack recognises 52 different pronunciations of the word Heathrow so our airline customers can cater to those whose first language isnt English. When differences become too big, we create separate models in some cases. Users of Dragon speech recognition software can choose between variations of English and between Flemish (for Belgium) and Dutch (for the Netherlands). Occasionally this is done under the hood, so to speak. Even in the Dragon US English version, there are several dialect models. We use a classifier (another application of Machine learning) to detect which package fits best to the users dialect and use that for recognising the dialect. We also verify that it works by measuring accuracy gains per variant, e.g. Dragon Professional Individual English has an accuracy improvement (over the previous version) of 22.5 per cent error reduction for speakers of English with a Hispanic accent, 16.5 per cent for southern (US) dialects, 13.5 per cent for Australian English, 18.8 per cent for UK English, 17.4 per cent for Indian English and 17.4 per cent for Southeast Asian speakers of English. Finally, we have adaptation to help us with the challenge: dictation software like Dragon will adapt overtime to a users specific dialect. When the usage deviates from how we thought it would be used during training, ASR may not work for every dialect at every time. However, speech recognitions accuracy across a number of languages has risen considerably by upwards of 99 per cent, and is evidenced by the broad and global integration of our cloud based ASR and NLU, used by thousands of apps in cars, IoT devices, smart phones etc. Linguistic variety is as important to us as it is important to you; which is why we support more than 80 languages (including regional languages like Catalan and Basque, which we developed in cooperation with regional governments ), and as I have outlined, we do a lot more to cover variation and dialects beyond that number. So, we welcome the challenge of dialect  even if its in the form of an amusing YouTube spoof. Nils Lenke is Senior Director, Corporate Research Nuance Communications. He oversees the coordination of various research initiatives and activities across many of Nuances business units. ",http://www.itproportal.com/features/does-speech-recognition-understand-your-dialect/,Many wonder though – how do we train speech recognition devices to understand such unique regional languages? So what is a dialect?,Does speech recognition understand your dialect?,Max Weinreichs Automatic ASR Do X Bavarian Scottish English German Canadian French Recall Baden-Wrttemberg Karl Benz Johann Kepler Albert Einstein Nuances Ulm ] High German Dialect YouTube ASR Scots ASR Machine Learning Deep Learning Neural Nets UK UK UK English Heathrow English Dragon English Flemish Belgium Dutch Netherlands Dragon US English Machine Dragon Professional Individual English English US English UK English English Southeast English Dragon ASR ASR NLU IoT Catalan Basque YouTube Nils Lenke Senior Director Corporate Research Nuance Communications,1
120,"This is just the ticket... Who knows where the Dow DJIA, +0.01% and the S&P 500 SPX, +0.07%  are headed Maybe only the algo traders, who are mostly calling the shots nowadays . Which brings us to this call from Jefferies analysts looking to make money on the artificial intelligence trend. They suggest betting on hot chip-stock Nvidia NVDA, -2.55% and car-parts supplier Delphi Automotive DLPH, +0.52% , as theyre likely to benefit from the greater usage of AI across industries. Artificial intelligence and deep learning will be among the most important investment themes in the next 3-5 years, says Jefferies in a note, citing research from its analyst Mark Lipacis. Nvidia has emerged as the de facto standard in AI, providing the graphic chips used in Amazon AMZN, +1.36% and Googles GOOG, +0.01% GOOGL, -0.05% personal assistants, Baidus BIDU, +1.03% speech recognition, Skypes translation tool and Netflixs NFLX, +0.52% recommendations, according to Lipacis. Self-driving cars and other deep-learning related opportunities should also mean more business for Nvidia, he says. Meanwhile, Delphi is particularly well-positioned to get a boost from all of the innovation around how people get from point A to point B, says Jefferies analyst David Kelley. Driverless cars have way more electronics than regular models, and that should play right into Delphis hands, Kelley suggests. He touts Delphi and Mobileyes MBLY, -0.02% partnership to develop a fully autonomous driving system for car makers by 2019. Not won over by Nvidia and Delphi Other analysts have said their AI stocks include AMD, Intel and Qualcomm  or Amazon and IBM . ",http://www.marketwatch.com/story/these-stocks-let-you-bet-on-artificial-intelligence-and-welcome-our-robot-overlords-2017-03-31,"... Baidu's BIDU, +1.03% speech recognition, Skype's translation tool and Netflix's NFLX, +0.52% recommendations, according to Lipacis.",These stocks let you bet on artificial intelligence and welcome our ...,Dow DJIA +0.01 S P SPX +0.07 Nvidia NVDA -2.55 Delphi Automotive DLPH +0.52 AI Jefferies Mark Lipacis Nvidia AI Amazon AMZN +1.36 Googles GOOG +0.01 GOOGL -0.05 Baidus BIDU +1.03 Skypes Netflixs NFLX +0.52 Lipacis Nvidia Delphi B Jefferies David Kelley Driverless Delphis Kelley Delphi Mobileyes MBLY -0.02 Nvidia Delphi AMD Intel Qualcomm Amazon IBM,-1
121,"Plenty of companies have wished they had access to the same call center technology Amazon uses for its customer service centers. Now, they can. Today, Amazon Web Services (AWS) announced the launch of Amazon Connect, a self-service, cloud-based contact center technology service that is based on the same technology the company uses to power its own customer service operations. According to the company, the service will allow enterprises to set up their own call center operations without having to invest heavily in proprietary hardware and software systems. Instead, Amazon's enterprise clients will be able to set up and configure their own ""virtual contact centers"" in a matter of minutes, offering better customer service at a lower cost. Traditional contact centers have typically been complicated, expensive operations to set up, often taking months or even years to fully deploy. They often involve proprietary technologies that require special skills to operate, and frequently come with restrictive licensing agreements that can make it difficult for companies to scale their call center operations in response to changes in call volumes due to short-term promotions, seasonal spikes, or new product launches. Amazon said that was the reason it decided to develop its own call center technology. ""Ten years ago, we made the decision to build our own customer contact center technology from scratch because legacy solutions did not provide the scale, cost structure, and features we needed to deliver excellent customer service for our customers around the world,"" said Tom Weiland, vice president of worldwide customer service, Amazon, in a statement. This choice has been a differentiator for us, as it is used today by our agents around the world in the millions of interactions they have with our customers,"" he added. ""We're excited to offer this technology to customers as an AWS service -- with all of the simplicity, flexibility, reliability, and cost-effectiveness of the cloud."" According to the company, Amazon Connect doesn't require any infrastructure to deploy or manage, so customers can scale their Amazon Connect virtual contact centers up or down, onboarding any number of agents in response to business cycles and paying only for the time callers interact with the service. The platform features a self-service graphical interface that Amazon said makes it easy for non-technical users to design contact flows, manage agents, and track performance metrics, without the need for any specialized skills. The new platform also makes it possible for enterprises to design contact flows that adapt the caller experience, the company said. Contact flows can change based on information retrieved by Amazon Connect from other AWS services or third-party systems including CRM (customer relationship management) or analytics solutions. Businesses can also build natural language contact flows using Amazon Lex, an artificial intelligence service that has the same automatic speech recognition technology and natural language understanding that powers Amazon Alexa. Amazon Lex enables callers to say what they want instead of having to listen to long lists of menu options and guess which one is most closely related to what they want to do. @Jerry: Yes, it sounds like a possible option for financial institutions as well as other customer-facing businesses. Captures my attention for sure...Would/could this be fitting to a Financial Institution ",http://www.toptechnews.com/article/index.php?story_id=023000DQELCN,... service that has the same automatic speech recognition technology and natural language understanding that powers Amazon Alexa.,Amazon Launches Its Own Call Center Platform,Plenty Amazon Amazon Web Services AWS Amazon Connect Amazon Traditional Amazon Tom Weiland Amazon AWS Amazon Connect Amazon Connect Amazon Contact Amazon Connect AWS CRM Amazon Lex Amazon Alexa Amazon Lex Jerry Financial,6
122,"According to the new market research reportIntelligent Virtual Assistant (IVA) Market, by Technology (Speech Recognition, Text-to- Speech Recognition), Applications (Messenger Bots, Websites, Contact Centers), End-users (Small Enterprises, Medium Enterprises, Large Enterprises, Individual Users) - Global Revenue, Trends, Growth, Share, Size and Forecast to 2022 published by Scalar Market Research, the global intelligent transportation systems market is expected to grow at a healthy rate during the forecast period. Early Buyers Can Get 10% Free Customization On This Report. The objective of this report is to describe the market trends and revenue forecasts for the intelligent virtual assistant (IVA) market for the next five years. The report focuses on defining and describing the key influencing factors for the growth of the market. It also offers an in-depth analysis of the market size (revenue), market share, major market segments, different geographic regions, key market players, and premium industry trends. The report tracks the major market events including product launches, technological developments, mergers & acquisitions, and the innovative business strategies opted by key market players. Along with strategically analyzing the key micro markets, the report also focuses on industry-specific drivers, restraints, opportunities and challenges in the intelligent virtual assistant (IVA) market. The scope of this report covers the intelligent virtual assistant (IVA) market by its major segments, which include the technologies, applications, end-users, and the major geographic regions. The key reasons for the growth of the intelligent virtual assistant (IVA) market include enhanced customer services, the rise in adoption of smartphones, and the cost-effectiveness of these systems. However, the lack of awareness about this technology is the major factor that may hamper the growth of the market in the coming years. The addition of natural language processing is expected to be the key growth opportunity factor for the intelligent virtual assistant (IVA) market during the forecast period. Scalar Market Research Inc. aspires to assist organizations from around the world to achieve their business goal with premium market research reports and consulting services. Our real-time industry tracking with the help of advanced analytics offers a crystal clear view of all the activities in niche markets. Our team, with thorough global understanding, works relentlessly to gather the necessary market insights, including customer analysis, competitions and global forecast. Find out more about our services at: www.scalarmarketresearch.com 8770 W Bryn Mawr Ave., ",http://www.openpr.com/news/496842/Intelligent-Virtual-Assistant-IVA-Market-Forecast-to-2022-Scalar-Market-Research.html,"According to the new market research reportIntelligent Virtual Assistant (IVA) Market, by Technology (Speech Recognition, Text-to- Speech ...",Intelligent Virtual Assistant (IVA) Market Forecast to 2022 - Scalar ...,Virtual Assistant IVA Market Technology Text-to- Applications Messenger Bots Websites Contact Centers End-users Small Enterprises Medium Enterprises Large Enterprises Individual Users Trends Growth Share Size Forecast Market Research Report IVA IVA IVA IVA IVA Market Research Inc. W Bryn Mawr Ave.,-1
123,"Amazon Web Services (AWS), a subsidiary of Amazon.com, has revealed Amazon Connect, a self-service, cloud-based contact centre service, is now available to other businesses. According the global company, the contact centre technology is based on the same contact centre technology used by Amazon customer service associates. ""Companies can set up and configure a virtual contact centre using the AWS management console which should make it easy for any business to deliver better customer service at lower cost,"" the company said. It added: ""The AWS management console operates in 42 availability zones within 16 geographic regions around the world. With the virtual contact centre, there are no up-front payments or long-term commitments and no infrastructure to manage. Customers also pay by the minute for usage, paying only for the time callers are interacting with the service plus any other associated telephony charges."" Since there is no infrastructure to deploy or manage, companies which use Amazon connect can onboard as many agents as possible for various business cycles. ""The service allows users to design contact flows that adapt the caller experience. For example, an airline could design a contact flow to recognise a caller's phone number, look up their travel schedule in a booking database, and present options like ""rebook,"" or ""cancel"" if the caller just missed a flight. Users can also build a natural language contact flows using Amazon Lex, an AI service that has the same automatic speech recognition technology and natural language understanding that powers Amazon Alexa."" Earlier this year AWS introduced a cloud-based video conferencing service that competes with Skype for Business and Google Hangouts. At the time the company said Amazon Chime supports video conferencing, calls, chat, and the sharing of content. ""The product runs on Windows and Mac OS X, Android and iOS. The video-conferencing tool encrypts all communications and doesn't store chat history while making work conferences more seamless and easier to start."" Speaking on the launch of the service, Tom Weiland, vice president of worldwide customer service at Amazon, said AWS's decision to build their own customer contact centre technology was a definite differentiator as the technology is now used by customers. ""Ten years ago, we made the decision to build our own customer contact centre technology from scratch because legacy solutions did not provide the scale, cost structure, and features we needed to deliver excellent customer service for our customers around the world. We're excited to offer this technology to customers as an AWS service."" Amazon Connect will be made available to the SA market in coming months. ",http://www.itweb.co.za/index.php?option=com_content&view=article&id=160742,"Users can also build a natural language contact flows using Amazon Lex, an AI service that has the same automatic speech recognition ...",Amazon makes self-service contact centre service available,Amazon Web Services AWS Amazon.com Amazon Connect Amazon AWS AWS Customers Amazon Amazon Lex AI Amazon Alexa AWS Skype Business Google Hangouts Amazon Chime Mac OS X Android Tom Weiland Amazon AWS AWS Amazon Connect SA,6
124,"This story was delivered to BI Intelligence Apps and Platforms Briefing subscribers. To learn more and subscribe, please click here . The news brings the company one step closer to getting computers to understand speech as well as a person, and helps in its efforts to provide conversation as a service through tech like Cortana, Skype Translator, and other language-related cognitive services. Voice is going to emerge as a dominant computing interface.Digital voice assistants, like Apples Siri, have long been a part of the mobile device ecosystem, but the technology has often been slow or returned irrelevant information. Now, recent technological advancements in search functionality and language understanding  through AI tech like deep neural networks  are vastly improving the accuracy and convenience of voice assistants. And in an attempt to get ahead of the hype, tech companies including Amazon, Apple, Google, and IBM, are already deploying voice to consumers. Apple, for instance, recently added third-party integration to Siri, which will allow it to communicate with other apps. This means that users can order an Uber through Siri. However, voice assistants still need to improve.before voice becomes the primary platform for consumers.Speech recognition needs to reach roughly 99% (it's at approximately 90% now) in order for voice to become the most efficient form of computing input, according to Kleiner Perkins analyst Mary Meeker. Meanwhile, companies that arent investing in voice technology could run the risk of being left behind. This point was reiterated in a recent interview at TechCrunch Disrupt SF with VP of Messenger David Marcus who conceded that Facebooks emphasis on text could result in it not having a horse in the voice race. ",http://www.businessinsider.com/mircosoft-surpasses-ibms-watson-in-speech-recognition-2016-9,"A team of Microsoft researchers has achieved the lowest error rate for speech recognition on record at 6.3%, overtaking prior record holder IBM ...",Mircosoft surpasses IBM's Watson in speech recognition,BI Intelligence Apps Platforms Briefing Cortana Skype Translator Voice Apples Siri AI Amazon Apple Google IBM Apple Siri Uber Siri Kleiner Perkins Mary Meeker TechCrunch Disrupt SF VP Messenger David Marcus Facebooks,4
125,"I'm not going to sugar coat things, there are a lot of crazy things going on in the world right now... Will everyone soon be able to say they live in beachfront property Will anyone have health insurance to cover us when we are drowning from our global warming produced beachfront property These are big questions for sure, but let's not forget the elephant in the room. The question everyone is pondering and needs to be answered, right now... Is the HELLO BARBIE HOLOGRAM Creepy or Cool Barbie Dabbing...need I say more Yes, the Hello Barbie Hologram is a hologram of Barbie that responds to your voice commands. Think Amazon Echo...with a Barbie Hologram. Take a look at this video and then leave a comment in the video - CREEPY or COOL Hello Barbie Hologram combines the power of artificial intelligence with the fun of an expressive holographic character, giving girls an entirely new way to engage with Barbie throughout the day. Kid friendly speech recognition puts girls in the drivers seat, giving them easy access to customizable play, animated weather reports, alarms, night lights, daily reminders and more. As a Bluetooth speaker, Hello Barbie Hologram allows girls to choreograph Barbies dance moves while listening to their favorite playlists. Does Hello Barbie Hologram Represent the Next Level of Convergence You can make the argument that the major technological advances of the last 20 years have been all about convergence of technology. First we hadPCs in our home and that was something. The Internet of course changed our lives. Without it, we would have never known that cats could play piano and fit into tiny little boxes. It was the convergence of computers, the internet andportable technology that gave us smart phones and this truly changed lives. Now we are constantly connected and have instant access to basically all the information in the world...and the ability to see photos of what all our friends had for dinner. Amazon Echo and Google Home have taken the convergence of speech recognition, cloud knowledge and virtual assistants and given us the ability to yell out ""Alexa tell me a Joke"" and be instantly amused. What if your virtual assistant appeared as a hologram in your room This is the idea of Hello Barbie Hologram, but in truth, this might just be the start. What if Barbie could float in the air and come out of her box Yes, that is a Dad, interacting with his kid...who is just a hologram. I will give you a minute to pick up the pieces of your blown mind. Barbie Hologram Might Freak You Out, But She is Just the Start The Hello Barbie Hologram is being marketed to kids. It is a connected device, that asks your kid all types of personal info, collects things like birthdate, favorite music and other stuff that stalkers love. What could possibly go wrong Yes, there are all types of possibilities for Hologram Barbie to be a creepy spy device, but the concept and technology is impressive. Sure, it is technically not a hologram at all, but a reflected image that appears to dance in the air is still cool. Holograms will continue to advance. Voice activated virtual assistants will only get smarter, faster and more accurate. It's only a matter of time before I'm asking my Hologram of Donald Trump to give me my Fake News update for the day. This post is hosted on the Huffington Post's Contributor platform. Contributors control their own work and post freely to our site. If you need to flag this entry as abusive, send us an email . Start your workday the right way with the news that matters most. Learn more ",http://www.huffingtonpost.com/entry/hologram-barbie-creepy-or-cool-the-question-we-must_us_58d534d3e4b0f633072b36cb,"Kid friendly speech recognition puts girls in the driver's seat, giving them easy access to customizable play, animated weather reports, alarms, ...","Hologram Barbie, CREEPY or COOL? The Question We MUST ...",Will Will HELLO BARBIE HOLOGRAM Creepy Cool Barbie Dabbing Hello Barbie Hologram Barbie Amazon Echo Barbie Hologram COOL Hello Barbie Hologram Barbie Kid Bluetooth Hello Barbie Hologram Barbies Does Hello Barbie Hologram Represent Next Level Convergence First Internet Amazon Echo Google Home Alexa Joke Hello Barbie Hologram Barbie Dad Barbie Hologram Might Freak Start Hello Barbie Hologram Yes Hologram Barbie Holograms Voice Hologram Donald Trump Fake News Huffington Post Contributor,-1
126,"Facebook today announced the launch of a voice search feature for its Oculus Rift virtual reality (VR) headset and Samsungs Gear VR headset. Its available now in beta for people who use English as their primary language, the Oculus team said in a blog post . This feature lets you perform voice searches from Oculus Home to intuitively navigate games, apps, and experiences, the Facebook subsidiary said. I tested out the feature on an Oculus Rift just now, and it does work. Once you enable it, while youre in the Home portion of the Oculus app, you can say the phrase Hey Oculus to get the app to listen for your next voice command. If Oculus doesnt know how to respond to you, it will let you know that with a line of text below an orb floating directly in front of you. Look down and youll see a tablet with a few suggested actions. It notes that you can search for apps in the store, launch apps from your library, or step away from the speech recognition area by saying cancel that. You can even turn off Oculus ability to listen to the Hey Oculus wakeword. If you can access Oculus Rooms for the Gear VR, voice search is there, too. But theres a lot that Oculus voice search cant do. It wont let you look at your friends, your notifications, your profile, your avatar, app updates that you havent installed, or individual settings. And Oculus certainly isnt up for answering random questions, like Who is Mark Zuckerberg Youre better off asking Siri, Alexa, Cortana, or Google. And if you want to talk to Oculus but you have no internet connection, youre out of luck. I guess I shouldnt be too disappointed about Oculus speech shortcomings, because the team does point out in todays blog post that it will be adding more functionality over time. But I cant help it. As far as I can tell, this is the first time that Facebook itself as a company is unveiling a speech recognition capability. The company has been developing artificial intelligence technology for years, particularly since the formation of the Facebook Artificial Intelligence Research (FAIR) lab in 2013. Ive been wondering how good Facebooks underlying speech technology is. Amazon, Apple, Google, and Microsoft make a big deal about their speech-activated virtual assistants, while Facebook has been silent all this time. Now that changes. It turns out that Facebooks system does appear to be good at understanding what Im saying. And thats a good start. Triggering the right action based on voice input is the next step, and Facebook will be able to perfect that part over time. If engagement is good and people like it, the speech technology might well end up becoming a part of Facebooks more popular services, like WhatsApp, Messenger, Instagram, and even good old Facebook itself. Its time to learn how AI can transform your business at the 2017 VB Summit, June 5th  6th in Berkeley. Meet AI ecosystem leaders who are shaping a new digital economy. Request an Invite - Save 50%! ",https://venturebeat.com/2017/03/10/facebook-adds-speech-recognition-to-oculus-rift-gear-vr/,"It notes that you can search for apps in the store, launch apps from your library, or step away from the speech recognition area by saying ...","Facebook adds speech recognition to Oculus Rift, Gear VR",Facebook Oculus Rift VR Gear VR English Oculus Oculus Home Facebook Oculus Rift Home Oculus Hey Oculus Oculus Hey Oculus Oculus Rooms Gear VR Oculus Oculus Who Mark Zuckerberg Youre Siri Alexa Cortana Google Oculus Oculus Facebook Facebook Artificial Intelligence Research FAIR Ive Facebooks Amazon Apple Google Microsoft Facebook Facebooks Im Facebook Facebooks WhatsApp Messenger Instagram Facebook AI VB Summit June Berkeley Meet AI Request Invite,4
127,"Speech recognition software used to be awful. It couldn't deal with background noise, its vocabulary was limited and it was awkward to use in public. But we've come a long way. A new speech recognition system can transcribe English or Mandarin about three times faster than humans can type on a smartphone, according to a recent study . Developers from the popular Chinese search engine Baidu created the program back in December. It's called Deep Speech 2 , and it uses machine learning to vastly improve speech recognition. SEE ALSO: Mark Zuckerberg's First Stop in China: Baidu Headquarters The study, a collaboration between Stanford University, Baidu and the University of Washington, also found that the system produced 20.4 percent fewer errors than people typing in English and 63.8 percent fewer than people working in Mandarin. To make the conditions as authentic as possible, the human subjects were also allowed to use autocorrect and word suggestions. And all participants were between 19 and 32 years old. The system produced 20.4 percent fewer errors than people typing in English and 63.8 percent fewer than people working in Mandarin. Overall, participants preferred the speech recognition system, though they had some reservations. ""When I made mistakes it seemed like it took longer to correct, because I was switching from holding the phone to speak vs. holding to type,"" said one participant, according to the study. Another participant admitted to simply being more comfortable on their keyboard. ""I could correct errors as they were made,"" the participant said ""That being said, it seemed like I introduced more errors typing that the speech system did."" The researchers suggested some improvements for future versions of the system. For example, it would be great if the system could auto-detect when a speaker was finished instead of pressing a button. The researchers would also like to test it in a variety of noise levels. Baidu doesn't plan to make the software available to the public, but they are integrating it into Baidu apps in China. You could imagine an interface where you use speech to start and then it switches to a graphical interface that you can touch and control with your finger, Landay said. We should put speech in more applications than just typing an email or text message."" Watch an in-depth explanation of the experiment below: ",http://mashable.com/2016/08/29/baidu-deep-speech-2-fast-speech-recognition/,"Speech recognition software used to be awful. It couldn't deal with background noise, its vocabulary was limited and it was awkward to use in ...",This speech recognition software is much faster than human typists,English Mandarin Baidu December Deep Speech SEE ALSO Mark Zuckerberg First Stop China Baidu Headquarters Stanford University Baidu University Washington English Mandarin English Mandarin Overall Baidu Baidu China Landay,3
128,"Get today's popular DigitalTrends articles in your inbox: Speech-recognition systems may not yet be perfect, but as the likes of Amazon Echo show, theyre getting both better and more ubiquitous all the time. A new piece of research by investigators at The Massachusetts Institute of Technologys Computer Science and Artificial Intelligence Laboratory (CSAIL) suggests a new technique for training these systems  by getting them to learn by looking at images. This is an attempt to get machines to require less supervised training to learn about spoken language, Jim Glass , a senior research scientist at CSAIL, told Digital Trends. The conventional way to train speech recognition systems is by using recordings of people talking and, for each utterance, transcribing exactly what words have been said. Ideally, you have hundreds or thousands of hours of speech in order for the system to work properly. Some of the biggest companies doing this  like Baidu and Google  are using tens ofthousands of hours for training. The more annotated data that they have, the better these systems perform. So whats wrong with that After all, as noted, speech-recognition tech is continuously getting better. Whatever computer scientists are doing is obviously working. That may be true, but this new approach is interesting for a couple of reasons. Firstly, opening up the ability of a machine to train itself to understand by looking at combined images and audio (eventually, you could imagine it training by watching YouTube) is much closer to the way that we learn as human beings. Secondly  and arguably more importantly  is the fact that it could help bring speech recognition to parts of the world that mightgreatly benefit from this kind of technology. Annotated data is expensive to produce, Glass continued. Speech recognition has been going on for decades and the majority of it has been for languages in countries which can afford to invest in these kind of resources. When it comes to language, it tends to be those which companies think will help them make a profit. English has received by far the most attention, followed by western European languages, and other languages like Japanese and Mandarin. The problem is that there are around 7,000 languages spoken in the worldand around 300 that are spoken by more than 1 million people. A lot of these just havent received much attention  if any. In parts of the world where literacy levels are low, its easy to see how speech recognition could be a game changer in terms of providing people with access to information. Hopefully, this technology can help toward that goal. As exciting as the research is, however, Glass notes itis still in its very early stages. At present, CSAIL researchers have been feeding their system with a database of 1,000 images, each with a free-form verbal description that relates to it in some way. They then test the system by giving it a recording and asking it to retrieve 10 images which best match what it is hearing. Over time, the hope is that such approaches to speech recognition will improve in their effectiveness to the point where laborious labeling of speech training datais no longer considered a necessity. If all goes according to plan, that should be better for everyone  whether youre an English speaker in the U.S. ora speaker of Xhosa in South Africa. ",http://www.digitaltrends.com/cool-tech/new-approach-speech-recognition/,"Speech-recognition systems may not yet be perfect, but as the likes of Amazon Echo show, they're getting both better and more ubiquitous all ...",Clever new speech recognition system from MIT learns language ...,Get DigitalTrends Amazon Echo Massachusetts Institute Technologys Computer Science Artificial Intelligence Laboratory CSAIL Jim Glass CSAIL Digital Trends Ideally Baidu Google YouTube Annotated Glass English Mandarin Hopefully Glass CSAIL U.S. Xhosa South Africa,-1
129,"Baidu , the Chinese company operating a search engine, a mobile browser, and other web services, is announcing today the launch of SwiftScribe , a web app thats meant to help people transcribe audio recordings more quickly, using  you guessed it!  artificial intelligence (AI). Baidu in the past few years has been honing its DeepSpeech software for speech recognition. Last year, the company introduced TalkType , an Android keyboard that, using DeepSpeech, puts speech input first and typing second, based on the idea that you can enter information more quickly when you say it than when you peck. Now Baidu is coming out with another app enhanced with DeepSpeech, one that could arguably find better footing in a professional setting. Amazon, Apple, Google, and Microsoft have all been working on speech recognition right alongside Baidu, but none of those four has come up with something aimed at longer-form transcription. In SwiftScribe, once you choose a file to upload in .wav or .mp3 format, the system goes to work processing it. For me, a 30-second file was ready in 10 seconds, and a one-minute file was ready in less than 30 seconds. SwiftScribe can handle up to an hour of audio in any given file, butthat will take 20 minutes to process, Baidu project manager Tian Wu told VentureBeat in an interview. From there, youll need to go in and change some things, like capitalizing, adding punctuation, and changing the spelling of certain words. Keyboard shortcuts help you more efficiently change the speed of audio, rewind, and add a line break. SwiftScribe was inspired partly by Wus experience transcribing many interviews during her time in graduate school at the University of California, Santa Barbara. English is not my first language, said Wu, who is from China. It took 10 hours to transcribe one hour of audio. Thats my personal experience. Usually it will take a professional four to six hours to transcribe a one-hour audio clip. But Wu and her colleague Nina Wei also took inspiration from conversations with several transcriptionists. Wus team believes SwiftScribe can help people transcribe audio 1.67 times faster  in 40 percent less time  than they would on their own. That would imply that they could do more work and ultimately get paid more for their work, Wu said. While the product is certainly designed for transcriptionists  who are used to working on computers as opposed to mobile devices, hence the fact that SwiftScribe is only available as a web app  SwiftScribe could also come in handy for other people, like journalists and historians. Today, Baidu is providing SwiftScribe as a free service  unlike Nuances Dragon software. But in the future, we hope to turn it into a business, Wu said. In the future the teamcould enhance the app with video transcription and captioning, support for more file formats, and an option for automatically adding punctuation, she said. A blog post has more detail. Its time to learn how AI can transform your business at the 2017 VB Summit, June 5th  6th in Berkeley. Meet AI ecosystem leaders who are shaping a new digital economy. Request an Invite - Save 50%! ",https://venturebeat.com/2017/03/13/baidu-launches-swiftscribe-an-app-that-transcribes-audio-with-ai/,"Baidu in the past few years has been honing its DeepSpeech software for speech recognition. Last year, the company introduced TalkType, an ...","Baidu launches SwiftScribe, an app that transcribes audio with AI",Baidu SwiftScribe AI Baidu DeepSpeech TalkType Android DeepSpeech Baidu DeepSpeech Amazon Apple Google Microsoft Baidu SwiftScribe SwiftScribe Baidu Tian Wu VentureBeat Keyboard SwiftScribe Wus University California Santa Barbara English Wu China Usually Wu Nina Wei Wus SwiftScribe Wu SwiftScribe SwiftScribe Baidu SwiftScribe Nuances Dragon Wu A AI VB Summit June Berkeley Meet AI Request Invite,-1
130,"15 Sep 2016 at 16:59, Katyanna Quach Microsoft researchers working on AI computer speech recognition have reached a word error rate of 6.3 per cent, claiming to be the lowest in the industry. Hot on the heels of Google DeepMind announcing a breakthrough in AI speech recognition, Microsoft was quick to respond by saying it, too, has reached a milestone while using neural networks. A paper released on arXiv shows the researchers have combined neural-network-based acoustic and language modelling on the US National Institute of Standards and Technology (NIST) 2000 Switchboard task - a conversational telephone speech recognition test used as an industry standard. Artificial neural networks which are modelled loosely on how scientists believe the brain performs calculations are often used in speech processing. Microsoft has used a mixture of convolutional neural networks that dont form a closed cycle of nodes and a recurrent neural network that does. Both are useful for analysing the large data sets used to train the networks to process language, which requires a lot of computing power. Human language is difficult for computers to understand. To make it easier, models are used to predict the probability distribution over sequences of words using audio signals from speech. Microsoft uses a 30k-word vocabulary database derived from the most common words in the Switchboard and Fisher corpora to recognise words from speech. Computer speech recognition has come a long way. Twenty years ago the error rate of the best published research system had a word error rate that was greater than 43 percent, Microsoft said . Speech recognition is a hot trend in AI as companies race to build the best AI personal assistant. Microsoft has Cortana, Apple uses Siri, Google has its WaveNet system and Amazon has just announced a UK release for its Echo device.  ",https://www.theregister.co.uk/2016/09/15/microsoft_lowest_error_rate_ai_speech_recognition/,"Microsoft researchers working on AI computer speech recognition have reached a word error rate of 6.3 per cent, claiming to be the lowest in ...",Microsoft: Our AI speech recognition mangles your words the least,Katyanna Quach Microsoft AI Hot Google DeepMind AI Microsoft US National Institute Standards Technology NIST Switchboard Microsoft Microsoft Switchboard Fisher Computer Microsoft AI AI Microsoft Cortana Apple Siri Google WaveNet Amazon UK Echo,8
131,"Computers have got much better at translation, voice recognition and speech synthesis, says Lane Greene. But they still dont understand the meaning of language IM SORRY, Dave. Im afraid I cant do that. With chilling calm, HAL 9000, the on-board computer in 2001: A Space Odyssey, refuses to open the doors to Dave Bowman, an astronaut who had ventured outside the ship. HALs decision to turn on his human companion reflected a wave of fear about intelligent computers. When the film came out in 1968, computers that could have proper conversations with humans seemed nearly as far away as manned flight to Jupiter. Since then, humankind has progressed quite a lot farther with building machines that it can talk to, and that can respond with something resembling natural speech. Even so, communication remains difficult. If 2001 had been made to reflect the state of todays language technology, the conversation might have gone something like this: Open the pod bay doors, Hal. Im sorry, Dave. I didnt understand the question. Open the pod bay doors, Hal. I have a list of eBay results about pod doors, Dave. Creative and truly conversational computers able to handle the unexpected are still far off. Artificial-intelligence (AI) researchers can only laugh when asked about the prospect of an intelligent HAL, Terminator or Rosie (the sassy robot housekeeper in The Jetsons). Yet although language technologies are nowhere near ready to replace human beings, except in a few highly routine tasks, they are at last about to become good enough to be taken seriously. They can help people spend more time doing interesting things that only humans can do. After six decades of work, much of it with disappointing outcomes, the past few years have produced results much closer to what early pioneers had hoped for. Speech recognition has made remarkable advances. Machine translation, too, has gone from terrible to usable for getting the gist of a text, and may soon be good enough to require only modest editing by humans. Computerised personal assistants, such as Apples Siri, Amazons Alexa, Google Now and Microsofts Cortana, can now take a wide variety of questions, structured in many different ways, and return accurate and useful answers in a natural-sounding voice. Alexa can even respond to a request to tell me a joke, but only by calling upon a database of corny quips. Computers lack a sense of humour. When Apple introduced Siri in 2011 it was frustrating to use, so many people gave up. Only around a third of smartphone owners use their personal assistants regularly, even though 95% have tried them at some point, according to Creative Strategies, a consultancy. Many of those discouraged users may not realise how much they have improved. In 1966 John Pierce was working at Bell Labs, the research arm of Americas telephone monopoly. Having overseen the team that had built the first transistor and the first communications satellite, he enjoyed a sterling reputation, so he was asked to take charge of a report on the state of automatic language processing for the National Academy of Sciences. In the period leading up to this, scholars had been promising automatic translation between languages within a few years. But the report was scathing. Reviewing almost a decade of work on machine translation and automatic speech recognition, it concluded that the time had come to spend money hard-headedly toward important, realistic and relatively short-range goalsanother way of saying that language-technology research had overpromised and underdelivered. In 1969 Pierce wrote that both the funders and eager researchers had often fooled themselves, and that no simple, clear, sure knowledge is gained. After that, Americas government largely closed the money tap, and research on language technology went into hibernation for two decades. The story of how it emerged from that hibernation is both salutary and surprisingly workaday, says Mark Liberman. As professor of linguistics at the University of Pennsylvania and head of the Linguistic Data Consortium, a huge trove of texts and recordings of human language, he knows a thing or two about the history of language technology. In the bad old days researchers kept their methods in the dark and described their results in ways that were hard to evaluate. But beginning in the 1980s, Charles Wayne, then at Americas Defence Advanced Research Projects Agency, encouraged them to try another approach: the common task. Researchers would agree on a common set of practices, whether they were trying to teach computers speech recognition, speaker identification, sentiment analysis of texts, grammatical breakdown, language identification, handwriting recognition or anything else. They would set out the metrics they were aiming to improve on, share the data sets used to train their software and allow their results to be tested by neutral outsiders. That made the process far more transparent. Funding started up again and language technologies began to improve, though very slowly. Many early approaches to language technologyand particularly translationgot stuck in a conceptual cul-de-sac: the rules-based approach. In translation, this meant trying to write rules to analyse the text of a sentence in the language of origin, breaking it down into a sort of abstract interlanguage and rebuilding it according to the rules of the target language. These approaches showed early promise. But language is riddled with ambiguities and exceptions, so such systems were hugely complicated and easily broke down when tested on sentences beyond the simple set they had been designed for. Nearly all language technologies began to get a lot better with the application of statistical methods, often called a brute force approach. This relies on software scouring vast amounts of data, looking for patterns and learning from precedent. For example, in parsing language (breaking it down into its grammatical components), the software learns from large bodies of text that have already been parsed by humans. It uses what it has learned to make its best guess about a previously unseen text. In machine translation, the software scans millions of words already translated by humans, again looking for patterns. In speech recognition, the software learns from a body of recordings and the transcriptions made by humans. Thanks to the growing power of processors, falling prices for data storage and, most crucially, the explosion in available data, this approach eventually bore fruit. Mathematical techniques that had been known for decades came into their own, and big companies with access to enormous amounts of data were poised to benefit. People who had been put off by the hilariously inappropriate translations offered by online tools like BabelFish began to have more faith in Google Translate. Apple persuaded millions of iPhone users to talk not only on their phones but to them. The final advance, which began only about five years ago, came with the advent of deep learning through digital neural networks (DNNs). These are often touted as having qualities similar to those of the human brain: neurons are connected in software, and connections can become stronger or weaker in the process of learning. But Nils Lenke, head of research for Nuance, a language-technology company, explains matter-of-factly that DNNs are just another kind of mathematical model, the basis of which had been well understood for decades. What changed was the hardware being used. Almost by chance, DNN researchers discovered that the graphical processing units (GPUs) used to render graphics fluidly in applications like video games were also brilliant at handling neural networks. In computer graphics, basic small shapes move according to fairly simple rules, but there are lots of shapes and many rules, requiring vast numbers of simple calculations. The same GPUs are used to fine-tune the weights assigned to neurons in DNNs as they scour data to learn. The technique has already produced big leaps in quality for all kinds of deep learning, including deciphering handwriting, recognising faces and classifying images. Now they are helping to improve all manner of language technologies, often bringing enhancements of up to 30%. That has shifted language technology from usable at a pinch to really rather good. But so far no one has quite worked out what will move it on from merely good to reliably great. WHEN a person speaks, air is forced out through the lungs, making the vocal chords vibrate, which sends out characteristic wave patterns through the air. The features of the sounds depend on the arrangement of the vocal organs, especially the tongue and the lips, and the characteristic nature of the sounds comes from peaks of energy in certain frequencies. The vowels have frequencies called formants, two of which are usually enough to differentiate one vowel from another. For example, the vowel in the English word fleece has its first two formants at around 300Hz and 3,000Hz. Consonants have their own characteristic features. In principle, it should be easy to turn this stream of sound into transcribed speech. As in other language technologies, machines that recognise speech are trained on data gathered earlier. In this instance, the training data are sound recordings transcribed to text by humans, so that the software has both a sound and a text input. All it has to do is match the two. It gets better and better at working out how to transcribe a given chunk of sound in the same way as humans did in the training data. The traditional matching approach was a statistical technique called a hidden Markov model (HMM), making guesses based on what was done before. More recently speech recognition has also gained from deep learning. English has about 44 phonemes, the units that make up the sound system of a language. P and b are different phonemes, because they distinguish words like pat and bat. But in English p with a puff of air, as in party, and p without a puff of air, as in spin, are not different phonemes, though they are in other languages. If a computer hears the phonemes s, p, i and n back to back, it should be able to recognise the word spin. But the nature of live speech makes this difficult for machines. Sounds are not pronounced individually, one phoneme after the other; they mostly come in a constant stream, and finding the boundaries is not easy. Phonemes also differ according to the context. (Compare the l sound at the beginning of light with that at the end of full.) Speakers differ in timbre and pitch of voice, and in accent. Conversation is far less clear than careful dictation. People stop and restart much more often than they realise. All the same, technology has gradually mitigated many of these problems, so error rates in speech-recognition software have fallen steadily over the yearsand then sharply with the introduction of deep learning. Microphones have got better and cheaper. With ubiquitous wireless internet, speech recordings can easily be beamed to computers in the cloud for analysis, and even smartphones now often have computers powerful enough to carry out this task. Bear arms or bare arms Perhaps the most important feature of a speech-recognition system is its set of expectations about what someone is likely to say, or its language model. Like other training data, the language models are based on large amounts of real human speech, transcribed into text. When a speech-recognition system hears a stream of sound, it makes a number of guesses about what has been said, then calculates the odds that it has found the right one, based on the kinds of words, phrases and clauses it has seen earlier in the training text. At the level of phonemes, each language has strings that are permitted (in English, a word may begin with str-, for example) or banned (an English word cannot start with tsr-). The same goes for words. Some strings of words are more common than others. For example, the is far more likely to be followed by a noun or an adjective than by a verb or an adverb. In making guesses about homophones, the computer will have remembered that in its training data the phrase the right to bear arms came up much more often than the right to bare arms, and will thus have made the right guess. Training on a specific speaker greatly cuts down on the softwares guesswork. Just a few minutes of reading training text into software like Dragon Dictate, made by Nuance, produces a big jump in accuracy. For those willing to train the software for longer, the improvement continues to something close to 99% accuracy (meaning that of each hundred words of text, not more than one is wrongly added, omitted or changed). A good microphone and a quiet room help. Advance knowledge of what kinds of things the speaker might be talking about also increases accuracy. Words like phlebitis and gastrointestinal are not common in general discourse, and uncommon words are ranked lower in the probability tables the software uses to guess what it has heard. But these words are common in medicine, so creating software trained to look out for such words considerably improves the result. This can be done by feeding the system a large number of documents written by the speaker whose voice is to be recognised; common words and phrases can be extracted to improve the systems guesses. As with all other areas of language technology, deep learning has sharply brought down error rates. In October Microsoft announced that its latest speech-recognition system had achieved parity with human transcribers in recognising the speech in the Switchboard Corpus, a collection of thousands of recorded conversations in which participants are talking with a stranger about a randomly chosen subject. Error rates on the Switchboard Corpus are a widely used benchmark, so claims of quality improvements can be easily compared. Fifteen years ago quality had stalled, with word-error rates of 20-30%. Microsofts latest system, which has six neural networks running in parallel, has reached 5.9% (see chart), the same as a human transcribers. Xuedong Huang, Microsofts chief speech scientist, says that he expected it to take two or three years to reach parity with humans. It got there in less than one. The improvements in the lab are now being applied to products in the real world. More and more cars are being fitted with voice-activated controls of various kinds; the vocabulary involved is limited (there are only so many things you might want to say to your car), which ensures high accuracy. Microphonesor often arrays of microphones with narrow fields of pick-upare getting better at identifying the relevant speaker among a group. Some problems remain. Children and elderly speakers, as well as people moving around in a room, are harder to understand. Background noise remains a big concern; if it is different from that in the training data, the software finds it harder to generalise from what it has learned. So Microsoft, for example, offers businesses a product called CRIS that lets users customise speech-recognition systems for the background noise, special vocabulary and other idiosyncrasies they will encounter in that particular environment. That could be useful anywhere from a noisy factory floor to a care home for the elderly. But for a computer to know what a human has said is only a beginning. Proper interaction between the two, of the kind that comes up in almost every science-fiction story, calls for machines that can speak back. ILL be back. Hasta la vista, baby. Arnold Schwarzeneggers Teutonic drone in the Terminator films is world-famous. But in this instance film-makers looking into the future were overly pessimistic. Some applications do still feature a monotonous robot voice, but that is changing fast. Examples of speech synthesis from OSX synthesiser: Example from Amazon's ""Polly"" synthesiser: Creating speech is roughly the inverse of understanding it. Again, it requires a basic model of the structure of speech. What are the sounds in a language, and how do they combine What words does it have, and how do they combine in sentences These are well-understood questions, and most systems can now generate sound waves that are a fair approximation of human speech, at least in short bursts. Heteronyms require special care. How should a computer pronounce a word like lead, which can be a present-tense verb or a noun for a heavy metal, pronounced quite differently Once again a language model can make accurate guesses: Lead us not into temptation can be parsed for its syntax, and once the software has worked out that the first word is almost certainly a verb, it can cause it to be pronounced to rhyme with reed, not red. Traditionally, text-to-speech models have been concatenative, consisting of very short segments recorded by a human and then strung together as in the acoustic model described above. More recently, parametric models have been generating raw audio without the need to record a human voice, which makes these systems more flexible but less natural-sounding. DeepMind, an artificial-intelligence company bought by Google in 2014, has announced a new way of synthesising speech, again using deep neural networks. The network is trained on recordings of people talking, and on the texts that match what they say. Given a text to reproduce as speech, it churns out a far more fluent and natural-sounding voice than the best concatenative and parametric approaches. The last step in generating speech is giving it prosodygenerally, the modulation of speed, pitch and volume to convey an extra (and critical) channel of meaning. In English, a German teacher, with the stress on teacher, can teach anything but must be German. But a German teacher with the emphasis on German is usually a teacher of German (and need not be German). Words like prepositions and conjunctions are not usually stressed. Getting machines to put the stresses in the correct places is about 50% solved, says Mark Liberman of the University of Pennsylvania. Many applications do not require perfect prosody. A satellite-navigation system giving instructions on where to turn uses just a small number of sentence patterns, and prosody is not important. The same goes for most single-sentence responses given by a virtual assistant on a smartphone. But prosody matters when someone is telling a story. Pitch, speed and volume can be used to pass quickly over things that are already known, or to build interest and tension for new information. Myriad tiny clues communicate the speakers attitude to his subject. The phrase a German teacher, with stress on the word German, may, in the context of a story, not be a teacher of German, but a teacher being explicitly contrasted with a teacher who happens to be French or British. Text-to-speech engines are not much good at using context to provide such accentuation, and where they do, it rarely extends beyond a single sentence. When Alexa, the assistant in Amazons Echo device, reads a news story, her prosody is jarringly un-humanlike. Talking computers have yet to learn how to make humans want to listen. Computer translations have got strikingly better, but still need human input IN STAR TREK it was a hand-held Universal Translator; in The Hitchhikers Guide to the Galaxy it was the Babel Fish popped conveniently into the ear. In science fiction, the meeting of distant civilisations generally requires some kind of device to allow them to talk. High-quality automated translation seems even more magical than other kinds of language technology because many humans struggle to speak more than one language, let alone translate from one to another. The idea has been around since the 1950s, and computerised translation is still known by the quaint moniker machine translation (MT). It goes back to the early days of the cold war, when American scientists were trying to get computers to translate from Russian. They were inspired by the code-breaking successes of the second world war, which had led to the development of computers in the first place. To them, a scramble of Cyrillic letters on a page of Russian text was just a coded version of English, and turning it into English was just a question of breaking the code. Scientists at IBM and Georgetown University were among those who thought that the problem would be cracked quickly. Having programmed just six rules and a vocabulary of 250 words into a computer, they gave a demonstration in New York on January 7th 1954 and proudly produced 60 automated translations, including that of Mi pyeryedayem mislyi posryedstvom ryechyi, which came out correctly as We transmit thoughts by means of speech. Leon Dostert of Georgetown, the lead scientist, breezily predicted that fully realised MT would be an accomplished fact in three to five years. Instead, after more than a decade of work, the report in 1966 by a committee chaired by John Pierce, mentioned in the introduction to this report, recorded bitter disappointment with the results and urged researchers to focus on narrow, achievable goals such as automated dictionaries. Government-sponsored work on MT went into near-hibernation for two decades. What little was done was carried out by private companies. The most notable of them was Systran, which provided rough translations, mostly to Americas armed forces. The scientists got bogged down by their rules-based approach. Having done relatively well with their six-rule system, they came to believe that if they programmed in more rules, the system would become more sophisticated and subtle. Instead, it became more likely to produce nonsense. Adding extra rules, in the modern parlance of software developers, did not scale. Besides the difficulty of programming grammars many rules and exceptions, some early observers noted a conceptual problem. The meaning of a word often depends not just on its dictionary definition and the grammatical context but the meaning of the rest of the sentence. Yehoshua Bar-Hillel, an Israeli MT pioneer, realised that the pen is in the box and the box is in the pen would require different translations for pen: any pen big enough to hold a box would have to be an animal enclosure, not a writing instrument. How could machines be taught enough rules to make this kind of distinction They would have to be provided with some knowledge of the real world, a task far beyond the machines or their programmers at the time. Two decades later, IBM stumbled on an approach that would revive optimism about MT. Its Candide system was the first serious attempt to use statistical probabilities rather than rules devised by humans for translation. Statistical, phrase-based machine translation, like speech recognition, needed training data to learn from. Candide used Canadas Hansard, which publishes that countrys parliamentary debates in French and English, providing a huge amount of data for that time. The phrase-based approach would ensure that the translation of a word would take the surrounding words properly into account. But quality did not take a leap until Google, which had set itself the goal of indexing the entire internet, decided to use those data to train its translation engines; in 2007 it switched from a rules-based engine (provided by Systran) to its own statistics-based system. To build it, Google trawled about a trillion web pages, looking for any text that seemed to be a translation of anotherfor example, pages designed identically but with different words, and perhaps a hint such as the address of one page ending in /en and the other ending in /fr. According to Macduff Hughes, chief engineer on Google Translate, a simple approach using vast amounts of data seemed more promising than a clever one with fewer data. Training on parallel texts (which linguists call corpora, the plural of corpus) creates a translation model that generates not one but a series of possible translations in the target language. The next step is running these possibilities through a monolingual language model in the target language. This is, in effect, a set of expectations about what a well-formed and typical sentence in the target language is likely to be. Single-language models are not too hard to build. (Parallel human-translated corpora are hard to come by; large amounts of monolingual training data are not.) As with the translation model, the language model uses a brute-force statistical approach to learn from the training data, then ranks the outputs from the translation model in order of plausibility. Statistical machine translation rekindled optimism in the field. Internet users quickly discovered that Google Translate was far better than the rules-based online engines they had used before, such as BabelFish. Such systems still make mistakessometimes minor, sometimes hilarious, sometimes so serious or so many as to make nonsense of the result. And language pairs like Chinese-English, which are unrelated and structurally quite different, make accurate translation harder than pairs of related languages like English and German. But more often than not, Google Translate and its free online competitors, such as Microsofts Bing Translator, offer a usable approximation. Such systems are set to get better, again with the help of deep learning from digital neural networks. The Association for Computational Linguistics has been holding workshops on MT every summer since 2006. One of the events is a competition between MT engines turned loose on a collection of news text. In August 2016, in Berlin, neural-net-based MT systems were the top performers (out of 102), a first. Now Google has released its own neural-net-based engine for eight language pairs, closing much of the quality gap between its old system and a human translator. This is especially true for closely related languages (like the big European ones) with lots of available training data. The results are still distinctly imperfect, but far smoother and more accurate than before. Translations between English and (say) Chinese and Korean are not as good yet, but the neural system has brought a clear improvement here too. Neural-network-based translation actually uses two networks. One is an encoder. Each word of an input sentence is converted into a multidimensional vector (a series of numerical values), and the encoding of each new word takes into account what has happened earlier in the sentence. Marcello Federico of Italys Fondazione Bruno Kessler, a private research organisation, uses an intriguing analogy to compare neural-net translation with the phrase-based kind. The latter, he says, is like describing Coca-Cola in terms of sugar, water, caffeine and other ingredients. By contrast, the former encodes features such as liquidness, darkness, sweetness and fizziness. Once the source sentence is encoded, a decoder network generates a word-for-word translation, once again taking account of the immediately preceding word. This can cause problems when the meaning of words such as pronouns depends on words mentioned much earlier in a long sentence. This problem is mitigated by an attention model, which helps maintain focus on other words in the sentence outside the immediate context. Neural-network translation requires heavy-duty computing power, both for the original training of the system and in use. The heart of such a system can be the GPUs that made the deep-learning revolution possible, or specialised hardware like Googles Tensor Processing Units (TPUs). Smaller translation companies and researchers usually rent this kind of processing power in the cloud. But the data sets used in neural-network training do not need to be as extensive as those for phrase-based systems, which should give smaller outfits a chance to compete with giants like Google. Fully automated, high-quality machine translation is still a long way off. For now, several problems remain. All current machine translations proceed sentence by sentence. If the translation of such a sentence depends on the meaning of earlier ones, automated systems will make mistakes. Long sentences, despite tricks like the attention model, can be hard to translate. And neural-net-based systems in particular struggle with rare words. Training data, too, are scarce for many language pairs. They are plentiful between European languages, since the European Unions institutions churn out vast amounts of material translated by humans between the EUs 24 official languages. But for smaller languages such resources are thin on the ground. For example, there are few Greek-Urdu parallel texts available on which to train a translation engine. So a system that claims to offer such translation is in fact usually running it through a bridging language, nearly always English. That involves two translations rather than one, multiplying the chance of errors. Even if machine translation is not yet perfect, technology can already help humans translate much more quickly and accurately. Translation memories, software that stores already translated words and segments, first came into use as early as the 1980s. For someone who frequently translates the same kind of material (such as instruction manuals), they serve up the bits that have already been translated, saving lots of duplication and time. A similar trick is to train MT engines on text dealing with a narrow real-world domain, such as medicine or the law. As software techniques are refined and computers get faster, training becomes easier and quicker. Free software such as Moses, developed with the support of the EU and used by some of its in-house translators, can be trained by anyone with parallel corpora to hand. A specialist in medical translation, for instance, can train the system on medical translations only, which makes them far more accurate. At the other end of linguistic sophistication, an MT engine can be optimised for the shorter and simpler language people use in speech to spew out rough but near-instantaneous speech-to-speech translations. This is what Microsofts Skype Translator does. Its quality is improved by being trained on speech (things like film subtitles and common spoken phrases) rather than the kind of parallel text produced by the European Parliament. Translation management has also benefited from innovation, with clever software allowing companies quickly to combine the best of MT, translation memory, customisation by the individual translator and so on. Translation-management software aims to cut out the agencies that have been acting as middlemen between clients and an army of freelance translators. Jack Welde, the founder of Smartling, an industry favourite, says that in future translation customers will choose how much human intervention is needed for a translation. A quick automated one will do for low-stakes content with a short life, but the most important content will still require a fully hand-crafted and edited version. Noting that MT has both determined boosters and committed detractors, Mr Welde says he is neither: If you take a dogmatic stance, youre not optimised for the needs of the customer. Translation software will go on getting better. Not only will engineers keep tweaking their statistical models and neural networks, but users themselves will make improvements to their own systems. For example, a small but much-admired startup, Lilt, uses phrase-based MT as the basis for a translation, but an easy-to-use interface allows the translator to correct and improve the MT systems output. Every time this is done, the corrections are fed back into the translation engine, which learns and improves in real time. Users can build several different memoriesa medical one, a financial one and so onwhich will help with future translations in that specialist field. TAUS, an industry group, recently issued a report on the state of the translation industry saying that in the past few years the translation industry has burst with new tools, platforms and solutions. Last year Jaap van der Meer, TAUSs founder and director, wrote a provocative blogpost entitled The Future Does Not Need Translators, arguing that the quality of MT will keep improving, and that for many applications less-than-perfect translation will be good enough. The translator of the future is likely to be more like a quality-control expert, deciding which texts need the most attention to detail and editing the output of MT software. That may be necessary because computers, no matter how sophisticated they have become, cannot yet truly grasp what a text means. Meaning and machine intelligence: What are you talking about IN BLACK MIRROR, a British science-fiction satire series set in a dystopian near future, a young woman loses her boyfriend in a car accident. A friend offers to help her deal with her grief. The dead man was a keen social-media user, and his archived accounts can be used to recreate his personality. Before long she is messaging with a facsimile, then speaking to one. As the system learns to mimic him ever better, he becomes increasingly real. This is not quite as bizarre as it sounds. Computers today can already produce an eerie echo of human language if fed with the appropriate material. What they cannot yet do is have true conversations. Truly robust interaction between man and machine would require a broad understanding of the world. In the absence of that, computers are not able to talk about a wide range of topics, follow long conversations or handle surprises. Machines trained to do a narrow range of tasks, though, can perform surprisingly well. The most obvious examples are the digital assistants created by the technology giants. Users can ask them questions in a variety of natural ways: Whats the temperature in London Hows the weather outside Is it going to be cold today The assistants know a few things about users, such as where they live and who their family are, so they can be personal, too: Hows my commute looking Text my wife Ill be home in 15 minutes. And they get better with time. Apples Siri receives 2bn requests per week, which (after being anonymised) are used for further teaching. For example, Apple says Siri knows every possible way that users ask about a sports score. She also has a delightful answer for children who ask about Father Christmas. Microsoft learned from some of its previous natural-language platforms that about 10% of human interactions were chitchat, from tell me a joke to whos your daddy, and used such chat to teach its digital assistant, Cortana. The writing team for Cortana includes two playwrights, a poet, a screenwriter and a novelist. Google hired writers from Pixar, an animated-film studio, and The Onion, a satirical newspaper, to make its new Google Assistant funnier. No wonder people often thank their digital helpers for a job well done. The assistants replies range from My pleasure, as always to You dont need to thank me. How do natural-language platforms know what people want They not only recognise the words a person uses, but break down speech for both grammar and meaning. Grammar parsing is relatively advanced; it is the domain of the well-established field of natural-language processing. But meaning comes under the heading of natural-language understanding, which is far harder. First, parsing. Most people are not very good at analysing the syntax of sentences, but computers have become quite adept at it, even though most sentences are ambiguous in ways humans are rarely aware of. Take a sign on a public fountain that says, This is not drinking water. Humans understand it to mean that the water (this) is not a certain kind of water (drinking water). But a computer might just as easily parse it to say that this (the fountain) is not at present doing something (drinking water). As sentences get longer, the number of grammatically possible but nonsensical options multiplies exponentially. How can a machine parser know which is the right one It helps for it to know that some combinations of words are more common than others: the phrase drinking water is widely used, so parsers trained on large volumes of English will rate those two words as likely to be joined in a noun phrase. And some structures are more common than others: noun verb noun noun may be much more common than noun noun verb noun. A machine parser can compute the overall probability of all combinations and pick the likeliest. A lexicalised parser might do even better. Take the Groucho Marx joke, One morning I shot an elephant in my pyjamas. How he got in my pyjamas, Ill never know. The first sentence is ambiguous (which makes the joke)grammatically both I and an elephant can attach to the prepositional phrase in my pyjamas. But a lexicalised parser would recognise that I [verb phrase] in my pyjamas is far more common than elephant in my pyjamas, and so assign that parse a higher probability. But meaning is harder to pin down than syntax. The boy kicked the ball and The ball was kicked by the boy have the same meaning but a different structure. Time flies like an arrow can mean either that time flies in the way that an arrow flies, or that insects called time flies are fond of an arrow. Who plays Thor in Thor Your correspondent could not remember the beefy Australian who played the eponymous Norse god in the Marvel superhero film. But when he asked his iPhone, Siri came up with an unexpected reply: I dont see any movies matching Thor playing in Thor, IA, US, today. Thor, Iowa, with a population of 184, was thousands of miles away, and Thor, the film, has been out of cinemas for years. Siri parsed the question perfectly properly, but the reply was absurd, violating the rules of what linguists call pragmatics: the shared knowledge and understanding that people use to make sense of the often messy human language they hear. Can you reach the salt is not a request for information but for salt. Natural-language systems have to be manually programmed to handle such requests as humans expect them, and not literally. Shared information is also built up over the course of a conversation, which is why digital assistants can struggle with twists and turns in conversations. Tell an assistant, Id like to go to an Italian restaurant with my wife, and it might suggest a restaurant. But then ask, is it close to her office, and the assistant must grasp the meanings of it (the restaurant) and her (the wife), which it will find surprisingly tricky. Nuance, the language-technology firm, which provides natural-language platforms to many other companies, is working on a concierge that can handle this type of challenge, but it is still a prototype. Such a concierge must also offer only restaurants that are open. Linking requests to common sense (knowing that no one wants to be sent to a closed restaurant), as well as a knowledge of the real world (knowing which restaurants are closed), is one of the most difficult challenges for language technologies. Common sense, an old observation goes, is uncommon enough in humans. Programming it into computers is harder still. Fernando Pereira of Google points out why. Automated speech recognition and machine translation have something in common: there are huge stores of data (recordings and transcripts for speech recognition, parallel corpora for translation) that can be used to train machines. But there are no training data for common sense. Knowledge of the real world is another matter. AI has helped data-rich companies such as Americas West-Coast tech giants organise much of the worlds information into interactive databases such as Googles Knowledge Graph. Some of the content of that appears in a box to the right of a Google page of search results for a famous figure or thing. It knows that Jacob Bernoulli studied at the University of Basel (as did other people, linked to Bernoulli through this node in the Graph) and wrote On the Law of Large Numbers (which it knows is a book). Organising information this way is not difficult for a company with lots of data and good AI capabilities, but linking information to language is hard. Google touts its assistants ability to answer questions like Who was president when the Rangers won the World Series But Mr Pereira concedes that this was the result of explicit training. Another such complex queryWhat was the population of London when Samuel Johnson wrote his dictionarywould flummox the assistant, even though the Graph knows about things like the historical population of London and the date of Johnsons dictionary. IBMs Watson system, which in 2011 beat two human champions at the quiz show Jeopardy!, succeeded mainly by calculating huge numbers of potential answers based on key words by probability, not by a human-like understanding of the question. Making real-world information computable is challenging, but it has inspired some creative approaches. Cortical.io, a Vienna-based startup, took hundreds of Wikipedia articles, cut them into thousands of small snippets of information and ran an unsupervised machine-learning algorithm over it that required the computer not to look for anything in particular but to find patterns. These patterns were then represented as a visual semantic fingerprint on a grid of 128x128 pixels. Clumps of pixels in similar places represented semantic similarity. This method can be used to disambiguate words with multiple meanings: the fingerprint of organ shares features with both liver and piano (because the word occurs with both in different parts of the training data). This might allow a natural-language system to distinguish between pianos and church organs on one hand, and livers and other internal organs on the other. Proper conversation between humans and machines can be seen as a series of linked challenges: speech recognition, speech synthesis, syntactic analysis, semantic analysis, pragmatic understanding, dialogue, common sense and real-world knowledge. Because all the technologies have to work together, the chain as a whole is only as strong as its weakest link, and the first few of these are far better developed than the last few. The hardest part is linking them together. Scientists do not know how the human brain draws on so many different kinds of knowledge at the same time. Programming a machine to replicate that feat is very much a work in progress. IN WALL-E, an animated childrens film set in the future, all humankind lives on a spaceship after the Earths environment has been trashed. The humans are whisked around in intelligent hovering chairs; machines take care of their every need, so they are all morbidly obese. Even the ships captain is not really in charge; the actual pilot is an intelligent and malevolent talking robot, Auto, and like so many talking machines in science fiction, he eventually makes a grab for power. Speech is quintessentially human, so it is hard to imagine machines that can truly speak conversationally as humans do without also imagining them to be superintelligent. And if they are super intelligent, with none of humans flaws, it is hard to imagine them not wanting to take over, not only for their good but for that of humanity. Even in a fairly benevolent future like WALL-Es, where the machines are doing all the work, it is easy to see that the lack of anything challenging to do would be harmful to people. Fortunately, the tasks that talking machines can take off humans to-do lists are the sort that many would happily give up. Machines are increasingly able to handle difficult but well-defined jobs. Soon all that their users will have to do is pipe up and ask them, using a naturally phrased voice command. Once upon a time, just one tinkerer in a given family knew how to work the computer or the video recorder. Then graphical interfaces (icons and a mouse) and touchscreens made such technology accessible to everyone. Frank Chen of Andreessen Horowitz, a venture-capital firm, sees natural-language interfaces between humans and machines as just another step in making information and services available to all. Silicon Valley, he says, is enjoying a golden age of AI technologies. Just as in the early 1990s companies were piling online and building websites without quite knowing why, now everyone is going for natural language. Yet, he adds, were in 1994 for voice. 1995 will soon come. This does not mean that people will communicate with their computers exclusively by talking to them. Websites did not make the telephone obsolete, and mobile devices did not make desktop computers obsolete. In the same way, people will continue to have a choice between voice and text when interacting with their machines. Not all will choose voice. For example, in Japan yammering into a phone is not done in public, whether the interlocutor is a human or a digital assistant, so usage of Siri is low during business hours but high in the evening and at the weekend. For others, voice-enabled technology is an obvious boon. It allows dyslexic people to write without typing, and the very elderly may find it easier to talk than to type on a tiny keyboard. The very young, some of whom today learn to type before they can write, may soon learn to talk to machines before they can type. Those with injuries or disabilities that make it hard for them to write will also benefit. Microsoft is justifiably proud of a new device that will allow people with amyotrophic lateral sclerosis (ALS), which immobilises nearly all of the body but leaves the mind working, to speak by using their eyes to pick letters on a screen. The critical part is predictive text, which improves as it gets used to a particular individual. An experienced user will be able to speak at around 15 words per minute. People may even turn to machines for company. Microsofts Xiaoice, a chatbot launched in China, learns to come up with the responses that will keep a conversation going longest. Nobody would think it was human, but it does make users open up in surprising ways. Jibo, a new social robot, is intended to tell children stories, help far-flung relatives stay in touch and the like. Another group that may benefit from technology is smaller language communities. Networked computers can encourage a winner-take-all effect: if there is a lot of good software and content in English and Chinese, smaller languages become less valuable online. If they are really tiny, their very survival may be at stake. But Ross Perlin of the Endangered Languages Alliance notes that new software allows researchers to document small languages more quickly than ever. With enough data comes the possibility of developing resourcesfrom speech recognition to interfaces with softwarefor smaller and smaller languages. The Silicon Valley giants already localise their services in dozens of languages; neural networks and other software allow new versions to be generated faster and more efficiently than ever. There are two big downsides to the rise in natural-language technologies: the implications for privacy, and the disruption it will bring to many jobs. Increasingly, devices are always listening. Digital assistants like Alexa, Cortana, Siri and Google Assistant are programmed to wait for a prompt, such as Hey, Siri or OK, Google, to activate them. But allowing always-on microphones into peoples pockets and homes amounts to a further erosion of traditional expectations of privacy. The same might be said for all the ways in which language software improves by training on a single users voice, vocabulary, written documents and habits. All the big companies location-based serviceseven the accelerometers in phones that detect small movementsare making ever-improving guesses about users wants and needs. The moment when a digital assistant surprises a user with The chemist is nearbydo you want to buy more haemorrhoid cream, Steve could be when many may choose to reassess the trade-off between amazing new services and old-fashioned privacy. The tech companies can help by giving users more choice; the latest iPhone will not be activated when it is laid face down on a table. But hackers will inevitably find ways to get at some of these data. The other big concern is for jobs. To the extent that they are routine, they face being automated away. A good example is customer support. When people contact a company for help, the initial encounter is usually highly scripted. A company employee will verify a customers identity and follow a decision-tree. Language technology is now mature enough to take on many of these tasks. For a long transition period humans will still be needed, but the work they do will become less routine. Nuance, which sells lots of automated online and phone-based help systems, is bullish on voice biometrics (customers identifying themselves by saying my voice is my password). Using around 200 parameters for identifying a speaker, it is probably more secure than a fingerprint, says Brett Beranek, a senior manager at the company. It will also eliminate the tedium, for both customers and support workers, of going through multi-step identification procedures with PINs, passwords and security questions. When Barclays, a British bank, offered it to frequent users of customer-support services, 84% signed up within five months. Digital assistants on personal smartphones can get away with mistakes, but for some business applications the tolerance for error is close to zero, notes Nikita Ivanov. His company, Datalingvo, a Silicon Valley startup, answers questions phrased in natural language about a companys business data. If a user wants to know which online ads resulted in the most sales in California last month, the software automatically translates his typed question into a database query. But behind the scenes a human working for Datalingvo vets the query to make sure it is correct. This is because the stakes are high: the technology is bound to make mistakes in its early days, and users could make decisions based on bad data. This process can work the other way round, too: rather than natural-language input producing data, data can produce language. Arria, a company based in London, makes software into which a spreadsheet full of data can be dragged and dropped, to be turned automatically into a written description of the contents, complete with trends. Matt Gould, the companys chief strategy officer, likes to think that this will free chief financial officers from having to write up the same old routine analyses for the board, giving them time to develop more creative approaches. Carl Benedikt Frey, an economist at Oxford University, has researched the likely effect of artificial intelligence on the labour market and concluded that the jobs most likely to remain immune include those requiring creativity and skill at complex social interactions. But not every human has those traits. Call centres may need fewer people as more routine work is handled by automated systems, but the trickier inquiries will still go to humans. Much of this seems familiar. When Google search first became available, it turned up documents in seconds that would have taken a human operator hours, days or years to find. This removed much of the drudgery from being a researcher, librarian or journalist. More recently, young lawyers and paralegals have taken to using e-discovery. These innovations have not destroyed the professions concerned but merely reshaped them. Machines that relieve drudgery and allow people to do more interesting jobs are a fine thing. In net terms they may even create extra jobs. But any big adjustment is most painful for those least able to adapt. Upheavals brought about by social changeslike the emancipation of women or the globalisation of labour marketsare already hard for some people to bear. When those changes are wrought by machines, they become even harder, and all the more so when those machines seem to behave more and more like humans. People already treat inanimate objects as if they were alive: who has never shouted at a computer in frustration The more that machines talk, and the more that they seem to understand people, the more their users will be tempted to attribute human traits to them. That raises questions about what it means to be human. Language is widely seen as humankinds most distinguishing trait. AI researchers insist that their machines do not think like people, but if they can listen and talk like humans, what does that make them As humans teach ever more capable machines to use language, the once-obvious line between them will blur. ",http://www.economist.com/technology-quarterly/2017-05-01/language,"Computers have got much better at translation, voice recognition and speech synthesis, says Lane Greene. But they still don't understand the meaning of ...",Finding a voice,Lane Greene IM SORRY Dave Im HAL Space Odyssey Dave Bowman HALs Jupiter Hal Im Dave Hal Dave Creative AI HAL Terminator Rosie Jetsons Machine Apples Siri Amazons Alexa Google Microsofts Cortana Alexa Apple Siri Creative Strategies John Pierce Bell Labs Americas National Academy Sciences Pierce Mark Liberman University Pennsylvania Linguistic Data Consortium Charles Wayne Americas Defence Advanced Research Projects Agency Researchers Mathematical BabelFish Google Translate Apple DNNs Nils Lenke Nuance DNNs DNN GPUs GPUs DNNs English Markov HMM English P English Compare Speakers Bear English English Dragon Dictate Nuance Advance October Microsoft Switchboard Corpus Switchboard Corpus Microsofts Xuedong Huang Microsofts Microphonesor Children Background Microsoft CRIS Hasta Arnold Schwarzeneggers Teutonic Terminator OSX Amazon Polly Creating Again Heteronyms DeepMind Google English German Mark Liberman University Pennsylvania Myriad German Alexa Amazons Echo Computer IN STAR TREK Universal Translator Hitchhikers Guide Galaxy Babel Fish High-quality MT Cyrillic English English IBM Georgetown University New York January Mi Leon Dostert Georgetown MT John Pierce MT Systran Americas Yehoshua Bar-Hillel MT IBM MT Candide Candide Canadas Hansard French English Google Systran Google /en Macduff Hughes Google Translate Internet Google Translate BabelFish Chinese-English English German Google Translate Microsofts Bing Translator Association Computational Linguistics MT MT August Berlin MT Google English Chinese Korean Marcello Federico Italys Fondazione Bruno Kessler Coca-Cola GPUs Googles Tensor Processing Units TPUs Smaller Google Training European Unions EUs Greek-Urdu English MT EU MT Microsofts Skype Translator European Parliament Translation MT Jack Welde Smartling MT Mr Welde Translation Lilt MT MT TAUS Jaap Meer TAUSs MT MT IN BLACK MIRROR London Hows Hows Text Ill Siri Apple Siri Father Christmas Microsoft Cortana Cortana Google Pixar Onion Google Assistant My Grammar First Humans English Groucho Marx Ill Time Thor Thor Your Australian Norse Marvel Siri Thor Thor IA US Thor Iowa Thor Siri Tell Id Common Fernando Pereira Google Knowledge AI Americas West-Coast Googles Knowledge Graph Google Jacob Bernoulli University Basel Bernoulli Graph Law Large Numbers AI Google Who Rangers World Series Mr Pereira London Samuel Johnson Graph London Johnsons IBMs Watson Jeopardy Cortical.io Wikipedia Clumps IN WALL-E Earths Auto Speech WALL-Es Soon Frank Chen Andreessen Horowitz Silicon Valley AI Japan Siri Microsoft ALS Microsofts Xiaoice China Jibo English Chinese Ross Perlin Endangered Languages Alliance Silicon Valley Digital Alexa Cortana Siri Google Assistant Hey Siri OK Google Steve Language Brett Beranek PINs Barclays Digital Nikita Ivanov Datalingvo Silicon Valley California Datalingvo Arria London Matt Gould Carl Benedikt Frey Oxford University Google AI,4
132,"15 Sep 2016 at 16:59, Katyanna Quach Microsoft researchers working on AI computer speech recognition have reached a word error rate of 6.3 per cent, claiming to be the lowest in the industry. Hot on the heels of Google DeepMind announcing a breakthrough in AI speech recognition, Microsoft was quick to respond by saying it, too, has reached a milestone while using neural networks. A paper released on arXiv shows the researchers have combined neural-network-based acoustic and language modelling on the US National Institute of Standards and Technology (NIST) 2000 Switchboard task - a conversational telephone speech recognition test used as an industry standard. Artificial neural networks which are modelled loosely on how scientists believe the brain performs calculations are often used in speech processing. Microsoft has used a mixture of convolutional neural networks that dont form a closed cycle of nodes and a recurrent neural network that does. Both are useful for analysing the large data sets used to train the networks to process language, which requires a lot of computing power. Human language is difficult for computers to understand. To make it easier, models are used to predict the probability distribution over sequences of words using audio signals from speech. Microsoft uses a 30k-word vocabulary database derived from the most common words in the Switchboard and Fisher corpora to recognise words from speech. Computer speech recognition has come a long way. Twenty years ago the error rate of the best published research system had a word error rate that was greater than 43 percent, Microsoft said . Speech recognition is a hot trend in AI as companies race to build the best AI personal assistant. Microsoft has Cortana, Apple uses Siri, Google has its WaveNet system and Amazon has just announced a UK release for its Echo device.  ",https://www.theregister.co.uk/2016/09/15/microsoft_lowest_error_rate_ai_speech_recognition/,"Microsoft researchers working on AI computer speech recognition have reached a word error rate of 6.3 per cent, claiming to be the lowest in ...",Microsoft: Our AI speech recognition mangles your words the least,Katyanna Quach Microsoft AI Hot Google DeepMind AI Microsoft US National Institute Standards Technology NIST Switchboard Microsoft Microsoft Switchboard Fisher Computer Microsoft AI AI Microsoft Cortana Apple Siri Google WaveNet Amazon UK Echo,8
133,"Computers have got much better at translation, voice recognition and speech synthesis, says Lane Greene. But they still dont understand the meaning of language IM SORRY, Dave. Im afraid I cant do that. With chilling calm, HAL 9000, the on-board computer in 2001: A Space Odyssey, refuses to open the doors to Dave Bowman, an astronaut who had ventured outside the ship. HALs decision to turn on his human companion reflected a wave of fear about intelligent computers. When the film came out in 1968, computers that could have proper conversations with humans seemed nearly as far away as manned flight to Jupiter. Since then, humankind has progressed quite a lot farther with building machines that it can talk to, and that can respond with something resembling natural speech. Even so, communication remains difficult. If 2001 had been made to reflect the state of todays language technology, the conversation might have gone something like this: Open the pod bay doors, Hal. Im sorry, Dave. I didnt understand the question. Open the pod bay doors, Hal. I have a list of eBay results about pod doors, Dave. Creative and truly conversational computers able to handle the unexpected are still far off. Artificial-intelligence (AI) researchers can only laugh when asked about the prospect of an intelligent HAL, Terminator or Rosie (the sassy robot housekeeper in The Jetsons). Yet although language technologies are nowhere near ready to replace human beings, except in a few highly routine tasks, they are at last about to become good enough to be taken seriously. They can help people spend more time doing interesting things that only humans can do. After six decades of work, much of it with disappointing outcomes, the past few years have produced results much closer to what early pioneers had hoped for. Speech recognition has made remarkable advances. Machine translation, too, has gone from terrible to usable for getting the gist of a text, and may soon be good enough to require only modest editing by humans. Computerised personal assistants, such as Apples Siri, Amazons Alexa, Google Now and Microsofts Cortana, can now take a wide variety of questions, structured in many different ways, and return accurate and useful answers in a natural-sounding voice. Alexa can even respond to a request to tell me a joke, but only by calling upon a database of corny quips. Computers lack a sense of humour. When Apple introduced Siri in 2011 it was frustrating to use, so many people gave up. Only around a third of smartphone owners use their personal assistants regularly, even though 95% have tried them at some point, according to Creative Strategies, a consultancy. Many of those discouraged users may not realise how much they have improved. In 1966 John Pierce was working at Bell Labs, the research arm of Americas telephone monopoly. Having overseen the team that had built the first transistor and the first communications satellite, he enjoyed a sterling reputation, so he was asked to take charge of a report on the state of automatic language processing for the National Academy of Sciences. In the period leading up to this, scholars had been promising automatic translation between languages within a few years. But the report was scathing. Reviewing almost a decade of work on machine translation and automatic speech recognition, it concluded that the time had come to spend money hard-headedly toward important, realistic and relatively short-range goalsanother way of saying that language-technology research had overpromised and underdelivered. In 1969 Pierce wrote that both the funders and eager researchers had often fooled themselves, and that no simple, clear, sure knowledge is gained. After that, Americas government largely closed the money tap, and research on language technology went into hibernation for two decades. The story of how it emerged from that hibernation is both salutary and surprisingly workaday, says Mark Liberman. As professor of linguistics at the University of Pennsylvania and head of the Linguistic Data Consortium, a huge trove of texts and recordings of human language, he knows a thing or two about the history of language technology. In the bad old days researchers kept their methods in the dark and described their results in ways that were hard to evaluate. But beginning in the 1980s, Charles Wayne, then at Americas Defence Advanced Research Projects Agency, encouraged them to try another approach: the common task. Researchers would agree on a common set of practices, whether they were trying to teach computers speech recognition, speaker identification, sentiment analysis of texts, grammatical breakdown, language identification, handwriting recognition or anything else. They would set out the metrics they were aiming to improve on, share the data sets used to train their software and allow their results to be tested by neutral outsiders. That made the process far more transparent. Funding started up again and language technologies began to improve, though very slowly. Many early approaches to language technologyand particularly translationgot stuck in a conceptual cul-de-sac: the rules-based approach. In translation, this meant trying to write rules to analyse the text of a sentence in the language of origin, breaking it down into a sort of abstract interlanguage and rebuilding it according to the rules of the target language. These approaches showed early promise. But language is riddled with ambiguities and exceptions, so such systems were hugely complicated and easily broke down when tested on sentences beyond the simple set they had been designed for. Nearly all language technologies began to get a lot better with the application of statistical methods, often called a brute force approach. This relies on software scouring vast amounts of data, looking for patterns and learning from precedent. For example, in parsing language (breaking it down into its grammatical components), the software learns from large bodies of text that have already been parsed by humans. It uses what it has learned to make its best guess about a previously unseen text. In machine translation, the software scans millions of words already translated by humans, again looking for patterns. In speech recognition, the software learns from a body of recordings and the transcriptions made by humans. Thanks to the growing power of processors, falling prices for data storage and, most crucially, the explosion in available data, this approach eventually bore fruit. Mathematical techniques that had been known for decades came into their own, and big companies with access to enormous amounts of data were poised to benefit. People who had been put off by the hilariously inappropriate translations offered by online tools like BabelFish began to have more faith in Google Translate. Apple persuaded millions of iPhone users to talk not only on their phones but to them. The final advance, which began only about five years ago, came with the advent of deep learning through digital neural networks (DNNs). These are often touted as having qualities similar to those of the human brain: neurons are connected in software, and connections can become stronger or weaker in the process of learning. But Nils Lenke, head of research for Nuance, a language-technology company, explains matter-of-factly that DNNs are just another kind of mathematical model, the basis of which had been well understood for decades. What changed was the hardware being used. Almost by chance, DNN researchers discovered that the graphical processing units (GPUs) used to render graphics fluidly in applications like video games were also brilliant at handling neural networks. In computer graphics, basic small shapes move according to fairly simple rules, but there are lots of shapes and many rules, requiring vast numbers of simple calculations. The same GPUs are used to fine-tune the weights assigned to neurons in DNNs as they scour data to learn. The technique has already produced big leaps in quality for all kinds of deep learning, including deciphering handwriting, recognising faces and classifying images. Now they are helping to improve all manner of language technologies, often bringing enhancements of up to 30%. That has shifted language technology from usable at a pinch to really rather good. But so far no one has quite worked out what will move it on from merely good to reliably great. WHEN a person speaks, air is forced out through the lungs, making the vocal chords vibrate, which sends out characteristic wave patterns through the air. The features of the sounds depend on the arrangement of the vocal organs, especially the tongue and the lips, and the characteristic nature of the sounds comes from peaks of energy in certain frequencies. The vowels have frequencies called formants, two of which are usually enough to differentiate one vowel from another. For example, the vowel in the English word fleece has its first two formants at around 300Hz and 3,000Hz. Consonants have their own characteristic features. In principle, it should be easy to turn this stream of sound into transcribed speech. As in other language technologies, machines that recognise speech are trained on data gathered earlier. In this instance, the training data are sound recordings transcribed to text by humans, so that the software has both a sound and a text input. All it has to do is match the two. It gets better and better at working out how to transcribe a given chunk of sound in the same way as humans did in the training data. The traditional matching approach was a statistical technique called a hidden Markov model (HMM), making guesses based on what was done before. More recently speech recognition has also gained from deep learning. English has about 44 phonemes, the units that make up the sound system of a language. P and b are different phonemes, because they distinguish words like pat and bat. But in English p with a puff of air, as in party, and p without a puff of air, as in spin, are not different phonemes, though they are in other languages. If a computer hears the phonemes s, p, i and n back to back, it should be able to recognise the word spin. But the nature of live speech makes this difficult for machines. Sounds are not pronounced individually, one phoneme after the other; they mostly come in a constant stream, and finding the boundaries is not easy. Phonemes also differ according to the context. (Compare the l sound at the beginning of light with that at the end of full.) Speakers differ in timbre and pitch of voice, and in accent. Conversation is far less clear than careful dictation. People stop and restart much more often than they realise. All the same, technology has gradually mitigated many of these problems, so error rates in speech-recognition software have fallen steadily over the yearsand then sharply with the introduction of deep learning. Microphones have got better and cheaper. With ubiquitous wireless internet, speech recordings can easily be beamed to computers in the cloud for analysis, and even smartphones now often have computers powerful enough to carry out this task. Bear arms or bare arms Perhaps the most important feature of a speech-recognition system is its set of expectations about what someone is likely to say, or its language model. Like other training data, the language models are based on large amounts of real human speech, transcribed into text. When a speech-recognition system hears a stream of sound, it makes a number of guesses about what has been said, then calculates the odds that it has found the right one, based on the kinds of words, phrases and clauses it has seen earlier in the training text. At the level of phonemes, each language has strings that are permitted (in English, a word may begin with str-, for example) or banned (an English word cannot start with tsr-). The same goes for words. Some strings of words are more common than others. For example, the is far more likely to be followed by a noun or an adjective than by a verb or an adverb. In making guesses about homophones, the computer will have remembered that in its training data the phrase the right to bear arms came up much more often than the right to bare arms, and will thus have made the right guess. Training on a specific speaker greatly cuts down on the softwares guesswork. Just a few minutes of reading training text into software like Dragon Dictate, made by Nuance, produces a big jump in accuracy. For those willing to train the software for longer, the improvement continues to something close to 99% accuracy (meaning that of each hundred words of text, not more than one is wrongly added, omitted or changed). A good microphone and a quiet room help. Advance knowledge of what kinds of things the speaker might be talking about also increases accuracy. Words like phlebitis and gastrointestinal are not common in general discourse, and uncommon words are ranked lower in the probability tables the software uses to guess what it has heard. But these words are common in medicine, so creating software trained to look out for such words considerably improves the result. This can be done by feeding the system a large number of documents written by the speaker whose voice is to be recognised; common words and phrases can be extracted to improve the systems guesses. As with all other areas of language technology, deep learning has sharply brought down error rates. In October Microsoft announced that its latest speech-recognition system had achieved parity with human transcribers in recognising the speech in the Switchboard Corpus, a collection of thousands of recorded conversations in which participants are talking with a stranger about a randomly chosen subject. Error rates on the Switchboard Corpus are a widely used benchmark, so claims of quality improvements can be easily compared. Fifteen years ago quality had stalled, with word-error rates of 20-30%. Microsofts latest system, which has six neural networks running in parallel, has reached 5.9% (see chart), the same as a human transcribers. Xuedong Huang, Microsofts chief speech scientist, says that he expected it to take two or three years to reach parity with humans. It got there in less than one. The improvements in the lab are now being applied to products in the real world. More and more cars are being fitted with voice-activated controls of various kinds; the vocabulary involved is limited (there are only so many things you might want to say to your car), which ensures high accuracy. Microphonesor often arrays of microphones with narrow fields of pick-upare getting better at identifying the relevant speaker among a group. Some problems remain. Children and elderly speakers, as well as people moving around in a room, are harder to understand. Background noise remains a big concern; if it is different from that in the training data, the software finds it harder to generalise from what it has learned. So Microsoft, for example, offers businesses a product called CRIS that lets users customise speech-recognition systems for the background noise, special vocabulary and other idiosyncrasies they will encounter in that particular environment. That could be useful anywhere from a noisy factory floor to a care home for the elderly. But for a computer to know what a human has said is only a beginning. Proper interaction between the two, of the kind that comes up in almost every science-fiction story, calls for machines that can speak back. ILL be back. Hasta la vista, baby. Arnold Schwarzeneggers Teutonic drone in the Terminator films is world-famous. But in this instance film-makers looking into the future were overly pessimistic. Some applications do still feature a monotonous robot voice, but that is changing fast. Examples of speech synthesis from OSX synthesiser: Example from Amazon's ""Polly"" synthesiser: Creating speech is roughly the inverse of understanding it. Again, it requires a basic model of the structure of speech. What are the sounds in a language, and how do they combine What words does it have, and how do they combine in sentences These are well-understood questions, and most systems can now generate sound waves that are a fair approximation of human speech, at least in short bursts. Heteronyms require special care. How should a computer pronounce a word like lead, which can be a present-tense verb or a noun for a heavy metal, pronounced quite differently Once again a language model can make accurate guesses: Lead us not into temptation can be parsed for its syntax, and once the software has worked out that the first word is almost certainly a verb, it can cause it to be pronounced to rhyme with reed, not red. Traditionally, text-to-speech models have been concatenative, consisting of very short segments recorded by a human and then strung together as in the acoustic model described above. More recently, parametric models have been generating raw audio without the need to record a human voice, which makes these systems more flexible but less natural-sounding. DeepMind, an artificial-intelligence company bought by Google in 2014, has announced a new way of synthesising speech, again using deep neural networks. The network is trained on recordings of people talking, and on the texts that match what they say. Given a text to reproduce as speech, it churns out a far more fluent and natural-sounding voice than the best concatenative and parametric approaches. The last step in generating speech is giving it prosodygenerally, the modulation of speed, pitch and volume to convey an extra (and critical) channel of meaning. In English, a German teacher, with the stress on teacher, can teach anything but must be German. But a German teacher with the emphasis on German is usually a teacher of German (and need not be German). Words like prepositions and conjunctions are not usually stressed. Getting machines to put the stresses in the correct places is about 50% solved, says Mark Liberman of the University of Pennsylvania. Many applications do not require perfect prosody. A satellite-navigation system giving instructions on where to turn uses just a small number of sentence patterns, and prosody is not important. The same goes for most single-sentence responses given by a virtual assistant on a smartphone. But prosody matters when someone is telling a story. Pitch, speed and volume can be used to pass quickly over things that are already known, or to build interest and tension for new information. Myriad tiny clues communicate the speakers attitude to his subject. The phrase a German teacher, with stress on the word German, may, in the context of a story, not be a teacher of German, but a teacher being explicitly contrasted with a teacher who happens to be French or British. Text-to-speech engines are not much good at using context to provide such accentuation, and where they do, it rarely extends beyond a single sentence. When Alexa, the assistant in Amazons Echo device, reads a news story, her prosody is jarringly un-humanlike. Talking computers have yet to learn how to make humans want to listen. Computer translations have got strikingly better, but still need human input IN STAR TREK it was a hand-held Universal Translator; in The Hitchhikers Guide to the Galaxy it was the Babel Fish popped conveniently into the ear. In science fiction, the meeting of distant civilisations generally requires some kind of device to allow them to talk. High-quality automated translation seems even more magical than other kinds of language technology because many humans struggle to speak more than one language, let alone translate from one to another. The idea has been around since the 1950s, and computerised translation is still known by the quaint moniker machine translation (MT). It goes back to the early days of the cold war, when American scientists were trying to get computers to translate from Russian. They were inspired by the code-breaking successes of the second world war, which had led to the development of computers in the first place. To them, a scramble of Cyrillic letters on a page of Russian text was just a coded version of English, and turning it into English was just a question of breaking the code. Scientists at IBM and Georgetown University were among those who thought that the problem would be cracked quickly. Having programmed just six rules and a vocabulary of 250 words into a computer, they gave a demonstration in New York on January 7th 1954 and proudly produced 60 automated translations, including that of Mi pyeryedayem mislyi posryedstvom ryechyi, which came out correctly as We transmit thoughts by means of speech. Leon Dostert of Georgetown, the lead scientist, breezily predicted that fully realised MT would be an accomplished fact in three to five years. Instead, after more than a decade of work, the report in 1966 by a committee chaired by John Pierce, mentioned in the introduction to this report, recorded bitter disappointment with the results and urged researchers to focus on narrow, achievable goals such as automated dictionaries. Government-sponsored work on MT went into near-hibernation for two decades. What little was done was carried out by private companies. The most notable of them was Systran, which provided rough translations, mostly to Americas armed forces. The scientists got bogged down by their rules-based approach. Having done relatively well with their six-rule system, they came to believe that if they programmed in more rules, the system would become more sophisticated and subtle. Instead, it became more likely to produce nonsense. Adding extra rules, in the modern parlance of software developers, did not scale. Besides the difficulty of programming grammars many rules and exceptions, some early observers noted a conceptual problem. The meaning of a word often depends not just on its dictionary definition and the grammatical context but the meaning of the rest of the sentence. Yehoshua Bar-Hillel, an Israeli MT pioneer, realised that the pen is in the box and the box is in the pen would require different translations for pen: any pen big enough to hold a box would have to be an animal enclosure, not a writing instrument. How could machines be taught enough rules to make this kind of distinction They would have to be provided with some knowledge of the real world, a task far beyond the machines or their programmers at the time. Two decades later, IBM stumbled on an approach that would revive optimism about MT. Its Candide system was the first serious attempt to use statistical probabilities rather than rules devised by humans for translation. Statistical, phrase-based machine translation, like speech recognition, needed training data to learn from. Candide used Canadas Hansard, which publishes that countrys parliamentary debates in French and English, providing a huge amount of data for that time. The phrase-based approach would ensure that the translation of a word would take the surrounding words properly into account. But quality did not take a leap until Google, which had set itself the goal of indexing the entire internet, decided to use those data to train its translation engines; in 2007 it switched from a rules-based engine (provided by Systran) to its own statistics-based system. To build it, Google trawled about a trillion web pages, looking for any text that seemed to be a translation of anotherfor example, pages designed identically but with different words, and perhaps a hint such as the address of one page ending in /en and the other ending in /fr. According to Macduff Hughes, chief engineer on Google Translate, a simple approach using vast amounts of data seemed more promising than a clever one with fewer data. Training on parallel texts (which linguists call corpora, the plural of corpus) creates a translation model that generates not one but a series of possible translations in the target language. The next step is running these possibilities through a monolingual language model in the target language. This is, in effect, a set of expectations about what a well-formed and typical sentence in the target language is likely to be. Single-language models are not too hard to build. (Parallel human-translated corpora are hard to come by; large amounts of monolingual training data are not.) As with the translation model, the language model uses a brute-force statistical approach to learn from the training data, then ranks the outputs from the translation model in order of plausibility. Statistical machine translation rekindled optimism in the field. Internet users quickly discovered that Google Translate was far better than the rules-based online engines they had used before, such as BabelFish. Such systems still make mistakessometimes minor, sometimes hilarious, sometimes so serious or so many as to make nonsense of the result. And language pairs like Chinese-English, which are unrelated and structurally quite different, make accurate translation harder than pairs of related languages like English and German. But more often than not, Google Translate and its free online competitors, such as Microsofts Bing Translator, offer a usable approximation. Such systems are set to get better, again with the help of deep learning from digital neural networks. The Association for Computational Linguistics has been holding workshops on MT every summer since 2006. One of the events is a competition between MT engines turned loose on a collection of news text. In August 2016, in Berlin, neural-net-based MT systems were the top performers (out of 102), a first. Now Google has released its own neural-net-based engine for eight language pairs, closing much of the quality gap between its old system and a human translator. This is especially true for closely related languages (like the big European ones) with lots of available training data. The results are still distinctly imperfect, but far smoother and more accurate than before. Translations between English and (say) Chinese and Korean are not as good yet, but the neural system has brought a clear improvement here too. Neural-network-based translation actually uses two networks. One is an encoder. Each word of an input sentence is converted into a multidimensional vector (a series of numerical values), and the encoding of each new word takes into account what has happened earlier in the sentence. Marcello Federico of Italys Fondazione Bruno Kessler, a private research organisation, uses an intriguing analogy to compare neural-net translation with the phrase-based kind. The latter, he says, is like describing Coca-Cola in terms of sugar, water, caffeine and other ingredients. By contrast, the former encodes features such as liquidness, darkness, sweetness and fizziness. Once the source sentence is encoded, a decoder network generates a word-for-word translation, once again taking account of the immediately preceding word. This can cause problems when the meaning of words such as pronouns depends on words mentioned much earlier in a long sentence. This problem is mitigated by an attention model, which helps maintain focus on other words in the sentence outside the immediate context. Neural-network translation requires heavy-duty computing power, both for the original training of the system and in use. The heart of such a system can be the GPUs that made the deep-learning revolution possible, or specialised hardware like Googles Tensor Processing Units (TPUs). Smaller translation companies and researchers usually rent this kind of processing power in the cloud. But the data sets used in neural-network training do not need to be as extensive as those for phrase-based systems, which should give smaller outfits a chance to compete with giants like Google. Fully automated, high-quality machine translation is still a long way off. For now, several problems remain. All current machine translations proceed sentence by sentence. If the translation of such a sentence depends on the meaning of earlier ones, automated systems will make mistakes. Long sentences, despite tricks like the attention model, can be hard to translate. And neural-net-based systems in particular struggle with rare words. Training data, too, are scarce for many language pairs. They are plentiful between European languages, since the European Unions institutions churn out vast amounts of material translated by humans between the EUs 24 official languages. But for smaller languages such resources are thin on the ground. For example, there are few Greek-Urdu parallel texts available on which to train a translation engine. So a system that claims to offer such translation is in fact usually running it through a bridging language, nearly always English. That involves two translations rather than one, multiplying the chance of errors. Even if machine translation is not yet perfect, technology can already help humans translate much more quickly and accurately. Translation memories, software that stores already translated words and segments, first came into use as early as the 1980s. For someone who frequently translates the same kind of material (such as instruction manuals), they serve up the bits that have already been translated, saving lots of duplication and time. A similar trick is to train MT engines on text dealing with a narrow real-world domain, such as medicine or the law. As software techniques are refined and computers get faster, training becomes easier and quicker. Free software such as Moses, developed with the support of the EU and used by some of its in-house translators, can be trained by anyone with parallel corpora to hand. A specialist in medical translation, for instance, can train the system on medical translations only, which makes them far more accurate. At the other end of linguistic sophistication, an MT engine can be optimised for the shorter and simpler language people use in speech to spew out rough but near-instantaneous speech-to-speech translations. This is what Microsofts Skype Translator does. Its quality is improved by being trained on speech (things like film subtitles and common spoken phrases) rather than the kind of parallel text produced by the European Parliament. Translation management has also benefited from innovation, with clever software allowing companies quickly to combine the best of MT, translation memory, customisation by the individual translator and so on. Translation-management software aims to cut out the agencies that have been acting as middlemen between clients and an army of freelance translators. Jack Welde, the founder of Smartling, an industry favourite, says that in future translation customers will choose how much human intervention is needed for a translation. A quick automated one will do for low-stakes content with a short life, but the most important content will still require a fully hand-crafted and edited version. Noting that MT has both determined boosters and committed detractors, Mr Welde says he is neither: If you take a dogmatic stance, youre not optimised for the needs of the customer. Translation software will go on getting better. Not only will engineers keep tweaking their statistical models and neural networks, but users themselves will make improvements to their own systems. For example, a small but much-admired startup, Lilt, uses phrase-based MT as the basis for a translation, but an easy-to-use interface allows the translator to correct and improve the MT systems output. Every time this is done, the corrections are fed back into the translation engine, which learns and improves in real time. Users can build several different memoriesa medical one, a financial one and so onwhich will help with future translations in that specialist field. TAUS, an industry group, recently issued a report on the state of the translation industry saying that in the past few years the translation industry has burst with new tools, platforms and solutions. Last year Jaap van der Meer, TAUSs founder and director, wrote a provocative blogpost entitled The Future Does Not Need Translators, arguing that the quality of MT will keep improving, and that for many applications less-than-perfect translation will be good enough. The translator of the future is likely to be more like a quality-control expert, deciding which texts need the most attention to detail and editing the output of MT software. That may be necessary because computers, no matter how sophisticated they have become, cannot yet truly grasp what a text means. Meaning and machine intelligence: What are you talking about IN BLACK MIRROR, a British science-fiction satire series set in a dystopian near future, a young woman loses her boyfriend in a car accident. A friend offers to help her deal with her grief. The dead man was a keen social-media user, and his archived accounts can be used to recreate his personality. Before long she is messaging with a facsimile, then speaking to one. As the system learns to mimic him ever better, he becomes increasingly real. This is not quite as bizarre as it sounds. Computers today can already produce an eerie echo of human language if fed with the appropriate material. What they cannot yet do is have true conversations. Truly robust interaction between man and machine would require a broad understanding of the world. In the absence of that, computers are not able to talk about a wide range of topics, follow long conversations or handle surprises. Machines trained to do a narrow range of tasks, though, can perform surprisingly well. The most obvious examples are the digital assistants created by the technology giants. Users can ask them questions in a variety of natural ways: Whats the temperature in London Hows the weather outside Is it going to be cold today The assistants know a few things about users, such as where they live and who their family are, so they can be personal, too: Hows my commute looking Text my wife Ill be home in 15 minutes. And they get better with time. Apples Siri receives 2bn requests per week, which (after being anonymised) are used for further teaching. For example, Apple says Siri knows every possible way that users ask about a sports score. She also has a delightful answer for children who ask about Father Christmas. Microsoft learned from some of its previous natural-language platforms that about 10% of human interactions were chitchat, from tell me a joke to whos your daddy, and used such chat to teach its digital assistant, Cortana. The writing team for Cortana includes two playwrights, a poet, a screenwriter and a novelist. Google hired writers from Pixar, an animated-film studio, and The Onion, a satirical newspaper, to make its new Google Assistant funnier. No wonder people often thank their digital helpers for a job well done. The assistants replies range from My pleasure, as always to You dont need to thank me. How do natural-language platforms know what people want They not only recognise the words a person uses, but break down speech for both grammar and meaning. Grammar parsing is relatively advanced; it is the domain of the well-established field of natural-language processing. But meaning comes under the heading of natural-language understanding, which is far harder. First, parsing. Most people are not very good at analysing the syntax of sentences, but computers have become quite adept at it, even though most sentences are ambiguous in ways humans are rarely aware of. Take a sign on a public fountain that says, This is not drinking water. Humans understand it to mean that the water (this) is not a certain kind of water (drinking water). But a computer might just as easily parse it to say that this (the fountain) is not at present doing something (drinking water). As sentences get longer, the number of grammatically possible but nonsensical options multiplies exponentially. How can a machine parser know which is the right one It helps for it to know that some combinations of words are more common than others: the phrase drinking water is widely used, so parsers trained on large volumes of English will rate those two words as likely to be joined in a noun phrase. And some structures are more common than others: noun verb noun noun may be much more common than noun noun verb noun. A machine parser can compute the overall probability of all combinations and pick the likeliest. A lexicalised parser might do even better. Take the Groucho Marx joke, One morning I shot an elephant in my pyjamas. How he got in my pyjamas, Ill never know. The first sentence is ambiguous (which makes the joke)grammatically both I and an elephant can attach to the prepositional phrase in my pyjamas. But a lexicalised parser would recognise that I [verb phrase] in my pyjamas is far more common than elephant in my pyjamas, and so assign that parse a higher probability. But meaning is harder to pin down than syntax. The boy kicked the ball and The ball was kicked by the boy have the same meaning but a different structure. Time flies like an arrow can mean either that time flies in the way that an arrow flies, or that insects called time flies are fond of an arrow. Who plays Thor in Thor Your correspondent could not remember the beefy Australian who played the eponymous Norse god in the Marvel superhero film. But when he asked his iPhone, Siri came up with an unexpected reply: I dont see any movies matching Thor playing in Thor, IA, US, today. Thor, Iowa, with a population of 184, was thousands of miles away, and Thor, the film, has been out of cinemas for years. Siri parsed the question perfectly properly, but the reply was absurd, violating the rules of what linguists call pragmatics: the shared knowledge and understanding that people use to make sense of the often messy human language they hear. Can you reach the salt is not a request for information but for salt. Natural-language systems have to be manually programmed to handle such requests as humans expect them, and not literally. Shared information is also built up over the course of a conversation, which is why digital assistants can struggle with twists and turns in conversations. Tell an assistant, Id like to go to an Italian restaurant with my wife, and it might suggest a restaurant. But then ask, is it close to her office, and the assistant must grasp the meanings of it (the restaurant) and her (the wife), which it will find surprisingly tricky. Nuance, the language-technology firm, which provides natural-language platforms to many other companies, is working on a concierge that can handle this type of challenge, but it is still a prototype. Such a concierge must also offer only restaurants that are open. Linking requests to common sense (knowing that no one wants to be sent to a closed restaurant), as well as a knowledge of the real world (knowing which restaurants are closed), is one of the most difficult challenges for language technologies. Common sense, an old observation goes, is uncommon enough in humans. Programming it into computers is harder still. Fernando Pereira of Google points out why. Automated speech recognition and machine translation have something in common: there are huge stores of data (recordings and transcripts for speech recognition, parallel corpora for translation) that can be used to train machines. But there are no training data for common sense. Knowledge of the real world is another matter. AI has helped data-rich companies such as Americas West-Coast tech giants organise much of the worlds information into interactive databases such as Googles Knowledge Graph. Some of the content of that appears in a box to the right of a Google page of search results for a famous figure or thing. It knows that Jacob Bernoulli studied at the University of Basel (as did other people, linked to Bernoulli through this node in the Graph) and wrote On the Law of Large Numbers (which it knows is a book). Organising information this way is not difficult for a company with lots of data and good AI capabilities, but linking information to language is hard. Google touts its assistants ability to answer questions like Who was president when the Rangers won the World Series But Mr Pereira concedes that this was the result of explicit training. Another such complex queryWhat was the population of London when Samuel Johnson wrote his dictionarywould flummox the assistant, even though the Graph knows about things like the historical population of London and the date of Johnsons dictionary. IBMs Watson system, which in 2011 beat two human champions at the quiz show Jeopardy!, succeeded mainly by calculating huge numbers of potential answers based on key words by probability, not by a human-like understanding of the question. Making real-world information computable is challenging, but it has inspired some creative approaches. Cortical.io, a Vienna-based startup, took hundreds of Wikipedia articles, cut them into thousands of small snippets of information and ran an unsupervised machine-learning algorithm over it that required the computer not to look for anything in particular but to find patterns. These patterns were then represented as a visual semantic fingerprint on a grid of 128x128 pixels. Clumps of pixels in similar places represented semantic similarity. This method can be used to disambiguate words with multiple meanings: the fingerprint of organ shares features with both liver and piano (because the word occurs with both in different parts of the training data). This might allow a natural-language system to distinguish between pianos and church organs on one hand, and livers and other internal organs on the other. Proper conversation between humans and machines can be seen as a series of linked challenges: speech recognition, speech synthesis, syntactic analysis, semantic analysis, pragmatic understanding, dialogue, common sense and real-world knowledge. Because all the technologies have to work together, the chain as a whole is only as strong as its weakest link, and the first few of these are far better developed than the last few. The hardest part is linking them together. Scientists do not know how the human brain draws on so many different kinds of knowledge at the same time. Programming a machine to replicate that feat is very much a work in progress. IN WALL-E, an animated childrens film set in the future, all humankind lives on a spaceship after the Earths environment has been trashed. The humans are whisked around in intelligent hovering chairs; machines take care of their every need, so they are all morbidly obese. Even the ships captain is not really in charge; the actual pilot is an intelligent and malevolent talking robot, Auto, and like so many talking machines in science fiction, he eventually makes a grab for power. Speech is quintessentially human, so it is hard to imagine machines that can truly speak conversationally as humans do without also imagining them to be superintelligent. And if they are super intelligent, with none of humans flaws, it is hard to imagine them not wanting to take over, not only for their good but for that of humanity. Even in a fairly benevolent future like WALL-Es, where the machines are doing all the work, it is easy to see that the lack of anything challenging to do would be harmful to people. Fortunately, the tasks that talking machines can take off humans to-do lists are the sort that many would happily give up. Machines are increasingly able to handle difficult but well-defined jobs. Soon all that their users will have to do is pipe up and ask them, using a naturally phrased voice command. Once upon a time, just one tinkerer in a given family knew how to work the computer or the video recorder. Then graphical interfaces (icons and a mouse) and touchscreens made such technology accessible to everyone. Frank Chen of Andreessen Horowitz, a venture-capital firm, sees natural-language interfaces between humans and machines as just another step in making information and services available to all. Silicon Valley, he says, is enjoying a golden age of AI technologies. Just as in the early 1990s companies were piling online and building websites without quite knowing why, now everyone is going for natural language. Yet, he adds, were in 1994 for voice. 1995 will soon come. This does not mean that people will communicate with their computers exclusively by talking to them. Websites did not make the telephone obsolete, and mobile devices did not make desktop computers obsolete. In the same way, people will continue to have a choice between voice and text when interacting with their machines. Not all will choose voice. For example, in Japan yammering into a phone is not done in public, whether the interlocutor is a human or a digital assistant, so usage of Siri is low during business hours but high in the evening and at the weekend. For others, voice-enabled technology is an obvious boon. It allows dyslexic people to write without typing, and the very elderly may find it easier to talk than to type on a tiny keyboard. The very young, some of whom today learn to type before they can write, may soon learn to talk to machines before they can type. Those with injuries or disabilities that make it hard for them to write will also benefit. Microsoft is justifiably proud of a new device that will allow people with amyotrophic lateral sclerosis (ALS), which immobilises nearly all of the body but leaves the mind working, to speak by using their eyes to pick letters on a screen. The critical part is predictive text, which improves as it gets used to a particular individual. An experienced user will be able to speak at around 15 words per minute. People may even turn to machines for company. Microsofts Xiaoice, a chatbot launched in China, learns to come up with the responses that will keep a conversation going longest. Nobody would think it was human, but it does make users open up in surprising ways. Jibo, a new social robot, is intended to tell children stories, help far-flung relatives stay in touch and the like. Another group that may benefit from technology is smaller language communities. Networked computers can encourage a winner-take-all effect: if there is a lot of good software and content in English and Chinese, smaller languages become less valuable online. If they are really tiny, their very survival may be at stake. But Ross Perlin of the Endangered Languages Alliance notes that new software allows researchers to document small languages more quickly than ever. With enough data comes the possibility of developing resourcesfrom speech recognition to interfaces with softwarefor smaller and smaller languages. The Silicon Valley giants already localise their services in dozens of languages; neural networks and other software allow new versions to be generated faster and more efficiently than ever. There are two big downsides to the rise in natural-language technologies: the implications for privacy, and the disruption it will bring to many jobs. Increasingly, devices are always listening. Digital assistants like Alexa, Cortana, Siri and Google Assistant are programmed to wait for a prompt, such as Hey, Siri or OK, Google, to activate them. But allowing always-on microphones into peoples pockets and homes amounts to a further erosion of traditional expectations of privacy. The same might be said for all the ways in which language software improves by training on a single users voice, vocabulary, written documents and habits. All the big companies location-based serviceseven the accelerometers in phones that detect small movementsare making ever-improving guesses about users wants and needs. The moment when a digital assistant surprises a user with The chemist is nearbydo you want to buy more haemorrhoid cream, Steve could be when many may choose to reassess the trade-off between amazing new services and old-fashioned privacy. The tech companies can help by giving users more choice; the latest iPhone will not be activated when it is laid face down on a table. But hackers will inevitably find ways to get at some of these data. The other big concern is for jobs. To the extent that they are routine, they face being automated away. A good example is customer support. When people contact a company for help, the initial encounter is usually highly scripted. A company employee will verify a customers identity and follow a decision-tree. Language technology is now mature enough to take on many of these tasks. For a long transition period humans will still be needed, but the work they do will become less routine. Nuance, which sells lots of automated online and phone-based help systems, is bullish on voice biometrics (customers identifying themselves by saying my voice is my password). Using around 200 parameters for identifying a speaker, it is probably more secure than a fingerprint, says Brett Beranek, a senior manager at the company. It will also eliminate the tedium, for both customers and support workers, of going through multi-step identification procedures with PINs, passwords and security questions. When Barclays, a British bank, offered it to frequent users of customer-support services, 84% signed up within five months. Digital assistants on personal smartphones can get away with mistakes, but for some business applications the tolerance for error is close to zero, notes Nikita Ivanov. His company, Datalingvo, a Silicon Valley startup, answers questions phrased in natural language about a companys business data. If a user wants to know which online ads resulted in the most sales in California last month, the software automatically translates his typed question into a database query. But behind the scenes a human working for Datalingvo vets the query to make sure it is correct. This is because the stakes are high: the technology is bound to make mistakes in its early days, and users could make decisions based on bad data. This process can work the other way round, too: rather than natural-language input producing data, data can produce language. Arria, a company based in London, makes software into which a spreadsheet full of data can be dragged and dropped, to be turned automatically into a written description of the contents, complete with trends. Matt Gould, the companys chief strategy officer, likes to think that this will free chief financial officers from having to write up the same old routine analyses for the board, giving them time to develop more creative approaches. Carl Benedikt Frey, an economist at Oxford University, has researched the likely effect of artificial intelligence on the labour market and concluded that the jobs most likely to remain immune include those requiring creativity and skill at complex social interactions. But not every human has those traits. Call centres may need fewer people as more routine work is handled by automated systems, but the trickier inquiries will still go to humans. Much of this seems familiar. When Google search first became available, it turned up documents in seconds that would have taken a human operator hours, days or years to find. This removed much of the drudgery from being a researcher, librarian or journalist. More recently, young lawyers and paralegals have taken to using e-discovery. These innovations have not destroyed the professions concerned but merely reshaped them. Machines that relieve drudgery and allow people to do more interesting jobs are a fine thing. In net terms they may even create extra jobs. But any big adjustment is most painful for those least able to adapt. Upheavals brought about by social changeslike the emancipation of women or the globalisation of labour marketsare already hard for some people to bear. When those changes are wrought by machines, they become even harder, and all the more so when those machines seem to behave more and more like humans. People already treat inanimate objects as if they were alive: who has never shouted at a computer in frustration The more that machines talk, and the more that they seem to understand people, the more their users will be tempted to attribute human traits to them. That raises questions about what it means to be human. Language is widely seen as humankinds most distinguishing trait. AI researchers insist that their machines do not think like people, but if they can listen and talk like humans, what does that make them As humans teach ever more capable machines to use language, the once-obvious line between them will blur. ",http://www.economist.com/technology-quarterly/2017-05-01/language,"Computers have got much better at translation, voice recognition and speech synthesis, says Lane Greene. But they still don't understand the meaning of ...",Finding a voice,Lane Greene IM SORRY Dave Im HAL Space Odyssey Dave Bowman HALs Jupiter Hal Im Dave Hal Dave Creative AI HAL Terminator Rosie Jetsons Machine Apples Siri Amazons Alexa Google Microsofts Cortana Alexa Apple Siri Creative Strategies John Pierce Bell Labs Americas National Academy Sciences Pierce Mark Liberman University Pennsylvania Linguistic Data Consortium Charles Wayne Americas Defence Advanced Research Projects Agency Researchers Mathematical BabelFish Google Translate Apple DNNs Nils Lenke Nuance DNNs DNN GPUs GPUs DNNs English Markov HMM English P English Compare Speakers Bear English English Dragon Dictate Nuance Advance October Microsoft Switchboard Corpus Switchboard Corpus Microsofts Xuedong Huang Microsofts Microphonesor Children Background Microsoft CRIS Hasta Arnold Schwarzeneggers Teutonic Terminator OSX Amazon Polly Creating Again Heteronyms DeepMind Google English German Mark Liberman University Pennsylvania Myriad German Alexa Amazons Echo Computer IN STAR TREK Universal Translator Hitchhikers Guide Galaxy Babel Fish High-quality MT Cyrillic English English IBM Georgetown University New York January Mi Leon Dostert Georgetown MT John Pierce MT Systran Americas Yehoshua Bar-Hillel MT IBM MT Candide Candide Canadas Hansard French English Google Systran Google /en Macduff Hughes Google Translate Internet Google Translate BabelFish Chinese-English English German Google Translate Microsofts Bing Translator Association Computational Linguistics MT MT August Berlin MT Google English Chinese Korean Marcello Federico Italys Fondazione Bruno Kessler Coca-Cola GPUs Googles Tensor Processing Units TPUs Smaller Google Training European Unions EUs Greek-Urdu English MT EU MT Microsofts Skype Translator European Parliament Translation MT Jack Welde Smartling MT Mr Welde Translation Lilt MT MT TAUS Jaap Meer TAUSs MT MT IN BLACK MIRROR London Hows Hows Text Ill Siri Apple Siri Father Christmas Microsoft Cortana Cortana Google Pixar Onion Google Assistant My Grammar First Humans English Groucho Marx Ill Time Thor Thor Your Australian Norse Marvel Siri Thor Thor IA US Thor Iowa Thor Siri Tell Id Common Fernando Pereira Google Knowledge AI Americas West-Coast Googles Knowledge Graph Google Jacob Bernoulli University Basel Bernoulli Graph Law Large Numbers AI Google Who Rangers World Series Mr Pereira London Samuel Johnson Graph London Johnsons IBMs Watson Jeopardy Cortical.io Wikipedia Clumps IN WALL-E Earths Auto Speech WALL-Es Soon Frank Chen Andreessen Horowitz Silicon Valley AI Japan Siri Microsoft ALS Microsofts Xiaoice China Jibo English Chinese Ross Perlin Endangered Languages Alliance Silicon Valley Digital Alexa Cortana Siri Google Assistant Hey Siri OK Google Steve Language Brett Beranek PINs Barclays Digital Nikita Ivanov Datalingvo Silicon Valley California Datalingvo Arria London Matt Gould Carl Benedikt Frey Oxford University Google AI,4
134,"IBM Breaks Industry Record for Conversational Speech Recognition by Extending Deep Learning Technologies PR Newswire Wednesday, March 8, 2017 ARMONK, N.Y., March 8,2017 /PRNewswire/ --IBM (NYSE: IBM )today announced that it broke the industry record for speech recognition, creating a technology that recognizes spoken words ever closer to human parity. Last year, IBM announced a major improvement in conversational speech recognition: a system that achieved a 6.9 percent word error rate . Since then, IBM Researchers have continued to push the boundaries of accuracy rates, achieving this historic milestone and setting an industry record of 5.5 percent, a 20% improvement from the rate than was reported six months prior. ""These speech developments build on decades of research, and achieving speech recognition comparable to that of humans is a complex task. At IBM, we are dedicated to creating the technology that will one day match the complexity of how the human ear, voice and brain interact,"" said Michael Karasick, IBM Vice President, Cognitive Computing. ""This progress will have important implications for how man and machine collaborate in the future, making the interactions more natural and productive. We believe it is only a matter of time before we achieve parity on speech recognition with humans."" The success of speech recognition technology is measured against human parity, an error rate on par with that of two humans speaking. Previously, human parity was considered a 5.9 percent word error rate; IBM partnered with Appen, a speech and technology service provider, to reassess the industry benchmark and determined that human parity is lower than what anyone has yet achieved: 5.1 percent. In the face of other industry claims, this research, in partnership with Appen, shows finding a standard measurement for human parity across the industry is more complex than it seems. As IBM continues to develop and improve upon this technology, its researchers will remain accountable to the highest standards of accuracy when measuring for it for the findings to be truly valuable. ""In spite of impressive advances in recent years, reaching human-level performance in AI tasks such as speech recognition or object recognition remains a scientific challenge. Indeed, standard benchmarks do not always reveal the variations and complexities of real data,"" says Yoshua Bengio, leader of University of Montreal's Institute for Learning Algorithms. ""IBM continues to make significant strides in advancing speech recognition by applying neural networks and deep learning into acoustic and language models."" ""The ability to recognize speech as well as humans do is a continuing challenge, since human speech, especially during spontaneous conversation, is extremely complex,"" Julia Hirschberg, a professor and Chair at the Department of Computer Science at Columbia University. ""IBM's recent achievements in speech recognition are quite impressive, as is IBM's dedication to better understand how we measure the success speech technology and industry benchmarks."" Today's achievement builds upon IBM's recent advancements in language and speech technology, gained from IBM's decades of experience researching, developing and investing in AI technology. These research developments are critical to advancing the development and adoption of cognitive around the globe; as we continue to strengthen and improve upon our speech and language technology, these updates will be embedded in the cognitive capabilities we offer via the Watson Developer Cloud. For more than seven decades, IBM Research has defined the future of information technology with more than 3,000 researchers in 12 labs located across six continents. Scientists from IBM Research have produced six Nobel Laureates, 10 U.S. National Medals of Technology, five U.S. National Medals of Science, six Turing Awards, 19 inductees in the National Academy of Sciences and 20 inductees into the U.S. National Inventors Hall of Fame. For more information about IBM Research, visit www.ibm.com/research . ",http://finance.yahoo.com/news/ibm-breaks-industry-record-conversational-212800586.html,"ARMONK, N.Y., March 8, 2017 /PRNewswire/ -- IBM (NYSE: IBM) today announced that it broke the industry record for speech recognition, ...",IBM Breaks Industry Record for Conversational Speech Recognition ...,IBM Breaks Industry Record Conversational Deep Learning Technologies PR Newswire Wednesday March ARMONK N.Y. March /PRNewswire/ IBM NYSE IBM IBM IBM Researchers IBM Michael Karasick IBM Vice President Cognitive Computing IBM Appen Appen IBM AI Yoshua Bengio University Montreal Institute Learning Algorithms IBM Julia Hirschberg Chair Department Computer Science Columbia University IBM IBM IBM IBM AI Watson Developer Cloud IBM Research IBM Research Nobel Laureates U.S. National Medals Technology U.S. National Medals Science Turing Awards National Academy Sciences U.S. National Inventors Hall Fame IBM Research,0
135,"As movements to protest and silence controversial campus speakers have become common, the president of a new Harvard University student group intends to saturate the campus with those types of talks -- to challenge established ideologies that he said administrators there blatantly promote. Open Campus Initiative was organized this year, its president, Harvard sophomore Conor Healy, said in an interview Friday. Already, the group of roughly 25 students, Healy said, has secured commitments from two right-leaning, controversial figures to address the campus. One, writer Charles Murray, made headlines in March after his lecture at Middlebury College was drowned out by student chants, forcing him to stop. Murray is often accused of promoting racist ideals. Open Campus Initiative has not yet pegged a day for his talk. The pick of Murray was deliberate, Healy said. He was horrified by the disruptions at Middlebury and said he wanted to prove Harvard could serve as a role model institution for free expression. Most of the community wants to hear from the people were inviting, they want to critique them, ask them hard questions, and theyre willing to be convinced, Healy said. If theyre not convinced, their perception of the truth can be reinforced by the opposing view. The other speaker, University of Toronto psychology professor Jordan Peterson, is due to lead a discussion this week. Late last month, Peterson also faced protesters -- equipped with cowbells and air horns -- at his speaking engagement at McMaster University in Ontario. Peterson has been criticized for his perspective on gender identity. He has rejected a Canadian policy that would have offered protections for gender nonconforming people and refuses to use gender-neutral descriptors, such as ""they"" as a singular pronoun, as a matter of policy. Healy debated whether to invite ex-Breitbart News editor Milo Yiannopoulos, a deeply divisive media personality who resigned from the conservative website in February after a video emerged of him seemingly endorsing sexual relationships between men and underage boys. But Healy said Yiannopoulos has been known to occasionally heckle students from stage. Thats not really what were about, Healy said. Healy said the groupalso intends to ask liberal speakers to visit campus. Inspiration for the group came after Healy enrolled in a freshman seminar course on free speech last year. A Canadian, Healy was struck by the integral part that freedom of expression played in Americas history and found himself mulling what he considered to be threats to open dialogue on Harvards campus. Healy, who subscribes to libertarian philosophies, including a distaste for government interference in peoples personal lives, said some students try to curtail opposing viewpoints with shut it down mentality and behaviors. Ousted pharmaceutical mogul Martin Shkreli came to Harvard in February, but his scheduled speech was halted when someone intentionally set off the fire alarm in the building. Protesters later interrupted Shkreli, who has been indicted on federal stock fraud charges, but he did continue with his talk. Healy attended that event and said he was offended that other students would try to completely shut down Shkreli. That is so profoundly misguided in my opinion, Healy said. Dangers to First Amendment rights manifest in other ways, and the Harvard administration seems to endorse left-leaning values, Healy said. He referenced a December 2015 controversy, in which Harvard distributed laminated place mats in the dining halls with tips on how to address hot-button issues with family members during the holidays. The Harvard Undergraduate Council wrote at the time that university officials shouldnt prescribe party-line talking points for such concepts as Islamophobia. We do not think the offices of the university should be in the business of disseminating approved positions on complex and divisive political issues, the council wrote. Helene Lovett, a sophomore involved with lesbian, gay, bisexual, transgender and queer groups on campus, has organized a demonstration against the Peterson event. Outside, students will be ""aggressively"" handing out fliers and flags representing the transgender community. Inside at the talk, Lovett hopes that students will hold up those flags when Peterson says something ""problematic."" This way they won't disrupt his narrative, but it will be a way to demonstrate silently, she said. Lovett said she disagrees with Healy's interpretation that free speech is squashed on campus. ""Im not willing to buy that students feel more threatened that they cant speak than compared to the threats that marginalized communities feel every day,"" she said. Rachael Dane, a university spokeswoman, said in an email that the university does not necessarily approve or endorse an independent student organizations goals, activities or viewpoints. Independent student organizations are independent and distinct from Harvard University. The colleges recognition of, and provision of benefits and privileges to, an independent student organization does mean that the independent student organization is a unit of the university or controlled by the university. The university is not responsible for an independent student organizations contracts or other acts or omissions, Dane said. The faculty adviser to Open Campus Initiative, Harry R. Lewis, a former dean of the college, said he was traveling and declined an interview Friday. For the Peterson event, the group has paid for two security officers and five officers from the campus police force, costing about $1,000, Healy said. He anticipates some sort of problem, though not to the same level as Murrays Middlebury talk. Thus far, the group has raised about $9,000, Healy said, donations largely from disgruntled Harvard alumni who agree that the campus climate has become too restrictive. Its fund-raising goal is $100,000 by the end of next semester, Healy said. ",https://www.insidehighered.com/news/2017/04/10/harvard-student-group-promotes-free-expression-through-controversial-speakers,"... enrolled in a freshman seminar course on free speech last year. ... The college's recognition of, and provision of benefits and privileges to, ...",Harvard student group promotes free expression through ...,Harvard University Campus Initiative Harvard Conor Healy Friday Already Healy Charles Murray March Middlebury College Murray Campus Initiative Murray Healy Middlebury Harvard Healy University Toronto Jordan Peterson Peterson McMaster University Ontario Peterson Healy News Milo Yiannopoulos February Healy Yiannopoulos Healy Healy Healy Healy Americas Harvards Healy Martin Shkreli Harvard February Shkreli Healy Shkreli Healy First Amendment Harvard Healy December Harvard Harvard Undergraduate Council Islamophobia Helene Lovett Peterson Outside Inside Lovett Peterson Lovett Healy Im Rachael Dane Independent Harvard University Dane Campus Initiative Harry R. Lewis Friday Peterson Healy Murrays Middlebury Healy Harvard Healy,3
136,"Samsung has officially jumped into the digital-assistant field, pitting it against Amazon.com ( AMZN ) and Alphabet ( GOOGL ), as well as Apple ( AAPL ) and Microsoft ( MSFT ), in one of the hottest fields in consumer electronics . Samsung on Tuesday unveiled Bixby, an artificially intelligent software program that will first appear on the upcoming Samsung Galaxy 8 smartphone, expected to debut March 29. Bixby is designed to work across a range of Samsung devices and appliances, wrote InJong Rhee, executive vice president and head of research and development for the company'ssoftware and services division, in a blog post announcing Bixby . ""Bixby will be a new intelligent interface on our devices,"" Rhee wrote. He said Bixby is ""fundamentally different"" from other digital assistants, such as the Amazon Alexa line or the Google Home by Alphabet. He said Bixby offers ""a deeper experience"" than other offerings. Microsoft offers a digital assistant in the form of Cortana, an artificial intelligence platform available as a software app for Android smartphones. Apple's digital assistant is available through Siri, an iPhone software application. Microsoft in December gave a sneak peek at a new device that looks similar to the Amazon Echo device voiced by Alexa, but voice-controlled through Cortana, Microsoft's artificial intelligence platform. Microsoft said the device will ship this year. Shares of Amazon dropped 1.6% to close at 843.20 on the stock market today , dipping below buy range. Alphabet lost 2%, also falling out of buy zone. Apple and Microsoft both lost 1.1%. IBD'S TAKE: Amazon, Apple, Alphabet and Microsoft are strengthening their connection to customers with mobile and in-home devices infused with advanced speech recognition and artificial intelligence. The devices could evolve into the next major disruption in computing and put the smart home on a fast track . The Amazon and Alphabet devices also have the potential to be the central hub of a smart-home ecosystem. As to which device is better, that's open to opinion. Some analysts think Google Home will be the winner, based on its strength in artificial intelligence , while others think Amazon has a good shot at remaining the market leader with its Alexa line of products. In his blog post, Rhee suggested that Bixby will outperform the devices from Alphabet, Amazon and others by being easier to use. ""We know that adopting new ways to interact with your devices will require a change in user behavior,"" Rhee wrote. ""At the same time we believe the key to success for a new voice interface is to design a scheme that reduces friction and makes the experience significantly more rewarding than the existing interface."" ",http://www.investors.com/news/technology/samsung-joins-digital-assistant-fray-with-aim-to-tackle-amazon-alphabet/,... their connection to customers with mobile and in-home devices infused with advanced speech recognition and artificial intelligence.,"Samsung Joins Digital Assistant Fray To Take On Amazon, Alphabet",Samsung Amazon.com AMZN Alphabet GOOGL Apple AAPL Microsoft MSFT Samsung Tuesday Bixby Samsung Galaxy March Bixby Samsung InJong Rhee Bixby Bixby Rhee Bixby Amazon Alexa Google Home Alphabet Bixby Microsoft Cortana Android Apple Siri Microsoft December Amazon Echo Alexa Cortana Microsoft Microsoft Shares Amazon Alphabet Apple Microsoft IBD Amazon Apple Alphabet Microsoft Amazon Alphabet Google Home Amazon Alexa Rhee Bixby Alphabet Amazon Rhee,7
137,"You are not permitted to download, save or email this image. Visit image gallery to purchase the image. Wakatipu High School pupils Leah Kissick (16), left, and Olivia Ray (17), who will make speeches at the official Anzac Day services in Queenstown and Arrowtown after winning the Combined Queenstown and Arrowtown RSA speech competition last week. PHOTO: TRACEY ROXBURGH A century after New Zealand's deadliest day in history, two Wakatipu High School pupils will publicly pay tribute to two men who had very different war-time experiences. Last week Olivia Ray (17) and Leah Kissick (16) were judged the winners of the Combined Queenstown and Arrowtown RSA school competition. This year the competition focused on the First Battle of Passchendaele, on October 12, 1917 in which 2735 New Zealanders were killed or wounded. Leah's speech focused on Dunedin-born Sir Harold Delf Gillies who pioneered plastic surgery during the war. Sir Harold studied medicine at Cambridge University, in the United Kingdom and following the outbreak of WWI joined the Royal Army Medical Corps. He ultimately helped establish a hospital devoted to facial repairs in Sidcup, London, where he and his colleagues developed plastic surgery techniques, performing more than 11,000 operations on more than 5000 men, primarily soldiers with facial injuries from gunshot wounds. He was knighted in 1930. Leah said much of her education around conflicts had been based on the ``negative side of war.'' ``I wanted to focus on an individual who's taken the opportunity to have a positive impact on society ... if he did it now, he'd get so [much recognition] ... but he was in the shadows. ``I wanted to show that the war can, in some cases, bring positive ... inspiration.'' One example was Private Victor Spencer who suffered post-traumatic stress disorder but was executed for desertion in 1918 when he was 21 years old. Pte Spencer was born near Bluff, in Southland, and was raised by his aunt Sarah Goomes after both his parents died. An apprentice engineer and cox for a local rowing crew, he enlisted in August 1915 and was sent to Gallipoli with the Otago Regiment of the New Zealand Division in WWI when he was 18 years old and then on to the Western Front. By July 1916 he and some of his Otago Battalion had spent a month in the trenches without respite and stress had taken its toll - while in the trenches at Armentiers, in northern France, he was blown up and spent a month in hospital suffering shell-shock. He deserted and was sentenced to nine months' hard labour and then sent back into the firing line before deserting again. He was found two months later with a French woman and her two children. Pte Spencer was executed on February 24, 1918 but was formally pardoned under the provisions of the Pardon for Soldiers of the Great War Act 2000, on grounds that his execution was not a fate he deserved. While both girls had some experience in public speaking, giving their speeches at the Anzac Day services in Queenstown and Arrowtown would be a different experience. ``I think it will definitely be challenging because I'm not naturally a public speaker ... it's nerve wracking because we just want to do it justice and take the right tone,'' Olivia said. ",https://www.odt.co.nz/regions/queenstown/pupil-speeches-honour-soldiers,"Leah's speech focused on Dunedin-born Sir Harold Delf Gillies who ... did it now, he'd get so [much recognition] ... but he was in the shadows.",Pupil speeches honour soldiers,Visit Wakatipu High School Leah Kissick Olivia Ray Anzac Day Queenstown Arrowtown Combined Queenstown Arrowtown RSA TRACEY ROXBURGH A New Zealand Wakatipu High School Olivia Ray Leah Kissick Combined Queenstown Arrowtown RSA First Battle Passchendaele October New Zealanders Leah Dunedin-born Sir Harold Delf Gillies Sir Harold Cambridge University United Kingdom WWI Royal Army Medical Corps Sidcup London Leah Private Victor Spencer Pte Spencer Bluff Southland Sarah Goomes August Gallipoli Otago Regiment New Zealand Division WWI Front July Otago Battalion Armentiers France Pte Spencer February Pardon Soldiers Great War Act Anzac Day Queenstown Arrowtown Olivia,2
138,"We may be familiar with Siri, Google Now, and Cortana as the mobile operating systems native voice-operated assistants, but Apple has gone a step further and published an API that allows developers to recognise speech and make use of it. iOS users are already used to Siri to interact with apps and dictate text, and now developers have direct access to that text. With great power comes great responsibility, as they say on the web. What are some new powers we are getting thanks to this technology, and what are some of the risks An evident advantage is that by combining the speech APIs with APIs like NSLinguisticTagger we have the tools we need to make our app understand a users intent. Rather than instructing the user to chose from a set of buttons they can tap on, we can let the user express their fine-grained wishes. The difference is similar to that between voice messages and text messages: the latter tends to contain much more information we can use to understand a person and a context. But, as is also the case with voice messages, we often have noise or too much information, and writing algorithms to filter through it is not trivial. Another caveat is that speech recognition is carried out on Apples servers, and therefore, as Apple themselves advise, you should instruct the user not to send any sensitive information (such as health data or passwords) to the cloud. That said, the risks are manageable. If you want to really listen to your users, put your code where your mouth is. ",https://www.sitepoint.com/editorial-put-code-mouth/,"Another caveat is that speech recognition is carried out on Apple's servers, and therefore, as Apple themselves advise, you should instruct the ...",Editorial: Put Your Code Where Your Mouth Is,Siri Google Cortana Apple API Siri APIs APIs NSLinguisticTagger user Apples Apple,4
139,"StreamingMedia.com provides this section as a service to its readers and customers. Press releases are subject to approval by the editorial staff of StreamingMedia.com and may be edited or altered for length and clarity, or to remove unsubstantiated and unverifiable claims. All content presented within the press release section is that of the submitter. StreamingMedia.com does not necessarily endorse such content and bears no responsibility or liability for its accuracy. Valossa AI Enables Real-Time Image and Emotion Detection; Enhanced Metadata Management and Movie Search Using Text and Voice Create New Revenue Engines for Content Owners Valossa, a leader in artificial intelligence (AI)-based video identification technology, today announced the release of its Valossa AI product suite, which is the industrys first complete AI video recognition platform designed for professional broadcast and event-based programming. Valossa AI is capable of analyzing video content in real-time to identify more than one thousand concepts (e.g., people, places, objects and themes) from any video stream. The Valossa technology reaches down into video content to identify it and make it searchable in real-timetargeted video-related advertising to building up next-generation personalized, relevant content delivery services. The Valossa AI suite will be demonstrated at the NAB 2017 show in Las Vegas, from April 24-27 in the Valossa booth #N2935SP-B in the Sprockit start-up demo area of the North Hall of the Las Vegas Convention Center. Valossa AI is comprised of Valossa Core API, a high-level metadata management tool; the Valossa Image Tagger AI, for image tagging; the Valossa Emotion SDK, for emotion and sentiment analysis, and the Valossa Movie Search tool, based on Valossas popular demo site, WhatIsMyMovie.com. Valossa AI has been tested globally by leading content producers and distributors, and has been benchmarked in tests that demonstrate superior performance to competing solutions from Google, Clarifai, and Microsoft, among others. Valossa built the first visual AI search skill for Alexa, and has developed a heart-beat detection tool that is part of Valossa AI. Valossa AI is capable of analyzing movies and videos in real-time to identify more than one thousand concepts (e.g., places, objects and themes) from any video stream. Based on prime research and essential patents from one of the worlds leading computer vision and AI labs at the University of Oulu in northern Finland, the Valossa AI technology can reach down into video content, identify it and make it searchable in real-time. Valossas product also includes a descriptive deep search engine that enables natural, verbose and flexible querying for voice-controlled movie services and entertainment platforms. The world has moved into a post-textual modality, with video playing a greater role in our daily communication, and we are applying technology to understand what we are viewing, as much as actually consuming contact, said Mika Rautiainen, founder and CEO of Valossa. There are many ways in which machine learning can determine specific details of visual content, and we are now beginning to apply the latest techniques to meet emerging needs of content producers globally. Valossa deploys a wide range of computer vision, machine learning and natural language processing (NLP) to extract deep content metadata from video, including frame analysis and multilingual keyword search from speech transcripts and metadata. Valossas approach delivers holistic video content descriptions at the scene level as well as descriptive keyword annotation for video overviews. We view AI as a significant part of the programmatic advertising industry going forward, and as brands realize the value of serving more contextually relevant content to audiences, solutions such as Valossa AI will deliver greater value, said Jacques Cazin, CEO of Adways, which provides a cloud-based platform for interactive video. The era of AI-driven interactive engagement is here, and we see Valossa as a major player in this emerging space. Valossa AI offers users a much richer set of analytics, expanding the contextual semantics of video data for what is the most holistic, multi-modal video analysis solution commercially available today. Valossa, the deep content company, is a leading provider of advanced solutions for AI-based visual content search and recognition. The Valossa technology represents years of prime research and essential patents from the University of Oulu in northern Finland. Valossa is headquartered in Oulu, and maintains offices in New York and San Francisco. For more information, please visit the web site at www.valossa.com . ",http://www.streamingmedia.com/PressRelease/Valossa-to-Release-Industrys-First-Complete-AI-Video-Recognition-Platform_44124.aspx,Valossa to Release Industry's First Complete AI Video Recognition ... and multilingual keyword search from speech transcripts and metadata.,Valossa to Release Industry's First Complete AI Video Recognition ...,StreamingMedia.com StreamingMedia.com StreamingMedia.com Valossa AI Enables Real-Time Image Emotion Detection Enhanced Metadata Management Movie Search Using Text Voice Create New Revenue Engines Content Owners Valossa AI Valossa AI AI Valossa AI Valossa Valossa AI NAB Las Vegas April Valossa N2935SP-B Sprockit North Hall Las Vegas Convention Center Valossa AI Valossa Core API Valossa Image Tagger AI Valossa Emotion SDK Valossa Movie Search Valossas WhatIsMyMovie.com Valossa AI Google Clarifai Microsoft Valossa AI Alexa Valossa AI Valossa AI AI University Oulu Finland Valossa AI Valossas Mika Rautiainen CEO Valossa Valossa NLP Valossas AI Valossa AI Jacques Cazin CEO Adways AI-driven Valossa AI Valossa Valossa University Oulu Finland Valossa Oulu New York San Francisco,3
140,"A new password verification technique expected to be adopted for financial transactions in the near future exploits the way our lips form the passwords that we choose, whilst providing a language-agnostic security platform. The innovation comes out of the Hong Kong Baptist Universitys Department of Computer Science, which has developed the lip motion password (LMP) as a semi-biometric solution to the increasingly acknowledged Mexican stand-off between the need for password complexity and rigorous policies, and the need for passwords which the user can remember and rely on. Developed by Professor Cheung Yiu-ming, the LMP provides authentication context to passwords, potentially radically reducing the need that they observe the rules (uppercase, lowercase, non-alphabetic characters, and so forth) that often frustrate users. Lip topology is read via built-in cameras in smartphones or other devices, and correlated to registered password wave-forms, so that congruence is expected between the correct phonic password and the way that the users lips sound it out. The underlying research for the technique employed not only a study of the way lip-shape changes during the formation of words, but also texture  though presumably chapped lips would not render a password invalid. The method addresses the growing problem of password recycling, adding a crucial biometric element that is, apparently, impossible to simulate. According to the research, the dynamic characteristics of lip movement is highly resistant to mimicry, and also reduces the problem of background noise  often an obstacle in using speech recognition as a biometric barrier. Concentrating on the mouth area as a biometric factor  particularly with the involvement of shape recognition as the lips deform  addresses also the difficulties of using face recognition as a bare biometric passport, and guards against the more easily duplicable methods of fingerprint recognition. The lip-reading technique also has the advantage of storing a password as a waveform sound, obviating the frequent difficulties which can occur in attempting to incorporate various text-language encodings (such as ANSI and UTF-8) into passwords, opening up the possibility for omnilingual passwords which are unbreakable  even if they are as guessable as the evergreen password1. ",https://thestack.com/security/2017/03/14/new-technique-authenticates-passwords-via-lip-reading/,"... resistant to mimicry, and also reduces the problem of background noise – often an obstacle in using speech recognition as a biometric barrier ...",New technique authenticates passwords via lip reading,Hong Kong Baptist Universitys Department Computer Science LMP Mexican Professor Cheung Yiu-ming LMP Lip ANSI UTF-8,9
141,"By using this website, you consent to our use of cookies. For more information on cookies see our Cookie Policy . It was feared ATMs would displace bank tellers. However, deployment of ATMs reduces bank running costs, allowing banks to open more branches Artificial intelligence is the simulation of human intelligence by machines. The term was coined in a 1955 research proposal stating that good progress could be made in research to get machines to solve the kinds of problems reserved for humans. This prediction was over-optimistic at the time and AI garnered a reputation for promising more than it could deliver. An exciting breakthrough was eventually made in 2012 and AI is now making rapid progress. However, many people fear that AI will cause massive unemployment. (The Economist published a comprehensive report on AI last June). AI is already all around us and expanding rapidly. AI is used for detecting credit card fraud, speech recognition (such as Apples Siri), translation ( Google Translate), legal discovery, photo search, self-driving cars, automated customer service and so on. The 2012 AI breakthrough was made in an online contest called ImageNet Challenge, a competition that assesses AI researchers success in getting computers to recognise and label images automatically. The 2011 winner correctly labelled 25 per cent of the images, but the 2012 winner correctly labelled 85 per cent of the images using a novel technique called deep learning. Further progress allowed the 2015 winner to correctly label 96 per cent of the images, surpassing the human ability level of 95 per cent. And, in May 2016, Googles AI program AlphaGo defeated champion Lee Sedol in the complex game of Go. The game had been thought too difficult for a computer to master but Googles AI programme learned to play by observing millions of games. Deep learning is making rapid progress in training artificial neural networks (ANN). ANNs are computer networks of artificial neurons (mathematical functions) arranged in layers, mimicking the human brain organisation of densely connected biological neurons (brain cells). Information supplied to the input layer is passed on to the next layer where the signals are combined, applying different weights to them and passing the results on to the next layer, and so on and on until the output layer is reached on the other side of the ANN. All the artificial neurons in a layer are connected to all neurons in the layers on either side. The deeper the ANN the greater the ability to recognise subtle information in the input data. Training the ANN involves adjusting its internal weights to give the desired response to particular inputs. A widely cited 2013 study by Carl Frey and Michael Osborne published by the Oxford Martin Programme on Technology and Employment estimates that 47 per cent of American jobs are at high risk of being computerised soon  including most workers in logistics and transport (such as taxi and delivery drivers), office support (such as receptionists), security guards, sales and services (such as cashiers, counter-clerks), legal discovery, accountancy, etc. Automation by AI will relatively spare high skill, non-routine occupations (such as architects and senior managers) on the one hand and unskilled workers (such as cleaners and burger-tossers) on the other hand. The most routine jobs are most at risk. In the following occupations the figure in brackets is the probability it will become automated, where 1.0 is certain: dentists (0.004), clergy (0.008), editors (0.06), actors (0.37), economists (0.43), machinists (0.56), typists (0.81), retail sales (0.92), accountants/auditors (0.94). Great fears were expressed at the start of the industrial revolution that machines would displace most workers from their jobs. However, automating a task, allowing it to be done faster and cheaper, increases the demand for human workers to do associated work that has not been automated. This happened in the weaving trade when machines arrived. A single worker could now run several machines. The amount of cloth produced per weaver per hour increased 50 fold, making cloth cheaper, increasing demand for cloth and creating more jobs. The number of weavers quadrupled between 1830 and 1900. The same thing is happening today. It was feared ATMs would displace bank tellers. However, deployment of ATMs reduces bank running costs, allowing banks to open more branches. Between 1988 and 2004, urban bank branch numbers increased by 43 per cent in the US and total employment increased. ATMs relieve employees of routine work freeing them for tasks machines cannot do, such as customer services and sales. We probably shouldnt worry much about being replaced by machines. ",http://www.irishtimes.com/news/science/fears-of-robots-stealing-our-jobs-are-misplaced-1.2995116,"AI is used for detecting credit card fraud, speech recognition (such as Apple's Siri), translation (Google Translate), legal discovery, photo search ...",Fears of robots stealing our jobs are misplaced,ATMs ATMs AI AI AI Economist AI June AI AI Apples Siri Google Translate AI ImageNet Challenge AI May Googles AI AlphaGo Lee Sedol Go Googles AI Deep ANN ANN ANN ANN Carl Frey Michael Osborne Oxford Martin Programme Technology Employment American AI ATMs ATMs US ATMs,4
142,"I have a bit of a love-hate relationship with Google Nows voice commands (or rather Google Assistant as it is now called). I love the idea of what it will be able to do one day, and I love what it can do when it works. If it works. And therein lies the hate. That delay. The unnatural half second pause between saying Ok Google and continuing your command. The half second pause that made most of my favorite custom activation ideas on the Moto X Play almost unbearable to actually use (I was a fan of Would You Kindly, Hello Moto, and Bridge to Engineering personally, but the pause made them all fall flat). That half-second pause is often just enough to break your speech pattern, and just enough to prevent you from speaking the sentence normally. It changes the command from Would you kindly play music by The Weeknd into Would you kindly  play music by The Weeknd. It forces you to hesitate, sometimes even throwing you off for the rest of the sentence (resulting in the occasional flubbed command). And being thrown off by a pause is a major issue for Google Assistant. Despite all the talk about natural language processing and being able to have a conversation with Google Assistant, they give you almost no space to correct yourself. If you dont have a perfectly canned and practiced line ready in your head, if you say the wrong word because you were thrown off by the pause, if you misspeak, if you make a common conversational error, if you do anything wrong, it can and will give you inaccurate results. My first instinct if I know it has a mistake is to go and try to correct it (I mean ), but you cant just talk back to it. You need to use the activation command first if youre making a correction (which even then only is sometimes successful), and I can honestly say that I have never remembered to do that. Now, that goes beyond just the delay, and Google claims that they are working on it, but Google Assistantstill has a long way to come. The pause wouldnt be as bad if it was consistent. One of the biggest issues with the pause isnt the pause itself, but rather how sometimes its quick, and sometimes its slow. If youre on a rock-solid network connection it can be fast enough for you to just keep talking through it right after your activation phrase, but if youre on a slow network oh boy. If youre on a slow network connection, you could be waiting a couple of seconds before it starts recognizing anything. An issue that is only exacerbated by the lack of a beep now. I fully understand why the beep was removed (to make speaking regular sentences possible, with the goal of real conversations with Google Assistant), but it isnt quite there yet. This could potentially be solved in the future by handling more of the transcription locally, but right now it is extremely frustrating. Using OK Google over Bluetooth is still a pain. Not only is the aforementioned lag still there, but you also run into additional lag from the Bluetooth connection itself, which varies from device to device. On some devices the lag is very low and OKGoogle can be used easily. On others, like my car, the lag is so long that if you attempt to use it while playing music, it will stop listening before the music cuts out and you hear the Google Now is ready beep (which is still around for Bluetooth connections). Im actually rather surprised that they dont use the beep on Bluetooth connections to help mark where the recording might begin (and to make sure it doesnt time out too early). It would be a relatively simple addition, and would go a long way towards making Bluetooth use of OKGoogle easier (especially since it would allow the phone to guarantee that the speakers had stopped playing music, reducing the amount of background noise). I also have some Bluetooth speakers that OK Google doesnt seem to work with at all (beyond the activation phrase), but I havent had the opportunity to test for what is causing that issue yet, so I cant really blame Google for it. Google has been trying to fix this issue for a while, and Google Home is part of their latest attempt. With Google Home, they are trying to compete with the Amazon Echo and their Alexa assistant (which is extremely fluid compared to Googles current implementation), and it was honestly looking pretty solid in the demo. Yes, it was a quiet room and the commands were relatively simple and they probably have a fantastic internet connection, but Rishi Chandra sounded relatively natural when interacting with Google Home. The pauses seemed like a part of his regular speech pattern, rather than something extra that he needed to account for. It honestly got me excited that maybe, just maybe, Google had fixed the hesitation issue. Unfortunately, then the ad spot was presented, and the illusion came crashing down. The actors didnt have Rishis calm cadence. They were speaking in their normal voices, and with that, the pauses rang out like a bell. It may have just been because of the contrast against Rishi, but the pauses were noticeable. They were enough to stop it from being a smooth sentence. They made it feel disjointed (and that was just from listening to the sentence, let alone speaking it). And thats before even getting into the fact that there was a big disclaimer on the bottom of the screen during the commercial. Sequences simulated and shortened, implying that the response time is actually even slower than what was shown. Google was speeding up the response time for the commercial (which is fine. thats standard practice), and it still felt too slow. I think it really needs to be stressed at this point that Google Homes main competitor, Amazon Echo, is extremely fluid in operation. The pause between the activation phrase and it starting to listen isnt just short, its almost unnoticeable . Amazon Echo is truly at the point where, at least from a pacing perspective, you can speak a natural sentence to it. Its not perfect, the Amazon Echo can definitely continue to see substantial improvement, but in this particular area they have a monumental lead over Google at the moment. Im still excited for Google Assistant, and I cant wait to see how the Internet of Things evolves. There is certainly impressive polishpresent inthe way Assistant (and Googles iconic voice in general) sounds and how humanly it carries itself by keeping track of conversations. But when it comes to actual operation, the users cannot interact neither as humanly nor as fluidly as they interact with other people. That is, I think, a key point Google needs to address to really sell artificial intelligence as more than an input-output Assistant. Do you have a home automation hub What do you think of Google Home Do you plan on buying one Let us know in the comments below! ",https://www.xda-developers.com/a-note-on-google-assistants-fluidity-and-speech-recognition/,"That half-second pause is often just enough to break your speech pattern, and just enough to prevent you from speaking the sentence normally.",A Note on Google Assistant's Fluidity and Speech Recognition,Google Nows Google Assistant Ok Google Moto X Play Would Hello Moto Bridge Engineering Would Weeknd Would Weeknd Google Assistant Google Assistant Google Google Assistantstill Google Assistant Google Bluetooth Bluetooth OKGoogle Google Bluetooth Im Bluetooth Bluetooth OKGoogle Bluetooth Google Google Google Google Home Google Home Amazon Echo Alexa Googles Rishi Chandra Google Home Google Rishis Rishi Google Google Homes Amazon Echo Amazon Echo Amazon Echo Google Im Google Assistant Internet Things Assistant Googles Google Assistant Google Home Do Let,4
143,"Skype for Business is getting a few more enterprise features, likely in response to the likes of Amazon and Google who are both ramping up and changing their enterprise comms applications. Among the new Skype for Business features are an app for Mac users and new features including Call Queues and Auto Attendant, as well as a preview of Skype for Business Online Call Analytics Call Queues enable incoming calls to be routed to the next available live attendant and Auto Attendant provides an automated system to answer and route inbound calls using dial pad inputs and speech recognition - basically IVR functions that traditional PBX systems have been delivering for years. ",https://www.lifehacker.com.au/2017/03/skype-for-business-gets-business-class-upgrade/,... calls using dial pad inputs and speech recognition - basically IVR functions that traditional PBX systems have been delivering for years.,Skype For Business Gets Business Class Upgrade,Skype Business Amazon Google Skype Business Mac Call Queues Auto Attendant Skype Business Online Call Analytics Call Queues Auto Attendant IVR PBX,-1
144,"Bringing the power of IoT and AI solutions to energy, industry and          society Toshiba's booth image at CeBIT 2017 (Graphic: Business Wire) TOKYO--( BUSINESS WIRE )-- Toshiba        Corporation (TOKYO:6502) will be at CeBIT 2017, and looking to the        future to demonstrate how its extensive know-how IoT and AI will contribute to solutions in energy, industry and society. CeBIT, the        worlds largest exhibition for digital business, will take place in        Hannover, Germany over March 20 to March 24, and Toshiba will be in the        Japan Pavilion at Hall 4, A38, (68), showcasing essential technologies        under the theme of Create a New World with Japan - Society 5.0 Another        Perspective -. The main exhibit will spotlight        SPINEX, a Toshiba-developed IoT architecture that fuses know-how the        company has cultivated in its industrial businesses over the long term.        SPINEX has three main features: Edge Computing that intelligently        performs data processing to improve time to action; Digital Twin that        digitally reproduces the actual operating status of things in real time        for accurate emulation and simulation; and Media Intelligence that        integrates image and speech recognition technologies for recognizing        human intentions. Toshiba will also highlight how its        longstanding experience in power generation and infrastructure systems        contributes to products and solutions that apply IoT technology in such        areas as remote monitoring and control. This will include the virtual        power plants demonstration initiative, a demonstration of how bringing        Toshibas highly        reliable rechargeable batteries, SCiB into the grid can realize        advanced energy management capabilities, including demand response and        integration of renewable energy sources. Toshiba fully understands        power of IoT and AI        in the industrial field such as applying deep learning to clearing        bottlenecks and error-checking at Yokkaichi Operations, Toshibas        world-leading facility for NAND flash memories. Another display will        show how the first deep learning testbed approved by the Industrial        Internet Community is improving facility management at Toshibas Smart        Community Center in Kawasaki, Japan. Recognizing that deep        learning has potential in many areas, Toshiba has introduced it to a media-intelligence image and speech recognition system for application in sports. This        technology can track the ball and players in both teams and        automatically detect and track their movements, using only from video        data, without any need for specific camera and sensors. It can be used        live, during games, to assist developing offensive and defensive        strategy. Toshiba further plans to fine its analyzing technology. For more information on Toshiba IoT architecture SPINEX, please visit ",http://www.businesswire.com/news/home/20170316006454/en/Toshiba-Showcases-IoT-Solutions-CeBIT-2017,"Recognizing that deep learning has potential in many areas, Toshiba has introduced it to a media-intelligence image and speech recognition ...",Toshiba Showcases IoT Solutions at CeBIT 2017,IoT AI Toshiba CeBIT Graphic Business Wire TOKYO BUSINESS WIRE Toshiba Corporation TOKYO:6502 CeBIT IoT AI CeBIT Hannover Germany March March Toshiba Japan Pavilion Hall A38 Create New World Japan Perspective SPINEX IoT SPINEX Edge Computing Digital Twin Media Intelligence Toshiba IoT Toshibas SCiB Toshiba IoT AI Yokkaichi Operations Toshibas NAND Industrial Internet Community Toshibas Smart Community Center Kawasaki Japan Toshiba Toshiba Toshiba IoT SPINEX,-1
145,"It's like iMovie for the Snapchat generation, with speech-to-text transcription and facial recognition. Your message has been sent. Head on over to the App StoreApple launched its new Clips video-editing app for iOS on Thursday.Clipsis essentially a streamlined version of iMovie for creating short mobile videos with filters and text overlays. The app also lets you stitch together several clips from your camera rollwithout timelines, tracks, or complicated editing tools. Like other mobile video editing apps, users will be able to record video or take photos from within the app, and then stylize them with text, filters, speech bubbles, and emoji. You can also add elevator music as a soundtrack and create animated backdrops. The standout feature in Clips is Live Titles, a speech-to-text transcription so you can dictate what words you want overlaid, either as titles or animated captions. After youre done dictating, you can edit the transcription to add punctuation marks and (more!) emoji. According to Apple,Live Titles supports 36 different languages. After youve finished, you can export your video creation to share on other platforms, including Facebook, Instagram, YouTube, Vimeo, and iMessage.Clips also has a face recognition component, so the app will suggest iMessage contacts for you to share your video with based on who appears in it. Clips has been obviously inspired by Snapchat, Instagram, and even Vineall mobile apps that helped fuel stylized video-sharing. But unlike Snapchat and Instagram, Clips does not have a social network built around its community of usersinstead, the videos you create on Clips are intended for external distribution. In this sense, Clips may be more alike Instagrams Boomerang or the new redesigned Vine Camera after it got rid of the social component to become a strictly functional capturing and editing app. Clips is free, but it only works with iOS 10.3 devices, including the iPhone 5s and up, the new 9.7-inch iPad, all iPad Air and iPad Pro models, the iPad mini 2 and up, and the sixth-generation iPod Touch.Stay tuned for our full hands-on review of Clips to see if we think this appand its facial recognition capabilityis creepy or cool. To comment on this article and other Macworld content, visit our Facebook page or our Twitter feed. ",http://www.macworld.com/article/3188036/ios/apples-new-clips-app-for-making-stylized-videos-is-available-now.html,"... Snapchat generation, with speech-to-text transcription and facial recognition. ... Clips also has a face recognition component, so the app will ...",Apple's new Clips app for making stylized videos is available now,Snapchat App StoreApple Clips Thursday.Clipsis Clips Titles Apple Live Titles Facebook Instagram YouTube Vimeo Clips Snapchat Instagram Vineall Snapchat Instagram Clips Clips Instagrams Boomerang Vine Clips Air Pro Touch.Stay Clips Macworld Facebook Twitter,4
146,"As commercial speech recognition systems are derived largely from adult speech, Disney researchers have developed a new speech technology system that could make playing certain video games or interactions with robots more fun for kids.The new technology sorts through the overlapping speech, social side talk and creative pronunciations of young children to help them better control a video game called Mole Madness that require kids to say say just two words  jump and go. The multi-keyword-spotting system, developed by Disney Research, could make it possible to design any number of speech-based game or entertainment applications for children, including interactions with robots, the study said.Speech recognition applications have become increasingly commonplace as the technology has matured, but understanding what kids say when they play remains difficult, said Jessica Hodgins, vice president at Disney Research  an international network of research labs in US. A screenshot from mole madness. Image: Disney Research Kids dont necessarily pronounce words quite like adults and when they are playing together, as they like to do, they often engage in side banter, or exclamations of excitement, or simply talk over each other, Lehman said.In the cooperative Mole Madness  a two-player video game, kids needed to say just two words  jump and go while moving an animated mole through its environment, gathering rewards as they avoid obstacles. During game play, the players often say their commands simultaneously. In other cases, they make statements to each other, such as Dont say go yet, that can be misinterpreted by the system.This technology can be reproduced with other vocabulary, allowing designers and developers to build novel childrens applications that use limited speech as an input method, Lehman said.The findings were presented at the Workshop on Child Computer Interaction in San Francisco. ",http://tech.firstpost.com/news-analysis/disney-develops-speech-recognition-software-for-very-young-children-334012.html,"As commercial speech recognition systems are derived largely from adult speech, Disney researchers have developed a new speech ...",Disney develops speech recognition software for very young children,Disney Mole Madness Disney Research Jessica Hodgins Disney Research US Disney Research Kids Lehman Mole Madness Dont Lehman Workshop Child Computer Interaction San Francisco,3
147,"The researchers hypothesized that speech recognition, which has come a long way in recent times thanks to advancements in big data analysis and deep learning, is more accurate and faster than humans typing on keyboards. They tested this in a study with 38 participants, who used Baidus Deep Speech 2 system, as well as standard software keyboards on the iPhone . The test subjects performance in dictating and typing 120 phrases each in English and Mandarin was monitored and compared. The study revealed that not only was speech recognition 3x faster than typing in English and 2.8x faster in Mandarin, but the error rates were 20.4 percent and 63.4 percent lower respectively. Not bad, huh Those figures are significant enough to convince me to try using voice commands and dictations tools more often. Of course, Its worth noting that this test was conducted using Baidus latest speech recognition system  so your mileage with Google Now, Siri and Cortana will certainly vary. Deep Speech 2 powers Baidus Duer personal assistant app and has found favor among Mandarin speaker because typing in that language is time-consuming and because not everyone is familiar with the Pinyin phonetic system used for transcribing on software keyboards. Do you use speech recognition often Whats your experience with it been lately Let us know in the comments. ",https://thenextweb.com/insider/2016/08/26/stanford-speech-recognition-study-suggests-you-should-try-using-dictation-apps-more-often/,"According to the paper (PDF), which was published earlier this week, speech recognition tools have been found to be three times faster than ...",Stanford speech recognition study suggests you should give ...,Baidus Deep Speech English Mandarin English Mandarin Baidus Google Siri Cortana Deep Speech Baidus Duer Mandarin Pinyin Whats,7
148,"IBM's early work on speech recognition. IBM reached a new milestone in speech recognition this week. The company unveiled that it has reached an industry record of 5.5% word error rate by using the Switchboard linguistic corpus, which brings IBM closer to the considered human error rate of 5.1%. IBM was able to accomplish this by extending its deep learning technologies and incorporating an acoustic model that learns from positive examples while taking advantage of negative ones. Essentially, it gets smarter and performs better when similar speech patterns are repeated. We will continue to work toward creating the technology that will one day match the complexity of how the human ear, voice and brain interact, wrote George Saon, principal research scientist at IBM, in a blog post . While we are energized by our progress, our work is dependent on further researchand most importantly, staying accountable to the highest standards of accuracy possible. According to several reports, Google is planning to acquire Kaggle, a coding competition and data science platform. According to a Business Insider report by Sam Shead, Kaggle allows developers and data scientists to run machine learning contests, host datasets, and to write and share code. Right now, Google didnt comment on the plan to acquire Kaggle, but Kaggle cofounder and CEO Anthony Goldbloom didnt deny the acquisition while on a call with TechCrunch. The Open API Initiative is continuing its mission to create a vendor-neutral API description. The organization has announced the release of the OpenAPI specification version 3 Implementers Draft. This release features improved documentation as well as resolves structural changes, and it clarifies some issues and concerns. Notable changes include finalized support for URL templating, enhanced callbacks expression language, and updated schema object. The specification is now feature-complete. More information is available here . ",http://sdtimes.com/ibm-reaches-new-record-speech-recognition-google-may-acquire-kaggle-open-api-releases-draft-oas-sd-times-news-digest-march-8-2017/,IBM reached a new milestone in speech recognition this week. The company unveiled that it has reached an industry record of 5.5% word error ...,"IBM reaches new record in speech recognition, Google may acquire ...",IBM IBM Switchboard IBM IBM George Saon IBM Google Kaggle Business Insider Sam Shead Kaggle Google Kaggle Kaggle CEO Anthony Goldbloom TechCrunch API Initiative API OpenAPI Implementers Draft URL,-1
149,"Learning a new language is an amazing skill, and research has shown that the brains of those who speak two or more languages operate differently to single language speakers. Because of this (and the added benefits of being able to order a coffee in another country), a lot of us want to learn a new language as adults, but struggle with conventional programs, or aren't sure of where to start. Rosetta Stone Language Learning is a different way to learn a language - instead of translations between your native language and the one you want to learn, the software uses Dynamic Immersion to teach you. It teaches you in the same way that you originally learnt your native language as a child  starting by matching words with images. When you've mastered that, the program moves on to interactive lessons using speech recognition software. We've teamed up with Stack Commerce to offer you the software at a reduced cost  instead of US$249, you can start learning and mastering a new language for only US$149 . But this deal won't last forever, so get started now . This is a promotional ScienceAlert Academy post, in partnership with StackCommerce. We carefully vet all courses and products to make sure they're relevant to our readers, and make a share in the profits of any sales. Nanoparticles can be pushed by pure nothingness. Say hello to your new ancestors. The moment that changed spaceflight forever. ",http://www.sciencealert.com/sciencealert-deal-learn-a-new-language-fast-with-this-award-winning-software,"When you've mastered that, the program moves on to interactive lessons using speech recognition software. It was the PC Magazine Editor's ...",ScienceAlert Deal: Learn a New Language With This Award ...,Rosetta Stone Language Learning Dynamic Immersion Stack Commerce US US ScienceAlert Academy StackCommerce Say,-1
150,"Despite the fact that Google has developed its own custom machine learning chips, the company is well-known as a user of GPUs internally, particularly for its deep learning efforts, in addition to offering GPUs in its cloud. At last years Nvidia GPU Technology Conference, Jeff Dean, Senior Google Fellow offered a vivid description of how the search giant has deployed GPUs for a large number of workloads, many centered around speech recognition and language-oriented research projects as well as various computer vision efforts . What was clear from Deans talkand from watching other deep learning shops with large GPU cluster countsis that the real innovation is around tightening deep learning algorithms and frameworks to hum on GPU clusters. The Google Brain team, which focuses on many of the areas cited above, is working on software tuning to keep pushing the limits of GPU backed machine learning. Most recently, a group there has put together a detailed analysis of architectural hyperparameters for neural machine translation systems, an effort that required more than 250,000 GPU hours on their in-house cluster, which is based on Nvidia Tesla K40m and Tesla K80 GPUs, distributed over 8 parallel workers and 6 parameter servers. New work will help push current hardware and software approaches to the NMT problem beyond current limitations. One shortcoming of current NMT architectures is the amount of compute required to train them. Training on real-world datasets of several million examples typically requires dozens of GPUs and convergence time is on the order of days to weeks. While sweeping across large hyper-parameter spaces is common in computer vision, such exploration would be prohibitively expensive for NMT models, limiting researchers to well-established architectures and hyper-parameter choices. Even though GPUs are best suited to the NMT task for the Google Brain team, making the models more comprehensive and richer requires co-design and optimization, something the researchers focused on during the architectural analysis. This analysis, while focused on architecture, goes far beyond hardware. The architectural conditions that provided the best performance have been packaged up and provided as an open source software framework based on TensorFlow to allow reproducible sequence-to-sequence model deployments. In the new work, the team showed how small changes in the values of hyper-parameters and different initializations can affect results. We purposely built this framework to enable reproducible state of the art implementations of Neural Machine Translation architectures. As part of our contribution we are releasing the framework and all configuration files needed to reproduce our results, the Google Brain researchers explain. The code, found on GitHub , is a modular software framework that allows researchers to explore novel architectures with minimal code changes, and define experimental parameters in a reproducible manner. They note that while they have achieved good results with this approach, the same framework can extend to other problems, including summarization, conversational modeling and image-to-text. On the WMT14 English-to-French and English-to-German benchmarks, Googles Neural Machine Translation achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Googles phrase-based production system. Googles internal neural machine translation work was made public at the end of 2016 and is the driving neural network force behind Google Translate. Using millions of training examples, the translation service is able to pick up on nuances that go far beyond simply providing word-by-word literal translations, grabbing semantics of full sentences. This is similar to the services Chinese search giant Baidu provides and like Google, GPUs are at the heart of much of the research work there, as we have described in depth in the past. ",https://www.nextplatform.com/2017/03/20/google-team-refines-gpu-powered-neural-machine-translation/,"... of workloads, many centered around speech recognition and language-oriented research projects as well as various computer vision efforts.",Google Team Refines GPU Powered Neural Machine Translation,Google GPUs GPUs Nvidia GPU Technology Conference Jeff Dean Senior Google Fellow GPUs Deans GPU GPU Google Brain GPU GPU Nvidia Tesla K40m Tesla K80 GPUs New NMT NMT GPUs NMT GPUs NMT Google Brain TensorFlow Neural Machine Translation Google Brain GitHub WMT14 Googles Neural Machine Translation Googles Googles Google Translate Baidu Google GPUs,4
151,"On April 5, Jana Kramer was honored by Safe Horizon, an agency for victims of domestic violence, during their 2017 Champion Awards ceremony. Kramer received their Voice of Empowerment Award, in recognition of her efforts to speak out against domestic violence, and gave a powerful, emotional speech, which readers can watch in full above. In October, Kramer revealed that her first ex-husband, Michael Gambino, was physically and emotionally abusive to her , even once nearly killing her. Kramer admitted at the time that she stayed married to Gambino because she was ashamed and terrified to tell anybody. During her Champion Awards speech, Kramer recounted Gambinos abuse and how it affected her professionally and personally. I thought that if I shared my story, that people would view me as broken  [so] I held it in for 14 years, Kramer told the audience. It kept getting worse, and I didnt know how to get out  I was a girl full of light, energy and charm, and this man just broke me down to absolutely nothing. Following her marriage to Gambino, Kramerwas married to actor Johnathon Schaech and thenwas engaged to Brantley Gilbert . She married Michael Caussin in 2015, and the couplewelcomed their first child, daughter Jolie Rae Caussin , in January of 2016; however, they are currently separated . Before accepting her award from Safe Horizon, Kramer visited one of the organizations shelters, where she met some of the children staying there. During her speech, Kramer noted that to go in there and see these beautiful children being protected means the absolute world to me as a mother. The 22nd annual Champion Awards ceremony and gala raised $1.4 million to support Safe Horizons programs, a press release reports . ",http://theboot.com/jana-kramer-speech-about-domestic-violence-safe-horizon-champion-awards/,"Kramer received their Voice of Empowerment Award, in recognition of ... and gave a powerful, emotional speech, which readers can watch in ...",Watch Jana Kramer's Powerful Speech About Domestic Violence at ...,April Jana Kramer Safe Horizon Champion Awards Kramer Empowerment Award October Kramer Michael Gambino Kramer Gambino Champion Awards Kramer Gambinos [ Kramer Gambino Kramerwas Johnathon Schaech Brantley Gilbert Michael Caussin Jolie Rae Caussin January Safe Horizon Kramer Kramer Champion Awards Safe Horizons,-1
152,"The business of voice control: Mastering Alexa The business of voice control: Mastering Amazon Alexa This year has started with a loud and clear message: voice is the next big thing in home automation control. At CES and ISE we have seen an explosion of companies showing voice operated solutions, creating interfaces, and opening up the possibilities that voice command can bring to our daily lives. At ISE 60 per cent of manufacturers showed integrations with Alexa, Amazon Echos sophisticated voice control platform. Recently, CEDIA teamed up with Amazon to organize a fascinating talk addressing the use of voice commands to manage and operate devices within the home and how custom installers can do their own programming using Amazon Smart Home Skill Kit. Naturally, Inside CI was there... Dean Bryen, solutions architect at Amazon and Alexa and Echo evangelist, opened his Mastering the Skills of Voice Control in the Home presentation quoting AI visionary Tim Tuttle: Soon devices wont have keyboards. Hands free, voice operated systems are not new. The first time the term voice technology intelligence emerged, was in the 1950s. In 2007, a CNN business article reported that voice command was over a billion-dollar industry and that companies like Google and Apple were trying to create speech recognition features. Apple introduced Siri in 2010, and Amazon Echo and Google Home followed suit. Two main factors are making voice control more appealing: the increased accuracy of Automatic Speech Recognition (ASR) and advanced machine learning. Both of these have vastly improved in the last four to five years, according to Bryen. The difference between 95 per cent and 99 per cent accuracy when it comes to voice is amazing, said Bryen and added, In 2014, voice search traffic was negligible. Today it exceeds 10 per cent of all search traffic. Siri, Google and Cortana exceeded 50bl voice searches per month. By 2018 30 per cent of interaction with smart machines will be through conversation. By 2020, over 200 billion searches per month will be done with voice. Voice commands are by nature conversational. People using a voice command will have their own accents, ways of calling things, and, there might also be a wide range of interlocutors of different ages that will want to use it. Installers will require intuitive innovation and a sense of playfulness to create Voice user Interfaces (VUI) that, literally, resonate with their clients. Conversational use of voice commands will mean that, instead of asking for something to be switched on or off, a user will say Good morning, or Im Home, or Im babysitting, or Good night and this will be linked to a set of actions such as drawing the curtains and starts the coffee, turns on the lights and the TV, etc, creating the desired scene. Bryen admits that Amazons Echo still faces some challenges, such as security issues, hierarchies differentiation for users, and location and contextual awareness. When Alexa is in a room, it would be easy to get a bit of spy paranoia, admits Bryen, but Amazon has earner the trust of its customer before and it will do it again. In 1995, Amazon asked its customers to add their credit card details to its platform and, although some people might not have felt at ease to start with, today is normal. Alexa will only save information on the cloud after its name is mentioned. Voice commands, using Natural Language Use to recognise the intent, are sorted in the cloud. All consequent machine learning is anonymized. The process between the voicing of the command and it being carried out can take 0.5 seconds and it only requires a 16GB broadband capacity, so any home in the UK could have Alexa. Bryen painted a very favourable picture for integrators and the possibilities that VUI (voice user interface) customisation can bring. Studies have shown that 50 per cent of consumers say they plan to buy at least one smart home product in the next year. Although 60 per cent of consumers self- install smart home devices, the majority of device owners said that they would prefer professional assistance. Integrators that are able to develop a good VUI will also reap the benefits of being able to provide a better customer service and maintenance. Updates on the cloud make the added value of the service provided by the installer a reality as well as it opens up for hardware upgrades and better margins. How can it be done By using Amazons Smart Home Skill API. The skill kit requires a bit of programming knowledge but its not too dissimilar to HTML language. To carry out your own VUI youll need: an Amazon Developer Account, an AWS Account and knowledge of Java, Node.js, or Python. Skills are made up of two components Skill configuration in the Amazon Developer Portal and Your skill code, hosted in AWS Lambda and the developers own HTTPS endpoint. The language used is called JSON and a line of code will look something like this: Very simplified, the elements on the programming narration will include: invocation names or launch request (i.e. Alexa), actions, custom slots (which identify an argument within an intent), and utterances. The system seemed simple enough to try it out! Matt Nimmons, Managing Director of CEDIA EMEA concluded, Voice control is one of the greatest technological advancements our industry has seen in recent years and is fast becoming common place in the home. It is imperative for home technology professionals to be knowledgeable on how to integrate voice control into the home and how to use and promote this technology to ensure it is a profitable business venture. As one of the leading voice control platforms in todays market, we are delighted that Amazon has chosen to host this event, exclusive for CEDIA, in order to help professionals get the most out of this ongoing phenomenon. ",http://www.insideci.co.uk/articles/the-business-of-voice-control-mastering-alexa.aspx,Two main factors are making voice control more appealing: the increased accuracy of Automatic Speech Recognition (ASR) and advanced ...,The business of voice control: Mastering Amazon Alexa,Mastering Alexa Mastering Amazon Alexa CES ISE ISE Alexa Amazon Echos CEDIA Amazon Amazon Smart Home Skill Kit Inside CI Dean Bryen Amazon Alexa Echo Skills Voice Control Home AI Tim Tuttle Hands CNN Google Apple Apple Siri Amazon Echo Google Home Automatic ASR Bryen Bryen Siri Google Cortana Voice Voice Interfaces VUI Conversational Im Home Im Bryen Amazons Echo Alexa Bryen Amazon Amazon Alexa Voice Natural Language Use UK Alexa Bryen VUI VUI Amazons Smart Home Skill API HTML VUI Amazon Developer Account AWS Account Java Node.js Python Skill Amazon Developer Portal Your AWS Lambda HTTPS JSON Alexa Matt Nimmons Managing Director CEDIA EMEA Voice Amazon CEDIA,-1
153,"In our series 6sqft Studio Visits , we take you behind the scenes of the citys up-and-coming and top designers, artists, and entrepreneursto give you a peek into the minds, and spaces, of NYCs creative force. In this installment we take a tour of the Bed-Stuyurban farm Square Roots. Want to see your studio featured here, or want to nominate a friend Get in touch! In a Bed-Stuy parking lot, across from the Marcy Houses (youll know this as Jay-Zs childhood home) and behind the hulking Pfizer Building, is an urban farming accelerator thats collectively producing the equivalent of a 20-acre farm. An assuming eye may see merely a collection of 10 shipping containers, but inside each of these is a hydroponic, climate-controlled farmgrowing GMO-free, spray-free, greensreal food, as Square Roots calls it.The incubatoropened just this past November, a responseby co-founders Kimbal Musk (Yes, Elon s brother) and Tobias Peggs against the industrial food system as a way to bring local food to urban settings. Each vertical farm is run by its own entrepreneur who runs his or her own sustainable business, selling directly to consumers. 6sqft recently visited Square Roots, went insideentrepreneur Paul Philpott s farm, and chatted with Tobias about the evolution of the company, its larger goals, and how food culture is changing. Tell us how you got interested in and involved with the urban agriculture movement And how did you and Kimbal start Square Roots I came to the U.S. from my native UK in 2003 to run U.S. operations for a UK-based Speech Recognition software company (i.e. a tech startup). I have a PhD in AI and have always been in tech. Through tech, I first met Kimbal Muskhes on the board of companies like SpaceX and Teslawho at the time was setting up a new social media analytics tech company called OneRiot, which I joined him on in 2006. Since then, Kimbals been working on a mission to bring real food to everyone. Even while I was working with him in tech, he had a restaurant called The Kitchen in Boulder, Colorado that sourced food from local farmers and made farm-to-table accessible in terms of menu and price point. His journey in real food started in the late 90s, when he sold his first tech company, Zip2, andmoved to NYC and trained to become a chef, his real passion. When9/11 happened he cooked for firefighters at Ground Zero. It was during that time  where people would come together around a freshly cooked meal  that he began to see the power of real food and its ability to strengthen communities, even in the most awful conditions imaginable. In 2009, while we were both working at OneRiot,Kimbal had a skiing accident and broke his neck. Realizing life can be short, he decided to focus on this idea of bringing real food to everyone. So he left OneRiot to focus on The Kitchen, which is now a family of restaurants across Chicago, Boulder, Denver, Memphis, and more. That organization ploughs millions of dollars into local food economies across the country by sourcing food from local farmers and giving its customers access to healthy, nutritious food. They also run a nonprofit, The Kitchen Community ,thats built hundreds of learning gardens in schools across the country, serving almost 200,000 school children a day. After Kimbals accident, I became CEO of OneRiot, which was acquired by Walmart in 2011, where I ended up running mobile commerce for international markets. I learned a lot about the industrial food system there byworking with huge data sets of the groceries people were buying across the globe and researching where those foods were being grown. I began to visualize food being shipped across the world, thousands of miles, before consumers bought it. Its well known that the average apple you buy in a supermarket has been traveling for ninemonths and is coated in wax. You think youre making a healthy choice, but the nutrients have all broken down and youre basically eating a ball of sugar. That is industrial food. I left Walmart a year later and became CEO of an NYC photo editing software startup called Aviary, but I couldnt get this map of the industrial food system out of my head. When Aviary was acquired by Adobe in 2014, I re-joined Kimbal at the Kitchen and we started developing the idea for Square Roots. What we saw was that millions of people, especially those in our biggest cities, were at the mercy of industrial food. This is high calorie, low nutrient food, shipped in from thousands of miles away. It leaves people disconnected from their food and the people who grow it. And the results are awful  from childhood obesity to adult diabetes, to a total loss of community around food. (Not to mentioned environmental factors like chemical fertilizers and greenhouse gases.) We also saw thatthese people were losing trust in the industrial food system and wanted what we call real food. Essentially, this is local food where you know your farmer. (This isnt just a Brooklyn hipster foodie thing. Organic food has come from nowhere to be a $40billion industry in the last decade. Local is the food industrys fastest growing sector.) Meanwhile, the worlds population is growing and urbanizing quickly. By 2050 there will be nine billionpeople on the planet, and 70percent will live in cities.So if we have more people living in the city, demanding local food, the only conclusion you can draw is that weve got to figure out how to grow real food in the city, at scale, as quickly as possible. In many ways NYC is a template for what that future world will look like. So our thinking was: if we can figure out a solution in NYC, then it will be a solution for the rest of the world as it increasingly begins to look like NYC. The industrial food system is not going to solve this problem. Instead, this presents an extraordinary opportunity for a new generation of entrepreneurs  those who understand urban agriculture, community, and the power of real, local food. Kimbal and I believe that this opportunity is bigger than the internet was when we started our careers 20 years ago. So we set up Square Roots as a platform to empower the next generation to become entrepreneurial leaders in this real food revolution.At Square Roots, we build campuses of urban farms located in the middle of our biggest cities. The first campus is in Brooklyn and has 10 modular, indoor, controlled climate farms that can grow spray-free, GMO-free, nutritious, tasty greens all year round. On those farms, we coach young passionate people to grow real food, to sell real food, and to become real food entrepreneurs. Square Roots entrepreneurs are surrounded and supported by our team and about 120 mentors with expertise in farming, marketing, finance, and sellingbasically everything you need to become a sustainable, thriving business. Why did you choose to set up at Bed-StuysPfizer Building We believe in strengthening community through food, and hopefully by joining forces with all the awesome local food companies already in Pfizer, were doing our part towards that. Next,in the lead up the first World War, that factory was the U.S.s largest manufacturer of ammonia, which at the time was used for explosives. Post war, the U.S. had excessive amounts of ammonia, and it started being used as fertilizer. So in many ways, that building is the birth place of industrial food. I like the act of poetic justice that we now have a local farm on the parking lot. Paul Philpotts company is called Gateway Greens . His membership-based business model is that his members pay a premium to subsidize food for low-income New Yorkers. He grows oregano, parsley, sage, thyme, and cilantro, as well as swiss chard, collard greens, and kale. You received more than 500 entrepreneur applications; how did you narrow it down to just 10 Lots of late nights watching video applications! We were looking for people with shared values and mission  a belief in the power of real, local food. And we needed to see a passion for entrepreneurship. Being an entrepreneur inSquare Roots is hard and we needed to make sure that first 10 were coming in with eyes wide open. They are really kicking ass now! Each farm can produce 50,000 mini-heads of lettuce per year. The greens are grown hydroponically, meaning the nutrients are mixed with the water that feeds the roots,since the system is soil-free and uses LED lights. Each farm uses about 10 gallons of water a dayless than a typical shower. For someone whos not familiar with this type of technology, can you give us a basic rundown of how it works and compares with traditional farming The first thing weve got to do is build farms in the middle of the city. In Bushwick, these are modular, indoor, controlled climate, farms. You can put them in the neighborhood right next to the people whoare going to eat the food. To set this up, we literally rent spaces in a parking lot and drop the farms in there. Its scrappy, but they enable year-round growing and support the annual yield equivalent of two acres of outdoor farmland inside a climate-controlled container with a footprint of barely 320 square feet. These systems also use 80 percent less water than outdoor farms. Thats the potential for a lot of real food grown in a very small space using very few resources. Each of our ten farms is capable of growing about 50 poundsof produce per week. Most of that today goes to customers of the Farm to Local program,where a local farmer will deliver freshly harvested greens direct to your office(people love having a farmer show up at their desk with freshly harvest greens right before lunch!) Some of the farmers sell to local restaurants also. Why do you think consumers in general respond so well to this type of local farming This generation of consumers want food you can trust, and when you know your farmer, you trust the food. There are so many layers between the farmer and the consumer in industrial foodagents, manufacturers, wholesalers, retailers, the list goes on. And every one of them takes their cut, leaving the farmer with paper thin margins and the consumer with no connection to the food or the people who grow it. Thats 20th century food, where it takes weeks to get to you andthe food has to be grown to travel. Square Roots farmers can harvest and deliver within hours  meaning food is grown for taste and nutrition. Moving forward, how do you hope urban farming will coincide with more traditional agriculture The consumer wants local food where they know the farmer and the food tastes great. Whether thats grown on a no-till organic soil farm or in a container on a parking lot, if its local food its food you can trust  and were all on the same side. The common enemy here is industrial food. Where do you hope Square Roots will be in a year from now What about 10 years We grow a ton of food in the middle of the city and sell locally. So we see revenue from direct-to-consumer food sales and were building a very valuable local food brand. But as we replicate campuses and our program to new cities, were building that local food brand at a national and then ultimately global scale. At the same time, our model unleashes an army of new real food entrepreneurs who will graduate from Square Roots and start their own amazing businesses, who we will invest in. Ive been quoted on this before, but Id like to think I can open Fortune Magazine in 2050 and see a list of Top 100 Food Companies in the world, which includes Square Roots and 99 others that have been started by graduates of Square Roots, who all share our same values. That would mean were truly bring real food to everyone. All photos taken by James and Karla Murray exclusively for 6sqft. Photos are not to be reproduced without written permission from 6sqft . ",https://www.6sqft.com/studio-visits-go-inside-square-roots-futuristic-shipping-container-farm-in-bed-stuy/,I came to the U.S. from my native UK in 2003 to run U.S. operations for a UK-based Speech Recognition software company (i.e. a tech startup).,Studio Visits: Go inside Square Roots' futuristic shipping container ...,Studio Visits NYCs Bed-Stuyurban Square Roots Get Marcy Houses Jay-Zs Pfizer Building Square Roots November Kimbal Musk Elon Tobias Peggs Square Roots Paul Philpott Tobias Kimbal Square Roots U.S. UK U.S. PhD AI Kimbal Muskhes SpaceX Teslawho OneRiot Kimbals Kitchen Boulder Colorado Zip2 NYC When9/11 Ground Zero OneRiot Kimbal OneRiot Kitchen Chicago Boulder Denver Memphis Kitchen Community Kimbals CEO OneRiot Walmart CEO NYC Aviary Aviary Adobe Kimbal Kitchen Square Roots Brooklyn Local NYC NYC NYC Kimbal Square Roots Square Roots Brooklyn GMO-free Square Roots Bed-StuysPfizer Building Pfizer World War U.S.s Post U.S. Paul Philpotts Gateway Greens New Yorkers Roots LED Bushwick Thats Farm Local Square Roots Whether Square Roots Square Roots Ive Id Fortune Magazine Square Roots Square Roots James Karla Murray,10
154,"Covering future technology and cutting-edge innovations like new drone technology, artificial intelligence, and news on the future of transportation. Microsoft has become the worlds first company to develop speech recognition software that is more accurate than humans. In the paper  Achieving Human Parity in Conversational Speech Recognition  published Monday, the software produced transcripts that contained half a percent fewer errors than human efforts, which is incredible considering how good people are at understanding speech. The breakthrough opens the door for new A.I. assistants that are more accurate than ever before. Its impressive how far the technology has come in such a short space of time. It wasnt too long ago that computer speech recognition was a weird niche that seemed hopelessly distant. Watch this demonstration of Windows Vistas speech software from 2006: The team used the National Institute of Standards and Technology (NIST) 2000 test, used across the industry to measure the reliability of speech transcriptions. A conversation takes place between two participants over the phone, turn by turn, before the resultant script is compared and checked against dictionary spellings. In the switchboard portion, where two strangers speak for the first time, the human error rate is around 5.9 percent, while on the call home portion, where two people that know each other speak, the error rate is around 11.3 percent. Microsofts software scored around a 0.4 percent lower error rate. The breakthrough will help bring new forms of immersive A.I.. In August, student Joshua Browder took the wraps off his DoNotPay chatbot, which can help homeless people get free legal advice. Combined with recognition advancements, its easy to picture a future where people ask a virtual assistant for help with housing by having a regular conversation with their computer. At this stage, researchers are considering how A.I.-powered speech recognition can give smarter responses. Sensay , an anger-detecting A.I. from the lab behind Siri, uses advanced recognition capabilities to detect if a user is feeling angry or confused, changing its answers to suit the situation. Removing the barrier of error-prone voice recognition, creates exciting new opportunities for virtual assistance. ",https://www.inverse.com/article/22329-microsoft-research-speech-recognition,Microsoft has become the world's first company to develop speech recognition software that is more accurate than humans. In the paper ...,Microsoft Invents Better-Than-Human Speech Recognition,Microsoft Achieving Human Parity Conversational Monday A.I Windows Vistas National Institute Standards Technology NIST Microsofts A.I.. August Joshua Browder DoNotPay Sensay A.I Siri,2
155,"On March 18th, the CDT accorded Rep. Glenn Thompson a top, wall-to-wall large font headline, Rep. Thompson says he will not vote in favor of proposed GOP health care bill. He got everything he wanted: instant recognition; strong position taken, using equivocation only in the very first line in the body of the statement (... the way it stands ...). Change one comma, and it is rendered useless for the headline readers. I suspected as much two days later when I could not find his name on any list published anywhere, so I called his office and was told that was no longer true. Damage done. Voters beware. I thought back to the Republican debates of the summer when I could hear let them die cries from the audience when health care would be the focus of the debate. On March 28th, the CDT published a story concerning Mara Einsteins speech at Penn State. Einstein, a Queens College media studies professor, focused on a different definition of fake news. In the same edition, the CDT published a letter headlined Thompson working hard to serve, by Laurence F. Lane, a 30-year friend of Thompsons  a spokesperson for the health industry. The CDT should have printed both of those articles side by side. The letter would have been a great example for the Einstein speech to explore as relevant to Thompsons obvious attempt at obfuscation. ",http://www.centredaily.com/opinion/letters-to-the-editor/article143867879.html,"He got everything he wanted: instant recognition; strong position taken, ... published a story concerning Mara Einstein's speech at Penn State.",Utilizing 'fake news',March CDT Rep. Glenn Thompson Rep. Thompson GOP Republican March CDT Mara Einsteins Penn State Einstein Queens College CDT Thompson Laurence F. Lane Thompsons CDT Einstein Thompsons,-1
156,"PUNE, India, Nov. 24, 2016 /PRNewswire/ --Market Research Future Published a Half Cooked Research Report on Speech Recognition Market. The global market for Speech Recognition is majorly driven by factors such as increasing security concerns of managing consumer devices and growing security application in internet-connected and digital devices. Global Speech Recognition Market is expected to grow with market size of US ~$13 billion by 2022. Growing mobile banking application and adoption of mobile- and cloud-based computer technology is driving the Speech Recognition Market. The growing adoption of automated and smart applications in consumer electronics and healthcare industries act as the major contributor for the growth of the speech recognition market during the forecasted period. Taste the market data and market information presented through more than 50 market data tables and figures spread over 110 numbers of pages of the project report. Avail the in-depth table of content TOC & market synopsis on ""The Global Speech Recognition Market Research Report -Forecast to 2022"". The prominent players in the market of Speech Recognition Market are- Nuance Communications, Inc. (U.S), Microsoft Corporation (U.S), Agnitio SL (Spain), VoiceVault (U.S.), VoiceBox Technologies Corp. (U.S), Google Inc. (U.S), LumenVox LLC.(U.S), Raytheon BBN Technologies (U.S), Advanced Voice Recognition Systems (U.S.), Sensory, Inc. (U.S) among others. Market Research Future has predicted that this market will be dominated by North America throughout the forecast period. Europe stands second biggest market in the year 2016 but is expected to be dominated by Asia-Pacific by the end of forecast period. Also, Asia-Pacific has emerged as fastest growing market for the Speech Recognition owing to factors such as growing economy of India and China and also due to the strong presence of manufacturing industry in the region. The Early Diners are Offered Free Customization - Up to 20% on this Report. Global Human Machine Interface Market, by Component (Control panel, LED indicator, Mechanical switches), by Type (Industrial PC, Interface software), by Application (Automation, Aerospace, Healthcare, Oil & gas, Defence) - Forecast 2027 At Market Research Future (MRFR) , we enable our customers to unravel the complexity of various industries through our Cooked Research Report (CRR), Half-Cooked Research Reports (HCRR), Raw Research Reports (3R), Continuous-Feed Research (CFR), and Market Research & Consulting Services. MRFR team have supreme objective to provide the optimum quality market research and intelligence services to our clients. Our market research studies by products, services, technologies, applications, end users, and market players for global, regional, and country level market segments, enable our clients to see more, know more, and do more, which help to answer all their most important questions. In order to stay updated with technology and work process of the industry, MRFR often plans & conducts meet with the industry experts and industrial visits for its research analyst members. ",http://www.prnewswire.com/news-releases/speech-recognition-market-is-expected-to-reach-usd-13-billion-by-2022-300368462.html,"PUNE, India, Nov. 24, 2016 /PRNewswire/ -- Market Research Future Published a Half Cooked Research Report on Speech Recognition ...",Speech Recognition Market is Expected to Reach USD 13 Billion by ...,PUNE India Nov. Market Research Future Half Cooked Research Report Market US ~ TOC Global Market Research Report Market Nuance Communications Inc. U.S Microsoft Corporation U.S Agnitio SL Spain VoiceVault U.S. VoiceBox Technologies Corp. U.S Google Inc. U.S LumenVox LLC U.S Raytheon BBN Technologies U.S Advanced Voice Recognition Systems U.S. Sensory Inc. U.S Market Research Future North America Europe Asia-Pacific India China Customization Report Global Human Machine Interface Market Component Control LED Mechanical Type Industrial PC Interface Application Automation Aerospace Healthcare Oil Defence Market Research Future MRFR Cooked Research Report CRR Research Reports HCRR Raw Research Reports Continuous-Feed Research CFR Market Research Consulting Services MRFR MRFR,3
157,"IBM is making significant strides in Speech Recognition, which could provide a boost to flattening revenues from its Cognitive Solutions Segment. IBM has come out in support of Trumps Tax Reform program but we would caution investors against expecting an earnings boost from a friendlier tax regime. IBM has come out in support of Trumps Tax Reform program but we would caution investors against expecting an earnings boost from a friendlier tax regime. IBM is currently trading above our Fair Value estimate and investors should avoid buying the stock for now even though its dividend yield is fairly attractive. Instead, they should take a position in the stock when it becomes clear that Trumps tax amnesty of foreign earnings is going to pass. With the increasing popularity of virtual assistants like Apple's (NASDAQ: AAPL ) Siri, Amazon's (NASDAQ: AMZN ) Alexa and Microsoft's (NASDAQ: MSFT ) Cortana, 2017 is emerging as the year when Virtual Assistants truly emerge. Even Samsung ( OTC:SSNLF ), Apple's erstwhile competitor in the smartphone space, is purportedly launching its own virtual assistant, Bixby. IBM (NYSE: IBM ) has long had skin in the virtual assistant (or 'agent') field with Watson, which was developed by IBM early in the last decade as a kind of follow-up to its well-known Deep Blue system. Recently, IBM indicated that its systems' ability to parse human speech had achieved a new error rate record of 5.5% , which is quite close to the 5.1% error rate that people have in understanding one another's speech. IBM's improved language-parsing technology should eventually make it to Watson, making it an all the more attractive proposition for IBM's enterprise customers who may be seeking new ways to automate their interactions with customers or help their employees access company resources. As virtual assistants become more and more mainstream, having a strong natural language parsing system will give IBM a potential 'long game' advantage over its competitors and could set it up for solid future growth . The virtual assistant market itself is estimated to grow to a $3.07 Billion industry by 2020 and $7.9 Billion by 2024 and IBM could seize a big portion of this market in the coming years - either indirectly by licensing its voice recognition technology or directly by aggressively marketing Watson to enterprise customers. This would be a welcome outcome for IBM's shareholders, who have witnessed its revenues contract by an average of 1.3% over the last 5 years as key areas such as cognitive solutions (+2% in Q4-16) have tapered off. Its margins have also been a cause for some concern, having slipped by nearly 200 basis points from 49.8% in 2015 to 47.9% in 2016. In any case, investors seem to be sanguine about IBM's prospects, perhaps considering its advances in areas such as machine learning and the Internet of Things (NYSEMKT: IOT ) - as well as its ability to put together 4 straight quarters of earnings that exceeded expectations . Another attraction for IBM: its 3.17% dividend yield, which is considerably higher than that of both the Dow Jones Industrial Average and the S&P500 , both of which count IBM among their components. It is also over 100-basis points better than that of its peer group . However, the attraction isn't exactly straightforward: IBM's effective tax rate in the fourth quarter was only 9.6% based on its 4th quarter earnings report. That's far lower than the 15% tax rate that Trump has proposed in his agenda . Moreover, IBM's effective tax rate has drifted down in recent years. After having reached a high of nearly 48% in 2005, its last 3 years' effective rates have fallen from 21% in 2014 to under 10% by the 4th quarter of 2016, thanks to a shift in its geographic mix and foreign tax credits. It's certainly possible that IBM achieved its low tax rate in much the same (and perhaps controversial) way that Apple did . In fact, if IBM had paid its 2015 effective tax rate of 16.2% in the fourth quarter of 2016, its non-GAAP earnings per share for the period would have been about $4.74 per share instead of $5.01, which would have caused it to miss earnings expectations of $4.89 per share. In our view, IBM's real angle - which could benefit its dividend investors - is Trump's proposed tax amnesty on earnings held overseas. IBM's cash trove is estimated at around $65 Billion that, while less Apple's $246 Billion could mean a huge tax savings of as much as $16.5 Billion - or equivalent to around 1.4-times its Net Income last year. If even a fourth of that went out to IBM's investors as a special dividend, it would be the equivalent to a one-time dividend payment of 2.4% based on IBM's current share price. Of course, a lower tax rate would also give IBM a reason to eschew various tax accounting maneuvers and simplify its tax administration. However, it could mean pain in the near-term - to wit, a 15% tax rate in 2016 would have meant non-GAAP earnings per share of $12.42 per share rather than the $13.49 it reported. All things considered, we believe that there is no 'short game' upside to IBM from its tax positioning. The likely result would be a one-time boost from a reassessment of its tax liabilities but going forward, it would likely have to adhere to a 15% tax rate, which would bring it back to its average over the 2nd to 4th quarters of 2016 . In short, there's nothing to see there. In terms of the 'long game' associated with the strides it has made in voice recognition that could potentially boost its Cognitive segment's revenues, there could be upside - but it's too early to invest on that basis and while the market for virtual assistants is growing, it won't be large enough to provide a sizable boost to IBM's top line. In short, we believe that IBM is fairly valued at this time and would caution investors against taking a significant position in the stock. Indeed, while the stock seems like a bargain at 14.2-times earnings, the fact that its revenues are contracting implies that it should be trading closer to 11- to 12-times our earnings estimate of 13.55 per share, which works out to a price of $163 per share on the high end (which is shy of the consensus of $165 per share ). Instead, the 'play' for dividend-focused investors would be to buy IBM when there is greater clarity of whether Trump's tax amnesty will get passed (perhaps in 2018). In this case, investors could be looking forward to capital gains associated as investors position for a special dividend. Disclosure: I am/we are long IBM. I wrote this article myself, and it expresses my own opinions. I am not receiving compensation for it (other than from Seeking Alpha). I have no business relationship with any company whose stock is mentioned in this article. Additional disclosure: Black Coral Research, Inc. is a team of writers who provide unique perspective to help inform dividend investors. This article was written by Jonathan Lara, one of our Senior Analysts. We did not receive compensation for this article (other than from Seeking Alpha), and we have no business relationship with any company whose stock is mentioned in this article. Company financial data is taken from the companys latest SEC filings unless attributed elsewhere. Black Coral Research, Inc. is not a registered investment advisor or broker/dealer. Readers are advised that the material contained herein should be used solely for informational purposes. Investing involves risk, including the loss of principal. Readers are solely responsible for their own investment decisions. Author payment: $35 + $0.01/page view. Authors of PRO articles receive a minimum guaranteed payment of $150-500. Become a contributor  Problem with this article Please tell us . Disagree with this article Submit your own . ",https://seekingalpha.com/article/4057013-ibm-watson-breaks-records-investors-wait-tax-reform,"IBM is making significant strides in Speech Recognition, which could provide a boost to flattening revenues from its Cognitive Solutions ...","IBM: Watson Breaks Records, But Investors Should Wait For Tax ...",IBM Solutions Segment IBM Trumps Tax Reform IBM Trumps Tax Reform IBM Fair Value Trumps Apple NASDAQ AAPL Siri Amazon NASDAQ AMZN Alexa Microsoft NASDAQ MSFT Cortana Virtual Assistants Samsung OTC Apple Bixby IBM NYSE IBM 'agent Watson IBM Deep Blue IBM IBM Watson IBM IBM Billion Billion IBM Watson IBM Q4-16 IBM Internet Things NYSEMKT IOT IBM Dow Jones Industrial Average S P500 IBM IBM Trump IBM IBM Apple IBM IBM Trump IBM Billion Apple Billion Billion IBM IBM IBM IBM Cognitive IBM IBM IBM Trump IBM Alpha Black Coral Research Inc. Jonathan Lara Alpha SEC Black Coral Research Inc. + PRO Problem Please Submit,0
158,"All design and technology integration work is complete ahead of          iFLYTEKS planned late 2016 launch. Sensor gives iFLYTEKs headset product measurable improvements in          automatic speech recognition (ASR) performance that are substantially          higher than was demonstrated in early lab tests. The two companies also initiate next design collaboration for an          automotive voice control and voice biometrics product that uses a          far-range version of VocalZooms Human to Machine Communication (HMC)          sensor. YOKNEAM ILLIT, Israel--( BUSINESS WIRE )-- VocalZoom ,        a leading supplier of Human-to-Machine Communication (HMC) optical        sensors that enable a more natural, personalized and secure        voice-control experience, today announced it has completed the final        stage of design work with iFLYTEK, the supplier of Chinas most popular        speech recognition platform. iFLYTEK is preparing to launch a headset        product by years end featuring the disruptive VocalZoom HMC solution,        which has demonstrated in recent tests of iFLYTEKs pre-production        headset product that it can deliver near-perfect speech recognition in        noisy voice-control environments, and even better performance than was originally demonstrated in early iFLYTEK lab tests of the sensor. Game-changing early test results for the VocalZoom sensor performance        have only improved as we now prepare to launch our first product based        on this truly innovative HMC solution, said Haikun Wang, senior        researcher at iFLYTEK. Our upcoming headset featuring the VocalZoom        sensor will redefine ASR performance in its product class and open up        new applications in noisy environments where voice control products        previously didnt work. We also have entered the development stage with        VocalZoom on another product for voice control in the connected car        using a forthcoming longer-range version of their sensor. We are extremely excited about the progress we have made with iFLYTEK        on the companys upcoming voice-control headset product, said Rammy        Bahalul, vice president of sales and business development for VocalZoom.        We believe that iFLYTEK is poised to revolutionize how users        communicate with devices, applications and machines, leveraging the        breakthrough capabilities of our optical HMC sensors. We also look        forward to working with iFLYTEK on additional projects including a        reference design that will simplify VocalZoom sensor design-in for        product developers using the iFLYTEK ASR, and iFLYTEKs next initiative        for voice control and voice biometric solutions in the connected car. Early tests of the VocalZoom sensor with iFLYTEKs Voice Cloud        intelligent speech technology platform showed performance improvements        of at least 50 percent compared to traditional speech-recognition        technology in a quiet environment, and even better in noisy environments        such as outdoors with street sounds, and in vehicles with windows open,        engine noise, music playing and other interference. Testing of iFLYTEKs        pre-production headset featuring the sensor are yielding even greater        improvements than those early results in quiet environments, while        continuing to deliver near-perfect word recognition performance in noisy        environments. This compares with traditional alternative solutions using        one, two or an array of acoustic microphones with noise reduction        technology, which cannot provide clean enough, isolated speaker input to        operate satisfactorily in noisy environments. VocalZooms compact, low-profile optical sensor solution takes a        completely new approach to improving voice recognition and voice        biometric, initially in headsets and wearables, and in the near future        for the connected car and other longer-range sensor applications. By        integrating the VocalZoom optical HMC sensor into a voice-control        solution and focusing it on facial skin around the cheeks, ears, neck        and throat, tiny vibrations during speech can be acquired, measured and        converted to an isolated, near-perfect reference signal that machines        can understand  regardless of noise levels. VocalZoom supplies Human-to-Machine Communication (HMC) sensors for        delivering a natural, personalized and secure voice-controlled user        experience in todays increasingly mobile and interconnected world. The        sensors enable accurate and reliable voice control and biometrics        authentication in any environment, regardless of noise. Applications        including mobile secure payments, headsets and wearables, mobile phones,        access control, smart home solutions, and hands-free automotive voice        control. For more information, visit www.VocalZoom.com and follow the company on LinkedIn . VocalZoom has completed the final stage of design work with iFLYTEK, the supplier of Chinas most popular speech recognition platform. ",http://www.businesswire.com/news/home/20161018005785/en/VocalZoom-Completes-Design-In-Phase-iFLYTEK-Speech-Recognition,Sensor gives iFLYTEK's headset product measurable improvements in automatic speech recognition (ASR) performance that are substantially ...,VocalZoom Completes Design-In Phase with iFLYTEK as the ...,Sensor ASR VocalZooms Human Machine Communication HMC YOKNEAM ILLIT Israel BUSINESS WIRE VocalZoom Human-to-Machine Communication HMC Chinas VocalZoom HMC VocalZoom HMC Haikun Wang VocalZoom ASR VocalZoom Rammy Bahalul VocalZoom HMC VocalZoom ASR VocalZoom Voice Cloud VocalZooms VocalZoom HMC VocalZoom Human-to-Machine Communication HMC LinkedIn VocalZoom Chinas,1
159,"LONDON, Feb. 9, 2017 /PRNewswire/ -- This report analyzes the worldwide markets for Voice and Speech Recognition Technology in US$ Thousand. The Global market is additionally analyzed by the following Segments: Voice Recognition, and Speech Recognition. The report provides separate comprehensive analytics for the US, Canada, Japan, Europe, Asia-Pacific, Latin America, and Rest of World. Annual estimates and forecasts are provided for the period 2015 through 2022. Also, a six-year historic analysis is provided for these markets. Market data and analytics are derived from primary and secondary research. Company profiles are primarily based on public domain information including company URLs. The report profiles 78 companies including many key and niche players such as - Study Reliability and Reporting Limitations I-1 Disclaimers.............. I-2 Data Interpretation & Reporting Level I-2 Quantitative Techniques & Analytics I-3 Product Definitions And Scope of Study I-3 Voice Recognition Technology..............I-3 Speech Recognition Technology I-3 1. INDUSTRY OVERVIEW.............. II-1 Speech Technology: Enabling Man-to-Machine Communication II-1 Value Chain of Voice Business II-2 Speech Technology: An Indispensable Part of New-Age Machines II-2 Speech Recognition Dominates Voice & Speech Recognition Technology Market..............II-4 Voice Recognition - The Faster Growing Technology Segment II-5 Automated Speech Recognition (ASR): The Widely Used Speech Recognition Technology..............II-7 Text-to-Speech Emerges as a Realistic, Natural Conversation Tool II-8 Text-to-speech (TTS) for Education Market - Significant Potential for Growth..............II-9 TTS Aids in Improving Customer Engagement II-10 Developed Markets to Maintain their Dominance II-11 Developing Countries to Drive Global Growth in the Short-to- Medium Term Period.............. II-11 Table 1: Global Voice and Speech Recognition Technology Market: Leading Countries by Percentage CAGR for the Period 2015-2022 (includes corresponding Graph/Chart) II-12 Competitive Landscape..............II-12 China's Speech Recognition Market - A Tough Nut to Crack for Google.............. II-13 Patent Activity Gains Momentum in Speech Technology Market II-14 A Review of Competition in Key Sectors II-15 Speech Engine.............. II-15 Analytics.............. II-15 Self-Service Suite..............II-15 Speech Security.............. II-15 Professional Services..............II-15 Major Challenges Impeding Speech & Voice Recognition Market II-16 Limited Understanding of Words and Inability to Process Large Vocabulary of Information..............II-16 Low Accuracy due to Ambient Noise II-16 Lack of Awareness..............II-16 2. MARKET TRENDS, ISSUES & DRIVERS II-17 Rapid Penetration of Mobile Devices: A Major Growth Driver II-17 Opportunity Indicators..............II-18 Table 2: World Market for Mobile Handsets: Breakdown of Volume Sales in Million Units for Years 2011, 2013 & 2015E (includes corresponding Graph/Chart) II-18 Table 5: Penetration Rate (%) of Voice Processors in Mobile Devices for 2012 through 2016 (includes corresponding Graph/Chart).............. II-20 Mobile SMS: An Underutilized Feature for Mobile Speech Recognition Technology..............II-20 Advancements in Processing Power Improves Speech Recognition Efficiency.............. II-21 Mobile Voice Portals Drive Adoption of Speech Recognition in Mobile Phones.............. II-21 Voice & Speech Recognition Become Part of Primary Interface for Smartphone Users..............II-22 Robust Growth of Smartphones Sets the Platform II-22 Table 6: Global Market for Smartphones: Volume Sales in Billion Units for 2014, 2016, 2018 & 2020 (includes corresponding Graph/Chart)..............II-23 Table 7: Smartphone Penetration Rate (%) for Select Countries Worldwide as a Percentage of Population Owning a Phone: 2016 (includes corresponding Graph/Chart) II-24 Growing Popularity of Tablet Phones Benefits the Market II-25 Table 8: Worldwide Shipments of Desktop PCs, Laptops and Tablets in Million Units for 2014 and 2016E (includes corresponding Graph/Chart)..............II-25 Wearable Devices: The Next Frontier for Speech Recognition Technology.............. II-25 Technology Innovations & Advancements: Spearheading Growth II-27 Intelligent Virtual Assistants - A Proactive Speech Technology-based Feature..............II-27 Multimodal Solutions: The New Focus Area II-28 Integration of Graphic User Interface (GUI) and Voice User Interface (VUI)..............II-29 Innovations in Voice-Interface Devices II-30 Technology Developers Focus on Phoneme Accuracy Feature II-30 Speech Recognition-enabled Self-Service Technology II-31 Promising Prospects for Speech Recognition Technology in Call Centers.............. II-31 Rising Significance of Speech Technology for Outsourced Customer Care Services Market..............II-32 Call Centers to Leverage Proficiency of Speech Analytics II-33 Expanding Use of IVR Systems in Call Centers Lends Traction to Market Growth..............II-34 Table 9: Global IVR Systems Market (2015): Percentage Breakdown of Installed Systems by Input Mode (includes corresponding Graph/Chart)..............II-34 Table 10: Global Interactive Voice Response (IVR) Systems by Geographic Region - Annual Revenues in US$ Million for the Years 2016 & 2022 (includes corresponding Graph/Chart) II-35 Growing Demand for Hosted IVR Services Fuels Demand for Speech Technology Solutions II-35 Rising Importance of Speech Technology in the IoT Era II-36 Cloud Computing Technology Revolutionizes Speech Technology Market.............. II-36 Healthcare Facilities Seek the Aid of Cloud for Voice Documentation.............. II-38 User Experience and Usability Take Center Stage II-38 Cloud Solutions Boost Adoption of Voice Recognition Technologies II-38 Cloud Services to Drive Mobility for Voice Authentication II-39 Globalization & Workforce Decentralization Spur Demand in the Enterprise Sector..............II-40 Table 11: Global Speech Recognition Market (2013): Percentage Share Breakdown of Revenues by End-Use Segment (includes corresponding Graph/Chart)..............II-40 Table 13: Global Breakdown of Mobile Workforce (in Million) for the Years 2010 & 2015 by Geographic Region (includes corresponding Graph/Chart)..............II-41 Speech Analytics Gather Steam II-41 Rising Popularity of Biometrics Offer Ample Growth Opportunities II-42 Voice Biometrics Emerges as a Game Changer for Authenticating Mobile Devices.............. II-42 Increasing Security Threats Drive the Voice Verification Biometrics Market.............. II-43 Favorable Trends in the IT Industry Strengthens Market Prospects II-44 Spurt in Internet Usage..............II-44 Table 14: Global Internet Usage (2012-2016): Number of Internet Users (in Billion) and Internet Penetration Rate (%) (includes corresponding Graph/Chart) II-44 Table 16: Global Internet Usage: Number of Internet Subscribers (in Million) and Penetration Rate (%) by Geographic Region for the Year 2016 (includes corresponding Graph/Chart).............. II-46 Move towards Convergence..............II-46 Proliferation of Embedded Devices II-47 Continued Expansion of Software Sector II-47 Rise in Social Networking..............II-47 VoIP Model Drives Demand..............II-47 Speech-based Searching Gains Momentum II-48 Speech Technology Solutions Improves Convenience and Safety of Automobiles.............. II-48 Telematics & Car Infotainment Offers Opportunities on a Platter II-50 Cloud-based Voice Recognition to Foster Use of Automotive Telematics.............. II-50 Growing Adoption of Speech Recognition Technology in ""In- Vehicle Systems""..............II-51 Overcoming Challenges in Comprehending Voice Commands in Automobiles.............. II-52 Suppression of In-Car Noise - Essential for Effectiveness of Voice Recognition Technology II-52 Blurring Demographic Barriers Encourage Voice-Enabled Car Systems.............. II-53 Growth in Automobiles Production Bodes Well for the Market II-53 Table 17: Global Automobile Production for the Years 2015, 2017 & 2020 (in '000 Units) (includes corresponding Graph/Chart).............. II-54 Speech Recognition Charms Connected Media Solutions II-54 Audio Mining for Searching Online Audio and Video Content II-55 Speech Recognition Seeks Improvements in Processor Technology II-55 Speech Recognition Technology Transforms Banking Industry II-56 Voice Authentication Systems for Mitigating Online Banking Fraud II-56 Healthcare Industry: A Major Untapped Opportunity II-57 Voice Documentation Emerge as a Preferred Means for Data Collection.............. II-58 Hospitals Look towards Front-end Speech Recognition Deployments II-58 Radiology Department & Point-of-Care: Potential Application Areas.............. II-59 Speech Recognition in Electronic Medical Records (EMR) II-59 Speech-Enabled Self-Service Solutions for the Healthcare Sector II-60 Rising Incidence of Cognitive Impairment Issues Drive Focus onto Assistive Speech Technology II-60 Rising Prominence of Voice Controls in Smart Homes II-61 Distribution and Warehouse Management Leverages Potential of Speech Technology..............II-62 How Speech Technology Operates in the Distribution Center II-62 Advantages of Speech Technology in the Distribution Center II-63 Growing Opportunities for Speech Recognition in the SOHO Market II-64 Speech Technology Finds Favor among Law Enforcement Agencies II-64 Law Enforcement Agencies Seek to Exploit Voice Stress Analyser II-64 Voice Stress Analyser vs. Polygraphs II-65 Increasing Use of Speech Technology in Next Generation Networks (NGNs).............. II-65 Speech Technology Facilitates Quick Information Access in the Military Sector.............. II-65 Open Standards: A Key Architectural Requisite II-66 3. PRODUCT OVERVIEW.............. II-67 Speech Technology: Processing Human Speech II-67 History of Speech Technology..............II-67 Landmarks in the Advancement of Speech Recognition Technology II-68 Speech Recognition Technology..............II-69 Speech Recognition Modes..............II-69 Dictation Mode.............. II-69 Command and Control Mode..............II-70 Speech Recognition Technology: How it Helps Developers II-70 Applications.............. II-70 Advantages.............. II-71 Cost Savings.............. II-71 Improvement in Call Completion Rates II-71 Delivery of Value-Added Content to Users II-71 Higher Customer Satisfaction II-71 Reduction in Agent Turnover Rate II-71 Other Advantages in Brief..............II-71 Types of Speech Technology..............II-72 Automatic Speech Recognition II-72 Types of Automatic Speech Recognition II-73 Discrete Word Recognition..............II-73 Continuous Word Recognition..............II-73 Word Spotting.............. II-73 Phoneme Recognition..............II-73 Natural Language - Grammar Based Recognition II-73 Types of ASR Technology Solutions II-74 Speaker-Independent Technology II-74 Speaker-Dependent Technology II-74 Isolated Speech Recognition Systems II-75 Connected Speech Recognition Systems II-75 Continuous Speech Recognition Systems II-75 Automatic Speech Recognition: Applications II-75 Principal Applications of Automatic Speech Recognition by Sector.............. II-76 Prospective Applications..............II-76 Document Editing..............II-76 Data Entry.............. II-77 Edutainment and Games..............II-77 Advantages.............. II-77 Limitations.............. II-78 Text-to-Speech or Speech Synthesis II-78 Process Description of Speech Synthesis II-79 Visual Text-to-Speech..............II-79 Text-to-Speech Vs. Digitized Voice II-79 Applications.............. II-79 Advantages.............. II-80 Voice Recognition Technology..............II-80 Speaker Verification..............II-81 Methods of Speaker Verification II-81 Text-Dependent Speaker Verification Methods II-81 Hidden Markov Modeling (HMW) II-81 Dynamic Time Warping (DTW) II-81 Text-Independent Speaker Verification Methods II-82 VQ-Based Method.............. II-82 Multivariate Auto-Regression (MAR) II-82 Average-Spectrum-Based Method II-82 Drawbacks.............. II-82 Distributed Speech Recognition System II-82 Distributed Speech Recognition Services II-83 Other Speech Technology Related Concepts II-83 Interactive Voice Response..............II-83 Phonetic Speech Recognition..............II-84 Integration of Speech Recognition in IVR Systems II-84 Attributes of Advanced Speech Solutions for IVRs II-85 Advantages of IVR for Customer Service II-85 Advantages of IVR for Management II-85 Voice Over Internet Protocol..............II-86 Natural Language Speech Recognition II-86 Design, Development, and Implementation of NLSR Applications II-86 Preliminary Collection of Data II-87 Transcription of Data..............II-87 Describing Key Reasons for Customer Calls II-87 Tagging.............. II-87 System Training..............II-87 Implementation.............. II-87 Speech Application Language Tags II-87 Wireless Application Protocol II-88 Voice Portal.............. II-88 VoiceXML.............. II-89 V-Commerce.............. II-89 Voice/Data Convergence..............II-90 Voice Browsers.............. II-90 4. PRODUCT & TECHNOLOGY INNOVATIONS/ INTRODUCTIONS II-91 Sensory Introduces TrulyHandsfree SDK II-91 Interactions Unveils Curo Speech Application II-91 VoiceBox Introduces New Embedded ASR Product for Automotive Applications.............. II-91 Nuance Releases Dragon for Mac Medical Speech Recognition Software.............. II-91 TranscribeMe Unveils High Accuracy Automated Speech Recognition Technology..............II-91 Uniphore Introduces Latest Version of auMina II-91 Ozonetel Introduces Speech Recognition Technology II-92 NTT Introduces Speech-Recognition API for Cross-Browser Use II-92 Listen Technologies Launches TalkPerfect DX Speech Enhancement System.............. II-92 Sensory Introduces TrulyNatural Platform II-92 Nuance Releases Dragon Dictate Medical for Mac II-92 BigHand Upgrades Server-Side Speech Recognition Module II-92 Azure Media Launches Speech Recognition Service to Index Video Content.............. II-92 Nuance Communications Launches Dragon NaturallySpeaking 13 for the PC.............. II-93 LumenVox Unveils LumenVox 12.1 II-93 Fujitsu Laboratories Develops New Speech Synthesis Technology II-93 Nuance Communications Upgrades PowerScribe 360 Platform for Healthcare.............. II-93 DSP Group and Sensory Rolls Out Voice Activation Solution II-93 Genesys Launches Actionable Analytics Solution II-93 Nuance Communications Rolls Out Dragon Drive-Powered Mercedes News App.............. II-93 IDS Unveils Voice2Dox 3.0 Speech Recognition Application for Clinical Reporting.............. II-94 Sensory and Tensilica Unveil Lowest Power Voice Activation Solution.............. II-94 Nuance Communications Launches Next Generation Voice Biometrics Platform..............II-94 NeoSpeech Unveils VoiceEZCALL English Learning Program Software II-94 Nuance Communications Rolls Out New Version of Dragon Medical Practice Edition for Smaller Practices II-94 Nuance Communications Launches Voice Ads II-94 Agnitio Unveils BATVOX 4.0 Forensic Speaker Verification System II-95 ReadSpeaker Unveils Web Application Service Platform for Text -to-Speech.............. II-95 5. RECENT INDUSTRY ACTIVITY..............II-96 Nuix and Voci Enter into Strategic Partnership for Speech Analytics Solution.............. II-96 Vuzix and Sensory in Partnership II-96 CHRISTUS Health Selects Nuance's Speech Recognition and Clinical Documentation Solutions II-96 Dataworxs Australia and Nuance to Resell Dragon Medical Speech Technology in Australia..............II-96 ReadSpeaker and Blackboard Team Up for Text-to-Speech Tech II-96 VocalZoom Collaborates with Cobalt Speech & Language II-96 VoiceBox to Acquire Telisma Speech Product Unit from OnMobile II-97 Sensory and Phillips Collaborate for Speech Recognition Technologies.............. II-97 Acapela Takes Over CreaWave..............II-97 Apple Acquires VocalIQ..............II-97 Facebook Takes Over Wit.ai..............II-97 Interactions Concludes Acquisition of AT&T Watsonsm Technology Program.............. II-97 Samsung's Galaxy Products Deploy Sensory' Embedded Speech Technologies.............. II-98 Convergys Partners with Voci Technologies II-98 Sena Technologies Selects Sensory's TrulyHandsfree Voice Trigger and Phrase Spotting Technology II-98 Sensory Receives Patent Claims on its TrulyHandsfree Voice Control Approach.............. II-98 British Telecommunications Deploys LumenVox Automatic Speech Recognizer.............. II-98 NXP Semiconductors Integrates Rubidium's Multi-Lingual ASR and Voice Trigger Engine into CoolFlux DSP II-98 Nuance Communications Delivers Speech and Natural Language Understanding Technology to China Mobile's Jiangsu Branch II-98 Honeywell Acquires Intermec..............II-99 Apple Acquires Novauris Technologies II-99 Applications Technology Takes Over Leidos' Omnifluent Human Language Technology Software and Associated Business II-99 Amazon.com Acquires IVONA Software II-99 Nuance Communications Signs Multi-Year Collaboration Deal with ZTE.............. II-99 Wolfson Microelectronics Selects Sensory's TrulyHandsfree Voice Control and Audio Detect Front-End II-99 Nuance Communications Integrates New DIRECTV Voice Search Feature for the DIRECTV App..............II-99 Aldebaran Robotics to Integrate Nuance's Conversational Voice Capabilities in its NAO Robot II-100 Nuance Communications Delivers Dragon Drive Voice Capabilities to Kenwood Audio Visual Navigation Systems II-100 Genesys Acquires Utopy..............II-100 Avaya Includes LumenVox in its DevConnect Select Product Program II-100 Nuance Communications Inaugurates Brand New Mobile Innovation Center in Massachusetts..............II-100 Nuance Communications to Take Over Tweddle Connect from Tweddle Group.............. II-100 Toyota Selects Nuance Dragon Drive for its Smart G-BOOK Smartphone Application..............II-101 ReadSpeaker Partners with Instructure II-101 Tatra Banka Deploys Nuance Communications' FreeSpeech Voice Biometrics Solution..............II-101 Nuance Communications Teams Up with Widespace II-101 Nuance Communications to Collaborate with AutoNavi II-101 6. FOCUS ON SELECT GLOBAL PLAYERS..............II-102 24/7 Customer, Inc. (US)..............II-102 Acapela Group (France)..............II-102 Advanced Voice Recognition Systems, Inc. (US) II-102 AGNITIO S.L. (Spain)..............II-103 Apple, Inc. (US).............. II-103 Applied Voice & Speech Technologies, Inc. (US) II-103 Avaya, Inc. (US).............. II-104 BioTrust ID B.V. (The Netherlands) II-104 Cisco Systems, Inc. (US)..............II-104 Convergys Corp. (US)..............II-105 Genesta Partnership (US)..............II-105 Genesys Telecommunications Laboratories, Inc. (US) II-106 Google, Inc. (US).............. II-106 Honeywell Scanning & Mobility (US) II-106 IBM Corporation (US)..............II-107 LumenVox LLC (US).............. II-107 M*Modal, Inc. (US)..............II-108 Microsoft Corporation (US)..............II-108 Netcall Telecom (UK)..............II-109 Nuance Communications, Inc. (US) II-109 Sensory, Inc. (US)..............II-110 SpeechFX, Inc. (US).............. II-110 VoiceBox Technologies Corporation (US) II-110 VoiceVault, Inc. (US)..............II-111 Voxware, Inc. (US)..............II-111 Wizzard Speech LLC (US)..............II-111 7. GLOBAL MARKET PERSPECTIVE..............II-112 Table 18: World Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology by Geographic Region - US, Canada, Japan, Europe, Asia-Pacific, Latin America and Rest of World Markets Independently Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) II-112 Table 19: World Historic Review for Voice and Speech Recognition Technology by Geographic Region - US, Canada, Japan, Europe, Asia-Pacific, Latin America and Rest of World Markets Independently Analyzed with Annual Revenue Figures in US$ Thousand for Years 2009 through 2014 (includes corresponding Graph/Chart)..............II-113 Table 20: World 14-Year Perspective for Voice and Speech Recognition Technology by Geographic Region - Percentage Breakdown of Revenues for US, Canada, Japan, Europe, Asia-Pacific, Latin America and Rest of World Markets for Years 2009, 2016 & 2022 (includes corresponding Graph/Chart) II-114 Table 23: World 14-Year Perspective for for Voice and Speech Recognition Technology by Geographic Region - Percentage Breakdown of Revenues for Voice Recognition and Speech Recognition Markets for Years 2009, 2016 & 2022 (includes corresponding Graph/Chart)..............II-117 1. THE UNITED STATES.............. III-1 A.Market Analysis.............. III-1 Speech & Voice Recognition Software Market: An Overview III-1 Speech Technology Market Remains Firm III-1 Voice Recognition Market for Smartphones III-2 Speech Recognition Technology: A Prominent Role in Healthcare Sector..............III-2 Growing Adoption of Speech Recognition Systems among American Doctors..............III-3 Amid Growing Threats Financial Sector Embraces Voice Biometrics III-3 Stricter Government Regulations Propel Demand for Interactive Analytics Systems III-4 Rising Demand for Speech Analytics Technology in Insurance Industry.............. III-4 Automotive Industry Looks to Voice Recognition Technology for Improving Driver Safety III-5 Speech Recognition Technology Making Way into Air Traffic Control.............. III-5 Product Launches..............III-6 Strategic Corporate Developments III-9 Key Players.............. III-13 B.Market Analytics..............III-22 Table 24: US Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-22 2. CANADA.............. III-24 A.Market Analysis.............. III-24 Current and Future Analysis..............III-24 Market Overview.............. III-24 VoiceTIMES: The Speech Technology Initiative III-24 B.Market Analytics..............III-25 Table 26: Canadian Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-25 3. JAPAN.............. III-27 A.Market Analysis.............. III-27 Current and Future Analysis..............III-27 Product Launches..............III-27 Strategic Corporate Development III-27 B.Market Analytics..............III-28 Table 28: Japanese Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-28 4. EUROPE.............. III-30 Market Analysis.............. III-30 Table 30: European Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology by Geographic Region - France, Germany, Italy, UK, Spain, Russia, and Rest of Europe Markets Independently Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-31 Table 31: European Historic Review for Voice and Speech Recognition Technology by Geographic Region - France, Germany, Italy, UK, Spain, Russia, and Rest of Europe Markets Independently Analyzed with Annual Revenue Figures in US$ Thousand for Years 2009 through 2014 (includes corresponding Graph/Chart)..............III-32 Table 32: European 14-Year Perspective for Voice and Speech Recognition Technology by Geographic Region - Percentage Breakdown of Revenues for France, Germany, Italy, UK, Spain, Russia, and Rest of Europe Markets for Years 2009, 2016 & 2022 (includes corresponding Graph/Chart) III-33 4a. FRANCE.............. III-34 A.Market Analysis.............. III-34 Current and Future Analysis..............III-34 Strategic Corporate Developments III-34 Acapela Group - A Key Player III-34 B.Market Analytics..............III-35 Table 33: French Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-35 4d. THE UNITED KINGDOM..............III-41 A.Market Analysis.............. III-41 Current and Future Analysis..............III-41 Strategic Corporate Developments III-41 Netcall Telecom - A Key Player III-41 B.Market Analytics..............III-42 Table 39: UK Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-42 4e. SPAIN.............. III-44 A.Market Analysis.............. III-44 Current and Future Analysis..............III-44 Product Launch.............. III-44 AGNITIO S.L. - A Key Player..............III-44 B.Market Analytics..............III-45 Table 41: Spanish Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-45 4g. REST OF EUROPE.............. III-49 A.Market Analysis.............. III-49 Current and Future Analysis..............III-49 Strategic Corporate Developments III-49 BioTrust ID B.V. (The Netherlands) - A Key Player III-50 B.Market Analytics..............III-51 Table 45: Rest of Europe Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-51 5. ASIA-PACIFIC.............. III-53 A.Market Analysis.............. III-53 Developing Asian Economies to Propel Future Growth III-53 Strong Demand for Smartphones Augurs Well for Speech Recognition in Asia-Pacific III-53 B.Market Analytics..............III-54 Table 47: Asia-Pacific Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology by Geographic Region - Australia, China, India, South Korea and Rest of Asia-Pacific Markets Independently Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-54 Table 48: Asia-Pacific Historic Review for Voice and Speech Recognition Technology by Geographic Region - Australia, China, India, South Korea and Rest of Asia-Pacific Markets Independently Analyzed with Annual Revenue Figures in US$ Thousand for Years 2009 through 2014 (includes corresponding Graph/Chart).............. III-55 Table 49: Asia-Pacific 14-Year Perspective for Voice and Speech Recognition Technology by Geographic Region - Percentage Breakdown of Revenues for Australia, China, India, South Korea and Rest of Asia-Pacific Markets for Years 2009, 2016 & 2022 (includes corresponding Graph/Chart) III-56 5a. AUSTRALIA.............. III-57 A.Market Analysis.............. III-57 Current and Future Analysis..............III-57 Strategic Corporate Development III-57 B.Market Analytics..............III-57 Table 50: Australian Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-57 5b. CHINA.............. III-59 A.Market Analysis.............. III-59 Current and Future Analysis..............III-59 Baidu Holds an Edge in China's Speech Recognition Market III-59 Strategic Corporate Developments III-60 B.Market Analytics..............III-61 Table 52: Chinese Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-61 5c. INDIA.............. III-63 A.Market Analysis.............. III-63 Current and Future Analysis..............III-63 Speech Technology to Aid Indian Farmers III-63 The Promise of Speech Recognition Technology for India's Agro Sector.............. III-63 Product Launches..............III-64 B.Market Analytics..............III-65 Table 54: Indian Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-65 5e. REST OF ASIA-PACIFIC..............III-69 A.Market Analysis.............. III-69 Current and Future Analysis..............III-69 Singapore: A Key Regional Market III-69 B.Market Analytics..............III-69 Table 58: Rest of Asia-Pacific Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-69 Table 62: Latin American 14-Year Perspective for Voice and Speech Recognition Technology by Product Segment - Percentage Breakdown of Revenues for Brazil and Rest of Latin America Markets for Years 2009, 2016 & 2022 (includes corresponding Graph/Chart)..............III-73 7. REST OF WORLD.............. III-74 Market Analysis.............. III-74 Table 63: Rest of World Recent Past, Current & Future Analysis for Voice and Speech Recognition Technology Analyzed with Annual Revenue Figures in US$ Thousand for Years 2015 through 2022 (includes corresponding Graph/Chart) III-74 ",http://www.prnewswire.com/news-releases/global-voice-and-speech-recognition-technology-industry-300405359.html,"9, 2017 /PRNewswire/ -- This report analyzes the worldwide markets for Voice and Speech Recognition Technology in US$ Thousand.",Global Voice and Speech Recognition Technology Industry,LONDON Feb. Voice Technology US Thousand Voice Recognition US Canada Japan Europe Asia-Pacific Latin America Rest World Market URLs Study Reliability Reporting Limitations I-1 Disclaimers I-2 Data Interpretation Reporting Level I-2 Quantitative Techniques Analytics I-3 Product Definitions Scope Study I-3 Voice Recognition Technology Technology I-3 INDUSTRY OVERVIEW II-1 Speech Technology Communication II-1 Value Chain Voice Business II-2 Speech Technology New-Age Machines II-2 Dominates Voice Technology Market Voice Recognition Faster Growing Technology Segment II-5 Automated ASR Widely Technology Realistic Natural Conversation Tool II-8 Text-to-speech TTS Market Potential Growth TTS Aids Customer Engagement II-10 Developed Markets Dominance II-11 Developing Countries Global Growth Short-to- Medium Term Period II-11 Table Voice Technology Market Percentage CAGR Period Graph/Chart II-12 Competitive Landscape China Market Tough Nut Crack Google Patent Activity Gains Momentum Speech Technology Market II-14 A Review Competition Key Sectors II-15 Speech Engine Self-Service Suite Speech Security II-15 Professional Services Major Challenges Impeding Speech Voice Recognition Market II-16 Limited Understanding Words Inability Process Large Vocabulary Information Low Accuracy Ambient Noise II-16 Lack Awareness TRENDS ISSUES DRIVERS II-17 Rapid Penetration Mobile Devices Growth Driver II-17 Opportunity Indicators Mobile Breakdown Million Units Years Graph/Chart II-18 Rate Voice Processors Mobile Devices Graph/Chart II-20 Mobile SMS Mobile Technology Power Improves Efficiency II-21 Mobile Voice Portals Drive Adoption Mobile Phones II-21 Voice Become Part Primary Interface Smartphone Users Robust Growth Smartphones Sets Platform II-22 Table Billion Units Graph/Chart Rate Countries Worldwide Population Graph/Chart II-24 Popularity Tablet Phones Market II-25 Table Desktop PCs Laptops Tablets Million Units Graph/Chart Next Frontier Technology II-25 Technology Innovations Growth II-27 Intelligent Virtual Assistants Speech Multimodal Solutions New Focus Area II-28 Integration Graphic User Interface GUI Voice User Interface VUI Voice-Interface Devices II-30 Technology Developers Focus Phoneme Accuracy Feature II-30 Self-Service Technology II-31 Promising Technology Call Centers Speech Technology Outsourced Customer Care Services Centers Proficiency Speech Analytics II-33 Expanding Use IVR Systems Call Centers Lends Traction Market Growth IVR Systems Market Breakdown Installed Systems Input Mode Graph/Chart Interactive Voice Response IVR Systems Geographic Region US Million Years Graph/Chart II-35 Demand Hosted IVR Services Fuels Demand Speech Technology Solutions II-35 Rising Importance Speech Technology IoT Era II-36 Cloud Computing Technology Revolutionizes Speech Technology Market II-36 Healthcare Facilities Seek Aid Cloud Voice Documentation II-38 User Experience Usability Take Center Stage II-38 Cloud Solutions Boost Adoption Voice Recognition Technologies II-38 Cloud Services Drive Mobility Voice Authentication II-39 Globalization Workforce Decentralization Spur Demand Enterprise Sector Share Breakdown Revenues End-Use Segment Graph/Chart Breakdown Mobile Workforce Million Years Region Graph/Chart Speech Analytics Gather Steam II-41 Rising Popularity Biometrics Offer Ample Growth Opportunities II-42 Voice Biometrics Emerges Game Changer Authenticating Mobile Devices Increasing Security Threats Drive Voice Verification Biometrics Market II-43 Favorable Trends IT Industry Market Prospects II-44 Spurt Internet Internet Usage Number Internet Users Billion Internet Penetration Rate Graph/Chart II-44 Internet Number Internet Subscribers Million Penetration Rate Geographic Region Graph/Chart Move Convergence Proliferation Embedded Devices II-47 Continued Expansion Software Sector II-47 Rise Social Networking VoIP Model Demand Searching Gains Momentum II-48 Speech Technology Solutions Convenience Safety Automobiles II-48 Telematics Car Infotainment Offers Opportunities Platter II-50 Voice Recognition Foster Use Automotive Telematics Adoption Technology Vehicle Systems Overcoming Challenges Comprehending Voice Commands Automobiles Suppression In-Car Noise Effectiveness Voice Recognition Technology II-52 Blurring Demographic Barriers Encourage Car Systems Growth Automobiles Production Bodes Well Market II-53 Table Automobile Production Graph/Chart Charms Connected Media Solutions II-54 Audio Mining Online Audio Video Content II-55 Seeks Improvements Processor Technology II-55 Technology Transforms Banking Industry II-56 Voice Authentication Systems Mitigating Online Banking Fraud II-56 Healthcare Industry Major Untapped Opportunity II-57 Voice Documentation Emerge Preferred Means Data Collection Front-end Deployments II-58 Radiology Department Point-of-Care Application Areas II-59 Electronic Medical Records EMR II-59 Self-Service Solutions Healthcare Sector II-60 Rising Incidence Cognitive Impairment Issues Drive Focus Assistive Speech Technology II-60 Rising Prominence Voice Controls Smart Homes II-61 Distribution Warehouse Management Leverages Potential Speech Technology How Speech Technology Distribution Center II-62 Advantages Speech Technology Distribution Center II-63 Growing Opportunities SOHO Market II-64 Speech Technology Finds Favor Law Enforcement Agencies II-64 Law Enforcement Agencies Seek Exploit Voice Stress Analyser II-64 Voice Stress Analyser Polygraphs II-65 Increasing Use Speech Technology Next Generation Networks NGNs II-65 Speech Technology Facilitates Quick Information Access Military II-65 Open Standards Key Architectural Requisite II-66 PRODUCT OVERVIEW II-67 Speech Technology Processing Human Speech II-67 History Speech Technology Landmarks Advancement Technology II-68 Technology Dictation Mode Command Control Mode Developers II-70 II-71 Cost Savings Improvement Call Completion Delivery Value-Added Content II-71 Higher Customer Satisfaction II-71 Reduction Agent Turnover Rate II-71 Advantages Brief Speech Technology Automatic II-72 Types Automatic II-73 Discrete Word Recognition Continuous Word Recognition Word Spotting Phoneme Recognition Natural Language Recognition II-73 Types ASR Technology Solutions II-74 Speaker-Independent Technology II-74 Speaker-Dependent Technology II-74 Isolated Systems II-75 Connected Systems II-75 Continuous Systems II-75 Automatic II-75 Principal Applications Automatic Sector II-76 Prospective Document Editing Data Edutainment Games Synthesis II-78 Process Description Speech Synthesis II-79 Visual Text-to-Speech Text-to-Speech Vs. Digitized Voice II-79 Voice Recognition Technology Speaker Verification Speaker Verification II-81 Text-Dependent Speaker Verification Methods II-81 Hidden Markov Modeling HMW II-81 Dynamic Time Warping DTW II-81 Speaker Verification Methods II-82 Method Multivariate Auto-Regression MAR II-82 Method II-82 Drawbacks II-82 Distributed System II-82 Distributed Services II-83 Speech Technology Related Concepts II-83 Interactive Voice Response Phonetic IVR Systems II-84 Attributes Advanced Speech Solutions IVRs II-85 Advantages IVR Customer Service II-85 Advantages IVR Management II-85 Voice Over Internet Protocol Language II-86 Design Development Implementation NLSR Applications II-86 Preliminary Collection Data II-87 Transcription Data Describing Key Reasons Customer Calls II-87 Tagging II-87 System Training II-87 Speech Application Language Tags II-87 Wireless Application Protocol II-88 Voice Portal II-88 VoiceXML II-89 Voice/Data Convergence Voice Browsers PRODUCT TECHNOLOGY INNOVATIONS/ INTRODUCTIONS II-91 Sensory Introduces TrulyHandsfree SDK II-91 Interactions Unveils Curo Speech Application II-91 VoiceBox Introduces New Embedded ASR Product Automotive Applications Nuance Releases Dragon Mac Medical Software TranscribeMe Unveils High Accuracy Automated Technology Uniphore Introduces Latest Version II-91 Ozonetel Introduces Technology II-92 NTT Introduces Speech-Recognition API Cross-Browser Use II-92 Listen Technologies Launches TalkPerfect DX Speech Enhancement System II-92 Sensory Introduces TrulyNatural Platform II-92 Nuance Releases Dragon Dictate Medical Mac II-92 BigHand Upgrades Server-Side Module II-92 Azure Media Launches Service Index Video Content Nuance Communications Launches Dragon NaturallySpeaking II-93 LumenVox Unveils LumenVox II-93 Fujitsu Laboratories Develops New Speech Synthesis Technology II-93 Nuance Communications Upgrades PowerScribe Platform Healthcare II-93 DSP Group Sensory Rolls Out Voice Activation Solution II-93 Genesys Launches Actionable Analytics Solution II-93 Nuance Communications Rolls Out Dragon Drive-Powered Mercedes News App IDS Unveils Voice2Dox Application Clinical Reporting II-94 Sensory Tensilica Unveil Lowest Power Voice Activation Solution Nuance Communications Launches Next Generation Voice Biometrics Platform NeoSpeech Unveils VoiceEZCALL English Learning Program Software II-94 Nuance Communications Rolls Out New Version Dragon Medical Practice Edition Smaller Practices II-94 Nuance Communications Launches Voice Ads II-94 Agnitio Unveils BATVOX Forensic Speaker Verification System II-95 ReadSpeaker Unveils Web Application Service Platform Text INDUSTRY ACTIVITY Nuix Voci Enter Strategic Partnership Speech Analytics II-96 Vuzix Sensory Partnership II-96 CHRISTUS Health Selects Nuance Documentation Solutions II-96 Dataworxs Australia Nuance Resell Dragon Medical Speech Technology Australia ReadSpeaker Blackboard Team Up Text-to-Speech Tech II-96 VocalZoom Cobalt Speech Language II-96 VoiceBox Acquire Telisma Speech Product Unit OnMobile II-97 Sensory Phillips Collaborate Technologies Acapela Takes Over CreaWave Apple Acquires VocalIQ Facebook Takes Over Wit.ai Interactions Concludes Acquisition AT T Watsonsm Technology Program II-97 Samsung Galaxy Products Deploy Sensory Embedded Speech Technologies II-98 Convergys Partners Voci Technologies II-98 Sena Technologies Selects Sensory TrulyHandsfree Voice Trigger Phrase Spotting Technology II-98 Sensory Receives Patent Claims TrulyHandsfree Voice Control Approach II-98 Telecommunications Deploys LumenVox Automatic Speech Recognizer II-98 NXP Semiconductors Integrates Rubidium ASR Voice Trigger Engine CoolFlux DSP II-98 Nuance Communications Delivers Speech Natural Language Understanding Technology China Mobile Jiangsu Branch II-98 Honeywell Acquires Intermec Apple Acquires Novauris Technologies II-99 Applications Technology Takes Over Leidos Omnifluent Human Language Technology Software Associated Business II-99 Amazon.com Acquires IVONA Software II-99 Nuance Communications Signs Collaboration Deal ZTE II-99 Wolfson Microelectronics Selects Sensory TrulyHandsfree Voice Control Audio Detect Front-End II-99 Nuance Communications Integrates New DIRECTV Voice Search Feature DIRECTV App Aldebaran Robotics Integrate Nuance Conversational Voice Capabilities NAO Robot II-100 Nuance Communications Delivers Dragon Drive Voice Capabilities Kenwood Audio Visual Navigation Systems II-100 Genesys Acquires Utopy Avaya Includes LumenVox DevConnect Select Product Program II-100 Nuance Communications Inaugurates Brand New Mobile Innovation Center Massachusetts Nuance Communications Tweddle Connect Tweddle Group II-100 Toyota Selects Nuance Dragon Drive Smart G-BOOK Smartphone Application ReadSpeaker Partners Instructure II-101 Tatra Banka Deploys Nuance Communications FreeSpeech Voice Biometrics Solution Nuance Communications Teams Up Widespace II-101 Nuance Communications AutoNavi II-101 ON SELECT GLOBAL PLAYERS Customer Inc. US Acapela Group France Advanced Voice Recognition Systems Inc. US II-102 AGNITIO S.L Spain Apple Inc. US II-103 Applied Voice Speech Technologies Inc. US II-103 Avaya Inc. US II-104 BioTrust ID B.V. Netherlands II-104 Cisco Systems Inc. US Convergys Corp. US Genesta Partnership US Genesys Telecommunications Laboratories Inc. US II-106 Google Inc. US II-106 Honeywell Scanning Mobility US II-106 IBM Corporation US LumenVox LLC US II-107 M*Modal Inc. US Microsoft Corporation US Netcall Telecom UK Nuance Communications Inc. US II-109 Sensory Inc. US SpeechFX Inc. US II-110 VoiceBox Technologies Corporation US II-110 VoiceVault Inc. US Voxware Inc. US Wizzard Speech LLC US MARKET PERSPECTIVE World Recent Past Current Future Analysis Voice Technology Geographic Region Canada Japan Europe Asia-Pacific Latin America Rest World Markets Independently Analyzed Annual Revenue Figures US Thousand Graph/Chart II-112 World Historic Review Voice Technology Geographic Region Canada Japan Europe Asia-Pacific Latin America Rest World Markets Independently Analyzed Annual Revenue Figures US Thousand Graph/Chart World Perspective Voice Technology Geographic Region Breakdown Revenues US Canada Japan Europe Asia-Pacific Latin America Rest World Markets Years Graph/Chart II-114 World Perspective Voice Technology Geographic Region Breakdown Revenues Voice Recognition Markets Years Graph/Chart UNITED STATES A.Market III-1 Speech Voice Recognition Software Market Overview III-1 Speech Technology Market Remains Firm III-1 Voice Recognition Market Smartphones III-2 Technology Role Healthcare Sector Adoption Systems Amid Growing Threats Financial Sector Embraces Voice Biometrics III-3 Stricter Government Regulations Propel Demand Interactive Analytics Systems III-4 Rising Demand Speech Analytics Technology Insurance Industry Automotive Industry Voice Recognition Technology Driver Safety III-5 Technology Making Way Air Traffic Control Launches Strategic Corporate Developments III-9 Key III-13 B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-22 A.Market III-24 Current Future Analysis Market Overview VoiceTIMES Speech Technology Initiative III-24 B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-25 A.Market III-27 Current Future Analysis Launches Strategic Corporate Development III-27 B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-28 Market III-30 Table Recent Past Current Future Analysis Voice Technology Geographic Region France Germany Italy UK Spain Russia Rest Europe Markets Independently Analyzed Annual Revenue Figures US Thousand Graph/Chart III-31 Historic Review Voice Technology Geographic Region France Germany Italy UK Spain Russia Rest Europe Markets Independently Analyzed Annual Revenue Figures US Thousand Graph/Chart Perspective Voice Technology Geographic Region Breakdown Revenues France Germany Italy UK Spain Russia Rest Europe Markets Years Graph/Chart III-33 A.Market III-34 Current Future Analysis Strategic Corporate Developments III-34 Acapela Group Key Player III-34 B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-35 UNITED KINGDOM A.Market III-41 Current Future Analysis Strategic Corporate Developments III-41 Netcall Telecom Key Player III-41 B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-42 III-44 A.Market Analysis III-44 Current Future Analysis Product III-44 AGNITIO S.L Key Player B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-45 EUROPE A.Market III-49 Current Future Analysis Strategic Corporate Developments III-49 BioTrust ID B.V. Netherlands Key Player III-50 B.Market Analytics Europe Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-51 A.Market Future Growth III-53 Strong Demand Smartphones Augurs Well Asia-Pacific III-53 B.Market Analytics Recent Past Current Future Analysis Voice Technology Geographic Region Australia China India South Korea Rest Asia-Pacific Markets Independently Analyzed Annual Revenue Figures US Thousand Graph/Chart III-54 Historic Review Voice Technology Geographic Region Australia China India South Korea Rest Asia-Pacific Markets Independently Analyzed Annual Revenue Figures US Thousand Graph/Chart III-55 Table Voice Technology Geographic Region Breakdown Revenues Australia China India South Korea Rest Asia-Pacific Markets Years Graph/Chart III-56 A.Market III-57 Current Future Analysis Strategic Corporate Development III-57 B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-57 A.Market III-59 Current Future Analysis Baidu Holds Edge China Market III-59 Strategic Corporate Developments III-60 B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-61 A.Market III-63 Current Future Analysis Speech Technology Aid Farmers III-63 Promise Technology India Agro Sector Launches B.Market Analytics Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-65 ASIA-PACIFIC A.Market III-69 Current Future Analysis Singapore Key Regional Market III-69 B.Market Analytics Asia-Pacific Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart III-69 Voice Technology Product Segment Breakdown Revenues Brazil Rest Latin America Markets Years Graph/Chart WORLD Market III-74 Table World Recent Past Current Future Analysis Voice Technology Analyzed Annual Revenue Figures US Thousand Graph/Chart,3
160,"PUNE, India, Jun 13, 2016 (PR Newswire Europe via COMTEX) -- PUNE, India, June 13, 2016 /PRNewswire/ -- The voice recognition market is expected to grow from $440.3 million in 2015 to reach $1.99 Billion by 2022, at a CAGR of 23.66% between 2016 and 2022 driven by factors such as the increasing acceptance of biometric technologies like voice verification for remote authentication applications. Complete report on global speech & voice recognition market spread across 164 pages, profiling 13 companies and supported with 82 tables and 54 figures is now available at http://www.reportsnreports.com/reports/590849-speech-voice-recognition-market-by-technology-speech-recognition-voice-recognition-application-ai-based-non-ai-based-vertical-automotive-consumer-finance-retail-military-healthcare-government-and-geography-global-forecast-to-2022.html . The installation of biometric security for mobile banking is driving the demand for voice recognition technology in the banking & finance sector. The governments in various countries are among the major end users of voice recognition technology. The growing adoption of automated and smart applications in consumer and healthcare industries is the major contributor for the growth of the speech recognition market. The speech recognition technology holds the major market size because of its increasing adoption by major players such as Google, Apple, and Microsoft. On the other hand, the speech recognition market is expected to grow from USD 3.73 billion in 2015 to reach USD 9.97 billion by 2022, at a CAGR of 15.78% during the forecast period. The factors driving the speech recognition market are the increasing acceptance of speech recognition solutions in the healthcare and banking, financial services, and insurance sectors for time and attendance monitoring. Self-service applications in Asia-Pacific emerged in 2008 as the developing economies witnessed strong organic growth in sectors such as telecom and banking. The growth in the customer care services in various verticals such as banks, telecom, travel, and hospitality contributes to the growth of the speech & voice recognition market to combat the increasing occurrence of fraudulent activities in these sectors. China, Japan, and India are the fastest-growing economies in Asia-Pacific. Their financial strength enables them to make significant investments in the biometric sector for security-related applications. Several government projects are increasingly using voice biometrics for the purpose of security and authentication. The presence of major consumer electronics device manufacturers such as Samsung (South Korea) and Sony (Japan) creates a lucrative market for speech & voice recognition technology in this region. Major players in the speech & voice recognition market are Nuance Communications (U.S.) , Microsoft Inc. (U.S), Agnitio SL (Spain), Biotrust (Netherland), VoiceVault (U.S.), VoiceBox Technologies Corp. (U.S.), LumenVox LLC. (U.S.), M2Sys LLC (U.S.), Raytheon BBN Technologies (U.S.), M2SyS LLC (U.S.) ValidSoft UK Limited (U.K.), Advanced Voice Recognition Systems (U.S.), Sensory Inc. (U.S.), and MMODAL Inc. (U.S.). Order a copy of Speech & Voice Recognition Market by Technology (Speech Recognition, Voice Recognition), Application (AI Based, Non AI Based), Vertical (Automotive, Consumer, Finance, Retail, Military, Healthcare & Government) and Geography - Global Forecast to 2022 research report at http://www.reportsnreports.com/purchase.aspxname=590849 . In the process of determining and verifying, the market size for several segments and sub segments gathered through secondary research, extensive primary interviews were conducted with key people. In Tier 1 (30%), Tier 2 (50%) and Tier 3 (20%) companies were contacted for primary interviews. The interviews were conducted with various key people such as C-level Executives (40%), Directors (25%) and others (35%) from various key organizations operating in the global speech & voice recognition market. The primary interviews were conducted worldwide covering regions such as North America (39%), Europe (31%), APAC (17%) and RoW (13%). On a related note, another research on Speech Analytics Market Global Forecast to 2020 says, the market size is to grow from USD 589.1 million in 2015 to USD 1.60 billion by 2020, at a CAGR of 22.0% from 2015 to 2020. Growing adoption of contact centers and increased importance of voice in the multichannel world are driving the market. Asia-Pacific is witnessing high growth potential during the forecast period. The report helps stakeholders to understand the pulse of the market and provides them information on key market drivers, restraints, challenges, and opportunities. Companies like Intelligence Interactive Group, Inc. NICE Systems Ltd. Verint Systems Avaya, Inc. Genesys Telecommunication Laboratories, Inc. HP Enterprise inContact, Inc. Calabrio, Inc. and Callminer, Inc. Calabridge, Inc. have been profiled in this 145 pages research report available at http://www.reportsnreports.com/reports/276472-speech-analytics-market-solutions-speech-engine-indexing-analysis-reports-dashboards-applications-business-process-agent-performance-market-intelligence-risk-compliance-management-cem-worldwide-market-forecast-2014-2019-.html . ReportsnReports.com is an online market research reports library of 500,000+ in-depth studies of over 5000 micro markets. Not limited to any one industry, ReportsnReports.com offers research studies on agriculture, energy and power, chemicals, environment, medical devices, healthcare, food and beverages, water, advanced materials and much more. Contact: Ritesh Tiwari UNIT no 802, Tower no. 7, SEZ Magarpatta city, Hadapsar Pune - 411013 Maharashtra, India. + 1 888 391 5441 sales@reportsandreports.com ",http://www.marketwatch.com/story/voice-recognition-market-to-rise-at-2366-cagr-while-speech-recognition-to-hit-1578-to-2022-2016-06-13-420338,"The voice recognition market is expected to grow from $440.3 million in 2015 to reach $1.99 Billion by 2022, at a CAGR of 23.66% between ...",Voice Recognition Market to Rise at 23.66% CAGR While Speech ...,PUNE India Jun PR Newswire Europe COMTEX PUNE India June Billion CAGR Google Apple Microsoft USD USD CAGR Asia-Pacific China Japan India Asia-Pacific Samsung South Korea Sony Japan Nuance Communications U.S. Microsoft Inc. U.S Agnitio SL Spain Biotrust Netherland VoiceVault U.S. VoiceBox Technologies Corp. U.S. LumenVox LLC U.S. M2Sys LLC U.S. Raytheon BBN Technologies U.S. M2SyS LLC U.S. ValidSoft UK Limited U.K. Advanced Voice Recognition Systems U.S. Sensory Inc. U.S. MMODAL Inc. U.S. Speech Voice Recognition Market Technology Voice Recognition Application AI Non AI Vertical Automotive Consumer Finance Retail Military Healthcare Government Geography Global Forecast Tier Tier Directors North America Europe APAC RoW Speech Analytics Market Global Forecast USD CAGR Asia-Pacific Intelligence Interactive Group Inc. NICE Systems Ltd. Verint Systems Avaya Inc. Genesys Telecommunication Laboratories Inc. HP Enterprise Inc. Calabrio Inc. Callminer Inc. Calabridge Inc. ReportsnReports.com ReportsnReports.com Ritesh Tiwari UNIT Tower SEZ Magarpatta Hadapsar Pune Maharashtra India,3
161,"Cambridge-based speech technology company Speechmatics has received investment from a range of investors to accelerate the commercial roll-out of its products. Founded by chief technology officer Tony Robinson, the firm received investment from several sources, including technology venture capitalist IQ Capital; AI/machine learning specialist and technology investors Amadeus Capital Partners; and a number of technology investors including Laurence Garrett (Highland Capital Europe), Cambridge professor Ted Briscoe, a specialist in Natural Language Processing as well as co-founders of CSR, and former executive chairman at SwiftKey Richard Gibson. Robinson said: ""We are at the forefront of how deep neural networks are changing speech recognition. With our ever expanding and highly experienced R&D team, we continue to push the boundaries in speech technology, especially around languages, accuracy and deployment."" Speechmatics' technology enables businesses to generate data about customers and employees which is harnessed to improve process, efficiency, and benefit the bottom line. In 2016, Speechmatics launched its new AI framework, Auto-Auto, which enables the company to add almost any language automatically. Since building the framework, Speechmatics has released a new language every two weeks, including most European languages, and Greek, Russian and Arabic. Benedikt von Thngen, chief executive of Speechmatics, said: ""Over the past two years Speechmatics has seen substantial growth, entirely through cash-flow. ",https://www.insidermedia.com/insider/central-and-east/speech-recognition-company-closes-growth-funding-round,Robinson said: We are at the forefront of how deep neural networks are changing speech recognition. With our ever expanding and highly ...,Speech recognition company closes growth funding round,Speechmatics Tony Robinson IQ Capital AI/machine Amadeus Capital Partners Laurence Garrett Highland Capital Europe Cambridge Ted Briscoe Natural Language Processing CSR SwiftKey Richard Gibson Robinson R D Speechmatics Speechmatics AI Auto-Auto Speechmatics Greek Russian Arabic Benedikt von Thngen Speechmatics Speechmatics,6
162,"$226.2 Billion Robotics Market Forecasts, 2021 Research and Markets has announced the addition of the ""Robotics Market Forecasts: Consumer Robots, Enterprise Robots, Industrial Robots, Healthcare Robots, Military Robots, Unmanned Aerial Vehicles and Autonomous Vehicles"" report to their offering. Driven mainly by growth in these new categories, the author forecasts that the global robotics industry will expand from $34.1 billion in 2016 to $226.2 billion by 2021, representing a compound annual growth rate (CAGR) of 46%. The robotics market is undergoing a major transformation, in which robots are growing beyond being the workhorses of industrial shop floors, and beginning to assume the roles of personal assistants, delivery vehicles, surgical assistants, exoskeletons, autonomous vehicles, and unmanned aerial vehicles (UAVs), among many others. Until now, industrial robots have made up more than 50% of overall robotics market revenue, however 2016 marks one of the critical turning points of this shift in the robotics market, as Tractica estimates that industrial robots will drop to 41% of total robotics revenue, with the remaining 59% coming from non-industrial robots. The non-industrial sector largely consists of consumer robots, enterprise robots, military robots, and UAVs. This report includes global market sizing, segmentation, and forecasts for the robotics industry within 23 distinct application markets in the consumer, enterprise, industrial, and military domains. Unit shipments and revenue forecasts are included for each application market, segmented by world region. The report also includes technology attach rates for machine vision, voice and speech recognition, gesture recognition, and tactile sensors, plus connectivity technologies for consumer robots. The forecast period for this report extends from 2016 through 2021. ",http://www.iconnect007.com/index.php/article/103144/2262-billion-robotics-market-forecasts-2021/103147/?skin=ein,"The report also includes technology attach rates for machine vision, voice and speech recognition, gesture recognition, and tactile sensors, ...","$226.2 Billion Robotics Market Forecasts, 2021",Billion Robotics Market Forecasts Research Markets Consumer Robots Enterprise Robots Industrial Robots Healthcare Robots Military Robots Vehicles CAGR UAVs Tractica UAVs,3
163,"Sign up for our newsletter and get the latest HPC news and analysis. Send me information from insideHPC: Today Mellanox announced that one of Chinas leading intelligent speech and language technologies companies, iFLYTEK , has chosen Mellanoxs end-to-end 25G and 100G Ethernet solutions based on ConnectX adapters and Spectrum switches for their next generation machine learning center. The partnership between Mellanox and iFLYTEK will enable iFLYTEK to achieve a high speech recognition rate of 97 percent. Mellanoxs solution has enabled iFLYTEK to build a next generation machine learning center that will be accelerate our application performance and provide us with our future needs, said Dr. Zhiguo Wang, executive vice president of iFLYTEK Research Institute. Moreover, we leverage the scalability of Mellanox Ethernet solutions to grow our compute and storage needs in the most efficient manner. iFLYTEK is a nationally renowned software enterprise based in China and dedicated to the research of intelligent speech and language technologies, provision of speech information services, and integration of E-government systems. To support a diverse number and growing type of applications, iFLYTEK requires a high performance and efficient data center network solution that needs to be both compatible with the companys current infrastructure and scalable for future computing and storage requirements. Speech recognition has made tremendous advancements in the past couple of years and we are excited to partner with iFLYTEK to build the next generation of machine learning systems for this technology development, said Gilad Shainer, vice president of marketing at Mellanox Technologies. Mellanoxs 25G/100G Ethernet solutions provide the most cost effective, high performance Ethernet connectivity allowing iFLYTEK to provide the needed compute and storage resources to their developers. Mellanoxs Open Ethernet 25G and 100G Spectrum switches enable iFLYTEK to build a high- performance architecture that is flexible, scalable and easily managed. The iFLYTEK machine learning systems can handle massive concurrent traffic and is capable of supporting unpredictable future business growth. Mellanoxs intelligent networking solutions provides iFLYTEK with automatic networking provision and management, convergence network infrastructure with Quality of Service and RDMA over Ethernet (RoCE). In addition, the solution features scalability via a hyperscale topology that offers full layer 3 BGP as routing and 8/16 ECMP. The Mellanox Spectrum switch-based leaf-spine topology architecture resolves the bottleneck challenges of horizontal network connections and provides excellent scalability, making it ideal for nearly all small-to-middle sized data centers. IT architectures for most enterprises tend to employ a convergent and highly virtualized spine structure and to address this trend, Mellanox provides best-in-class products for industrial users to build IT infrastructure that meet both current and future data network needs. ",http://insidehpc.com/2016/12/mellanox-25g100g-ethernet-solutions-enables-artificial-intelligence-speech-recognition-technology-at-iflytek/,"mellanox Today Mellanox announced that one of China's leading intelligent speech and language technologies' companies, iFLYTEK, has ...",Mellanox 25G/100G Ethernet Speeds Speech Recognition at iFLYTEK,Sign HPC Mellanox Chinas Mellanoxs Ethernet ConnectX Spectrum Mellanox Mellanoxs Dr. Zhiguo Wang Research Institute Mellanox Ethernet China Gilad Shainer Mellanox Technologies Mellanoxs Ethernet Ethernet Mellanoxs Open Ethernet Spectrum Mellanoxs Quality Service RDMA Ethernet RoCE BGP ECMP Mellanox Spectrum IT Mellanox IT,3
164,"Jackson Browne inducted folk legend Joan Baez at the 2017 Rock and Roll Hall of Fame ceremony at Brooklyn's Barclays Center Friday night. Bruce Springsteen inducted Browne into the Hall of Fame in 2004 and in 2013, Browne appeared at the ceremony to help honor inductee Randy Newman. Browne and Baez have crossed paths multiple times over the years, with Baez having covered and interpreted songs by Browne on her own albums. Last year, the folk legends performed on stage for Baez's 75th birthday celebration at New York's Beacon Theater, singing ""Before the Deluge"" and Woody Guthrie's ""Deportee (Plane Wreck at Los Gatos)"" together. The songwriter gave a deeply personal speech about Baez, tracing her involvement in his own musical upbringing and greater contribution to social causes. Read the full speech below. The changes that began happening in the Sixties - the civil rights movement, opposition to the Vietnam war, the sexual revolution, the spiritual exploration and consciousness expansion.... Women's Liberation..... None of that can be separated from the folk music that was being rediscovered and brought to the forefront of popular culture. On college campuses, in coffee houses, and at folk festivals, a new generation was discovering the true history of this country through the music of the people who built it. And we weren't just listening to it. We were learning to sing and play these songs that contained the hardship, and the struggles and the hopes of people who had come to this country as immigrants. And as slaves. Folk musicians began traveling to parts of the country where people still made this music, and they began finding out who was actually here. And that was something you couldn't find out in those days by watching TV or going to the movies. Of all the many great artists who were singing and recording this music, and who embodied the search for what was real, historic, and eternal - the one who suddenly emerged and came to national prominence - was Joan Baez. From the moment she appeared in the Cambridge folk scene, she had a spellbinding effect on her audiences. In 1960, at the age of 19, she released her first album, and then a second album, and then a live album. And then she was on the cover of Time Magazine, as the face and the voice of the new folk movement. The first record I ever bought with my own money, was Joan Baez's second album. I was 14. I went down to our little record store there in Fullerton - out in Orange County - where I had just moved with my family from L.A., and they had a listening booth, where you could play records before buying them. I saw this album with her picture on it, and she looked like the girls I had grown up with in Highland Park, my old neighborhood in L.A.  I went into that listening booth, and right away I was taken with - what was for me - completely new music. Just voice and guitar. But so ethereal. Powerfully in tune, deeply expressive. Dramatic. Hypnotic. By the third song I was completely mesmerized. I took the record home and started learning to play that third song, ""The Lily Of The West."" The purity of her voice was intoxicating - her enormous dynamics, and the command she had as a singer - mixed with the drama and the mystery of those old songs - led me into the world of folk and blues, and the voice-and-guitar driven narratives that have been at the center of my musical quest my whole life. Almost immediately she introduced her audience to the songs of Bob Dylan. Joan Baez gave Bob Dylan a national audience. When she began singing his songs, those who had been time traveling through folk music and discovering all the human drama, and the eternal truths of our shared history, were suddenly in the present! With God On Our Side. ""With God On Our Side"" - this Dylan song, which summarized and examined the history of US wars and the supposed rational for each of them, was one of the two songs Joan and Bob sang on her concert tour in 1963, catapulting the broadside - or what is now known as the protest song - into the consciousness of a whole generation. Both of these songs had a galvanizing effect on me and my friends. We joined CORE - the Congress of Racial Equality. We joined hands and sang, we demonstrated. We started writing songs. And we were doing this out in Orange County, the bastion of the ultra-conservative John Birch Society. But It was happening all across America. And when I saw that Joan Baez was marching with Martin Luther King, I felt that I was represented there. And that they were marching for all of us. And that they were marching for - to paraphrase Langston Hughes - ""the America that has never been yet, and yet must be."" There is no way to quantify how much Joan Baez means to the people of my generation who grew up listening to her voice, leading us in singing We Shall Overcome. When I hear the recording now - I feel a deep sadness that the song is as needed now as it was then. Now even more. The changes that began happening in the Sixties are still happening. And the injustice we opposed then we must still oppose. And we need to be empowered now as much as we ever have needed to be empowered. To track Joan Baez's involvement in human rights and social justice is to chart the evolution of our own moral awakening, and of our own growing planetary consciousness. Her example has been, from the beginning, empowering for women. And for men, also. Of course, the women are smarter, and it has taken men a little longer to realize that we were being empowered too. But that's right. Joan Baez empowered me to sing and play guitar when I was a kid, not that much younger than her, and to find my voice, and to eventually try to use it to make the changes I need to see in the world. So - it is my honor and my pleasure - and a recognition long, long overdue - To welcome Joan Baez to the Rock and Roll Hall of Fame. ",http://www.rollingstone.com/music/news/read-jackson-brownes-joan-baez-rock-hall-induction-speech-w475769,"The songwriter gave a deeply personal speech about Baez, tracing her .... So - it is my honor and my pleasure - and a recognition long, long ...",Watch Jackson Browne's Laudatory Joan Baez Rock Hall Induction ...,Jackson Browne Joan Baez Rock Roll Hall Fame Brooklyn Barclays Center Friday Bruce Springsteen Browne Hall Fame Browne Randy Newman Browne Baez Baez Browne Baez New York Beacon Theater Deluge Woody Guthrie Deportee Plane Wreck Los Gatos Baez Sixties Vietnam Women Folk Joan Baez Cambridge Time Magazine Joan Baez Fullerton Orange County L.A. Highland Park L.A Just Dramatic Hypnotic Lily West Almost Bob Dylan Joan Baez Bob Dylan God Side God Dylan US Joan Bob CORE Congress Racial Equality Orange County John Birch Society America Joan Baez Martin Luther King Langston Hughes America Joan Baez Shall Overcome Joan Baez Joan Baez Joan Baez Rock Roll Hall Fame,11
165,"Disney Research has developed a speech recognition system specifically for the speech patterns ofkids, and tested it with a game that allows kids to control a mole using simple voice commands (Credit: Disney Research) Barking voice commands at a phone , car , computer , or a dedicated voice assistant like Alexa , is pretty commonplace these days, but these systems are usually designed with an adult manner of speaking in mind. Kids have very different speech patterns, and Disney Research has developed a system that caters to a younger crowd, picking out key words from excited chatter and overlapping speech to let kids play a video game with their voice. Mole Madness is the name of the game, and kids control the character with just two simple voice commands. Playing in pairs (either with another child or a robot named Sammy), one player says ""go"" to get the mole moving across the screen, while their partner steers it upwards by saying ""jump"". As simple as that seems for a speech recognition system to identify, the kids threw a few spanners in the works with a tendency to chitchat and talk over each other. ""Kids don't necessarily pronounce words quite like adults and when they are playing together, as they like to do, they often engage in side banter, or exclamations of excitement, or simply talk over each other,"" says Jill Fain Lehman, lead researcher on the project. ""That makes it tough for a speech-based system, even one that just has to detect the words 'go' and 'jump' as in Mole Madness."" At first, the voice recognition system had some human training wheels, in the form of a ""wizard"" in another room who would press a button on a controller when he heard either a ""jump"" or ""go"" command. After 62 children between the ages of five and 10 had played the game, the researchers had enough data to train the system to recognize those key words, both individually or said together, and differentiate them from background noise and other bits of banter. Once the system was automated and the wizard removed from the equation, the researchers reported it could pick out the keywords 85 percent of the time. Not bad, considering 40 percent of those commands overlapped when two kids were playing, and 32 percent were said faster than usual. By comparison, a commercially available speech recognition system was put through the paces and could only recognize 50 percent of the commands, and struggled with overlapping and fast speech. The automated system was also judged to be more engaging to players than when a researcher was entering the commands manually. According to parents watching the video later, the kids were closer to feeling like they ""could take it or leave it"" than solidly enjoying themselves. Once the game was automated, and could process a command within 150 milliseconds, the kids showed more signs of engagement with the game. The system did run into some issues with the ""jump"" command when a new group of kids, some as young as four, tried out the game, but the researchers found that the participants soon modified their speech patterns to help the system through, repeating the commands or saying them more carefully. ""Speech recognition applications have become increasingly commonplace as the technology has matured, but understanding what kids say when they play remains difficult,"" says Jessica Hodgins, vice president at Disney Research. ""This latest work by our researchers could make it possible to design any number of speech-based game or entertainment applications for children, including interactions with robots."" The researchers are presenting the study at the Workshop on Child Computer Interaction this week, and at the International Conference on Intelligent Virtual Agents later this month. ",http://newatlas.com/disney-research-kids-speech/45290/,"Kids have very different speech patterns, and Disney Research has ... As simple as that seems for a speech recognition system to identify, the ...",Disney's speech recognition system for kids cuts through the chatter,Disney Research Disney Research Barking Alexa Disney Research Mole Madness Sammy Kids Jill Fain Lehman 'jump Mole Madness Jessica Hodgins Disney Research Workshop Child Computer Interaction International Conference Intelligent Virtual Agents,3
166,"The 3rd Edition of the Cape Town Zimbabwe Excellence Awards 2017 explodes this Saturday in the Mother City with several businesses, entrepreneurs and individuals being honoured. The awards ceremony will be held at Kelvin Grove Conference Centre in Newlands, Cape Town. The awards seek to recognise, acknowledge and celebrate Zimbabwean individuals and companies that are pushing boundaries and chasing their dreams in South Africa. The annual awards are organised by Shamstone Production and Entertainment. Spokesperson for the awards, Charlene Dhliwayo said in a statement the awards have increased from the initial seven categories in 2015 to 12 this year. The organisers also received an unprecedented 320 nominations. ""These category numbers reflect as proof that the awards have become more inclusive and have broaden the recognition of initiatives by Zimbabweans in the South African community,"" Dhliwayo said. The awards seek to showcase the efforts, abilities and perseverance by Zimbabweans flying both the local and South African flags high  thereby influencing global recognition for their hard work and determination, pushing for full solidarity and unity in a foreign land. ""The idea is mainly to encourage a spirit of togetherness and recognition and mostly for motivation and inspiration to the rest of the community. ""We should embrace each other, support each other and embrace diversity and unity, only then we can emancipate ourselves,"" she said. Zimbabwe's Cape Town consular general Mr Bonface Mugobogobo is the guest of honour. ""We also welcome this year a youthful delegation to grace the awards ceremony led by Mr Brian Mudimu, vice chairperson Youth Development and Empowerment at the National Business Council of Zimbabwe."" Mr Mudimu was named one of the youngest influential leaders in Zimbabwe. He will deliver a speech on entrepreneurship and opportunities available for investment in Zimbabwe. Besides the awards ceremony, there will be lots' of entertainment on the cards. ""Our MC this year is Floridah Mapeto who is going to be partnering Cape Town DJ Morgan Chasara aka Morgan Stitch. This will be the first time for Floridah to be hosting the event since busting on the social media scene where she commands a lot of followers and she said she is as enthusiastic and excited as the rest of us. ""If you are in Cape Town, you don't wanna miss this event. It's going to be a night with lots of entertainment and networking,"" Dhliwayo said. Further details are available on Facebook page  Cape Town Zimbabwe Excellence Awards  and the event website www.zimexcellenceawards.com. Some of the partners to the awards include Mukuru Money Transfer, Zororo Phumlani, Steward Bank, Maps Graphics, Uhuru Training, Ruziwo Cambridge, Impress Clothing, Shamstone Production, Bra Stu Driving School, Wakati Trust, Zuriey Web Solutions, Av Studios, Manlee Construction and IMB. ",http://bulawayo24.com/index-id-news-sc-national-byo-108063.html,... and have broaden the recognition of initiatives by Zimbabweans in the ... He will deliver a speech on entrepreneurship and opportunities ...,Cape Town - Zimbabwe Excellence Awards on this weekend,Cape Town Zimbabwe Excellence Awards Saturday Mother City Kelvin Grove Conference Centre Newlands Cape Town Zimbabwean South Africa Shamstone Production Entertainment Spokesperson Charlene Dhliwayo African Dhliwayo African Zimbabwe Cape Town Mr Bonface Mugobogobo Mr Brian Mudimu Youth Development Empowerment National Business Council Zimbabwe Mr Mudimu Zimbabwe Zimbabwe Floridah Mapeto Cape Town DJ Morgan Chasara Morgan Stitch Floridah Cape Town Dhliwayo Further Facebook Cape Town Zimbabwe Excellence Awards Mukuru Money Transfer Zororo Phumlani Steward Bank Maps Graphics Uhuru Training Ruziwo Cambridge Impress Clothing Shamstone Production Bra Stu Driving School Wakati Trust Zuriey Web Solutions Av Studios Manlee Construction IMB,2
167,"Arlington, VA, March  15, 2017  (GLOBE NEWSWIRE) -- Celebrating 25 years of continued innovation in the education technology sector, Rosetta Stone Inc. (NYSE: RST) today announced significant updates to its flagship Learn Languages with Rosetta Stone program for individuals and families. Various subscription offerings and new features  Phrasebook, Stories, and Audio Companion  provide learners with an updated user experience that will get them talking in their new language right from the first lesson. Phrasebook: Never be at a loss for words with easy-to-access greetings, phrases, and useful expressions available at any point in the learners language journey and perfect for traveling! Stories: Learners can read and record fun stories and get feedback on their pronunciation from Rosetta Stones TruAccentTM patented speech-recognition engine. Rosetta Stone samples your voice with 16,000 data points each second, showing those updates in nearly real time  up to 100 times per second  to help perfect pronunciation. Audio Companion: Learners can enjoy hands-free learning with downloadable audio lessons they can take with them anywhere. Learning a language takes commitment and time, but it also takes the right tools and resources to help you achieve your learning goals, said John Hass, Chief Executive Officer at Rosetta Stone. Our mobile-first product innovations are centered on the goal to get people talking in their new language and I believe our newest offering delivers on that promise with the most comprehensive language learning solution in the marketplace. The new Learn Languages with Rosetta Stone debuted today across web and iOS platforms. Android customers will experience a completely revamped user interface and onboarding experience, with full parity to iOS expected in the coming months. Learners have various subscription options to choose from ranging from $79 for 3 months access, to $179 for 12 months access. Were extremely proud of the new features and updated content that this new product delivers and believe this optimized experience will help learners start speaking a new language even faster than before, said Peter Brussard, Senior Vice President of Product, Rosetta Stone. Rosetta Stones immersive curriculum is developed by language and education experts, and is sequenced to introduce new skills by stimulating the brain's natural language learning ability. In addition to getting learners talking in their new language right from the very first lesson, Rosetta Stone TruAccentTM technology helps users learn to speak authentically. With the companys live tutoring offering, learners have another way to master pronunciation and accelerate their learning by practicing face-to-face with native speakers. For more information on Learn Languages with Rosetta Stone, visit www.rosettastone.com . To download the newRosetta Stoneapp in iOS, visit the Apple app store or https://itunes.apple.com/us/app/learn-languages-rosetta-stone/id435588892mt=8 ; the Android app can be downloaded via the Google Play Store or https://play.google.com/store/apps/detailsid=air.com.rosettastone.mobile.CoursePlayer&hl=en . Images of the new program are available upon request for the media. Rosetta Stone Inc. (NYSE: RST) is dedicated to changing peoples lives through the power of language and literacy education. The companys innovative digital solutions drive positive learning outcomes for the inspired learner at home or in schools and workplaces around the world. Founded in 1992, Rosetta Stones language division uses cloud-based solutions to help all types of learners read, write, and speak more than 30 languages. Lexia Learning, Rosetta Stone's literacy educationdivision, was founded more than 30 years ago and is a leader in the literacy education space. Today, Lexia helps students build fundamental reading skills through its rigorously researched, independently evaluated, and widely respected instruction and assessment programs.  For more information, visit www.rosettastone.com .Rosetta Stone is a registered trademark or trademark of Rosetta Stone Ltd. in the United States and other countries. Michelle Alvarez, Head of Global Communications, Rosetta Stone 703-387-5862 |malvarez@rosettastone.com  Kim Kirchner, Coburn Communication 212.536.9837 |kim.kirchner@coburnww.com other press releases by Rosetta Stone Ltd. ",https://globenewswire.com/news-release/2017/03/15/937916/0/en/Newly-Upgraded-Rosetta-Stone-Language-Offering-Provides-Even-More-Features-and-User-Benefits.html,... read and record fun stories and get feedback on their pronunciation from Rosetta Stone's TruAccentTM patented speech-recognition engine.,Newly Upgraded Rosetta Stone Language Offering Provides Even ...,Arlington VA March GLOBE NEWSWIRE Rosetta Stone Inc. NYSE Learn Languages Rosetta Stone Phrasebook Stories Audio Companion Rosetta Stones TruAccentTM Rosetta Stone John Hass Chief Executive Officer Rosetta Stone Learn Languages Rosetta Stone Android Were Peter Brussard Senior Vice President Product Rosetta Stone Rosetta Stones Rosetta Stone TruAccentTM Learn Languages Rosetta Stone Stoneapp Apple Android Google Play Store Rosetta Stone Inc. NYSE Rosetta Stones Lexia Learning Rosetta Stone Lexia .Rosetta Stone Rosetta Stone Ltd. United States Michelle Alvarez Head Global Communications Rosetta Stone |malvarez @ Kim Kirchner Coburn Communication |kim.kirchner @ Rosetta Stone Ltd,5
168,"VIENNA--( BUSINESS WIRE )-- Speech        Processing Solutions , the leader in professional dictation has        received positive feedback on its new dictation workflow software        SpeechLive and its smart voice recorder, Philips SpeechAir. Philips SpeechAir is perfectly suited for busy people who do not always        work from their desks. Combined with Philips SpeechLive speech        recognition service, they can create text files immediately from        anywhere and be productive on the go. Ingo Bischof, MBA, Managing        Director of IVAM Real Estate, is delighted with the advantages of these        two solutions: ""With Philips dictation solutions it is possible for        us to log and record issues in the properties that we maintain        significantly more quickly. The recordings are then sent to our back        office, saving us a considerable amount of time, as transcription can        begin before we return to the office."" The SpeechAir is equipped with three professional-quality        microphones providing ultimate sound quality in any recording        situation . Background noise is filtered and the voices are recorded        in perfect quality. This ensures excellent playback and speech        recognition results. At the start of the year, the SpeechAir was awarded        Nuance Communications' rating for best recording        accuracy . The slide switch provides an efficient and intuitive user        interface operated with just one hand. Recordings can be started,        paused, stopped, played back, rewound and fast-forwarded using the slide        switch . The Wi-Fi and Bluetooth connections allow wireless transfer of finished recordings and access to        the customer's data in the respective document management system, email        or calendar. Dr Doris Ulreich-Laussermayer, a doctor in obstetrics and        gynaecology, appreciates this function. ""I can send additions to the        PC in the medical practice via Wi-Fi on the move or from home and my        assistant can edit them. Another thing that I especially like about        SpeechAir is its portability, as I can move freely around the practice.        The SpeechAir's antimicrobial surface is another positive aspect for        hygiene in my medical practice,"" says the doctor. SpeechLive speech        recognition  speak, send, receive the finished document The Philips SpeechLive cloud dictation solution now includes speech        recognition available in 21 languages. Spoken words are converted into a        written text in next to no time. SpeechLive can be used at any time and        is easily accessible via any web browser with no software installation        required. ""Our company is split between two different locations. Last        week I was working in the office in Melbourne; this week I am outside of        the city in Mount Waverley. It's great to be able to work from anywhere,        without having to think about taking something with me. I can sign in        from any computer,"" raves Nicole Honan, Office Manager at the        law firm Hicks Oakley Chessell Williams in Australia. For more information about our solutions visit: Speech        Processing Solutions is the global leader in professional dictation        solutions. The company was founded in 1954 in Austria as a Philips        subsidiary, and has been a driving force for innovative speech-to-text        solutions for 60 years. The company developed ground-breaking        products such as the mobile Philips        SpeechAir , the Philips        Pocket Memo voice recorder , the Philips        SpeechMike Premium USB dictation microphone and the Philips        Dictation Recorder app for smartphones, thus meeting its demands for        excellence and superior quality. Thanks to the newest innovation, Philips        SpeechLive , dictations and recordings will become faster and easier        than ever before with cloud-based workflow services. Speech Processing        Solution's perfectly tailored offers and products help professionals        save time and resources and maximize efficiency. Connect with Speech Processing Solutions on: ",http://www.businesswire.com/news/home/20161213005092/en/Philips-Offers-Professional-Speech-Recognition,"VIENNA--(BUSINESS WIRE)--Speech Processing Solutions, the leader in professional dictation has received positive feedback on its new ...",Philips Now Offers Professional Speech Recognition,VIENNA BUSINESS WIRE Processing Solutions SpeechLive Philips SpeechAir Philips SpeechAir Philips SpeechLive Ingo Bischof MBA Managing Director IVAM Real Estate Philips SpeechAir Background SpeechAir Nuance Communications Bluetooth Dr Doris Ulreich-Laussermayer Wi-Fi SpeechAir SpeechAir Philips SpeechLive Spoken Melbourne Mount Waverley Nicole Honan Office Manager Hicks Oakley Chessell Williams Australia Processing Solutions Austria Philips Philips SpeechAir Philips Pocket Memo Philips SpeechMike Premium USB Philips Dictation Recorder Philips SpeechLive Speech Processing Solution Speech Processing,5
169,"European Union should strike a deal that offers both sides access to each others markets and goes beyond the option of regulatory equivalence, according to an advisory group set up by bank lobbying bodies. The International Regulatory Strategy Group, which is sponsored bythe City of London Corporation and TheCityUK, laid out a possible framework for the bespoke agreement between Britain and EU that the U.K. government has said it plans to negotiate. Along with criteria for access, the report proposes a joint committee that ensures regulations stay aligned and lays out ways to resolve disputes within a free trade agreement that covers financial services. What were trying to do is set out a mechanism by which we feel we can have mutual recognition of our regulatory systems, which will enable cross-border business to continue to flow post-Brexit, Mark Hoban, chairman of the IRSG, said in a telephone interview. We want to try and shape thinking on both sides of the debate. EU member states want to avoid granting a deal that gives the U.K. full advantages of membership in the bloc after it exits. British lawmakers have argued that EU companies also benefit from London being a financial capital. The report echoes that case, saying the U.K. is a significant source of funding for smaller EU corporations. As things stand, foreign firms are allowed to sell a limited suite of financial services in the single market if the European Commission, the blocs executive arm, deems their home rules and oversight of specific business lines to be equivalent to the EUs. The status is granted by the commission and can be withdrawn at any time without appeal, making it more restrictive than so-called passporting, which is a right of membership of the bloc. The report seeks a deal based on the understanding that the U.K. and EU will operate regulatory regimes that have consistent regulatory objectives and aim to deliver comparable outcomes. While the U.K. will be excluded from the setting of EU regulation post-Brexit, global standards such as those developed by the Basel Committee on Banking Supervision, Financial Stability Board and the International Organization of Securities Commissions could form the basis for mutual access to the two markets, according to the report. speech on April 7, in which he said that while regulations need not be exactly the same, they should deliver similar outcomes. This is now possible given progress at the FSB, where a series of agreed reforms are leveling the playing field and reducing opportunities for regulatory arbitrage. Your cheat sheet on life, in one weekly email. Get our weekly Game Plan newsletter. While the U.K. will have the same regulatory structure as the EU on the day it leaves the bloc, the issue for the country is that as time passes and new challenges appear, the two frameworks will diverge. That potentially will lead to disagreements about whether the outcomes of the two sets of rules remain similar enough for continued access, an issue that has to be settled using a dispute resolution mechanism. Where disputes arise, the report suggests the two sides should again lean on global standard setters such as the FSB and IOSCO in resolving the difference. If there is a material divergence, the agreement should allow for it to be dealt with and where that doesnt happen there should be agreed processes governing the withdrawal of the arrangement, including giving affected firms time to adjust, according to the report. Before it's here, it's on the Bloomberg Terminal. ",https://www.bloomberg.com/news/articles/2017-04-10/london-bank-group-offers-plan-for-eu-market-access-after-brexit,"... we feel we can have mutual recognition of our regulatory systems, ... of England Governor Mark Carney's statement in a speech on April 7, ...",London Bank Group Offers Plan for EU Market Access After Brexit,Union International Regulatory Strategy Group City London Corporation TheCityUK Britain EU U.K. Mark Hoban IRSG EU U.K. EU London U.K. EU Commission EUs U.K. EU U.K. EU Basel Committee Banking Supervision Financial Stability Board International Organization Securities Commissions April FSB Game Plan U.K. EU FSB IOSCO Bloomberg Terminal,-1
170,"MARKHAM, ONTARIO--(Marketwired - Nov. 28, 2016) - VIQ Solutions Inc. (""VIQ"", ""VIQ Solutions"" or the ""Company"") (TSX VENTURE:VQS), a global provider of cybersecurity protected technology and service platforms for digital evidence capture and content management, is pleased to announce that VIQ Dataworxs was awarded a speech recognition integration upgrade with Bazzani Scully Priddle Lawyers (""BSP"") of Melbourne, Australia. ""Seamless integration of speech recognition technology in our secure digital capture, management and transcription workflow sets VIQ Dataworxs apart from our competitors. It is a key part of our growth plan,"" said Sebastien Par, President and CEO of VIQ Solutions Inc. ""Our sophisticated workflow includes a flexible range of capture options, secure automated workflow and integrated speech recognition. It is highly regarded as a complete solution for legal, medical and law enforcement customers."" More than 500 legal firms around the world use the VIQ Dataworxs platform for secure digital capture and transcription. ""Law firms demand the highest standards in cybersecurity and data privacy and are increasingly turning to VIQ Dataworxs to meet their dictation and transcription needs,"" said Daryl Duda, VIQ Dataworxs Vice President of Sales. ""The addition of speech recognition integration through partners like Nuance enables VIQ to provide efficient capture and transcription workflows for law firms and other legal agencies."" The flexible VIQ Dataworxs workflow enables BSP lawyers to capture client notes and other key dictation using the MobileMic smartphone app or digital handheld devices for secure anytime, anywhere capture. The voice files are automatically routed through the secure VIQ Dataworxs workflow to the fully integrated Nuance speech recognition engine for automated transcription. The speech recognition technology integration enables BSP to speed document turnaround, reduce transcription costs and increase efficiency. ""The integration of Nuance speech recognition with the VIQ Dataworxs platform provides a streamlined transcription workflow, helping lawyers and law offices provide fast, efficient service while keeping their important client information private and secure,"" said Tina Di Marzio, National Client Relationships Manager at VIQ Dataworxs. For more information on what is making the news at VIQ Solutions, please visit our website at www.viqsolutions.com/news.html . Bazzani Scully Priddle Lawyers was formed in 2007 and services clients in the hospitality, gaming, property and sporting sectors, as well as broader business organisations and individuals. Located in Melbourne's central business district, Bazzani Scully Priddle's team of lawyers provide high quality, timely and commercially relevant legal advice across eight practice areas. Visit their website at http://bazzaniscullypriddle.com.au . VIQ Solutions is the leading technology and service platform provider for digital evidence capture and content management. Our secure modular software allows customers to onboard the VIQ platform at any stage of their organization's digitization, from the capture of digital content from video and audio devices through to online collaboration, mobility, data analytics and integration with sensors, facial recognition, speech recognition and case management or patient record systems. VIQ's technology leads the industry in security, meeting the highest international standards for digital/cyber security and privacy, including military and medical regulations. Our solutions are in use in over 20 countries with tens of thousands of users in over 200 government and private agencies including law enforcement, immigration, medical, legal, insurance, courts, transportation and transcription service providers. VIQ also provides end to end transcription services to several large government agencies through our Australia-based reporting and transcription partners. VIQ operates worldwide with partners like security integrators, audio-video specialists, and hardware and data storage suppliers. Managing digital media evidence is what we do, and we do it better than anyone else. For more information about VIQ Solutions, please visit www.viqsolutions.com . Neither the TSX Venture Exchange nor its Regulation Service Provider (as that term is defined in the policies of the Exchange) accepts responsibility for the adequacy or accuracy of this release. ",http://www.marketwired.com/press-release/viq-dataworxs-wins-secure-nuance-speech-recognition-integration-project-with-australian-tsx-venture-vqs-2178721.htm,"Seamless integration of speech recognition technology in our secure digital capture, management and transcription workflow sets VIQ ...",VIQ Dataworxs Wins Secure Nuance Speech Recognition ...,MARKHAM ONTARIO Marketwired Nov. VIQ Solutions Inc. VIQ Solutions Company VENTURE VQS VIQ Dataworxs Bazzani Scully Priddle Lawyers BSP Melbourne Australia VIQ Dataworxs Sebastien Par President CEO VIQ Solutions Inc. VIQ Dataworxs VIQ Dataworxs Daryl Duda VIQ Dataworxs Vice President Nuance VIQ VIQ Dataworxs BSP MobileMic VIQ Dataworxs Nuance BSP Nuance VIQ Dataworxs Tina Di Marzio National Client Relationships Manager VIQ Dataworxs VIQ Solutions Bazzani Scully Priddle Lawyers Melbourne Bazzani Scully Priddle VIQ Solutions VIQ VIQ VIQ VIQ VIQ Solutions TSX Venture Exchange Regulation Service Provider Exchange,11
171,"What is the future of machine learning in finance originally appeared on Quora - the place to gain and share knowledge, empowering people to learn from others and better understand the world. Machine learning has already helped a lot to solve complex problems in the domain of natural language processing, image and speech recognition, etc. Recently, deep learning or neural networks have emerged as one of the most popular and powerful methods for learning tasks. The financial sector is also not left untouched by the current wave of machine learning and artificial intelligence. The present financial market is already comprised of humans as well as machines. There are machines out there doing trades of billions of dollars every day in a response time measured in microseconds popularly known as high-frequency trading. According to stats, nearly 73% of the everyday trading is executed by machines. Every major financial firm is investing in algorithmic trading because the level and volume of trade carried out by these machines is out of human bounds to process and execute. Based on a very complex model, these machines take into account the past historical financial data available as well as other information available on the internet such as news. These systems make real-time trade decisions that maximize their returns. Flooded as the market is with such artificial trading systems, the market is becoming more and more sophisticated day by day. These systems compete in real-time for trading, and as part of these competitions, these systems often indulge in flooding the market with false data to slow down competitors and get an edge over them. Also, there might be times when algorithm may behave abnormally. One of the famous examples is the Flash Crash of 2010, where the market fell down abruptly and recovered in a short span of 36 minutes. Now, from a machine learning perspective, active research is going on in the field of stock trading, portfolio optimization, etc. Researchers are constantly trying to learn more and more information from the large volume of data available. Older models used only the numerical data available, but todays system takes into account the financial news before it even reaches humans and infers outcomes based on the news. In the future, we can expect machines to have greater control over the financial markets. Here are links to some good Ted Talks on algorithmic trading and use of machine learning in finance: ",http://www.huffingtonpost.com/entry/the-future-of-machine-learning-in-finance_us_58d55c99e4b06c3d3d3e6d42,"... problems in the domain of natural language processing, image and speech recognition, etc. Recently, deep learning or neural networks have ...",The Future of Machine Learning in Finance,Quora Flash Crash Ted Talks,-1
172,"by Samara Lynn   Posted: March 13, 2017 On Monday, March 6, Salesforce Chairman and CEO Marc Benioff and IBM Chairman, President, and CEO Ginni Rometty announced a global strategic partnership to deliver joint artificial intelligence solutions that will enable companies to make smarter decisions, faster than ever before. The newest Silicon Valley partnership is a meeting of minds, so to speak. IBM and Salesforcethe worlds most famous customer relationship management (CRM) software company, announced they are partnering to leverage IBM Watson s analytical capabilities with Salesforces artificial intelligence (AI) technology, Einstein. Einstein is the moniker for the artificial intelligence capabilities built into Salesforce products, including Sales Cloud, Service Cloud, Marketing Cloud, and others. The AI provides Salesforce customers real-time analytics and deep insight into their data. Additionally, the CRM titan recently introduced Einstein Vision, a developers platform that lets developers build customized add-ons to Salesforce solutions using Einstein APIs. These solutions can include image recognition within a CRM database and AI-powered apps. Watson is arguably among IBMs greatest achievements. Its a platform comprised of several technologies besides AI; machine learning, data analytics, neural networking, speech recognition and more. It can sift through and analyze a vast amount of dataits able to read 800 million pages per secondand can even understand nuances of human language. Within a few years, every major decisionpersonal or businesswill be made with the help of AI and cognitive technologies, said Rometty, chairman, president, and chief executive officer, IBM. This year, we expect Watson will touch one billion peoplethrough everything from oncology and retail, to tax preparation and cars. Now, with todays announcement, the power of Watson will serve the millions of Salesforce and Einstein customers and developers to provide an unprecedented understanding of customers. The combination of Einstein and Watson will make businesses smarter and our customers more successful, said Marc Benioff, chairman and CEO, Salesforce. Im thrilled to form an alliance with IBMno companys core values are as close to Salesforces as IBMs. Its the best of both worlds. ",http://www.blackenterprise.com/technology/new-tech-power-couple-ibm-watson-salesforce-einstein/,"It's a platform comprised of several technologies besides AI; machine learning, data analytics, neural networking, speech recognition and more.",New Tech Power Couple: IBM Watson and Salesforce Einstein,Samara Lynn March Monday March Salesforce Chairman CEO Marc Benioff IBM Chairman President CEO Ginni Rometty Silicon Valley IBM Salesforcethe CRM IBM Watson Salesforces AI Einstein Einstein Salesforce Cloud Service Cloud Marketing Cloud AI Salesforce CRM Einstein Vision Salesforce Einstein APIs CRM Watson IBMs AI AI Rometty IBM Watson Watson Salesforce Einstein Einstein Watson Marc Benioff CEO Salesforce Im IBMno Salesforces IBMs,0
173,"What is the future of machine learning in finance originally appeared on Quora - the place to gain and share knowledge, empowering people to learn from others and better understand the world. Machine learning has already helped a lot to solve complex problems in the domain of natural language processing, image and speech recognition, etc. Recently, deep learning or neural networks have emerged as one of the most popular and powerful methods for learning tasks. The financial sector is also not left untouched by the current wave of machine learning and artificial intelligence. The present financial market is already comprised of humans as well as machines. There are machines out there doing trades of billions of dollars every day in a response time measured in microseconds popularly known as high-frequency trading. According to stats, nearly 73% of the everyday trading is executed by machines. Every major financial firm is investing in algorithmic trading because the level and volume of trade carried out by these machines is out of human bounds to process and execute. Based on a very complex model, these machines take into account the past historical financial data available as well as other information available on the internet such as news. These systems make real-time trade decisions that maximize their returns. Flooded as the market is with such artificial trading systems, the market is becoming more and more sophisticated day by day. These systems compete in real-time for trading, and as part of these competitions, these systems often indulge in flooding the market with false data to slow down competitors and get an edge over them. Also, there might be times when algorithm may behave abnormally. One of the famous examples is the Flash Crash of 2010, where the market fell down abruptly and recovered in a short span of 36 minutes. Now, from a machine learning perspective, active research is going on in the field of stock trading, portfolio optimization, etc. Researchers are constantly trying to learn more and more information from the large volume of data available. Older models used only the numerical data available, but todays system takes into account the financial news before it even reaches humans and infers outcomes based on the news. In the future, we can expect machines to have greater control over the financial markets. Here are links to some good Ted Talks on algorithmic trading and use of machine learning in finance: ",http://www.huffingtonpost.com/entry/the-future-of-machine-learning-in-finance_us_58d55c99e4b06c3d3d3e6d42,"... problems in the domain of natural language processing, image and speech recognition, etc. Recently, deep learning or neural networks have ...",The Future of Machine Learning in Finance,Quora Flash Crash Ted Talks,-1
174,"by Samara Lynn   Posted: March 13, 2017 On Monday, March 6, Salesforce Chairman and CEO Marc Benioff and IBM Chairman, President, and CEO Ginni Rometty announced a global strategic partnership to deliver joint artificial intelligence solutions that will enable companies to make smarter decisions, faster than ever before. The newest Silicon Valley partnership is a meeting of minds, so to speak. IBM and Salesforcethe worlds most famous customer relationship management (CRM) software company, announced they are partnering to leverage IBM Watson s analytical capabilities with Salesforces artificial intelligence (AI) technology, Einstein. Einstein is the moniker for the artificial intelligence capabilities built into Salesforce products, including Sales Cloud, Service Cloud, Marketing Cloud, and others. The AI provides Salesforce customers real-time analytics and deep insight into their data. Additionally, the CRM titan recently introduced Einstein Vision, a developers platform that lets developers build customized add-ons to Salesforce solutions using Einstein APIs. These solutions can include image recognition within a CRM database and AI-powered apps. Watson is arguably among IBMs greatest achievements. Its a platform comprised of several technologies besides AI; machine learning, data analytics, neural networking, speech recognition and more. It can sift through and analyze a vast amount of dataits able to read 800 million pages per secondand can even understand nuances of human language. Within a few years, every major decisionpersonal or businesswill be made with the help of AI and cognitive technologies, said Rometty, chairman, president, and chief executive officer, IBM. This year, we expect Watson will touch one billion peoplethrough everything from oncology and retail, to tax preparation and cars. Now, with todays announcement, the power of Watson will serve the millions of Salesforce and Einstein customers and developers to provide an unprecedented understanding of customers. The combination of Einstein and Watson will make businesses smarter and our customers more successful, said Marc Benioff, chairman and CEO, Salesforce. Im thrilled to form an alliance with IBMno companys core values are as close to Salesforces as IBMs. Its the best of both worlds. ",http://www.blackenterprise.com/technology/new-tech-power-couple-ibm-watson-salesforce-einstein/,"It's a platform comprised of several technologies besides AI; machine learning, data analytics, neural networking, speech recognition and more.",New Tech Power Couple: IBM Watson and Salesforce Einstein,Samara Lynn March Monday March Salesforce Chairman CEO Marc Benioff IBM Chairman President CEO Ginni Rometty Silicon Valley IBM Salesforcethe CRM IBM Watson Salesforces AI Einstein Einstein Salesforce Cloud Service Cloud Marketing Cloud AI Salesforce CRM Einstein Vision Salesforce Einstein APIs CRM Watson IBMs AI AI Rometty IBM Watson Watson Salesforce Einstein Einstein Watson Marc Benioff CEO Salesforce Im IBMno Salesforces IBMs,0
175,"Hey Siri,how does voice recognition software understand me People are particularly attuned to listening to and comprehending speech.And while computers are still lagging, they're not bad for simple tasks. Jake Port explains how voice recognition software works. Talking to a computer is becoming normal, but there's still a ways to go before they're anywhere near as good as us. If you ring a call centre, the first voice you often hear isnt real, but a digitised human with whom you speak to process your request using voice recognition. In its most basic form, computerised voice recognition uses basic pattern matching, while more complex systems use detailed mathematical and predictive models. To find out how they work we will first delve down into why voice recognition is so difficult and explore three main methods that are found in everyday products. Understanding spoken words is something that we take for granted, but its a highly complex process that we learn during childhood. Packets of sound, called phones, are the building blocks of words. These are stored in our brain as phonemes, the ideal form of the phone. When chatting to someone, you hear phones and convert those blocks into speech that conveys a message. While it sounds straightforward, there are a few issues when a computer tries to do the same thing. Homophones, words that sound identical but mean different things, can confuse a voice recogniser unless the words are analysed in context (which we will get to later). Some people also speak very quickly, melding an entire sentence into a single unbroken sound that  for a computer  can be hard to separate into words. In loud environments, it can also be hard to separate the sound of a voice from background noise, particularly if the ambient noise comes from people talking. On top of all of this are dialect variations that change how the same word is said between different populations. Taking this all into account, lets now look at how voice recognition overcomes these issues. As far as voice recognition goes, this is as simple as it gets  it relies on a computer listening to a word and matching its audio pattern to a preloaded phrase. Its the type of recognition used by automated call centres where simple yes and no or one, two, three responses are enough to direct the caller. This small (around 10) group of words, known as a domain, allows the software to recognise a broad range of dialects. The system only works for words that sound completely different. Even then, it may have trouble, forcing the call to be directed to a human operator. This type of recognition is far more complex, looking at the individual components of each word such as the number of vowels. It relies on a system being able to identify a word from its audio footprint, a set of sounds called an utterance. Sound waves are converted into a spectrogram, a graph that shows how the sound changes over time. Each of the approximately 46 phonemes in the English language has a signature, which, when put together in various orders, form (in theory) a word. This is known as the beads-on-a-string model. For homophones, though, the system falls apart. Saying read (as in ""I read a book"") and red will give the same result. In order to overcome this, we need context. This type of voice recognition is found in mobile devices and speech recognition software. In English, adjectives generally come before a noun rather than the other way round (big truck versus truck big). Some words also generally precede others such as for, good, an in front of example, and nouns are not repeated. This is known as the language model. If a computer isnt sure of one of the words, it employs mathematical models and probability  looking at the words before and after it, for instance  to make an educated guess. At the moment, speech-to-text or speech-to-command is all voice recognition can do  and then, only some of the time. One possible idea for improving this is to build artificial neural networks, computers that use millions of electronic nodes to function much like a brain by activating different pathways. ",https://cosmosmagazine.com/technology/hey-siri-how-does-voice-recognition-work,"People are particularly attuned to listening to and comprehending speech. And while computers are still lagging, they're not bad for simple ...","Hey Siri, how does voice recognition software understand me?",Hey Siri People Jake Port Sound English,9
176,"Deep Speech 2 uses deep learning to recognise words in English and Mandarin, reports Tech in Asia Deep Speech 2 uses deep learning to recognise words in English and Mandarin, reports Tech in Asia Baidu wants to build a speech recognition engine thats 99 percent accurate, a threshold that Andrew Ng, chief scientist at Baidu and founder of Googles ""Google Brain"" deep learning project, believes will fundamentally change how humans interact with computers. Baidu, which opened its Silicon Valley AI Lab in 2014, is hoping to carve out a space for itself as a leader in speech recognition. So far, its making impressive headway. The companys latest speech recognition engine, dubbed Deep Speech 2, uses deep learning to recognise words spoken in English and Mandarin, at times outperforming humans in the latter, according to Baidu. ""We can train this giant neural network that eventually learns to recognise speech on its own as well as a human can, and not spend so much of our time thinking about how words are structured,"" says Adam. ""Instead, [we] can just ask the computer system to learn those things on its own."" The short answer to Baidus plan to conquer speech recognition is data  lots of it. Adam says Deep Speech 2 was trained on tens of thousands of hours of audio recordings. Some of it comes from public data, while another portion is from crowdsourcing services, such as Mechanical Turk, Amazons marketplace for odd jobs that require human intelligence. Deep Speech 2 is an example of supervised learning, a type of machine learning that uses labelled training data  such as transcribed audio  to teach a system new skills, like recognising handwritten numbers. Without labelled training data, however, the neural network wouldnt be able to differentiate right from wrong. This is an excerpt from an article published on TechInAsia. You can read the full story here Deep Speech 2 uses deep learning to recognise words in English and Mandarin, reports Tech in Asia Baidu wants to build a speech recognition engine thats 99 percent accurate, a threshold that Andrew Ng, chief scientist at Baidu and founder of Googles ""Google Brain"" deep learning project, believes will fundamentally change how humans interact with computers. Baidu, which opened its Silicon Valley AI Lab in 2014, is hoping to carve out a space for itself as a leader in speech recognition. So far, its making impressive headway. The companys latest speech recognition engine, dubbed Deep Speech 2, uses deep learning to recognise words spoken in English and Mandarin, at times outperforming humans in the latter, according to Baidu. ""We can train this giant neural network that eventually learns to recognise speech on its own as well as a human can, and not spend so much of our time thinking about how words are structured,"" says Adam. ""Instead, [we] can just ask the computer system to learn those things on its own."" The short answer to Baidus plan to conquer speech recognition is data  lots of it. Adam says Deep Speech 2 was trained on tens of thousands of hours of audio recordings. Some of it comes from public data, while another portion is from crowdsourcing services, such as Mechanical Turk, Amazons marketplace for odd jobs that require human intelligence. Deep Speech 2 is an example of supervised learning, a type of machine learning that uses labelled training data  such as transcribed audio  to teach a system new skills, like recognising handwritten numbers. Without labelled training data, however, the neural network wouldnt be able to differentiate right from wrong. This is an excerpt from an article published on TechInAsia. You can read the full story here ",http://www.business-standard.com/article/technology/how-baidu-s-ai-lab-plans-to-solve-speech-recognition-with-lots-of-data-117022300759_1.html,"Baidu wants to build a speech recognition engine that's 99 percent accurate, a threshold that Andrew Ng, chief scientist at Baidu and founder of ...",How Baidu's AI Lab plans to solve speech recognition - with lots of ...,Deep Speech English Mandarin Tech Asia Deep Speech English Mandarin Tech Asia Baidu Andrew Ng Baidu Googles Google Brain Baidu Silicon Valley AI Lab Deep Speech English Mandarin Baidu Adam Baidus Adam Deep Speech Mechanical Turk Amazons Deep Speech TechInAsia Deep Speech English Mandarin Tech Asia Baidu Andrew Ng Baidu Googles Google Brain Baidu Silicon Valley AI Lab Deep Speech English Mandarin Baidu Adam Baidus Adam Deep Speech Mechanical Turk Amazons Deep Speech TechInAsia,3
177,"Speechrecognitionmust get muchbetter if we are to speak naturally to ourgadgets. So the tech industry is vacuuming up all the conversations it can. Amazon's Echo has made tangible the promise of an artificially intelligent personal assistant in every home. Those whoownthevoice-activated gadget (known colloquially as Alexa, after its female interlocutor) are proneto proselytizing ""her""charms, applauding Alexa's ability to call anUber, order pizza or check a 10th-grader'smath homework. The company says more than 5,000 people a day profess their love for Alexa. On the other hand, Alexa devotees also know that unless youspeak to her very clearly . . . and . . . slowly, she's likely to say: Sorry, I don't have the answer to that question. ""I love her. I hate her, I love her,"" one customerwrote on Amazon's website, while still awarding Alexa five stars. ""You will very quickly learn how to talk to her in a way that she will understand and it's not unlike speaking to a small frustrating toddler."" Voice recognitionhas come a long way in the past few years. But it's still not good enough to popularize the technology for everyday use and usher in a new era ofhuman-machine interaction, allowing us to talk with all our gadgetscars, washing machines, televisions. Despite advances in speech recognition, most people continue to swipe, tap and click. And probably will for the foreseeable future. What's holding back progressPartly the artificial intelligence that powers the technology has room to improve. There's also a serious deficit of dataspecifically audio of human voices, speaking in multiple languages, accents and dialects in often noisy circumstances that can defeat the code. So Amazon, Apple, Microsoft and China's Baidu have embarked ona world-wide hunt for terabytes of human speech. Microsoft has set up mock apartments in cities around the globeto record volunteers speakingin a home setting. Every hour, Amazon uploads Alexa queries to a vast digital warehouse. Baidu is busily collecting every dialect in China. Then they take all that data and use it to teach their computers how to parse, understand and respond to commands and queries. The challenge is finding a way to capture natural, real-world conversations.Even 95 percent accuracy isn't enough, says Adam Coates, who runs Baidu'sartificial intelligencelab in Sunnyvale, California. ""Our goal is to push the error rate down to 1 percent,"" he says. ""That's where you can really trust the device to understand what you're saying, and that will be transformative."" Not so longago, voice recognition was comically rudimentary. An early version of Microsoft's technologyrunning in Windows transcribed ""mom"" as ""aunt"" during a 2006 demo before an auditorium of analysts and investors. When Apple debuted Siri five years back, the personal assistant's gaffes were widely mocked because it, too, routinely spat out incorrect results or didn't hearthe question correctly.When asked if Gillian Anderson is British, Siri provided a list of English restaurants. NowMicrosoft saysits speech engine makes the same number or fewer errors than professional transcribers, Siri is winning grudging respect, and Alexahasgiven usa tantalizingglimpse of the future. Much of that progress owes a debt to the magic of neural networks, a form of artificial intelligence based loosely on the architecture of thehuman brain. Neural networks learn without being explicitly programmed but generally require an enormous breadth and diversity of data. The more a speech recognition engine consumes, the better it gets at understanding different voices and the closer it gets to the eventual goal of having a natural conversation in manylanguages and situations. Hence the global scramble to capture a multitude of voices.""The more data we shove in our systems the better it performs,"" says Andrew Ng, Baidu's chief scientist. ""This is why speech is such a capital-intensive exercise; not a lot of organizations have this much data."" When the industrybegan working seriously on voice recognition in the 1990s, companies like Microsoft relied onpublicly available data from research institutes such as the Linguistics Data Consortium, a storehouse of voice and text data founded in 1992 with backing from the U.S. government and located at the University of Pennsylvania. Then tech companiesstarted collecting their own voice data, some of it garnered from volunteers who came in to read and be recorded. Now, with the popularity of speech-controlled software gaining ground, they harvest much of the datafrom theirown products and services. Whenyou tell your phone to search for something, play a song or guide you to a destination, chances are a company is recording it. (Apple, Google, Microsoft and Amazon emphasize that they anonymize user data to protect customer privacy.) When you ask Alexa what the weather is or the latest football score, the gadget uses the queriesto improve its understanding of natural language (although ""she"" isn't listening to your conversations unless you say her name). ""By design, Alexa gets smarter as you use her,"" says Nikko Strom, seniorprincipal scientist for the program. One of the key challenges is getting the technology conversant with multiple languages, accents and dialects. Nowhere, perhaps, is this more crucial than in China. Seeking to harvest dialects from all over the country, Baidu launched a marketing campaign duringChinese New Year earlier this year. Calling the pusha ""dialect conservation initiative,"" the search giant promised people that if they contributed they would help usher in a future when they wouldtalk to Baidu using their dialect. In two weeks, the company recordedmore than 1,000 hours of speech to plug into its computers. Many people did it for free simplybecause they were proud of their hometown dialects. A high school teacher in Sichuan was so excited about the program, he askeda class of students to record more than 1,000 ancient poems in Sichuanese. Another challenge: teaching voicerecognition technology to pick up commands over background noisethe clamor of happy hour, say, or the cacophony of asports stadium. Microsoft has deployed an Xbox app called Voice Studio to harvest conversation over the din of users shooting villains or watching movies.The company offered rewards including points and digital apparel for avatars and lured hundreds of subjects willing to contributetheir game chatter toMicrosoft's speech efforts. The program worked gangbusters in Brazil, where the local subsidiary promoted the app heavily on the main Xbox page. The data was used tocreate the Brazilian Portuguese version of Cortana, released earlier this year. Companies are also designing voice recognition systems for specific situations. Microsoft has been testing technology that can answer travelers' queries without being distracted by the constant barrage of flight announcements at airports. The company's technology is also being used inan automated ordering system for McDonald's drive-thrus. Trained to ignore scratchy audio, screaming kids and ""ums,"" it can spit out a complicatedorder, getting even the condiments right.Amazon is conducting tests in automobiles, challengingAlexa to work wellwith road noise and open windows. Even as companies scour the world for data, they'refiguring out ways to improve voice recognition with lessof it. The technology being tested at McDonald's is more accurate than other systems that use much more data, says Xuedong Huang, Microsoft's chief speech scientist, who has been working on voice recognition at the company for more than two decades.""You can always have breakthroughs even without using the most data."" Exclusive insights on technology around the world. Google generally subscribes to a less-is-more philosophy, deployinga piecemeal approachthat uses unintelligible units of sound to buildwords and phrases. With its speech recognition system, the company aims to solve multipleproblems with just one change. For its data sets, Google strings togethertens of thousands of audio snippets that are typically two to five seconds long. The process requires less computing power and can be more easily tested and tweaked, says Google researcher Franoise Beaufays. For its part, Baidu is working on more efficient algorithms where learning one language makes it easier to learn the next twelve. That's particularly important for those spoken by tens of thousands of people rather than millions, where there just won't be huge swaths of data no matter what, says Ng, the company's chief scientist. Ask researchers like Ng when it will be possible to speak naturally to your digital assistant and theyget wistful. No one really knows. Neural networks remain mysterious even to those who understand them best. And much of the work is trial and error; make a tweak here and you're never quite sure what will happen there. Based on the current technology and methods, the process will probably take years. But Ng, Huang, Beaufays and other scientists say you never know when a breakthrough will arrive, catapulting research forward and turningAlexa and Siriinto true conversationalists. Before it's here, it's on the Bloomberg Terminal. ",https://www.bloomberg.com/news/articles/2016-12-13/why-google-microsoft-and-amazon-love-the-sound-of-your-voice,Speech recognition must get much better if we are to speak naturally to our gadgets. So the tech industry is vacuuming up all the conversations ...,"Why Google, Microsoft and Amazon Love the Sound of Your Voice",Speechrecognitionmust Amazon Echo Alexa Alexa Alexa Alexa Sorry Amazon Alexa Voice Amazon Apple Microsoft China Baidu Microsoft Amazon Alexa Baidu China Adam Coates Baidu'sartificial Sunnyvale California Microsoft Windows Apple Siri Gillian Anderson Siri NowMicrosoft Siri Alexahasgiven usa Hence Andrew Ng Baidu Microsoft Linguistics Data Consortium U.S. University Pennsylvania Whenyou Apple Google Microsoft Amazon Alexa Nikko Strom China Baidu New Year Baidu Sichuan Sichuanese Microsoft Voice Studio Brazil Xbox Portuguese Cortana Microsoft McDonald McDonald Xuedong Huang Microsoft Google Google Google Franoise Beaufays Baidu Ng Ask Ng Ng Huang Beaufays Siriinto Bloomberg Terminal,7
178,"VIQ Solutions Integrates Nuance's Dragon Speech Recognition, Awarded New Contract With US Police Force MARKHAM, ONTARIO--(Marketwired - Oct. 3, 2016) - VIQ Solutions Inc. (""VIQ"", ""VIQ Solutions"" or the ""Company"") (TSX VENTURE:VQS) is pleased to announce that the Company has entered a new partnership with Nuance Communications, Inc. (""Nuance""), a leading provider of voice and language solutions for businesses and consumers, for the integration of Nuance's powerful Dragon speech recognition solution with the VIQ digital capture and management workflow. ""Nuance is an established global leader in advanced speech recognition solutions and we're delighted to partner with them to provide an integrated digital capture, management and transcription workflow that specifically targets our high-growth markets,"" said Sebastien Par, President and CEO of VIQ Solutions. ""With sophisticated speech recognition platforms designed specifically for the medical, law enforcement and legal markets, and a tireless focus on digital security and privacy that matches our own, Nuance is the ideal partner for VIQ."" ""VIQ's integration of Dragon speech recognition technology delivers a powerful solutions for its customers - allowing them to be more efficient and accurate in their documentation, so they can focus on what matters - public safety,"" said Peter Mahoney, senior vice president and general manager, Dragon. VIQ Solutions also announced today that the Company has been awarded a new contract with the Cedar Rapids, Iowa, police force using the integrated Nuance workflow. With this win, over 40 law enforcement and public safety agencies around the world are using the secure VIQ law enforcement digital capture and management platform. ""As law enforcement agencies turn to new digital technology to capture and manage incident audio and video, the VIQ Dataworxs footprint continues to grow as our experience and expertise is recognized,"" said Sebastien Par. ""VIQ's emphasis on mobility, flexibility and security, combined with speech recognition through partners like Nuance, makes the VIQ platform an unbeatable choice for police departments around the world."" The powerful VIQ Dataworxs solution enables Cedar Rapids officers to dictate incident or field reports via landlines, mobile phones, or interactively on workstations for secure anytime, anywhere capture, including during patrol. The secure technology routes the voice files through a sophisticated workflow to a fully integrated Nuance speech recognition engine for automated transcription. Nuance provides a library specifically designed for law enforcement, improving reporting detail and accuracy. Officers and others authorized can review transcripts via the author web portal. The transcripts are automatically encrypted and stored securely in a format compatible with the department's records management system. ""The VIQ Dataworxs voice management system instantly provides an immediate boost to Cedar Rapid's productivity by digitizing their incident capture procedures and automating their transcription and storage requirements via the integrated Nuance speech recognition engine,"" said Daryl Duda, VIQ Dataworxs Vice President of Sales and Business Development. ""The sophisticated digital workflow capability enables the Cedar Rapids police department to streamline their incident reporting and reduce paperwork for officers and staff."" ""The Cedar Rapids Police Department is fortunate to have this up-to-date technology that improves the efficiency of police officers. It is important to have accurate information in a timely manner that can be both processed and retrieved,"" added Greg Buelow of the Cedar Rapids City Manager's Office. ""Information is invaluable and we are pleased to have VIQ Solutions as a partner in providing accessibility to police records."" For more information on what is making the news at VIQ Solutions, please visit our website at www.viqsolutions.com/news . VIQ Solutions is the leading technology and service platform provider for digital evidence capture and content management. Our secure modular software allows customers to onboard the VIQ platform at any stage of their organization's digitization, from the capture of digital content from video and audio devices through to online collaboration, mobility, data analytics and integration with sensors, facial recognition, speech recognition and case management or patient record systems. VIQ's technology leads the industry in security, meeting the highest international standards for digital/cyber security and privacy, including military and medical regulations. Our solutions are in use in over 20 countries with tens of thousands of users in over 200 government and private agencies including law enforcement, immigration, medical, legal, insurance, courts, transportation and transcription service providers. VIQ also provides end to end transcription services to several large government agencies through our Australia-based reporting and transcription partners. VIQ operates worldwide with partners like security integrators, audio-video specialists, and hardware and data storage suppliers. Managing digital media evidence is what we do, and we do it better than anyone else. For more information about VIQ Solutions, please visit www.viqsolutions.com . Neither the TSX Venture Exchange nor its Regulation Service Provider (as that term is defined in the policies of the Exchange) accepts responsibility for the adequacy or accuracy of this release. ",http://www.marketwired.com/press-release/-2163325.htm,Nuance is an established global leader in advanced speech recognition solutions and we're delighted to partner with them to provide an ...,VIQ Solutions Integrates Nuance's Dragon Speech Recognition ...,VIQ Solutions Integrates Nuance Dragon Awarded New Contract US Police Force MARKHAM ONTARIO Marketwired Oct. VIQ Solutions Inc. VIQ Solutions Company VENTURE VQS Company Nuance Communications Inc. Nuance Nuance Dragon VIQ Nuance Sebastien Par President CEO VIQ Solutions Nuance VIQ VIQ Dragon Peter Mahoney Dragon VIQ Solutions Company Cedar Rapids Iowa Nuance VIQ VIQ Dataworxs Sebastien Par VIQ Nuance VIQ VIQ Dataworxs Cedar Rapids Nuance Nuance VIQ Dataworxs Cedar Rapid Nuance Daryl Duda VIQ Dataworxs Vice President Business Development Cedar Rapids Cedar Rapids Police Department Greg Buelow Cedar Rapids City Manager Office Information VIQ Solutions VIQ Solutions VIQ Solutions VIQ VIQ VIQ VIQ VIQ Solutions TSX Venture Exchange Regulation Service Provider Exchange,-1
179,"order Retin-a cheap In Retina-A contain are included such components as Retin acid and tretinoinum. where can i buy Allopurinol | buy Cipro canada To find an archived story, type a key word in the box below and hit 'enter' West firms have been given a unique glimpse of the top tech themes for this year  and the impacttheyare likely tohave on business at a special event staged by accountancy firm Deloitte. Ever-greater use of smartphones, digital navigation, machine learning and  on the downside  the growth in cyber crime were outlined by Deloittes technology, media and telecommunications (TMT) practice. The firms head of TMT Paul Lee led the briefing staged at the Engine Shed, Bristol, and discussed key trends for this year as highlighted in the 16th edition of the firms widely-respected TMT Predictions 2017 launched in January. He said the predictions particularly resonated with the West of Englands digital tech ecosystem as it continued to be among the UKs most innovative and fastest-growing. Deloittes key technology predictions for 2017 are: Deloitte predicts that one in20 uses of digital navigation in 2017 will be indoors. By 2022, at least a quarter of all human and machine uses of precision digital navigation will include an indoor leg or be for an entirely indoor journey. With every year, getting lost indoors will get increasingly harder. The growth of indoor navigation will be boosted by the growing availability of an array of indoor positioning data and improvements in the accuracy of data generated by beacons, wi-fi hotspots and cellular base stations. Advanced analytical tools will be required to interpret multiple location datasets in parallel in order to produce high-quality location. Precise indoor navigations potential is significant, and could be transformative. It is likely to benefit most vertical sectors, impacting government, business and consumers alike. Indoor navigation is also likely to be used for people and things, to locate rapidly items of value in a range of locations, from tools in a workshop, goods in a warehouse, parts on a factory floor, to suitcases in the hold of a plane. Deloitte lead partner for technology, media and telecommunications in the South West Laurence Hedditch, pictured right, said: We are all aware of the impact that satellite-based digital navigation, including the digitisation of street maps, has had on our daily way of living, and on business models. However, satellite navigation has one fundamental blind spot  its signals are often too weak to penetrate building roofs. Yet people spend over 90% of their time indoors. Billions of objects, from vehicles to tools to components, all of which may need to be located, are housed somewhere under a roof. Being able to locate people and objects when indoors is likely to add significant value, possibly at a level equivalent to or greater than the impact that outdoor digital navigation has had. Deloitte predicts that more than a fifth of smartphones sold in 2017 (more than 300m units) will have machine learning capabilities within the device in the next 12 months. Mobile devices will be able to perform machine learning tasks even without connectivity which will significantly alter how humans interact with technology across every industry, market and society Machine learning essentially means that our phones will be designed to mimic aspects of the human brains structure and function. This functionality will enhance applications including indoor navigation, image classification, augmented reality, speech recognition and language translation  even where there is little or no cellular or wi-fi connectivity. Recognising, for example, that an object is a face  and whose face it is  in a world of varying light sources, hats and glasses, is remarkably challenging for programmers. Its a challenge thats better dealt with by machine learning  the process by which computers get better at performing tasks through exposure to data, rather than through explicit programming. Although some smartphones in 2016 were capable of extremely limited machine-learning tasks such as recognising a fingerprint, more powerful cognitive tasks only worked when connected to large data centres far away. Laurence Hedditch added: Software and hardware developments mean that neural networks can now be provided at prices, sizes and power consumption that are compatible with smartphones. However, over time machine learning on-the-go will not just be limited to smartphones. These capabilities are likely to be found in tens of millions (or more) of drones, tablets, cars, virtual or augmented reality devices, medical tools, Internet of Things (IoT) devices and unforeseen new technologies. Machine learning is fascinating as it will revolutionise how we conduct simple tasks like translating content, but it also has major security and health consequences that can improve societies around the world, said Laurence Hedditch. For example, mobile machine learning is a strong entry point to improve responses to disaster relief, help save lives with autonomous vehicles, and even turn the tide against the growing wave of cyberattacks. According to Deloitte, the active base of fingerprint reader-equipped devices will top one billion for the first time in early 2017. Each sensor will be used, on average, 30 times a day, resulting in more than 10 trillion aggregate uses globally. Our research suggests that approximately 40% of all smartphones in developed countries will incorporate a fingerprint reader by the end of the year, up from 30% as of mid-2016. At least 80% of users with a fingerprint reader-equipped smartphone will use this sensor regularly; this compares to 69% of users in mid-2016. Paul Lee, pictured left,said: Billions of smartphones and tablets are expected to be capable of processing and collecting multiple types of biometric inputs, including face recognition, voice pattern and iris scan in 2017, but usage of fingerprints will lead the way. The rapid pace of adoption of this technology will likely be met with additional applications that could use fingerprint readers to provide fast and secure authentication. The fingerprint has proven to be the most popular form of biometric security on smartphones and the concept of fingerprint security is increasingly becoming normalised. This is important: as fingerprint security becomes more common, consumers will find it easier  and will be more willing to pay for goods and services using their smartphones. Deloitte also predicts that in 2017 Distributed Denial-of-Service (DDoS) attacks, a form of cyber attack, will become larger in scale, harder to mitigate, and more frequent. Deloitte expects there will be on average a terabit per second (Tbit/s) attack per month and over 10m attacks in total during the year. The average attack size will be between 1.25 and 1.5 gigabits per second (Gbit/s) of junk data being sent. An unmitigated Gbit/s attack (one whose impact was not contained), would be sufficient to take many organisations offline. Phill Everson, pictured right, the Bristol-based partner who also heads up cyber risk services at Deloitte in the UK, said: This escalation in the DDoS threat is largely due to the growing number of vulnerable Internet of Things (IoT) devices and online availability of IoT-focused malware (both of which allow relatively unskilled attackers to hijack IoT devices and use them to launch attacks), as well as access to ever higher bandwidth speeds. Insecure IoT devices are vulnerable to being compromised and taken over by malign third parties. Such internet-connected devices range from video cameras, digital video recorders, routers to appliances in peoples homes that sometimes have hard-coded, unchangeable user IDs and passwords. ",http://swindon-business.net/index.php/2017/04/02/smartphones-will-get-smarter-but-so-will-cyber-attacks-deloittes-tech-team-tells-west-businesses/,"This functionality will enhance applications including indoor navigation, image classification, augmented reality, speech recognition and ...",Smartphones will get smarter – but so will cyber attacks – Deloitte's ...,Retin-a Retina-A Retin Allopurinol | Cipro Deloitte Ever-greater Deloittes TMT TMT Paul Lee Engine Shed Bristol TMT Predictions January West Englands UKs Deloitte Advanced Precise Indoor Deloitte South West Laurence Hedditch Deloitte Mobile Machine Recognising Laurence Hedditch Software Internet Things IoT Machine Laurence Hedditch Deloitte Paul Lee Deloitte Distributed Denial-of-Service DDoS Deloitte Tbit/s Gbit/s Gbit/s Phill Everson Deloitte UK DDoS Internet Things IoT IoT Insecure IoT IDs,-1
180,"AUSTIN, Texas  The University of Texas at Austin has appointed Mark J.T. Smith as the next dean of the Graduate School and senior vice provost for academic affairs. His appointment begins Aug. 1. Smith comes to UT Austin from Purdue University, where he has served as the dean of the Graduate School since 2009. Mark is one of the most widely respected leaders in graduate education in the country. His experience in creating new interdisciplinary initiatives, expanding the global footprint, and leading diversity and inclusion efforts makes him an ideal addition to our leadership team, said Maurie McInnis, executive vice president and provost. We have some of the top graduate programs in the country, and we are thrilled Mark will soon help us do even more. In his concurrent role as senior vice provost for academic affairs, Smith will develop and implement strategic policies and practices that will allow the university to define and pursue future academic priorities. In addition to his efforts as a university leader, Smith is widely recognized for his academic contributions and research in speech and image processing as well as object detection and recognition. He was also a member of the U.S. Olympic fencing teams in 1980 and 1984. He holds an undergraduate degree from the Massachusetts Institute of Technology and a masters degree and doctorate from the Georgia Institute of Technology. Few universities have the reach and impact of The University of Texas at Austin. I have always admired the university and the success of the Graduate School, and I am excited to join this team at such an exciting time, explained Smith. My focus will be helping students succeed and creating opportunities for them to pursue their dreams. This is an amazing community, and I look forward to the road ahead. Smith succeeds Marvin Hackert, who has served as interim dean since June 2015. The university has benefitted tremendously from the unwavering leadership provided by Marv Hackert in his role as interim dean of the Graduate School. I commend him for his commitment and distinguished service to this campus, said McInnis. UT Austin has more than 40 graduate programs ranked among the top 10 in the nation and four programs ranked No. 1, according to U.S. News & World Report's 2018 edition of ""Best Graduate Schools. The university has top 10 programs in 12 of its colleges and schools including such disciplines as business, communication, education, engineering, Earth sciences, humanities, information, law, natural sciences, pharmacy, social work and the social sciences. For more information, contact: Joey Williams , Office of the Executive Vice President and Provost, 512-232-3716. ",https://news.utexas.edu/2017/04/10/mark-smith-appointed-dean-of-graduate-school,... widely recognized for his academic contributions and research in speech and image processing as well as object detection and recognition.,Mark Smith Appointed Dean of Graduate School and Senior Vice ...,AUSTIN Texas University Texas Austin Mark J.T Smith Graduate School Aug. Smith UT Austin Purdue University Graduate School Mark Maurie McInnis Mark Smith Smith U.S. Olympic Massachusetts Institute Technology Georgia Institute Technology University Texas Austin Graduate School Smith Smith Marvin Hackert June Marv Hackert Graduate School McInnis UT Austin No U.S. News World Report Best Graduate Schools Earth Joey Williams Office Executive Vice President Provost,2
181,"BURLINGTON, Ont.  Why type when you can talk Thats a marketing line from a new computer software company called LilySpeech that provides a service enabling computer operators to use speech recognition on their desk or laptop computers. Long story short, we found a way to leverage the fact that the Google speech-to-text engine is available in Google Chrome, and make that interact with any part of your Windows. So anywhere you could type in Windows, you could now talk-to-type, said Jonathan Adams, a former Truro resident now based in Burlington, Ont. We basically created a piece of software that makes the Google service available on a Windows desktop or laptop. And thats never been available before, he said. Google provides the service that enables speech recognition on android cellphones. There is another software on the market that can be used on the Windows platform, which Adams has been using for years. And while it has been a huge timesaver in his business, that software has been a love-hate relationship for him, he said, because of the amount of memory it takes up on a computer. LilySpeech does not use computer memory because of the way it works through the cloud-based Google system. Adams, whose parents Wayne and Judy still live in the area, moved to Ontario in 2000 after completing a multi-media course at the NSCC Truro campus. His initial reason for taking the course was to further his interest in using computers for musical recording. After doing some recording he helped form a Christian music band and toured the country a couple of times. Then he followed his entrepreneurial interests and started an online printing company called Printing Peaches. That company is now managed by his younger brother Joel out of Hubbards, N.S. And that was kind of my entry into all things web marketing, he said. We quickly kind of grew into a secondary business of helping other people create websites and market those websites online. Adams is currently in a soft-launch stage with LilySpeech and he anticipates going to full-scale launch within a months time. Right now were ironing some things out, he said. Its been pretty cool though. Ive never been part of anything where you did a little bit of marketing and then it seemed to take on a life of its own, Adams said of initial interest from people who have been using the system on a trial basis. ",http://www.trurodaily.com/business/2016/12/13/former-truro-man-creates-speech-recognition-software-.html,That's a marketing line from a new computer software company called LilySpeech that provides a service enabling computer operators to use ...,Former Truro man creates speech-recognition software,BURLINGTON Ont LilySpeech Long Google Google Chrome Windows Windows Jonathan Adams Truro Burlington Ont Google Windows Google Windows Adams LilySpeech Google Adams Wayne Judy Ontario NSCC Truro Printing Peaches Joel Hubbards N.S Adams LilySpeech Ive Adams,4
182,"Nuance's latest Dragon dictation software will steadily improve over time, the company says. Your message has been sent. With the trends in personal computing favoring software that gets to know its users, the newest version of Nuances Dragon voice dictation suite of softwareDragon 15, announced Tuesdayis right on track. Dragon 15including Dragon Professional Individual ($300), Dragon Professional Individual for Mac ($300), Dragon Legal ($500), and Nuance Dragon Anywhere (free to install; subscriptions run $15/month or $150/year)is based on Nuances new machine-learning technology. The company claims that this technology has improved recognition accuracy by at least 24 percent, thanks to its algorithm that learns your distinctive speech patterns over time combined with an improved capability to pick out speech from a noisy room. Lawyers may buy the expanded Legal version, which is trained using a legal vocabulary of more than 400 million words, according to the company. But the majority of Nuances customers will probably invest in the Dragon Professional Individual version, which is designed for a more general vocabulary. [ Further reading: Your new PC needs these 15 free, excellent programs ] According to Nuance, the Dragon software not only offers improvements in picking up and understanding your voice, but also features an improved transcription mode for working with recorded audio. Since Dragon works off of a model for each speaker, it could theoretically improve if it hears the same speaker in multiple recordings. Its still going to struggle, however, in accurately transcribing a recording of multiple speakers holding a conversation, executives said. But a batch mode can also transcribe multiple recordings quickly, they added. All three apps can also be synced with the Dragon Anywhere mobile app ( iOS and Android ), allowing transcriptions to begin on the go. Dragon Professional Individual version 15 for the PC and Mac will be available as a digital download on September 1 in the U.S., U.K., and Australia. Dragon Legal will only be available in U.S. English, also on September 1. (Physical copies ship on September 14, according to Nuances website.) Why this matters: Windows already has speech recognition built into itnot only can you use the traditional engine inside the Speech menu within the Control Panel, but you can dictate reminders and short emails using Cortana. But its mediocre, and cant handle longer documents. If youre the type of person who writes clearly but types slowly, however, dictation software may be for you. To comment on this article and other PCWorld content, visit our Facebook page or our Twitter feed. ",http://www.pcworld.com/article/3107943/software/nuance-taps-into-deep-learning-to-improve-dragon-speech-recognition-by-24-percent.html,The company claims that this technology has improved recognition ... Why this matters: Windows already has speech recognition built into ...,Nuance taps into deep learning to improve Dragon speech ...,Nuance Dragon Nuances Dragon Tuesdayis Dragon Dragon Professional Individual Dragon Professional Individual Mac Dragon Legal Nuance Dragon Anywhere Nuances Legal Nuances Dragon Professional Individual Nuance Dragon Dragon Dragon Anywhere Android Dragon Professional Individual Mac September U.S. U.K. Australia Dragon Legal U.S. English September September Nuances website Windows Speech Control Panel Cortana PCWorld Facebook Twitter,1
183,"Nuance's latest Dragon dictation software will steadily improve over time, the company says. Your message has been sent. With the trends in personal computing favoring software that gets to know its users, the newest version of Nuances Dragon voice dictation suite of softwareDragon 15, announced Tuesdayis right on track. Dragon 15including Dragon Professional Individual ($300), Dragon Professional Individual for Mac ($300), Dragon Legal ($500), and Nuance Dragon Anywhere (free to install; subscriptions run $15/month or $150/year)is based on Nuances new machine-learning technology. The company claims that this technology has improved recognition accuracy by at least 24 percent, thanks to its algorithm that learns your distinctive speech patterns over time combined with an improved capability to pick out speech from a noisy room. Lawyers may buy the expanded Legal version, which is trained using a legal vocabulary of more than 400 million words, according to the company. But the majority of Nuances customers will probably invest in the Dragon Professional Individual version, which is designed for a more general vocabulary. [ Further reading: Your new PC needs these 15 free, excellent programs ] According to Nuance, the Dragon software not only offers improvements in picking up and understanding your voice, but also features an improved transcription mode for working with recorded audio. Since Dragon works off of a model for each speaker, it could theoretically improve if it hears the same speaker in multiple recordings. Its still going to struggle, however, in accurately transcribing a recording of multiple speakers holding a conversation, executives said. But a batch mode can also transcribe multiple recordings quickly, they added. All three apps can also be synced with the Dragon Anywhere mobile app ( iOS and Android ), allowing transcriptions to begin on the go. Dragon Professional Individual version 15 for the PC and Mac will be available as a digital download on September 1 in the U.S., U.K., and Australia. Dragon Legal will only be available in U.S. English, also on September 1. (Physical copies ship on September 14, according to Nuances website.) Why this matters: Windows already has speech recognition built into itnot only can you use the traditional engine inside the Speech menu within the Control Panel, but you can dictate reminders and short emails using Cortana. But its mediocre, and cant handle longer documents. If youre the type of person who writes clearly but types slowly, however, dictation software may be for you. To comment on this article and other PCWorld content, visit our Facebook page or our Twitter feed. ",http://www.pcworld.com/article/3107943/software/nuance-taps-into-deep-learning-to-improve-dragon-speech-recognition-by-24-percent.html,The company claims that this technology has improved recognition ... Why this matters: Windows already has speech recognition built into ...,Nuance taps into deep learning to improve Dragon speech ...,Nuance Dragon Nuances Dragon Tuesdayis Dragon Dragon Professional Individual Dragon Professional Individual Mac Dragon Legal Nuance Dragon Anywhere Nuances Legal Nuances Dragon Professional Individual Nuance Dragon Dragon Dragon Anywhere Android Dragon Professional Individual Mac September U.S. U.K. Australia Dragon Legal U.S. English September September Nuances website Windows Speech Control Panel Cortana PCWorld Facebook Twitter,1
184,"Wong: What are the challenges associated with eliminating background noise Walker: In the case of noise control, much of todays technology traces back to patents that were filed as early as the 1930s. Historically, devices have required custom hardware to mask the noise. This means they need early-stage integration into product design, delaying schedules, limiting flexibility, and impacting costs. Making things worse, these methods are typically only effective on constant noise sources from fixed locations. When most noise-cancellation technology was invented, devices werent nearly as portable, ubiquitous, or capable as they are today. Bluetooth only recently reached a level where it was considered good enough to replace wires for carrying quality audio. Processors only recently became fast enough and affordable enough to appear in virtually any consumer electronics product. Finally, wireless networking speed and coverage only recently reached a point where cloud services can accommodate an ASR vocabulary library of 200 trillion+ possible word combinations and allow for real-time learning with contextual clues. Walker: As we continued to push voice as a control path into devices in uncontrolled noise environments, it became clear that we had reached the limits of a hardware-based, noise-centric approach. Building chips that tried to identify noise types and filter them out wasnt working. In our case, we learned early on that we wouldn't be able to isolate and block every random sound. That led us to the discovery and development of a machine-learning approach that enables us to isolate all of the sounds of the human voice via a deep neural network and only allow that through. By doing so, we block virtually all background noise. Wong: What is Cypher doing in the space Walker: Cypher was founded on the idea that while we increasingly use voice-centric devices in more places and rely on them to do more things, they havent undergone a significant improvement in noise-immunity quality. Our team set out to solve this problem in a novel way; treating it as a math problem rather than an audio problem. Three years later, we have a machine-learning-based software solution that provides over three times the noise suppression of the leading smartphone and improves speech-recognition performance in noisy conditions by over 120%. Cyphers technology is software-only. It is capable of running on the types of processors commonly found in smartphones and consumer electronics devices, and can identify the physical attributes of the human voice, regardless of age, gender, or language. Our approach does not rely on the acoustical design of hardware or on network bandwidth upgrades. Instead, Cyphers software approach can fit onto the existing processors of digital signal processors (DSPs) found in even simple consumer electronics devices. We use advanced math, neural networks, and pattern matching to detect the unique qualities of the human voice and ignore everything else,making a users voice crystal clear to the speaker on the other end of the lineeven in the noisiest environments. This is ideal for use in mobile phones, but has applications for any product that relies on voice. Our process also improves ASR accuracy. We just completed a test of the latest version of our ASR filtering on Amazon Echos Alexa, seeking to measure not only its ability to detect a single word, but its ability to accurately detect the entire command to optimize a correct responseusing over 1,000 different open-ended queries. Cypher improved Alexas ASR accuracy by as much as 121% in a noisy environment. Wong: Dont neural networks require substantial amounts of space and processing power within the device itself Walker: Many developers think you need expensive hardware or a lot of memory within the device to use a neural network to properly tackle background noise cancellation. These are myths. One of the biggest misconceptions about neural networks is that they require a lot of space and processing power. In Cyphers case, we take a very large database of human speech and train our deep-learning neural network on servers in advance. Once traineda process that can take hours or even daysthe resulting Cypher algorithm is capable of identifying the qualities of human voice versus other noise types. Because all of this training takes place offline, by the time a developer adds our software to a device, its pre-trained to start working immediately. It has a small-enough footprint to provide real-time speech enhancement on virtually any type of product, such as phones, fitness trackers, Bluetooth headsets, etc. There is no need for a persistent network connection when the software lives on the DSP itself. Wong: What is actionable advice to product manufacturers, and does it differ if youre adding to an existing product versus building something from ground up Walker: If you want to improve the background noise cancellation and/or speech-recognition accuracy in an existing product, its best to talk with your DSP chip manufacturer to see what options they have available. For instance, we are working with CEVA TeakLite and Qualcomm Snapdragon. If you are developing a product from the ground up, with some planning you can optimize everything from the number and placement of microphones to processing and memory requirements. Cypher software can be deployed in three different ways: Embedded in existing hardware, integrated into the OS, or as an application running on top of the OS. Cypher only needs input from two microphones in all implementations. ",http://electronicdesign.com/iot/qa-managing-background-noise-speech-recognition-systems,"When there aren't blenders blending, dogs barking, or TVs blaring in the background, automated speech recognition (ASR) technologies like ...",Q&A: Managing Background Noise for Speech-Recognition Systems,Walker Bluetooth ASR Building Cypher Walker Cypher Cyphers Cyphers DSPs ASR ASR Amazon Echos Alexa Cypher Alexas ASR Dont Walker Cyphers Cypher Bluetooth DSP Walker DSP CEVA TeakLite Qualcomm Snapdragon Cypher OS OS Cypher,-1
185,"Last week at FindBiometrics we continued to take a deep dive into the evolving world of connectivity and biometrics with our Mobile Biometrics Month 2017 coverage. The industry news was filled with more fingerprint innovation as we posted our recent interview with FPC CEO Christian Fredrikson, and the weeks headlines were rounded out with the launch of Aadhaar Pay in India and more. Take a look below at last weeks top stories in global identity management: Last week we opened registration to our first expert webinar of 2017,  From Revolution to Evolution: The New Mobile Biometrics Landscape . That live webcastwhich will feature guest presentations from Acuity Market Intelligence principal Maxine Most, and also David Pollington, GSMAs head of Applications & Servicesis part of Mobile Biometrics Month 2017, which we continued with in our featured articles section on Thursday with a look at how fingerprint biometrics have reached a point at which they can truly meet the ever changing design demands of the smartphone market. Check out our Mobile Biometrics Month coverage below and be sure to sign up for the webinar: Last week we published our interview with Fingerprint Cards CEO Christian Fredrikson conducted at Mobile World Congress 2017. The conversation gave insight into the companys recently announced acquisition of Delta ID as well as the companys plans for the future in an increasingly multimodal industry. In the meantime, Zwipe announced progress in its biometric payment card efforts and we saw new fingerprint products launched from GenKey and InvenSense respectively. In terms of deployments, NEC Chile announced that its fingerprint biometric technology is improving the cafeteria experience for students, and in terms of device integrations we saw a report predicting ultrasonic fingerprint sensors on the next iPhone iteration expected to be unveiled in the fall. Here is a look at all of last weeks big fingerprint news: ",http://findbiometrics.com/roundup-aadhaar-pay-403130/,"NICE released survey results illustrating consumer opinions regarding voice authentication in the call center, and speech recognition made ...",News Roundup: The Launch of Aadhaar Pay,FindBiometrics Biometrics FPC CEO Christian Fredrikson Aadhaar Pay India From Revolution Evolution New Mobile Biometrics Landscape Acuity Market Intelligence Maxine Most David Pollington GSMAs Applications Servicesis Mobile Biometrics Month Thursday Biometrics Month Fingerprint Cards CEO Christian Fredrikson Mobile World Congress Delta ID Zwipe GenKey InvenSense NEC Chile,3
186,"NICOSIA  Ahead of the 102nd anniversary of the Armenian Genocide, the Parliament of Cyprus has called on the international community to recognize the Armenian Genocide, Ermenihaber reports citing Kibrispostasi news agency. Addressing the House of Representatives at the start of the plenary session, Speaker Demetris Syllouris said that even if more than a century have gone by since the Armenian Genocide of 24th April 1915 when Turkey applied an ethnic cleansing plan against Armenians, Turkey refuses to acknowledge the murder Syllouris noted that Cyprus was among the first states in the world to recognize and condemn the Armenian Genocide, and in 1990, the Parliament of Cyprus declared April 24 as Armenian Genocide Remembrance Day. The Parliament has also adopted the law on the criminalization of the genocide denial and the war crimes against the humanity. Armenian MP Vartkes Mahdessian also delivered a speech at the Parliament, noting that the perpetrators of the Armenian Genocide still remain unpunished. More than one hundred year has passed. We do not seek vengeance, we seek justice. We want the historical facts over this tragedy to be unanimously accepted, he added. Cyprus Parliament Calls for International Recognition of the Armenian Genocide was last modified: April 10th, 2017 by MassisPost ",http://massispost.com/2017/04/cyprus-parliament-calls-international-recognition-armenian-genocide/,"Armenian MP Vartkes Mahdessian also delivered a speech at the Parliament, noting that the perpetrators of the Armenian Genocide still remain ...",Cyprus Parliament Calls for International Recognition of the ...,NICOSIA Ahead Genocide Parliament Cyprus Genocide Ermenihaber Kibrispostasi House Representatives Speaker Demetris Syllouris Genocide April Turkey Armenians Turkey Syllouris Cyprus Genocide Parliament Cyprus April Genocide Remembrance Day Parliament MP Vartkes Mahdessian Parliament Genocide Cyprus Parliament Calls International Recognition Genocide April MassisPost,10
187,"Microsoft has achieved a significant breakthrough. The companys researchers claim that they have succeeded in getting the lowest word error rate in the industry. Last week, Google parent Alphabet Inc (NASDAQ: GOOGL ) released a report about their new milestone in artificial inteligence [AI] speech recognition that according to the company outperforms existing technology by 50%. Microsoft Corporation (NASDAQ: MSFT ) did not want to be left behind as it too announced an amazing breakthrough for this market. According to Xuedong Huang, chief speech scientist for Microsoft, their researchers achieved a word error rate (WER) of 6.3%. It is considered the lowest in the industry. Microsoft recorded the benchmark rate by combining neural network based acoustic and language modeling on the US National Institute of Standards and Technology (NIST) 2000 Switchboard speech recognition task. This is a conversational telephone speech recognition test used as an industry standard. This document found on arXiv.org contained the email reporting the discoveries of the Microsoft researchers: We describe Microsofts conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.3%, representing an improvement over previously reported results on this benchmark task. Last weekend, the international conference speech communication and technology called Interspeech was held in San Francisco. During the event, IBM proudly announced that it was able to reach a WER of only 6.6%. Over two decades ago, the top error rate of the best published research system for computer speech recognition was at 43%. Experts say that these scientific achievements could open the doors to greater possibilities. Computers will soon be able to understand the words people say just like any ordinary person. This is practically what Microsoft aims to accomplish. The company wants to make computer-human interaction more personal. The gradual integration of this technology can be seen in different applications like the Cortana personal assistant, Apples Siri, Skype translator, and other services dependent on speech and language recognition. Microsofts achievement also takes them closer to their ultimate goal to provide AI technology that has the ability to predict the needs of the users without waiting for a command. It also paves the way to the development of Microsofts plan to build intelligent systems that can see, hear, speak, and understand. If this happens, everything will be made easier for man. Microsoft and IBM both claim that their advances in speech recognition were made because of the discovery of deep neural networks used for speech processing. The artificial neural networks were made according to the patterns of how scientists perceives the brain to function. For decades, scientists have been finding ways to teach computers to perform activities that are unique to humans like speech comprehension and image recognition. ",http://wallstreetpit.com/111999-microsoft-msft-overtakes-ibm-race-speech-recognition-supremacy/,"We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based ...",Microsoft (MSFT) Overtakes IBM in the Race for Speech Recognition ...,Microsoft Google Alphabet Inc NASDAQ GOOGL [ AI Microsoft Corporation NASDAQ MSFT Xuedong Huang Microsoft WER Microsoft US National Institute Standards Technology NIST Switchboard Microsoft Microsofts Switchboard MMI RNNLMs RNNLM NIST Switchboard Interspeech San Francisco IBM WER Microsoft Cortana Apples Siri Skype Microsofts AI Microsofts Microsoft IBM,2
188,"By Ken Briodagh October 12, 2016 In a release, Conexant Systems, a provider of audio and voice technology solutions, and SK Telecom, Koreas largest mobile carrier, have announced that they will work together on implementing AudioSmart far-field voice solutions in the new NUGU smart speaker. This will make the Smart Home hub hear users requests accurately from at a distance. With this collaboration, NUGU voice will offer the highest voice recognition accuracy rate in Korea, the two companies said. When developing the NUGU personal digital assistant, our engineers focused heavily on ensuring it would provide an optimal voice-enabled user experience, said Myung Soon Park, SVP, Future Technology R&D Center, SK Telecom. When choosing a technology partner to help us achieve the level of speech recognition accuracy we wanted, Conexant was an obvious choice. Their technology allows our NUGU personal digital assistant to overcome distance and noise related challenges and offer ideal near- and far-field speech recognition performance, as well as enables users to interrupt and interact with the device when it's playing media. Enabling speech recognition and voice control from a distance in smart platforms requires overcoming the challenges of echo cancellation, background noise, microphone speaker position and several more. Conexant solutions separate voice commands from background noise so that the speech recognition engine hears the user's command as clearly as possible. As the voice-enabled revolution continues to shift the way people interact with devices of all types, reliance on speech enhancement technologies that help devices hear user commands more accurately will continue to grow, said Saleel Awsare, President, Conexant. Conexant has been a leading provider of voice processing solutions that improve speech recognition accuracy and enable advanced voice capabilities for years. We are pleased to be working with SKT to bolster the speech recognition performance of their NUGU smart speaker. ",http://www.iotevolutionworld.com/smart-home/articles/426031-sk-telecom-conexant-use-far-field-speech-recognition.htm,"Enabling speech recognition and voice control from a distance in smart platforms requires overcoming the challenges of echo cancellation, ...",SK Telecom and Conexant Use Far-Field Speech Recognition for ...,Ken Briodagh October Conexant Systems SK Telecom Koreas AudioSmart NUGU Smart Home NUGU Korea NUGU Myung Soon Park SVP Future Technology R D Center SK Telecom Conexant Their NUGU Enabling Saleel Awsare President Conexant Conexant SKT NUGU,11
189,"Speechmatics, the Cambridge-based speech technology company, has received investment from multiple leading investors to accelerate the commercial roll-out of its products. Founded by Chief Technology Officer Dr Tony Robinson, who pioneered PhD research into Recurrent Neural Networks in the 1980s, Speechmatics has developed a unique machine learning technology to harness the full potential of speech technology.  This is an exciting time to be in speech recognition, comments Robinson. We are at the forefront of how deep neural networks are changing speech recognition. With our ever expanding and highly experienced R&D team, we continue to push the boundaries in speech technology, especially around languages, accuracy and deployment. The firm received investment from several sources, including technology venture capitalist IQ Capital, AI/machine learning specialist and technology investors Amadeus Capital Partners and a number of leading technology investors including Laurence Garrett (Highland Capital Europe), Cambridge Professor Ted Briscoe, a world expert in Natural Language Processing, as well as co-founders of CSR, and Richard Gibson (previously Exec Chairman at SwiftKey). Benedikt von Thngen, CEO of Speechmatics (pictured), said: Over the past two years Speechmatics has seen substantial growth, entirely through cash-flow. The addition of these highly-experienced investors as advisors to the business will help accelerate the on-going commercialisation of our technology and place speech recognition technology at the heart of all communications. Ed Stacey, Partner at IQ Capital Partners, added: Speechmatics disruptive technology has significantly greater accuracy than competitors such as Google, IBM or Microsoft, which opens up many new commercial opportunities for speech recognition  from call compliance driven by legislation such as MiFID II and PCI DSS, to content discovery and speech analytics in order to spot trends and understand the voice of the customer. With a simple set-up and a combination of cloud and on-premise based solutions, Speechmatics technology enables businesses to generate data about customers and employees which is harnessed to improve process, efficiency, and benefit the bottom line. Speechmatics is now even more equipped to help clients navigate the complex changes in technology, to generate business insights, and to facilitate more powerful communications in the digital age. In 2016, Speechmatics launched its new AI framework, Auto-Auto, which enables the company to add almost any language automatically. Since building the framework, Speechmatics has released a new language every two weeks, including most European languages, and Greek, Russian and Arabic. Speechmatics has a successful track record of delivering exceptional results across a wide range of applications and industries: for example, language assessment with Cambridge English or content discovery with Udemy. Other applications include call centre analytics, call compliance, sub-titling, interview & lecture transcription and media monitoring. Speechmatics has developed highly accurate universal models that work across use cases and industries and do not need to be individually trained. Tony Robinsons world-class research team utilises the latest ML and AI technology to offer continual improvements in accuracy, an ever-increasing range of languages and new business applications. This technology is also designed to integrate with other applications in the workplace to generate value and insight. Richard Gibson, recently appointed Chairman, said: I am excited to be working with another successful game-changing deep technology company out of Cambridge. With this funding, Speechmatics will be able to harness the true potential of its technology and accelerate its commercial roll-out. Speechmatics ( www.speechmatics.com ) provides the worlds smartest speech technology harnessing the latest advances in deep learning. Founded in 2006 Speechmatics draws on the technologies pioneered by Dr Tony Robinson during his PhD research into Recurrent Neural Networks in the 1980s and has developed new ways of using deep learning. These are combined with the very latest developments from academia and industry. The automatic speech recognition system powers a wide range of different applications that all depend on converting speech to text as the initial step. Speechmatics has developed universal models that can be used in any use case without any further training. As a result, Speechmatics clients are using technology to improve their commercial value, ranging from data predictive analytics to language assessment or content discovery. Speechmatics has a world class, international research R&D team comprising of multiple PhD researchers. Their unique approach to deep learning pushes the boundaries of what is possible in speech recognition, enabling faster, more accurate and more powerful communications in the digital age. This technology enables businesses to get a better understanding of their customers and employees, allowing organisations to improve efficiency and daily processes, across many languages, and re-establish speech as the main form of human: machine interaction. IQ Capital invests in ground-breaking technologies and ambitious founders, capable of dominating their respective markets. IQ Capital invests early and helps to drive growth for the long-term. The IQCP team has achieved significant exits since 1997 including Autonomy (IPO), Imsense (Apple), Neul (Huawei), Phonetic Arts (Google), KVS (Veritas), Stillfront (IPO) and Sirigen (BD). The most recent 50m 2015 fund is actively seeking new investment opportunities. For more information, please visit www.iqcapital.co.uk Amadeus Capital Partners is a global technology investor. Since its inception in 1997, the firm has raised over $1bn for investment and backed more than 100 companies in the software, mobile, internet, cyber security and medical technology sectors. The investment team is based in India, South Africa, Sweden, UK and USA, has deep experience in technology and invests in high-growth companies from early stage to pre-IPO. Major businesses built by Amadeus include CSR, a connectivity, audio, imaging and location semiconductor company, now owned by Qualcomm (NASDAQ: QCOM); Solexa, the developer of next generation genetic analysis systems, merged into Illumina (NASDAQ: ILMN) to create the world leader in gene-sequencing technology; Optos, the retinal imaging company, acquired by Nikon; ForeScout, a global enterprise security vendor; Octo Telematics, the provider of insurance telematics services acquired by Renova Group; and Tobii, the global innovator in eye tracking (STO: TOBII). w ww.amadeuscapital.com ",https://www.cambridgenetwork.co.uk/news/speechmatics-closes-growth-funding-round/?,"Speechmatics, the Cambridge-based speech technology company, has received investment from multiple leading investors to accelerate the ...",Cambridge speech recognition company closes growth funding ...,Chief Technology Officer Dr Tony Robinson PhD Recurrent Neural Networks Speechmatics Robinson R D IQ Capital AI/machine Amadeus Capital Partners Laurence Garrett Highland Capital Europe Cambridge Professor Ted Briscoe Natural Language Processing CSR Richard Gibson Exec Chairman SwiftKey Benedikt von Thngen CEO Speechmatics Speechmatics Ed Stacey Partner IQ Capital Partners Google IBM Microsoft MiFID II PCI DSS Speechmatics Speechmatics AI Auto-Auto Speechmatics Greek Russian Arabic Speechmatics Cambridge English Udemy Speechmatics Robinsons AI Richard Gibson Chairman Cambridge Speechmatics Speechmatics Dr Tony Robinson PhD Recurrent Neural Networks Speechmatics Speechmatics Speechmatics R D PhD IQ Capital IQ Capital IQCP Autonomy IPO Imsense Apple Neul Huawei Google KVS Veritas Stillfront IPO Sirigen BD Amadeus Capital Partners India South Africa Sweden UK USA Amadeus CSR Qualcomm NASDAQ QCOM Solexa Illumina NASDAQ ILMN Optos Nikon ForeScout Octo Telematics Renova Group Tobii STO,6
190,"The data volumes expected from the Internet of Things (IoT) are certain to be large  too large, in fact, for even an army of trained analysts to turn into useful information in a reasonable amount of time. This is why every solution aimed at the IoT relies heavily on automation, simply to manage the flow of information between devices and to centralized storage and analytics systems. But even this is not likely to be enough. To fully leverage the IoT, its becoming obvious that the enterprise will have to utilize new forms of artificial intelligence and machine learning to basically allow the environment to makes its own use of available data and tell human operators what needs to be done. Already, this is emerging on leading IoT platforms. Software developer C3 recently updated its IoT platform by pushing artificial intelligence to the edge where it can function on an application level for an improved user experience. The system uses Amazon Web Services for infrastructure and device management and provides AI and machine learning tools to supplement both out-of-the-box and homegrown applications. As well, the platform enables deep learning technology on the analytical side to improve its predictive capabilities, plus tools like object and facial recognition, natural language processing, and text analysis capable of interpreting even hand-written notes. Combining AI and the IoT is the best way to unleash the enormous potential that both technologies bring to the enterprise , says jaxenter.coms Rick Delgado. Without a high degree of system autonomy, the IoT is merely a massive data collection service, with no way to interpret or leverage that data for any meaningful purpose. Although the IoT brings many new technologies into play, it still suffers from the same vertical segmentation of any other silo-based infrastructure. Implemented properly, AI can break down the silos of pooled information to further the basic rationale behind the IoT: to find the connections between data sets that would otherwise remain hidden. A key application for AI in the IoT is the data catalog , according to Informaticas Amit Walia. Speaking to Silicon Angle at the Big Data SV conference in San Jose recently, Walia noted that its not just the amount of data that poses a problem for analysts, but the diversity of data types, streams and sources. With AI providing the metadata that gives meaning to all of this raw, unstructured data, human analysts can provide the higher-value service of actually analyzing information, rather than the rote processing and categorization. Its the difference between forming IoT infrastructure around a data lake or a data swamp. But AI has the potential to increase the IoTs value even further by extending its reach beyond highly trained data analysists and scientists to the average business user, or even the consumer. The best way to do that is by simplifying the user interface, which AI is uniquely suited for through its ability to recognize and emulate human speech. As TechRepublics Alison DeNisco points out, IBM recently brought the word error rate of its speech recognition system down to 5.5 percent , which is close to human parity. This will have enormous implications for the enterprise because it opens the possibility of speaking to digital systems as easily as we speak to co-workers. So instead of clicking, typing and texting our way through menus to get what we want, we can simply ask for it and the artificially intelligent system will be able to figure out on its own how to provide it. IBM says it is not popping the champagne yet over its speech recognition but is instead focusing on error rates of 5.1 percent or lower and tuning the system toward more complex contextualization. At the moment, the enterprise is focused on putting IoT infrastructure in place and getting it to function in a reasonably cohesive fashion. But it wont be long before data volumes start to mount and the pressure will be on to find the value that justifies the expense of all this massive collection and analytic infrastructure. Its a safe bet that the first enterprises to cross this line will have done it through artificial intelligence. Arthur Colewrites about infrastructure for IT Business Edge. Cole has been covering the high-tech media and computing industries for more than 20 years, having served as editor ofTV Technology, Video Technology News, Internet News and Multimedia Weekly. His contributions have appeared in Communications Today and Enterprise Networking Planet andas web content for numerous high-tech clients like TwinStrata and Carpathia. Follow Art on Twitter @acole602 . ",http://www.itbusinessedge.com/blogs/infrastructure/how-artificial-intelligence-will-make-the-iot.html,"As TechRepublic's Alison DeNisco points out, IBM recently brought the word error rate of its speech recognition system down to 5.5 percent, ...",How Artificial Intelligence Will Make the IoT,Internet Things IoT IoT IoT IoT Software C3 IoT Amazon Web Services AI AI IoT Rick Delgado IoT IoT AI IoT AI IoT Informaticas Amit Walia Silicon Angle Big Data SV San Jose Walia AI IoT AI IoTs AI TechRepublics Alison DeNisco IBM So IBM IoT Arthur IT Business Edge Cole Technology Video Technology News Internet News Multimedia Weekly Communications Today Enterprise Networking Planet TwinStrata Carpathia Follow Art Twitter @,-1
191,"Speechmatics breakthrough in speech recognition could be used in mobile phones, home entertainment and call centres Dr Tony Robinson at Speechmatics in Cambridge. Picture: Keith Heppell Cambridge technology firm Speechmatics has unveiled a breakthrough that could change how we interact with our mobile phones and home entertainment systems. To send a link to this page you must be logged in. Dr Tony Robinson at Speechmatics in Cambridge. Picture: Keith Heppell The Cambridge firm has launched a  real-time, embeddable continuous speech recognition system in many languages  a breakthrough that it says will provide the kind of high level of accuracy and speed usually only found in expensive cloud-based services. The speaker independent technology will be of use to a host of markets, from businesses requiring transcriptions to manufacturers of devices. It will enable improved live subtitling, or could be used to gather real-time data in call centres. It will also enable offline email dictation on mobile phones or allow home entertainment systems to feature true voice interaction. Dr Hermann Hauser, co-founder of Speechmatics investor Amadeus Capital Partners, said: We are seeing a shift in the tech industry as we move away from touchpad technology towards speech as the main form of communication. This shift is creating a need for businesses to gain immediate, actionable intelligence through highly accurate speech recognition technology, in many languages. There is strong demand in the market for Speechmatics, as it will allow businesses that work on an international scale to not only ensure speech is transcribed correctly, but also to improve everyday user experiences. The breakthrough comes through advances in recurrent neural networks  technology developed by founder Dr Tony Robinson during his PhD at Cambridge and enhanced over 30 years. Then, in 2016, Speechmatics R&D team redesigned the system. Systems with large vocabularies that have worked at high speed have typically lost accuracy. The technology has previously been limited to post-processing after an interaction or to the use of short phrases. It proved nearly impossible for large, continuous systems to process fast, accurate transcriptions over long periods. But Speechmatics new system has a 250,000-word vocabulary for each language, optimised for speed and accuracy on devices from mobile phones to servers. To improve data security, the system enables data to be held and processed by the user, running natively on a device, rather than in the cloud. Speechmatics says the offline capability takes us a step closer to using such technology anywhere, at any time. Dr Robinson said: This is the second big breakthrough we have had within nine months. First, we completed the first version of our Auto-Auto system. This was a massive step forward for speech science, as for the first time we could build new languages automatically with one-two orders less data and in a matter of weeks. For example, we built Japanese in a few weeks even though no one in the team spoke Japanese. Were delighted to continue making advances in speech recognition technology, building a strong R&D team and making our achievements globally available. A free trial demo is available at speechmatics.com. To send a link to this page you must be logged in. ",http://www.cambridgeindependent.co.uk/speechmatics-breakthrough-in-speech-recognition-could-be-used-in-mobile-phones-home-entertainment-and-call-centres-1-4889435,"The Cambridge firm has launched a real-time, embeddable continuous speech recognition system in many languages – a breakthrough that it ...",Speechmatics' breakthrough in speech recognition could be used in ...,Dr Tony Robinson Speechmatics Cambridge Keith Heppell Cambridge Speechmatics Dr Tony Robinson Speechmatics Cambridge Keith Heppell Cambridge Dr Hermann Hauser Speechmatics Amadeus Capital Partners Speechmatics Dr Tony Robinson PhD Cambridge Speechmatics R D Speechmatics Speechmatics Dr Robinson First R D,6
192,"VIENNA--( BUSINESS WIRE )-- Speech        Processing Solutions , the worlds number one in professional dictation        solutions , is proud to announce that thanks to its wide-spread        success, Philips SpeechLive is now also being made available in Italy,        Ireland, Finland, Spain, Sweden and South Africa. The dictation workflow solution Philips SpeechLive helps save time by        turning peoples voice into written text. Alongside a classic transcriptionist        service , it now also offers an on-demand speech        recognition service in over twenty languages, allowing users to get        more work done, in less time. The solution is perfectly suited for people who are on the go. Recordings can be sent using the Philips dictation app using a smartphone or a        Philips voice        recorder , such as the Philips Pocket Memo 8000 series. We        developed Philips SpeechLive to help improve the lives of busy        professionals. You can stop loosing time by focusing on your core tasks,        and have documents, notes and meeting minutes simply typed up for you explains        Dr Thomas Brauner, CEO of Speech Processing Solutions. He        continues: We received so much positive feedback since we        implemented our new speech recognition service feature that we decided        to expand our reach to new countries. The solution is extremely easy to adopt. It requires no installation and        can be accessed using a normal web browser. Tony Oakley, Principal at        the Australian law firm Hicks Oakley Chessell Williams in Melbourne        explains: Coming from an analog solution, we were very happy at how        easy and intuitive our dictation        workflow solution is. His officer manager, Nicole Honan        adds: For me, it is very convenient that I can work the same from        any location, I dont have to think about taking something with me. Regardless of the industry, Philips SpeechLive can help speed up users        daily workflow by saving them lost typing        time . Philips SpeechLive is as simple as speak, send, done concludes Dr Brauner. Philips is now offering a 30-day free trial of their software, including        10 free speech        recognition service minutes. Visit www.speechlive.com for more information. Speech        Processing Solutions is the global leader in professional dictation        solutions. The company was founded in 1954 in Austria as a Philips        subsidiary, and has been a driving force for innovative speech-to-text        solutions for 60 years. The company developed ground-breaking        products such as the mobile Philips        SpeechAir , the Philips        Pocket Memo voice recorder , the Philips        SpeechMike Premium USB dictation microphone and the Philips        Dictation Recorder app for smartphones, thus meeting its demands for        excellence and superior quality. Thanks to the newest innovation, Philips        SpeechLive , dictations and recordings will become faster and easier        than ever before with cloud-based workflow services. Speech Processing        Solution's perfectly tailored offers and products help professionals        save time and resources and maximize efficiency. Connect with Speech Processing Solutions on: ",http://www.businesswire.com/news/home/20161102005054/en/Philips-SpeechLive-Including-Speech-Recognition-Countries,"VIENNA--(BUSINESS WIRE)--Speech Processing Solutions, the world's number one in professional dictation solutions, is proud to announce ...",Philips SpeechLive Including Speech Recognition Now Available in ...,VIENNA BUSINESS WIRE Processing Solutions Philips SpeechLive Italy Ireland Finland Spain Sweden South Africa Philips SpeechLive Philips Philips Philips Pocket Memo Philips SpeechLive Dr Thomas Brauner CEO Speech Processing Solutions Tony Oakley Principal Hicks Oakley Chessell Williams Melbourne Nicole Honan Regardless Philips SpeechLive Philips SpeechLive Dr Brauner Philips Visit Solutions Austria Philips Philips SpeechAir Philips Pocket Memo Philips SpeechMike Premium USB Philips Dictation Recorder Philips SpeechLive Speech Processing Solution Speech Processing,5
193,"I'd Like to Thank the Academy, and These Other People... A few weeks ago, my friend David VonAllmen found out that he won a major award (and no, not a leg lamp or a bowling alley). It was a Writers of the Future award, and its given to only twelve people a year for science fiction writing. Over the thirty-three years since the inception of the event, some past winners have gone on to huge projects, including movies, TV shows, and books galore. So David and the other eleven winners were flown out to LA for some writing workshops, and then a red carpet event akin to any major award show youve ever seen. And after the ceremony, all of the winning stories were to be released in an anthology. I was absolutely thrilled for him. So imagine my stunned surprise when two of my friends said, Hey, did you hear David mentioned you by name in his thank-you speech Sure enough, I checked the link that my friends gave me and went to the 2.26.30 mark  and he absolutely killed his acceptance speech. It was really, really well done. He had the crowd eating out of his hands. And wouldnt you know it, he actually did mention me by name. And he didnt just mention me by name, but also by what I did to help inspire him. It was kind of a holy $#!+ moment for me. Awards are a big deal. Theyre recognition for a job well done, especially when given by people you look up to who consider your stab at creativity to be worthy of a statue. Or a plaque. Or a leg lamp. Id be lying if I didnt briefly think about who I would thank in an acceptance speech if I was ever given the opportunity. Definitely my wife. My kids. My folks. God. The person who invented the Reeses Peanut Butter Cup. But when I heard Davids speech, especially when I heard the people to whom he gave his very public gratitude, I decided to sit down and give it a whirl in the very, VERY off chance that I ever receive a similar honor. First off, Id like to thank God for giving me every gift I have, especially for the gifts of my wife Stephanie and my boys Sam and Ben who are my reason for everything. (Thats the four biggest biggies in one sentence. The Academy would be thrilled at my succinctness.) Id also like to thank my parents Ray and Marie, my brother Gavin, my folks and brother and sister-in-law, my nieces and nephews and all of my relatives who are continuous proof of the importance of family. Id like to thank my dog Summer and her lack of potty training skills for teaching me patience and humility. Id like to thank anyone who has ever hired me, and anyone who has ever taught me which gave me the skills that has led to more people hiring me. Id like to thank the people who constantly inspire me, including Kevin Smith who showed me that phenomenal comedic writing (plus a few maxed out credit cards) can be a catapult for your dreams. Id also like to thank Doug Lindsay , who not only discovered his own disease, but then found the previously unknown cure, and the medical team to actually pull off the never-been-done-in-a-human surgery. Yes, you read that correctly. Never underestimate the power of dogged research and unrestrained perseverance. Id like to thank the folks at Stanford Medicine X , TedX Sarasota , Gildas Club in Madison , WI, the Cancer Action Network , the Huffington Post , FCCLA , Lucky Bat Books , and anyone who has ever given me a voice or a platform to share what little I know. I would, indeed, like to thank the inventor of the Reeses Peanut Butter Cup. Hell, Ill thrown in Twix and Kit Kat, as well. Id like to thank people like Matthew Zachary and Doug Ulman and Cara and Billy Paymaster and Joe Farmer and Teri Griege and Renata Sledge and Mike Craycraft and Pat Taylor and Kenny Kane and April Dzubic and Ashley Swip and Toby Freeman and Marsha Lynn Hammond and everyone who strives to make cancer a thing of the past, and to make it less devastating for those who continue to live with it. As the band started playing me off over a minute ago, I know Im running out of time, so Ill make the rest briefer. Id like to thank all of the friends, both old friends and newer ones at my parish, my neighbors, Ryan Reynolds , Sponge Bob , barbecue grills, trampolines, airplanes, roller coasters, pianos, road trips, Bigfoot (both the truck, and the beast), Rush , U2 , red meat, beer and wine, Santa Claus, hiking, Blues Hockey and Cardinals Baseball , and too many countless things that just make life more interesting. And to my friend David, who Im not only grateful to for mentioning me by name in his acceptance speech, but for inspiring this little exercise in thinking about the people and things that make life grand. The 33rd edition of the Writers of the Future is available on Amazon. David will be signing copies on Saturday, April 15 from 2-4pm at the Barnes and Noble. Dan Duffy is a husband, dad, video producer, author, and testicular cancer survivor. His memoir, The Half Book: Hes Taking His Ball and Going Home , is available on Amazon in print and e-book versions. Meet up with Dan this year at Stupid Cancers CancerCon in Denver, the Midwest Young Adult Cancer Conference sponsored by Gildas Club in Madison, and Stanford Medicine X in Palo Alto. This post is hosted on the Huffington Post's Contributor platform. Contributors control their own work and post freely to our site. If you need to flag this entry as abusive, send us an email . Start your workday the right way with the news that matters most. Learn more ",http://www.huffingtonpost.com/entry/id-like-to-thank-the-academy-and-these-other-people_us_58ebe2bde4b0ea028d568be1,"They're recognition for a job well done, especially when given by people ... thank in an acceptance speech if I was ever given the opportunity.","I'd Like to Thank the Academy, and These Other People...",Academy David VonAllmen Writers Future David LA Hey David Sure Theyre Id Definitely God Reeses Peanut Butter Cup Davids VERY First Id God Stephanie Sam Ben Thats Academy Id Ray Marie Gavin Id Summer Id Id Kevin Smith Id Doug Lindsay Never Id Stanford Medicine X TedX Sarasota Gildas Club Madison WI Cancer Action Network Huffington Post FCCLA Lucky Bat Books Reeses Peanut Butter Cup Hell Ill Twix Kit Kat Id Matthew Zachary Doug Ulman Cara Billy Paymaster Joe Farmer Teri Griege Renata Sledge Mike Craycraft Pat Taylor Kenny Kane April Dzubic Ashley Swip Toby Freeman Marsha Lynn Hammond Im Ill Id Ryan Reynolds Sponge Bob Bigfoot Rush U2 Santa Claus Blues Hockey Cardinals Baseball David Writers Future Amazon David Saturday April Barnes Noble Dan Duffy Half Book Going Home Amazon Meet Dan Stupid Cancers CancerCon Denver Midwest Young Adult Cancer Conference Gildas Club Madison Stanford Medicine X Palo Alto Huffington Post Contributor,-1
194,"Philips Digital Voice Tracer (Photo: Business Wire) VIENNA--( BUSINESS WIRE )-- Speech        Processing Solutions , the world number 1 for professional dictation        solutions , has launched a new audio recorder. The Philips        VoiceTracer audio        recorder DVT7500 has been specially developed for music recordings.        Amongst its innovative features, it offers three high-quality        microphones, an XLR and line-in        connector , as well as 16GB of internal storage space. The new device is perfectly suitable for hobby musicians, composers,        song writers and music students alike. Stefan, guitarist in the Austrian        band Stan Green, explains: Its a must have! Its great for being        spontaneous when composing songs. Whenever I have an idea for a new        tune, I play it on my guitar and record it with the VoiceTracer. When I meet the rest of the band later, I let        them listen to it and we can work on the song from there. The new audio recorder features three high- quality        microphones and can record PCM (WAV) and MP3 files at up to 24        bit/96 kHz sampling rate, promising crystal-clear sound quality. The        device comes with an included XLR and line-in connectors, meaning        musicians can easily plug in their e-guitar or keyboard and record        directly, even from multiple sources at once. The input volume can also        be manually adjusted using a dial on the side of the device. This gives        users greater control over their recordings and helps minimize        background noises. The Philips VoiceTracer for music has a large color display and        conveniently placed buttons. Dr. Thomas Brauner explains: We made        sure the new Philips VoiceTracer for music recordings is easy and intuitive to use, so musicians can focus on the playing        their instrument and not waste any time. The high-capacity Li-polymer        battery offers extra-long battery life and the microSD card slot gives        them virtually unlimited storage space. The device also has a        tripod thread, allowing musicians to position the device into the        perfect distance from the instrument. The Philips VoiceTracers are also the ideal consumer products for        capturing notes, conversations, lectures, interviews and meetings. The        audio recorder for notes (DVT2710) comes with an included speech recognition software,        Dragon NaturallySpeaking from Nuance. Dr. Brauner goes on to explain: You        speak up to seven times quicker than you type. With the included        software you can quickly turn your voice into text and the software is        extremely reliable. It can achieve up to 99% accuracy. For more information, please visit Philips product page: www.philips.com/dictation Speech        Processing Solutions is the global leader in professional dictation        solutions. The company was founded in 1954 in Austria as a Philips        subsidiary, and has been a driving force for innovative speech-to-text        solutions for 60 years. The company developed ground-breaking        products such as the mobile Philips        SpeechAir , the Philips        Pocket Memo voice recorder , the Philips        SpeechMike Premium USB dictation microphone and the Philips        Dictation Recorder app for smartphones, thus meeting its demands for        excellence and superior quality. Thanks to the newest innovation, Philips        SpeechLive , dictations and recordings will become faster and easier        than ever before with cloud-based workflow services. Speech Processing        Solution's perfectly tailored offers and products help professionals        save time and resources and maximize efficiency. Connect with Speech Processing Solutions on: ",http://www.businesswire.com/news/home/20170321005128/en/Philips-Launches-Brand-New-Music-Audio-Recorder,"VIENNA--(BUSINESS WIRE)--Speech Processing Solutions, the world ... (DVT2710) comes with an included speech recognition software, ...",Philips Launches Brand-New Music Audio Recorder,Philips Digital Voice Tracer Photo Business Wire VIENNA BUSINESS WIRE Processing Solutions Philips VoiceTracer DVT7500 Amongst Stefan Stan Green VoiceTracer PCM WAV MP3 Philips VoiceTracer Dr. Thomas Brauner Philips VoiceTracer Philips VoiceTracers DVT2710 Dragon NaturallySpeaking Nuance Dr. Brauner Philips Speech Processing Solutions Austria Philips Philips SpeechAir Philips Pocket Memo Philips SpeechMike Premium USB Philips Dictation Recorder Philips SpeechLive Speech Processing Solution Speech Processing,5
195,"SK Telecom works with Conexant to produce first smart speaker capable of natural language processing of Korean language; Enables device to hear users accurately from a distance and in noise IRVINE, Calif., Oct. 12, 2016 /PRNewswire/ -- Conexant Systems, Inc., a leading provider of audio and voice technology solutions that enable a more natural user experience, today announced that SK Telecom, Korea's largest mobile carrier, is implementing AudioSmart far-field voice solutions to help the new NUGU smart speaker hear users' requests accurately from a considerable distance away from the device. Conexant provides advanced voice processing technologies that help NUGU voice services offer the highest voice recognition accuracy rate in Korea. Conexant AudioSmart technology enables highly accurate voice control in smart home applications, robots, IoT devices, and more. ""When developing the NUGU personal digital assistant, our engineers focused heavily on ensuring it would provide an optimal voice-enabled user experience,"" said Myung Soon Park, Senior Vice President, Future Technology R&D Center at SK Telecom. ""When choosing a technology partner to help us achieve the level of speech recognition accuracy we wanted, Conexant was an obvious choice. Their technology allows our NUGU personal digital assistant to overcome distance and noise related challenges and offer ideal near- and far-field speech recognition performance, as well as enables users to interrupt and interact with the device when it's playing media."" Enabling speech recognition and voice control from a distance in smart platforms requires overcoming substantial challenges related to echo cancellation, background noise, microphone speaker position and more. Conexant solutions are able to separate voice commands from background noise so that the speech recognition engine hears the user's command as clearly as possible, which in turn provides a far more accurate voice-enabled experience, even in noisy, real-world conditions, up to several meters away from the target device. The Conexant Far-Field Voice Input Processor solution for voice interactive devices is an industry first. This family of processors leverages Conexant AudioSmart far-field voice processing algorithms, enabling OEMs to deliver products, like NUGU, that offer a voice-enabled user experience. ""As the voice-enabled revolution continues to shift the way people interact with devices of all types, reliance on speech enhancement technologies that help devices hear user commands more accurately will continue to grow,"" said Saleel Awsare, President of Conexant. ""Conexant has been a leading provider of voice processing solutions that improve speech recognition accuracy and enable advanced voice capabilities for years. We are pleased to be working with SKT to bolster the speech recognition performance of their NUGU smart speaker."" Conexant Systems, Inc. is the CE industry's go-to source for hardware and software solutions that offer unrivaled performance for audio and voice applications. The company combines its significant IP portfolio in DSP, analog and mixed signal technology with embedded software to enrich and expand audio and voice capabilities of products across nearly every vertical of the CE space. Founded in 1999, Conexant is a privately-held fabless semiconductor and audio technology engineering company headquartered in Irvine, California with offices and design centers worldwide. ",http://www.prnewswire.com/news-releases/conexant-audiosmart-provides-superior-far-field-speech-recognition-performance-on-first-smart-speaker-for-korean-market-300335468.html,"When choosing a technology partner to help us achieve the level of speech recognition accuracy we wanted, Conexant was an obvious choice ...",Conexant AudioSmart Provides Superior Far-Field Speech ...,SK Telecom Conexant Enables IRVINE Calif. Oct. Systems Inc. SK Telecom Korea AudioSmart NUGU Conexant NUGU Korea Conexant AudioSmart IoT NUGU Myung Soon Park Senior Vice President Future Technology R D Center SK Telecom Conexant Their NUGU Far-Field Voice Input Processor Conexant AudioSmart OEMs NUGU Saleel Awsare President Conexant Conexant SKT NUGU Systems Inc. CE IP DSP CE Conexant Irvine California,11
196,"In this monthly feature I showcase 5 projects that I find interesting, since we are celebrating International Womens Day this month, I wanted to continue that theme by highlighting interesting projects that female innovators have shared with us on Intel Developer Mesh. I hope youll follow along and discover not just these few projects here but the multitude of amazing projects that our community is sharing on Mesh  with 29 new projects added to the site last month this site will quickly become your go-to place to see what is happening in the innovative tech arena. Agnes Duverger is putting together a multi-part series on how to build a connected longboard. In part 1 she shows you how to easily build the board itself in about 10 hours. Coming up next in part 2 will be adding the connected device to the board. Agnes works at SmartNotify which specializes in adding a communication layer to devices, so after losing her favorite longboard it makes sense that she wants to make the next one connected. Watching people skateboard can be really mesmerizing, but Ive never watched a board being built before. I found the process interesting and am looking forward to adding the tech to it in part 2. I was so bummed to have missed out on seeing Stoicheia twice in the past few weeks. It was displayed, locally for me, at both the Portland Winter Lights Festival and at a local Maker Open House celebrating Teen Tech Week and International Womens day. All I can say is kids are germ magnets and I look forward to getting another chance to view this in person in the future. Lilli Szafranksi designs digital stained-glass pieces which are controlled by procedurally-generated random code to light the +2,200 LEDs and create fantastic designs combining art and technology in a really unique way. I love a good quiz game, whether its on a kiosk at the restaurant table, a game of trivial pursuit, or trivia night at the pub. What Pooja Baraskar has done is connect an Intel Edison and an Intel RealSense camera and designed a quiz game to use facial recognition to adjust the game according to the users age. So kids wont find the game too challenging and give up, and adults wont find the game too easy and get bored. The game asks the questions in a human-like voice and uses speech recognition to capture your answer and process and validate it, then saves the scores to the cloud. Erika Harvey is working on a proactive tool for caregivers to stop Alzheimer's patients from wandering. Once an individuals baseline is learned, when they deviate from the norm a push alert is sent to the caregiver and a location tracker with a 3-mile radius is immediately activated and will even work in a heavily urban area. While this was designed with Alzheimers and memory care patients in mind I can easily see this as a useful tool for other vulnerable people, or even pets, that have a tendency to wander. I can have a hard time visualizing how to do something, especially when it is technical. So for me, watching it step-by-step helps it all to click and make sense. Manisha Biswas guide does just this  she has a both a video and a write-up that completely walk you through the process of implementing an augmented reality experience to your project. She makes it looks so simple, that I wouldnt hesitate to try it out myself. Although, I probably wouldnt choose a spider they give me the creepy-crawlies. Interested in getting your project featured Join us at Intel Developer Mesh today and become a member of our amazing community of developers. For more such intel Modern Code and tools from Intel, please visit the Intel Modern Code ",http://www.digit.in/apps/intel-developer-mesh-editors-picks-mar-2017-34547.html,"The game asks the questions in a human-like voice and uses speech recognition to capture your answer and process and validate it, then ...",Intel Developer Mesh: Editor's Picks Mar. 2017,"International Womens Day Intel Developer Mesh Mesh Agnes Duverger SmartNotify Ive Stoicheia Portland Winter Lights Festival Maker Open House Teen Tech Week International Womens Lilli Szafranksi +2,200 LEDs Pooja Baraskar Intel Edison Intel RealSense Erika Harvey Alzheimer Alzheimers Manisha Biswas Join Intel Developer Mesh Modern Code Intel Intel Modern Code",3
197,"BOSTON--( BUSINESS WIRE )--An advanced version of Automatic Speech Recognition (ASR) used in        computers and other devices is changing the way secondary and higher        education students learn world languages. Vista Higher Learningthe        leading specialized publisher of world language products in the United        Stateshas teamed with Cobalt Speech and Language, Inc. to develop a        Verification Automatic Speech Recognition (VASR) engine which determines        how well a learners speech compares to a specific, pre-determined word        or phrase. Most consumers have experienced Automatic Speech Recognition, the        process by which a computer or other device converts speech into text.        Vista Higher Learning has taken this technology one step further with        VASR, enabling the computer to determine what a learner is saying and        how accurate the speech is in both its form and substance. The Massachusetts-based company recently released VASR in Portales, its        groundbreaking introductory Spanish program in use at over 100 colleges        and universities. Fifteen thousand students currently rely on Portales every day to practice their vocabulary and communications skills,        improving their learning with instant and intelligent feedback which        highlights their successes and errors. Portales prompts learners        to state a word or phrase; then records the speech and provides        real-time feedback with an emphasis on vocabulary recognition and        speaking skills. This week, Vista Higher Learning announced that more        than 1.5 million speech recognition recordings have been submitted by        students using Portales. On average, 12,000 new recordings are        submitted for review each day. When we released VASR this summer, said Kurt Gerdenich, Chief        Technology Officer for Vista Higher Learning, we knew we were providing        students with educational learning technologies unlike anything        available today. Still, no one could predict that so many students would        benefit from our advanced speech recognition so quickly. Portales currently offers three different learning activities        featuring VASR: Vocabulary Tutorials, Fotonovela Tutorials, and        Pronunciation Tutorials. In all, the program features over 300        VASR activities covering approximately 4,000 unique Spanish words and        phrases. VASR enables students to receive accurate, intelligent guidance in real        time during both vocabulary and pronunciation instruction. And because        it reviews what is said and how words are pronounced, our        speech recognition activities are the ideal tool to engage and encourage        language learners, said David Mackin, Vice President of Sales and        Marketing. In 2017, Vista Higher Learning will feature VASR in select products for        high school Spanish, Intermediate Spanish, and Introductory French. Portales builds on the extensive catalogue of programs developed        by Vista Higher Learning which have made world languages accessible to        more than 500,000 digital language learners this year alone. In        addition, more than three million students and 20,000 instructors have        used Vista Higher Learnings textbooks and proprietary online learning        platform, Supersite, since its founding in 2000. Vista Higher Learning was founded in 2000 as a publisher focused        exclusively on world languages. Since that time, Vista has become the        market leader for course solutions in Spanish, French, German, and        Italian. Vista Higher Learning accomplishes this by producing highly        engaging and authentic learning materials, media, and instructional        technology. To learn more about Vista Higher Learning, visit www.vistahigherlearning.com/ . Jeff Adams founded Cobalt in 2014, after a successful career building        and managing world-class speech and language software groups at Nuance,        Yap, and Amazon. Cobalt brings cutting-edge speech and language        technology to commercial enterprises, enabling these enterprises to        create the next generation of speech recognition solutions. Prior to        joining the company, Cobalts speech and language scientists and        engineers worked at Nuance (Siri), Yap, and Amazon, supporting        applications like Dragon NaturallySpeaking and Amazon Echo. Cobalts        employees have over 100 years of combined experience and more than 50        patent applications in speech recognition technology. To learn more        about Cobalt Speech and Language, visit www.cobaltspeech.com/ . ",http://www.businesswire.com/news/home/20161205005010/en/Vista-Higher-Learning%E2%80%99s-Groundbreaking-Speech-Recognition-Transforms,BOSTON--(BUSINESS WIRE)--An advanced version of Automatic Speech Recognition (ASR) used in computers and other devices is changing ...,Vista Higher Learning's Groundbreaking Speech Recognition ...,BOSTON BUSINESS WIRE Automatic ASR Vista Higher Learningthe United Stateshas Cobalt Speech Language Inc. Verification Automatic VASR Automatic Vista Higher Learning VASR VASR Portales Fifteen Vista Higher Learning Portales VASR Kurt Gerdenich Chief Technology Officer Vista Higher Learning VASR Fotonovela Tutorials Pronunciation Tutorials VASR VASR David Mackin Vice President Marketing Vista Higher Learning VASR Spanish Intermediate Spanish Introductory French Portales Vista Higher Learning Vista Higher Learnings Supersite Vista Higher Learning Vista Vista Higher Learning Vista Higher Learning Jeff Adams Cobalt Nuance Yap Amazon Cobalt Prior Cobalts Nuance Siri Yap Amazon Dragon NaturallySpeaking Amazon Echo Cobalts Cobalt Speech Language,1
198,Markets data delayed by at least 15 minutes.  THE FINANCIAL TIMES LTD 2017. FT and Financial Times are trademarks of The Financial Times Ltd. ,https://www.ft.com/content/3cc18680-1b74-11e7-a266-12672483791a,"... with a sweeping speech on the challenges of Brexit from Mark Carney, the ... Mr Carney called for a system of mutual recognition of financial ...",Bank of England tells City to prepare better for 'no-deal' Brexit,FINANCIAL TIMES LTD FT Financial Times Financial Times Ltd,-1
199,"Get today's popular DigitalTrends articles in your inbox: Your skin can say a lot about you  from your age to the last time you saw the sun. Now, an Israeli startup called VocalZoom wants to examine skin to make much more complicated analyses: comprehending what we say. When we talk, the skin on our faces makes subtle vibrations too slight to be noticed by the naked human eye. While experimenting with an instrument known as an interferometer, VocalZoom CEO Tal Bakish and his team noticed it could detect peculiar measurements. When it measures the face, we found out that the vibrations were caused only by the speakers voice and werenot affected at all by any background voice, he told Digital Trends. At this point we realized that we have a disruptive technology to extract the voice of speaker in any noisy condition. By focusingon a speakers face from within a few feet, the VocalZoom sensor can measure changesin velocity and distance up to the micrometer. These vibrations are then translated into an acoustic signal, Bakish said. This acoustic output is then fed into a typical speech enhancement or noise-reduction [program] to be fused with an acoustic microphone to create a practically noise-free signal that is fed to an automatic speech recognition [program]. According to VocalZoom, the result is a signal with limited background noise thats clearer than those recorded with conventional microphones and noise-reduction units. This may help match facial cues with speech for more secure speech verification software. Bakish also hopes the measurementwill help improve speech recognitionwhere advancements in artificial intelligencehave not yet reached. Over the past decade, solutions have relied only on data collected for training and strong processing technologies, such as neural networks and deep learning, he said. Now it is clear that to reach the 100 percent performance required for user adoption, the microphone technology needs to improve. With current partnerships with Motorola Solutions, Intel, 3M, and others, VocalZoom hopes to have its technology featured in consumer devices as early as next year. The startup also said it is in talks with major car makers to include the feature for voice control in vehicles. ",http://www.digitaltrends.com/cool-tech/vocal-zoom/,Bakish also hopes the measurement will help improve speech recognition where advancements in artificial intelligence have not yet reached.,"What does your skin say about you? Apparently, it says what you say",Get DigitalTrends VocalZoom VocalZoom CEO Tal Bakish Digital Trends VocalZoom Bakish VocalZoom Bakish Motorola Solutions Intel VocalZoom,-1
200,"I read this article and found it very interesting, thought it might be something for you. The article is called Speech Recognition Integration with Epic EHR Saves Hospital Physicians 2 Hours Per Shiftand is located athttp://hitconsultant.net/2016/07/13/34731/. The solution, DragonMedical One was deployed to physicians in more than 100 facilities as a published Citrix XenAppto streamline clinical documentation, while delivering a consistent voice experience and supporting existing Epic EHR clinical workflows regardless of where they see patients. Our organization is always looking for technology that helps our users be more productiveno matter how they work, said Don Fosen, Director of Information Technology, Edward-Elmhurst Health. Nuances clinical speech recognition solution helped us achieve these goals and enabled physicians to saveup to two hours per shiftand cutting down on documentation done after hours or at home. Using Dragon Medical One, our physicians now speak their notes, and all of the documentation goes directly into our EHR system. These notes are available in real time, helping nurses and other physicians access critical patient information much more quickly than ever before. In many cases, our nurses are completely up to speed on a case before a patient arrives, which significantly expedites treatment and improves patient care. In the past few months alone, across the organization, providers have saved hundreds of hours on documentation, including saving an additional 150 hours per month by leveraging built-in productivity tools like Auto-texts. In addition to the time savings from dictating notes, when clinicians start using and become proficient with more advanced productivity tools in Dragon Medical One, they can save a projected additional 3hours per physician per month getting a high quality note in even less time. These efficiency gains allow them to see more patients, finish and share documentation immediately, and increase the overall quality of the patient experience. I read this article and found it very interesting, thought it might be something for you. The article is called Speech Recognition Integration with Epic EHR Saves Hospital Physicians 2 Hours Per Shiftand is located athttp://hitconsultant.net/2016/07/13/34731/. ",http://hitconsultant.net/2016/07/13/34731/,"Edward-Elmhurst Health, a Chicago area health system has helped their physicians save up to 2 hours per shift integrating Nuance's speech ...",Speech Recognition Integration with Epic EHR Saves Hospital ...,Epic EHR Saves Hospital Physicians Hours Per Shiftand DragonMedical One Citrix XenAppto Epic EHR Don Fosen Director Information Technology Edward-Elmhurst Health Nuances Dragon Medical EHR Auto-texts Dragon Medical Epic EHR Saves Hospital Physicians Hours Per Shiftand,1
201,"Mayfield's home robot Kuri only speaks in her own ""robot language,"" but she does recognize and respond to voice commands. Amazon's voice-activated assistant Alexa made a splash at CES in 2016 , and at this year's show, Alexa is just about everywhere you look. While Amazon has its own motivations for distributing its platform as broadly as possible, its momentum also represents a larger trend, according to Shawn DuBravac, chief economist for the Consumer Technology Association (CTA). Speech recognition and vocal computing have reached an inflection point, he said at the Las Vegas conference, now that the word error rate (WER) has reached about 5 percent, effectively achieving human parity. In the mid '90s, the WER was effectively 100 percent. By 2013, it was around 23 percent. ""We've seen more progress in this technology in the last 30 months than we saw in the last 30 years,"" DuBravac said. ""Ultimately vocal computing is replacing the traditional graphical user interface."" The CTA estimates about 5 million voice-activated digital assistants have been sold to date, and that this figure will double in 2017. There are other factors, along with better speech recognition, that are ""ushering in a new era of faceless computing,"" DuBravac said. GUIs started to disappear with wearables and other non-traditional computing applications around 2010, he noted. That trend is expected to continue with ""robots"" like Mayfield's Kuri or Samsung's POWERbot VR7000 vacuum cleaner -- two devices officially unveiled this week at CES. Kuri may be a bit pricey now -- it will cost $700 once it launches in the US -- but ultimately, removing GUIs should help lower the price and battery requirements of such devices. DuBravac noted that GUIs themselves were initially far too expensive, with the Xerox Star hitting the market in 1981 at $75,000. It took just a few years for GUIs to become commercially viable. Thanks to voice recognition, the CTA expects home robots to grow from around 2.9 million in 2016 to 5 million by 2020. And as the technology advances, voice recognition features will become more nuanced and useful. For instance, financial services companies are already adopting voice-activated functions. Voice recognition will also drive a more sophisticated smart home market. The company Somfy, which has for decades made automatically retractable awnings, unveiled at CES its new voice controlled, all-in-one home security system, the Somfy One . ""It's clear [voice] is the new interface of the home,"" Somfy's Jean-Marc Prunet told ZDNet. ",http://www.zdnet.com/article/ces-2017-voice-is-the-next-computer-interface/,"Speech recognition and vocal computing have reached an inflection point, he said at the Las Vegas conference, now that the word error rate ...",CES 2017: Voice is the next computer interface,Mayfield Kuri Amazon Alexa CES Alexa Amazon Shawn DuBravac Consumer Technology Association CTA Las Vegas WER WER DuBravac CTA DuBravac GUIs Mayfield Kuri Samsung POWERbot VR7000 CES Kuri US GUIs DuBravac GUIs Xerox Star GUIs CTA Voice Somfy CES Somfy One Somfy Prunet ZDNet,3
202,"Microsoft researchers from the Speech & Dialog Research Group (from left): Wayne Xiong, Xuedong Huang, Geoffrey Zweig, Dong Yu, Frank Seide, Mike Seltzer, Jasha Droppo and Andreas Stolcke. Photo by Dan DeLong. Microsoft is making waves with its artificial intelligence research by developing a piece of technology that recognizes words as well as humans do. According to a team of researchers from the companys AI and Research division, the companys speech recognition system has a word error rate of 5.9%, which is about equivalent to a human. Weve reached human parity, said Xuedong Huang, the companys chief speech scientist, in a statement . This is an historic achievement. The company will use this milestone to help improve their consumer and business products, and bring more features to its intelligent assistant Cortana. Node.js v6 is moving into Long Term Support with the release of version 6.9.0. According to the Node.js team, Long Term Support ensures enterprises and users can use a version of Node.js that is stable without having to continuously upgrade. Node.js v6, codenamed Boron, will remain as active in LTS until April 2018. Afterwards it will move to maintenance until end of life (April 2019). The new release features performance, reliability and security improvements, as well as the V8 JavaScript engine 5.1 and a majority of ECMAScript 6 feature support. Google is releasing its cloud-based registry platform into open source. Nomulus is a service that powers the companys top level domains (TLDs). TLDs are the top level of the Internet Domain Name System (DNS), and they collectively host every domain name on the Internet, wrote Ben McIlwain, software engineer at Google, in a blog post . To manage a TLD, you need a domain name registrya behind-the-scenes system that stores registration details and DNS information for all domain names under that TLD. Nomulus is designed to use Google Cloud Platform, manage any amount of TLDs, and support TLD functionality. We hope that by providing access to our implementation of core registry functions and up-and-coming services like Registration Data Access Protocol (RDAP), we can demonstrate advanced features of Google Cloud Platform and encourage interoperability and open standards in the domain name industry for registry operators, McIlwain wrote. Rust made its first-source pre-alpha release of the new Rust Language Server (RLS) project. The goal of RLS is to become a self-contained, full-featured application for IDE support and future Rust development. Rust is looking for developers, testers and people with expertise in various editors to flesh out support, wrote Jonathan Turner, a Mozilla engineer. The early release may eat a developers hard drive, so the Rust team recommended only using RLS with code you are making regular backups of, he wrote. The RLS uses the Language Server Protocol, a JSON-RPC protocol backed by Microsoft and Red Hat. Eclipse and Visual Studio Code also support this protocol, and it should be possible to add support to other editors. Google Cloud Platform users have been searching for a way to edit code and configuration files without leaving the browser, which is why the Google Cloud team created a new feature: an integrated code editor. Developers can go to the Google Cloud Console and click on the Cloud Shell icon on the top right section of the toolbar to get started with the editor. The new editor is based on Eclipse Orion, and its also a part of the Google Cloud Shell. The editor comes with the Cloud SDK and other essential tools. Other abilities of the new editor include the ability to write a sample app, push apps into a cloud source repository, and debut apps with Stackdriver Debugger. ",http://sdtimes.com/microsofts-milestone-speech-recognition-node-js-v6-9-0-released-google-open-sources-nomulus-sd-times-news-digest-oct-19-2016/,"Microsoft researchers from the Speech & Dialog Research Group (from left): Wayne Xiong, Xuedong Huang, Geoffrey Zweig, Dong Yu, Frank ...","Microsoft's milestone in speech recognition, Node.js v6.9.0 released ...",Microsoft Speech Dialog Research Group Wayne Xiong Xuedong Huang Geoffrey Zweig Dong Yu Frank Seide Mike Seltzer Jasha Droppo Andreas Stolcke Dan DeLong Microsoft AI Research Weve Xuedong Huang Cortana Node.js Term Support Node.js Term Support Node.js Node.js Boron LTS April April V8 JavaScript ECMAScript Google TLDs Internet Domain Name System DNS Internet Ben McIlwain Google TLD DNS TLD Google Cloud Platform TLDs TLD Registration Data Access Protocol RDAP Google Cloud Platform McIlwain Rust Rust Language Server RLS RLS IDE Rust Rust Jonathan Turner Mozilla Rust RLS RLS Language Server Protocol JSON-RPC Microsoft Red Hat Eclipse Visual Studio Code Google Cloud Platform Google Cloud Google Cloud Console Cloud Shell Eclipse Orion Google Cloud Shell Cloud SDK Stackdriver Debugger,4
203,"Linkplay Technology Releases WiFi Audio Speaker Solution With Amazon Alexa, Speech Recognition And Handsfree Control PALO ALTO, Calif., Dec.  30, 2016  (GLOBE NEWSWIRE) -- Linkplay Technology , a leading WiFi audio solution provider, is proud to be one of the first audio platformsapproved for integration with Alexa Voice Service for hands-free voice recognition capabilities with its WiFi audio platform. The smart audio solution provides AVS integration, global streaming music content integration, app and cloud development, and multi-room speaker capabilities. The initial product incorporating the hands-free technology is Linkplay customer, Omaker with the WoW portable speaker. It is one of five Powered by Linkplay AVS certified speakers being demonstrated at CES Unveiled and in suite 30-121 in the Venetian Hotel, during CES January 5-8, 2017 in Las Vegas, NV. WoW will be available exclusively through Amazon.com starting in mid-to-late January. We are thrilled to be one of the first companies approved for integration with AVS for both push to talk and hands-free voice recognition speaker technology, enabling our customers to launch products more quickly, at a fraction of the cost, said Jade Wu, chief business officer, Linkplay. Alexa Voice Service is creating a new category within the speaker industry and we are capitalizing on that innovation, bringing this solution to numerous manufacturers and ODMs. To bring this solution to life, Linkplay utilized Conexants 2 microphone solution with Sensorys Alexa wake word technology. The integration with AVS makes it simple, cost effective, and fast for speaker manufacturers and ODMS to bring devices with hands-free capabilities to their customers. Optimizing far field technology, the Alexa wake word can be activated from several meters away, anywhere in the room - even if the music is playing or people are talking. Linkplay makes it possible for speakers to be wireless and hands-free with consumers simply asking Alexa, through multiple WiFi speakers, to play their favorite music, turn on the lights in the living room, provide news, traffic and weather reports, set alarms, and even order food. As it relates to music specifically, Linkplays hardware and software speaker solution provides access to millions of songs through services such as Amazon Music, Audible, TuneIn and iHeartRadio, Spotify, TIDAL, Napster, and many others and operates with AirPlay, DLNA, Spotify Connect, and QPlay protocols, making it seamless for companies to bring music to their devices. Specializing in interruption-free music streaming from any iOS or Android device to speakers over a WiFi network, Linkplays solution eliminates the need to connect a phone to the speakers, allowing users to stream music without interference from calls or texts. In addition, Linkplays features include a holistic audio package with customized hardware, firmware, mobile app and cloud server for OTA and Amazon Alexa (with wake-up mode). Also, the platform offers multi-channel wireless stereo pairing, as well as left and right channel playback on two or more WiFi speakers simultaneously. The multi-source functionality supports audio streaming from the cloud, NAS, local music (from phone or tablet), USB disk/TF card or third party app and offers support for AUX-in, SD/USB, and Bluetooth re-transmission to multi-room via WiFi. Media may drop by CES Unveiled , table 129 on January 3rd at the Mandalay Bay Hotel or schedule a meeting in our suite at the Venetian Hotel to receive a demonstration. Additionally, contact Linda Ferguson, PR Director, Lindaf@Linkplay.com to schedule an interview. For more about the Alexa Voice Service, visit the Amazon developer portal . To learn more about how Linkplay can help you develop a product with Amazon Alexa, send a request to Alexa@linkplay.com . Linkplay Technology, headquartered in Silicon Valley, CA, was founded by a global team of hardware and software engineers, business executives, and wireless audio experts. Linkplays patent-pending technology is a turn-key WiFi audio platform with global streaming music content integration, app and cloud development for speaker brands and ODMs.Technology partners include original design manufacturers (ODMs), component suppliers and key technology investors. For more information about Linkplay, visit www.linkplay.com or email info@linkplay.com . Media Contact: Linda Ferguson PR Director, Linkplay Lindaf@linkplay.com 503-869-5827 www.linkplay.com Palo Alto, CA ",https://globenewswire.com/news-release/2016/12/30/902329/0/en/Linkplay-Technology-Releases-WiFi-Audio-Speaker-Solution-With-Amazon-Alexa-Speech-Recognition-And-Handsfree-Control.html,"... With Amazon Alexa, Speech Recognition And Handsfree Control ... for hands-free voice recognition capabilities with its WiFi audio platform.",Linkplay Technology Releases WiFi Audio Speaker Solution With ...,Linkplay Technology Releases WiFi Audio Speaker Solution Amazon Alexa Handsfree Control PALO ALTO Calif. Dec. GLOBE NEWSWIRE Linkplay Technology WiFi Alexa Voice Service WiFi AVS Linkplay Omaker WoW Linkplay AVS CES Unveiled Hotel CES January Las Vegas NV WoW Amazon.com January AVS Jade Wu Linkplay Alexa Voice Service ODMs Linkplay Sensorys Alexa AVS ODMS Alexa Linkplay Alexa WiFi Linkplays Amazon Music Audible TuneIn Spotify TIDAL Napster AirPlay DLNA Spotify Connect QPlay Android WiFi Linkplays Linkplays OTA Amazon Alexa NAS USB AUX-in SD/USB Bluetooth WiFi Media CES Unveiled January Mandalay Bay Hotel Hotel Linda Ferguson PR Director Lindaf @ Linkplay.com Alexa Voice Service Amazon Linkplay Amazon Alexa Alexa @ Linkplay Technology Silicon Valley CA WiFi ODMs.Technology ODMs Linkplay @ Media Linda Ferguson PR Director Linkplay Lindaf @ Palo Alto CA,3
204,"Microsoft will herald a global change in the realm of speech recognition. Its newest software appears to transcribe conversational speech as well as humans. Microsoft will herald a global change in the realm of speech recognition. Its newest software appears to transcribe conversational speech as good as humans. This is a historic achievement for the company, as a study just revealed that it can best professional human transcriptionists in conversational speech. It has a word error rate of 5.9 percent, lower than 6.3 percent just last month. Microsoft said this is the lowest ever recorded in the industry-standard Switchboard task, and it's even lower than the errors of human professional transcriptionists. Accordingto Futurism , the achievement came decades after speech pattern recognition was first studied in the 1970s. Given that Google's DeepMind is not far behind in the same field, Microsoft's achievement will herald more developments in the field of artificial intelligence research. This also means the new technology can be used for Microsoft's personal voice assistant Cortana for Windows and Xbox One. Aside from better speech-to-text transcription, it can help make Cortana more powerful and a ""true"" intelligent assistant. It can be remembered that other voice assistants exist, starting with the more popular Siri from Apple. Recent achievements from DeepMind include a ""voice synthesis program"" that can generate its own human-like speech. Still, Microsoft clarified that this didn't mean perfection. Their technology still cannot recognize words clearly, which something even humans cannot do efficiently. Regardless, their next goal is to make computers ""understand"" human conversation. Ergo, a transfer to the realm of understanding from recognition. However, this possibility appears to be very likely considering how the realm of artificial intelligence is already making leaps and bounds in various fields. For instance, it's reported that Google's DeepMind already has about 1,000 ongoing AI projects.  2017 NatureWorldNews.com All rights reserved. Do not reproduce without permission. ",http://www.natureworldnews.com/articles/32595/20161123/microsoft-officially-makes-first-humanly-accurate-speech-recognition-tech.htm,Microsoft will herald a global change in the realm of speech recognition. Its newest software appears to transcribe conversational speech as ...,Microsoft Officially Makes First Humanly Accurate Speech ...,Microsoft Microsoft Microsoft Switchboard Accordingto Futurism Google DeepMind Microsoft Microsoft Cortana Windows Xbox One Aside Cortana Siri Apple DeepMind Microsoft Their Regardless Ergo Google DeepMind AI NatureWorldNews.com All,8
205,"Can speech recognition software help prevent RSI Seans wife spends up to eight hours a day typing reports and he wonders which software works best. But speech recognition is already built into Windows There are several causes for RSI, including badly designed devices and bad working practices. Photograph: Zigy Kaluzny/Getty Images Can speech recognition software help prevent RSI Seans wife spends up to eight hours a day typing reports and he wonders which software works best. But speech recognition is already built into Windows My wife can spend up to eight hours a day typing reports in Microsoft Word 2010 and due to RSI issues, is thinking about investing in some speech recognition software. There seems to be a huge diversity in the price of these, and we need to know how much we should spend and how reliable they are. She would also be interested in a portable version, which she could use on the go. Ive been on a few websites and Ive identified the Dragon Naturally Speaking 13 Premium Wireless as a possible solution. Were running Windows 7 with the option to upgrade to Windows 10. Sean There are at least four issues here: technology, usability, ergonomics and economics. The technology aspect is pretty simple: if you want to buy a speech recognition program, Dragon Naturally Speaking has been the most accurate system since it first appeared in 1997. However, it doesnt matter how accurate it is if your wife ends up not using it. Fortunately, you already own a good speech recognition program. Its built into Windows 7 and other current versions of Windows. Its part of the accessibility suite through which Microsoft caters for people with various disabilities. Other components include a text-to-speech program or screen reader and a magnifier. Your wife should use the built-in program for a week or three to see if she can incorporate it into her workflow. If it does the job, that solves the problem. If it works well but she would benefit from more accurate transcription, then it would be worth investing in one of the versions of Dragon Naturally Speaking, which cover a wide price range. But if it turns out that she cant benefit from Windows Speech Recognition, buying a better program would be a waste of money. To run the Windows Speech Recognition program, type the word speech into Windows 7s search/run box: it should be the top result. It wont run if it cant find an audio input device, but almost every laptop and tablet has a built-in microphone. If it does run, a wizard will take you through the setup process, which involves reading a short text aloud to check the microphone. Once set up, Windows Speech Recognition runs in the background, but you can control it with either voice commands or mouse clicks. Its designed to be an assistive technology, so its easy to control via spoken commands such as start listening, open speech dictionary, comma, period, correct, delete, save and so on. SchoolFreeware has a series of YouTube tutorials, and the fifth one provides a good guide to dictating text into Word . However, Windows Speech Recognition will work with any program, including WordPad, the free word processor included with Windows. Speech recognition has improved dramatically thanks to the use of statistical models. Programs that struggle to recognise isolated words and letters can use a vast repository of stored language to get whole sentences correct. This is very evident with the simple stuff that Microsoft Cortana, Google Now and Apples Siri almost invariably get right nowadays. Longform dictation is somewhat trickier. Like its rivals, Windows Speech Recognition gets better the more you use it, because it learns your vocabulary. SchoolFreeware reckons you can get to around 140 words per minute, which is faster than most people can type. However, as with touch-typing, speeds slump dramatically if you have to make corrections. People who dont speak slowly and clearly, and who dont put in the time to train the software, usually get discouraged and give up. The people who use speech recognition successfully are usually highly motivated, often because they cant use their hands at all. This may be because of their job situation (eg dentists and pathologists) or because of RSI. In my experience, Windows Speech Recognition works very well when you read out proper texts. Its not as good if youre making stuff up as you go along, and frequently need to rewrite sentences, or if you use a lot of jargon. More advanced programs such as Dragon Naturally Speaking 13 can save and reload user profiles, and can import vocabularies for specialised fields such as medicine and the law. Dragon Naturally Speaking can also convert dictation files. That means you can dictate to a handheld digital recorder or other source and then load the audio file for processing. In fact, the Premium Mobile version includes a small voice recorder for people who want to dictate texts while on the move. Of course, you can use other recorders but you need to create a profile for each one. There are cheaper versions of Dragon Naturally Speaking: for example, you can get excellent results with Version 12 Home for 34.55. But for business users, the price is irrelevant. The 129.99 Premium 13 Wireless version, which includes a Bluetooth headset, can pay for itself in a few days. As usual, there are alternative programs but frankly, you may as well use the free one in Windows or pay for Dragon. One that might be worth a look is voice typing for Google Docs : you can dictate into an online document using a microphone and the Chrome browser, then download the document and load it into Microsoft Word. While on the go, you can dictate texts using the Google Docs app on a smartphone, but when I tried it on my Android phone, it was a nightmare. It got words wrong and it didnt recognise commands such as space, full stop, period, or delete, so my text included sequences like full stop delete full stop period correct period. Also, the mic kept turning itself off. RSI (repetitive strain injuries), RMD (repetitive motion disorders) and carpal tunnel syndrome dont come from nowhere. They are caused by badly designed devices  such as laptop computers  and bad working practices, such as not taking regular screen breaks. Ive written about this several times because Ive needed two sets of expert physiotherapy to survive, and other people have needed surgery. The problem is that everyone thinks theyll be fine until they arent, at which point its too late. Its important to use an ergonomic split keyboard for sustained work (at this instant, my B key is 7cm from my N key). To avoid the vulture posture, its important to have the screen raised to the correct height, which means using an external keyboard with a laptop. Its useful to remember keyboard commands  they can do almost everything in Windows  to avoid using a mouse. (Im currently trying a Penclic , which is still a mouse but pen-shaped, but a digitising pen would be another option.) Its important to take regular screen breaks: Im now using a Microsoft Band 2 to buzz me reminders. For more details, see my earlier answer: How can I use laptops and tablets without suffering from physical pains RSI can be a temporary inconvenience, but depending on which type it is , it can also be a life-changing and debilitating problem. Working through pain is the wrong answer. Speech recognition, improved ergonomics, screen breaks, gentle exercises ( Tai chi , yoga ) and physiotherapy can help, but if shes in pain, your wife should see her doctor. Have you got another question for Jack Email it to Ask.Jack@theguardian.com ",https://www.theguardian.com/technology/askjack/2016/apr/21/can-speech-recognition-software-help-prevent-rsi,Sean's wife spends up to eight hours a day typing reports and he wonders which software works best. But speech recognition is already built ...,Can speech recognition software help prevent RSI?,RSI Seans Windows RSI Zigy Kaluzny/Getty Images Can RSI Seans Windows My Microsoft Word RSI Ive Ive Dragon Naturally Speaking Premium Wireless Windows Dragon Naturally Speaking Windows Windows Microsoft Dragon Naturally Speaking Windows Windows Windows Windows SchoolFreeware YouTube Word Windows WordPad Windows Microsoft Cortana Google Apples Siri Longform Windows SchoolFreeware RSI Windows Dragon Naturally Dragon Premium Mobile Dragon Naturally Speaking Version Home Premium Wireless Bluetooth Windows Dragon Google Docs Chrome Microsoft Word Google Docs Android RSI RMD Ive Ive B N Windows Im Penclic Im Microsoft Band RSI Tai Jack Email Ask.Jack @,-1
206,"The success of Amazons Echo proves that we are slowly coming to terms with talking to machines. How long before digital assistants can do more than just control our music, and start having meaningful conversations Are you receiving me ... were not yet able to build a speech recognition system that understands the world. Photograph: Alamy The success of Amazons Echo proves that we are slowly coming to terms with talking to machines. How long before digital assistants can do more than just control our music, and start having meaningful conversations The problem with using the human voice to control computers is well known and well documented: it doesnt always work. You can find yourself adopting the aggressive tone of a belligerent tourist in a foreign land while digital assistants employ a range of apologetic responses (Im sorry, I didnt quite get that, Im sorry, I didnt understand the question). We throw our arms up and complain about their shortcomings. Plenty of us have tried them, plenty of us have dismissed them as a waste of time. We tend not to hear about them doing the job perfectly well, because few people write impassioned tweets or blog posts about things that work flawlessly. The evidence, however, shows that we are becoming more comfortable with using voice control as its capabilities improve. Back in May, Google announced that 20% of mobile search queries were now initiated by voice ; and it is predicted that this will rise  across all platforms  to 50% by the end of the decade. But its not phones leading the way in making voice control palatable. That honour goes to the Amazon Echo , the home-based ambient device inhabited by a digital assistant called Alexa. Quietly launched in late 2014, the success of the Echo (and its smaller siblings, the Dot and the Tap) has been described as unlikely, but sales have been increasing steadily quarter by quarter, with an estimated five million units sold in the US alone. Compared to the average smartphone, the Echo is comparatively modest: it is mainly dedicated to playing music and only does anything if you call its name. Alexa, play me a song by Hot Chip. Alexa, can I listen to Radio 2 Alexa, stop. It does these things efficiently, without complaint, and in doing so engenders a strange kind of affection. Alexa, goodnight. Goodnight, it replies. Sleep tight. Cookery tips from Amazon Echo ... it only works if you call its name. Photograph: Rachel Murray/WireImage The first successes in getting computers to recognise the spoken word were made in the 1950s, but more significant ground was gained in the early 1970s when two students at Carnegie Mellon University, James and Janet Baker , began to apply statistical modelling techniques to the recognition of speech patterns. These so-called  hidden Markov models turned out to be perfectly suited to machine learning; by absorbing thousands of examples, the machine became capable (in theory) of handling examples that it hadnt yet seen. The Bakers would go on to found Dragon Systems , which ultimately became Nuance, one of the architects of Apples Siri voice assistant. In the early days, Dragons software was used to power specialist accessibility and dictation applications, but when such techniques started being used in computer operating systems  most notably with Apples PlainTalk in 1993  our difficult relationship with voice control began. The first version of PlainTalk hogged computer resources, understood a limited number of phrases and wasnt particularly reliable. Ever since, weve subconsciously measured the effectiveness of each iteration of voice control in terms of the time we take to give up on it. Increases in processor power made things better. We were improving speech recognition year on year, says Nils Lenke, senior director of corporate research at Nuance, but it became tougher and tougher, because the old technology  the hidden Markov models  were nearing the end of their lifetime. When we started to use neural networks, accuracy went up a lot. The problem with machine learning techniques, according to Lenke, is that the model reflects what you show it. You need a lot of data covering all kinds of variants of speech, he says, accents, dialects, ages, gender, and different settings, different environments. But when cloud-based speech recognition came along, things got a lot better; now, as people use it, we can see that data on our servers. The right data, covering exactly what people are doing with the technology. Not what we thought people might be doing. Rich human-computer relationship ... Theodore consults Samantha in Spike Jonzes Her. Photograph: Allstar Collection/Warner Bros As speech recognition improves from 90% to 95% and beyond, the problem encountered by developers of voice assistants is not necessarily one of comprehension; its persuading us that its not just a novelty and that we should persist beyond uncovering its cute quirks. Speech has to solve a problem, says Lenke. But which problems can it solve Can I remember which ones it can solve and which it cant It can book a cinema ticket, but can it book a flight Were not yet able to build a speech recognition system that understands the world, and for human beings thats difficult to understand. The car is perhaps the best example of a single domain with well-defined problems (finding petrol stations, demisting windscreens) that can now be dealt with by voice commands. But this kind of understated ambition has, perhaps unwittingly, been the Echos strength, too. In terms of voice technology, its not revolutionary, says Simon Bryant, associate director at Futuresource Consulting, but people arent overwhelmed by it. They get it. The entrypoint is via controlling your media, but once you get comfortable with playing a track, or a radio station, and youre aware that its constantly learning, other applications will piggyback on to that. The potential is huge. A customer tries out an iPhones Siri at a Hong Kong Apple shop. Photograph: Jerome Favre/Bloomberg via Getty Images The Echos expandability comes in the form of skills, links with third-party services that range from time-wasting ephemera (trivia quizzes) to things that could be genuinely useful if you happened to have the right technology installed in your home (ie controlling room temperature). But the growing affection for the device is also linked to its immobility. Much of our interaction with a smartphone is conducted in public, where talking to it is simply too embarrassing. In the privacy of the home, however, we can explore its capabilities and slowly come to terms with the reality of talking to a machine. With Apples Siri, personality is crucial; behind its development seems to be a belief that the psychological rule of reciprocation (where we mirror the behaviour others dish out) also applies to machines. Google Now, meanwhile, is cooler, more utilitarian. Some people would say that building a persona makes people more comfortable and lowers the barrier to entry, says Lenke. Others would say look, its a machine, we should make that visible. Both approaches can be right, depending on the task. Ultimately, our enthusiasm for voice control may be defined by issues of trust. Theres trust in the device itself  that it will perform in the way we want it to  and theres trust in the company providing the service. Privacy concerns are never far from debates surrounding voice control; to function properly it requires data to be processed on external servers, but warnings of this within terms and conditions (if your spoken words include personal or other sensitive information, that information will be among the data captured and transmitted to a third party) can end up being interpreted as an Orwellian nightmare. The stuff of sci-fi ... Gary Lockwoods astronaut attempts to communicate with HAL in 2001: A Space Odyssey. Photograph: MGM/Everett/Rex Features These issues can be particularly sensitive when it comes to Amazon, whose ultimate aim with the Echo could reasonably be interpreted as frictionless purchasing  ie getting us to buy things as quickly and easily as possible. Detractors warn of an inglorious future where we casually mention that were out of washing up liquid, and an hour later a drone drops some off at our door having already billed our credit card. Amazons strategy is far reaching, says Simon Bryant, but a big aspect of it is Amazon Prime, which drives consumption of products and keeps people paying on an annual basis. Behind the cute replies and brisk efficiency of voice assistants is the aim of drawing us into an ecosystem; for example, the Echos ability to cue up sounds in response to a murmur is beautifully executed, but if your stash of personal music happens to be with Google Play, youll have to move everything over to Amazons Music Unlimited if you want to listen to it. The promise of voice control will continue to be restricted by these walled gardens. Advances in speech recognition could be seen as the fulfilling of a science fiction dream that extends from Star Trek through 2001: A Space Odyssey to Knight Rider and beyond. Its history has been characterised by disappointment, but its key attributes are clear: it is hands-free and fast, devices dont have to be unlocked and there are no menu structures to navigate. As more TVs and set-top boxes become speech savvy, the remote control will be consigned to history. As devices get smaller and lose their keyboards and screens, voice control will become crucial. And according to Bryant, the knock-on effects are already being seen. Were expecting 6.1m units of Echo-like devices to be sold by the end of this year, he says, which takes a huge chunk out of the audio market. And its going to boost radio audiences, because people are going into rooms and just want something to be playing. Alexas ability to instantly switch on Heart FM falls well short of the kind of rich human-computer relationship thats depicted in the Spike Jonze film Her , but while new apps like Hound are becoming more adept at having longer conversations and understanding context, there are limits to a computers ability to deal with conversational interaction, according to Mark Bishop, professor of cognitive computing at Goldsmiths University of London. Action-focused commands like tell me the weather in Seattle are much simpler things for a machine to parse and interact with than an open-ended narrative, he says. But there are fundamental problems in AI that, for me, mean that were some years away from having a machine that can have a meaningful, goal-directed conversation, if its ever possible at all. We may not see a convincingly empathetic machine in our lifetime, but in the meantime we can always ask Alexa to say something nice. You have a great taste in technology, it replies. But seriously, you rock. Im glad to know you. Same here, Alexa. I think. 1952 Scientists at Bell Laboratories build Audrey , the first documented speech recognition system, which can discern between the numbers zero to nine 1962 The Shoebox , built at IBM, adds the words plus, minus, total, subtotal, false and off to the 10 digits and can perform simple arithmetic 1990 Dragon launches a dictation software package for PC that recognises 30,000 words, but discretely (ie with pauses between words) for $4,995 1996 IBMs MedSpeak combines software with a noise-cancelling microphone to achieve continuous speech recognition; 25,000 words with an average accuracy of over 95% 2007 GOOG-411 , a telephone-based directory service, is launched by Google; the following year sees the technology deployed in a Voice Search app for iPhone 2011 Siri , previously available as a standalone app, is integrated into the operating system for the new iPhone 4S, launching the era of the digital assistant ",https://www.theguardian.com/technology/2016/dec/04/voice-control-amazon-echo-digital,"As speech recognition improves from 90% to 95% and beyond, the problem encountered by developers of voice assistants is not necessarily ...",Has voice control finally started speaking our language?,Amazons Echo How Alamy Amazons Echo How Im Im Back May Google Amazon Echo Alexa Echo Dot Tap US Echo Alexa Hot Chip Alexa Radio Alexa Alexa Goodnight Cookery Amazon Echo Rachel Murray/WireImage Carnegie Mellon University James Janet Baker Markov Bakers Dragon Systems Nuance Apples Siri Dragons Apples PlainTalk PlainTalk Nils Lenke Nuance Markov Lenke Theodore Samantha Spike Jonzes Her Allstar Collection/Warner Bros Lenke Can Were Echos Simon Bryant Futuresource Consulting Siri Hong Kong Apple Jerome Favre/Bloomberg Getty Images Echos Apples Siri Google Lenke Privacy Gary Lockwoods HAL Space Odyssey Amazon Echo Amazons Simon Bryant Amazon Prime Echos Google Play Amazons Music Unlimited Star Trek Space Odyssey Knight Rider Bryant Were Alexas Heart FM Spike Jonze Her Hound Mark Bishop Goldsmiths University London Seattle AI Alexa Im Alexa Bell Laboratories Audrey Shoebox IBM Dragon IBMs MedSpeak GOOG-411 Google Voice Search Siri,4
207,"On April 20, at 7 p.m., the public is invited to an open forum at Kenyon City Hall addressing the mental health and drug concerns in Kenyon. This meeting will be an open discussion where information will be given and received on drug abuse and addiction. The objectives of the forum are to inform advocates on how to report issues and seek help and how to recognize an abuse issue. Information on the physiology of drug use on the brain and body will be presented. The goals of the meeting are to educate and inform community members on abuse, addiction and recognition of these health concerns. Drug use is prevalent in our area as was indicated by the recent drug related death in Faribault. Opioids, meth, cocaine, alcohol and marijuana are known to be used in our society. The Kenyon Police Department has been trained on how to administer Narcan, an overdose antidote that can save lives. As Kenyon Police Chief Lee Sjolander said, We have all in some way been affected by drugs and drug abuse. The main purpose of the forum is not to judge people who are battling these life challenges, but to reach out to offer help and resources for them and the people who are concerned about them. On Saturday, Speech Team members Evelyn Humphrey, Melanie Knealing and Haha Shepard competed in the Sectional Speech Meet. Melanie completed a great senior season with a fifth-place finish. Evelyns season will continue, as she took first place in the Serious Prose category and will compete in the State Tournament. On April 3, the Region VIII FFA Annual Awards Ceremony was held at Pine Island High School. K-W FFA members receiving Region Awards were Madeline Patterson, Star Farmer and a Proficiency Award for Swine Production Entrepreneurship; Matthew Bauer, Proficiency Award in Forage Production; and Owen Scheffler place second in the individual Dairy Cattle Evaluation. The Dairy Cattle Evaluation team placed second and the Dairy Foods team finished third. Taylor Helland, who sang a song of her own composition, won the talent show and will compete at the State Convention Talent Show later in April. Michael Pliscott gave his first-place winning version of the FFA Creed and will also compete at the State Convention in FFA Creed Speaking. Chuck Larson, K-W FFA adviser, and Duane Pliscott, who lives in Kenyon and is the Cannon Falls FFA adviser, were made Honorary FFA Members. In her final address as Region VIII President, Emily Pliscott, of K-W, talked about using your super powers to make the world a better place. To support her talk, Emily used anecdotes about using the super powers of kindness, dedication and reaching out to know people. The Celebration of Student Scholarship, COSS, a daylong event showcasing research, scholarship and creative activity was held at Concordia College in Moorhead. K-W graduate and Concordia sophomore Rosie Breimhurst had a poster presentation on Feminism in Saudi Arabia in the COSS event. In her research, Breimhurst found that many conservative Saudi Arabian women do not support the loosening of gender roles and restrictions. The Western Media often present Saudi Arabia as an oppressor of women. Many Saudi Arabian people, including women, believe that the gender segregation they experience is just and equal under the Quran. This past weekend Rosie, a member of Concordias Chapel choir, was part of the world premiere performance of The Passion of Jesus Christ composed and conducted by Dr. Rene Clausen. Once again there will be a ski jumping hill in Goodhue County after the County Board approved construction of a jump at Mount Frontenac. For the past several winters Gary and Mary Skundberg have had a pair jumping skis displayed in front of their house. The jumping skis were used by Gary when he was growing up in Wisconsin. He was a ski jumper until he had to decide between ski jumping and basketball and chose basketball. Maybe this summer Gary will be getting his skis back into jumping condition or maybe not. ",http://www.southernminn.com/the_kenyon_leader/opinion/article_3872e979-f66d-54ef-8c70-cf78cf168d9c.html,"... inform community members on abuse, addiction and recognition of these ... On Saturday, Speech Team members Evelyn Humphrey, Melanie ...","Forum, COSS, ski jumping and more",April Kenyon City Hall Kenyon Drug Faribault Opioids Kenyon Police Department Narcan Kenyon Police Chief Lee Sjolander Saturday Speech Team Evelyn Humphrey Melanie Knealing Haha Shepard Sectional Speech Meet Melanie Evelyns Serious Prose State Tournament April Region VIII FFA Annual Awards Ceremony Pine Island High School K-W FFA Region Awards Madeline Patterson Star Farmer Proficiency Award Swine Production Entrepreneurship Matthew Bauer Proficiency Award Forage Production Owen Scheffler Dairy Cattle Evaluation Dairy Cattle Evaluation Dairy Foods Taylor Helland State Convention Talent Show April Michael Pliscott FFA Creed State Convention FFA Creed Speaking Chuck Larson K-W FFA Duane Pliscott Kenyon Cannon Falls FFA Honorary FFA Members Region VIII President Emily Pliscott K-W Student Scholarship COSS Concordia College Moorhead K-W Concordia Rosie Breimhurst Feminism Saudi Arabia COSS Breimhurst Saudi Arabian Media Saudi Arabia Saudi Arabian Quran Rosie Concordias Chapel Passion Jesus Christ Dr. Rene Clausen Goodhue County County Board Mount Frontenac Gary Mary Skundberg Gary Wisconsin Gary,2
208,"Google can recognize mens voices better. Voice recognition technology promises to make our lives easier, letting us control everything from our phones to cars to home appliances. Just talk to our tech, and it works. As the tech becomes more advanced, theres another issue thats not as obvious as a failure to process simple requests: Voice recognition technology doesnt recognize womens voices as well as mens. According to research from Rachael Tatman, linguist researcher and National Science Foundation Graduate Research Fellow at the University of Washington, Googles speech recognition software has gender bias.  Rachael Tatman (@rctatman) July 12, 2016 She realized significant differences in the way Googles speech recognition software auto-captions video on YouTube. It was much more consistent on male voices than female. She said the results were deeply disturbing. Its not that theres a consistent but small effect size, either, 13% is a pretty big effect. The Cohens d was 0.7 which means, in non-math-speak, that if you pick a random man and random woman from my sample, theres an almost 70% chance the transcriptions will be more accurate for the man. Thats pretty striking. Language varies in systematic ways depending on how youre talking, Tatman said in an interview. Differences could be based on gender, dialect, and other geographic and physical attributes that factor into how our voices sound. To train speech recognition software, developers use large datasets, either recorded on their own, or provided by other linguistic researchers. And sometimes, these datasets dont include diverse speakers. Generally, the people who are doing the training arent the people whose voices are in the dataset, Tatman said. Youll take a dataset thats out there that has a lot of different peoples voices, and it will work well for a large variety of people. I think the people who dont have socio-linguistic knowledge havent thought that the demographic of people speaking would have an effect. I dont think its maliciousness, I just think it wasnt considered. A representative for Google pointed us to a paper published last year that describes how Google built a speech recognizer specifically for children. In the paper, the company notes that speech recognition performed better for females. Additionally, Google said it trains voice recognition across gender, ages, and accents. Its hard to directly address the research in the article you shared, since the results reflect a relatively small sample size, without information on how the data was sampled or the number of speakers represented. That said, we recognize that its extremely important to have a diverse training set; our speech technology is trained on speakers across genders, ages, and accents. In a research paper we published last year ( Liao et al. 2015 ), we found that our speech recognizer performed better for females, with 10% lower Word Error Rate. (In that paper we actually measured 20% higher Word Error Rate for kidswhich weve been working hard to improve since then.)  Speech recognition has struggled to recognize female voices for years. Tatman cites a number of studies in her post, including voice tech working better for men in the medical field , and that it even performs better for young boys than girls. When the auto industry began implementing voice commands in more vehicles, women drivers struggled to tell their cars what to do, while their male counterparts had fewer problems getting vehicles to operate properly. Its a similar problem to biases in algorithms that control what we see on the webwomen get served ads for lower-paying jobs, criminal records turn up higher in searches for names commonly associated with someone of African-American descent, and the software some law enforcement agencies use is biased against people of color. And this isnt the first time Googles automated systems have failed; last year, the company came under fire when its image recognition technology labeled an African-American woman a gorilla. Tatman said the best first step to address issues in voice tech bias would be to build training sets that are stratified. Equal numbers of genders, different races, socioeconomic statuses, and dialects should be included, she said. Automated technology is developed by humans, so our human biases can seep into the software and tools we are creating to supposedly to make lives easier. But when systems fail to account for human bias, the results can be unfair and potentially harmful to groups underrepresented in the field in which these systems are built. ",https://www.dailydot.com/debug/google-voice-recognition-gender-bias/,She realized significant differences in the way Google's speech recognition software auto-captions video on YouTube. It was much more ...,Research shows gender bias in Google's voice recognition,Google Voice Voice Rachael Tatman National Science Foundation Graduate Research Fellow University Washington Googles Rachael Tatman @ July Googles YouTube Cohens Thats Tatman Tatman Youll Google Google Google Liao Word Error Rate Word Error Rate kidswhich Tatman Googles Tatman Automated,4
209,"On April 20, at 7 p.m., the public is invited to an open forum at Kenyon City Hall addressing the mental health and drug concerns in Kenyon. This meeting will be an open discussion where information will be given and received on drug abuse and addiction. The objectives of the forum are to inform advocates on how to report issues and seek help and how to recognize an abuse issue. Information on the physiology of drug use on the brain and body will be presented. The goals of the meeting are to educate and inform community members on abuse, addiction and recognition of these health concerns. Drug use is prevalent in our area as was indicated by the recent drug related death in Faribault. Opioids, meth, cocaine, alcohol and marijuana are known to be used in our society. The Kenyon Police Department has been trained on how to administer Narcan, an overdose antidote that can save lives. As Kenyon Police Chief Lee Sjolander said, We have all in some way been affected by drugs and drug abuse. The main purpose of the forum is not to judge people who are battling these life challenges, but to reach out to offer help and resources for them and the people who are concerned about them. On Saturday, Speech Team members Evelyn Humphrey, Melanie Knealing and Haha Shepard competed in the Sectional Speech Meet. Melanie completed a great senior season with a fifth-place finish. Evelyns season will continue, as she took first place in the Serious Prose category and will compete in the State Tournament. On April 3, the Region VIII FFA Annual Awards Ceremony was held at Pine Island High School. K-W FFA members receiving Region Awards were Madeline Patterson, Star Farmer and a Proficiency Award for Swine Production Entrepreneurship; Matthew Bauer, Proficiency Award in Forage Production; and Owen Scheffler place second in the individual Dairy Cattle Evaluation. The Dairy Cattle Evaluation team placed second and the Dairy Foods team finished third. Taylor Helland, who sang a song of her own composition, won the talent show and will compete at the State Convention Talent Show later in April. Michael Pliscott gave his first-place winning version of the FFA Creed and will also compete at the State Convention in FFA Creed Speaking. Chuck Larson, K-W FFA adviser, and Duane Pliscott, who lives in Kenyon and is the Cannon Falls FFA adviser, were made Honorary FFA Members. In her final address as Region VIII President, Emily Pliscott, of K-W, talked about using your super powers to make the world a better place. To support her talk, Emily used anecdotes about using the super powers of kindness, dedication and reaching out to know people. The Celebration of Student Scholarship, COSS, a daylong event showcasing research, scholarship and creative activity was held at Concordia College in Moorhead. K-W graduate and Concordia sophomore Rosie Breimhurst had a poster presentation on Feminism in Saudi Arabia in the COSS event. In her research, Breimhurst found that many conservative Saudi Arabian women do not support the loosening of gender roles and restrictions. The Western Media often present Saudi Arabia as an oppressor of women. Many Saudi Arabian people, including women, believe that the gender segregation they experience is just and equal under the Quran. This past weekend Rosie, a member of Concordias Chapel choir, was part of the world premiere performance of The Passion of Jesus Christ composed and conducted by Dr. Rene Clausen. Once again there will be a ski jumping hill in Goodhue County after the County Board approved construction of a jump at Mount Frontenac. For the past several winters Gary and Mary Skundberg have had a pair jumping skis displayed in front of their house. The jumping skis were used by Gary when he was growing up in Wisconsin. He was a ski jumper until he had to decide between ski jumping and basketball and chose basketball. Maybe this summer Gary will be getting his skis back into jumping condition or maybe not. ",http://www.southernminn.com/the_kenyon_leader/opinion/article_3872e979-f66d-54ef-8c70-cf78cf168d9c.html,"... inform community members on abuse, addiction and recognition of these ... On Saturday, Speech Team members Evelyn Humphrey, Melanie ...","Forum, COSS, ski jumping and more",April Kenyon City Hall Kenyon Drug Faribault Opioids Kenyon Police Department Narcan Kenyon Police Chief Lee Sjolander Saturday Speech Team Evelyn Humphrey Melanie Knealing Haha Shepard Sectional Speech Meet Melanie Evelyns Serious Prose State Tournament April Region VIII FFA Annual Awards Ceremony Pine Island High School K-W FFA Region Awards Madeline Patterson Star Farmer Proficiency Award Swine Production Entrepreneurship Matthew Bauer Proficiency Award Forage Production Owen Scheffler Dairy Cattle Evaluation Dairy Cattle Evaluation Dairy Foods Taylor Helland State Convention Talent Show April Michael Pliscott FFA Creed State Convention FFA Creed Speaking Chuck Larson K-W FFA Duane Pliscott Kenyon Cannon Falls FFA Honorary FFA Members Region VIII President Emily Pliscott K-W Student Scholarship COSS Concordia College Moorhead K-W Concordia Rosie Breimhurst Feminism Saudi Arabia COSS Breimhurst Saudi Arabian Media Saudi Arabia Saudi Arabian Quran Rosie Concordias Chapel Passion Jesus Christ Dr. Rene Clausen Goodhue County County Board Mount Frontenac Gary Mary Skundberg Gary Wisconsin Gary,2
210,"Google can recognize mens voices better. Voice recognition technology promises to make our lives easier, letting us control everything from our phones to cars to home appliances. Just talk to our tech, and it works. As the tech becomes more advanced, theres another issue thats not as obvious as a failure to process simple requests: Voice recognition technology doesnt recognize womens voices as well as mens. According to research from Rachael Tatman, linguist researcher and National Science Foundation Graduate Research Fellow at the University of Washington, Googles speech recognition software has gender bias.  Rachael Tatman (@rctatman) July 12, 2016 She realized significant differences in the way Googles speech recognition software auto-captions video on YouTube. It was much more consistent on male voices than female. She said the results were deeply disturbing. Its not that theres a consistent but small effect size, either, 13% is a pretty big effect. The Cohens d was 0.7 which means, in non-math-speak, that if you pick a random man and random woman from my sample, theres an almost 70% chance the transcriptions will be more accurate for the man. Thats pretty striking. Language varies in systematic ways depending on how youre talking, Tatman said in an interview. Differences could be based on gender, dialect, and other geographic and physical attributes that factor into how our voices sound. To train speech recognition software, developers use large datasets, either recorded on their own, or provided by other linguistic researchers. And sometimes, these datasets dont include diverse speakers. Generally, the people who are doing the training arent the people whose voices are in the dataset, Tatman said. Youll take a dataset thats out there that has a lot of different peoples voices, and it will work well for a large variety of people. I think the people who dont have socio-linguistic knowledge havent thought that the demographic of people speaking would have an effect. I dont think its maliciousness, I just think it wasnt considered. A representative for Google pointed us to a paper published last year that describes how Google built a speech recognizer specifically for children. In the paper, the company notes that speech recognition performed better for females. Additionally, Google said it trains voice recognition across gender, ages, and accents. Its hard to directly address the research in the article you shared, since the results reflect a relatively small sample size, without information on how the data was sampled or the number of speakers represented. That said, we recognize that its extremely important to have a diverse training set; our speech technology is trained on speakers across genders, ages, and accents. In a research paper we published last year ( Liao et al. 2015 ), we found that our speech recognizer performed better for females, with 10% lower Word Error Rate. (In that paper we actually measured 20% higher Word Error Rate for kidswhich weve been working hard to improve since then.)  Speech recognition has struggled to recognize female voices for years. Tatman cites a number of studies in her post, including voice tech working better for men in the medical field , and that it even performs better for young boys than girls. When the auto industry began implementing voice commands in more vehicles, women drivers struggled to tell their cars what to do, while their male counterparts had fewer problems getting vehicles to operate properly. Its a similar problem to biases in algorithms that control what we see on the webwomen get served ads for lower-paying jobs, criminal records turn up higher in searches for names commonly associated with someone of African-American descent, and the software some law enforcement agencies use is biased against people of color. And this isnt the first time Googles automated systems have failed; last year, the company came under fire when its image recognition technology labeled an African-American woman a gorilla. Tatman said the best first step to address issues in voice tech bias would be to build training sets that are stratified. Equal numbers of genders, different races, socioeconomic statuses, and dialects should be included, she said. Automated technology is developed by humans, so our human biases can seep into the software and tools we are creating to supposedly to make lives easier. But when systems fail to account for human bias, the results can be unfair and potentially harmful to groups underrepresented in the field in which these systems are built. ",https://www.dailydot.com/debug/google-voice-recognition-gender-bias/,She realized significant differences in the way Google's speech recognition software auto-captions video on YouTube. It was much more ...,Research shows gender bias in Google's voice recognition,Google Voice Voice Rachael Tatman National Science Foundation Graduate Research Fellow University Washington Googles Rachael Tatman @ July Googles YouTube Cohens Thats Tatman Tatman Youll Google Google Google Liao Word Error Rate Word Error Rate kidswhich Tatman Googles Tatman Automated,4
211,"9 Dec 2016 at 08:26, Katyanna Quach Machine learning researchers are on a mission to make machines understand speech directly from audio input, like humans do. At the Neural Information Processing Systems conference this week, researchers from Massachusetts Institute of Technology (MIT) demonstrated a new way to train computers to recognise speech without translating it to text first. The rise of interest in deep learning has accelerated the performance of computer speech recognition. Computers can achieve lower word error rates than professional transcriptionists , but it requires intense training. Researchers have to label audio input with transcriptions containing the right text in order for the machines to match sounds to words. It requires careful data collection, and thousands of languages are still unsupported by speech recognition systems. It's a problem that needs tackling if the technology is to be beneficial for society, said Jim Glass, coauthor of the paper and leader of the Spoken Language Systems Group at CSAIL. ""Big advances have been made  Siri, Google  but it's expensive to get those annotations, and people have thus focused on, really, the major languages of the world. There are 7,000 languages, and I think less than 2percent have ASR [automatic speech recognition] capability, and probably nothing is going to be done to address the others. ""So if you're trying to think about how technology can be beneficial for society at large, it's interesting to think about what we need to do to change the current situation,"" Glass said. Researchers from MIT have attempted to do this by mapping speech to images instead of text. The idea is that if words can be grouped together as a set of related images, and these images have associated text, then it should be possible to find a ""likely"" transcription of the audio without having to undergo rigorous training. To collect the training dataset, the researchers used the Places205 dataset , which contains over 2.5 million images categorised into 205 different subjects. People working on Human Intelligence Tasks on Amazon Mechanical Turk were paid to describe four random pictures from the Places205 dataset through audio recordings to provide the pictures with captions. (Just in case you're wondering, participants were paid three cents per recording.) Researchers have collected approximately 120,000 captions from 1,163 unique ""turkers"" and plan to make their dataset publicly available soon. Example of captions for an image in the dataset ... Source: MIT The model is trained to link words to relevant images, and gives a similarity scoring for each pairing. ""In its simplest sense, our model is designed to calculate a similarity score for any given image and caption pair, where the score should be high if the caption is relevant to the image and low otherwise,"" the paper said. During the test phase, the researchers fed the network audio recordings describing a picture in its database and it was asked to retrieve ten images that best matched the description. Out of those ten images, it would contain the correct one only 31per cent of the time. It's a low score, and the new method is a rudimentary way of giving computers a way of recognising words without any knowledge of text or language, but with improvement it could help speech recognition to adapt to different languages. Mapping audio input to images isn't immediately useful, but if the system is trained on images that are associated with words from different languages, it may provide a new way to translate speech into other languages. ""The goal of this work is to try to get the machine to learn language more like the way humans do,"" said Glass. ""The current methods that people use to train up speech recognisers are very supervised. You get an utterance, and you're told what's said. And you do this for a large body of data. ""I always emphasize that we're just taking baby steps here and have a long way to go,"" Glass says. ""But it's an encouraging start.""  ",https://www.theregister.co.uk/2016/12/09/improving_computers_learning_speech/,The rise of interest in deep learning has accelerated the performance of computer speech recognition. Computers can achieve lower word error ...,AI brains take a step closer to understanding speech just like humans,Katyanna Quach Machine Neural Information Processing Systems Massachusetts Institute Technology MIT Jim Glass Spoken Language Systems Group CSAIL Siri Google ASR [ ] Glass MIT Places205 Human Intelligence Tasks Amazon Mechanical Turk Places205 Just Researchers Example MIT Glass Glass,-1
212,"Cars are getting smarter for drivers, but new technology from Nuance Communications enables multi-passenger communications in connected vehicles. PALO ALTO, Calif.The hottest trend in automobiles is the self-driving car. But there are a lot of other developments worth noting. Companies such as Nuance Communications, for example, are using artificial intelligence (AI) and voice recognition to make rides safer and more enjoyable for both the driver and passengers. In a demonstration for eWEEK Nuance executives showed off a Chrysler Pacifica minivan customized by Nuance to include four built-in microphones (two in front, two in back) and the companys AI-powered Automotive Assistant software called Dragon Drive that displayed information on the vehicles in-dash display. Some of the features Nuance demonstrated are yet to be released, but many functions of Dragon Drive have already been incorporated in certain BMW vehicles letting front seat passengers interact with the BMW in-car infotainment system. Microphones installed in the Chrysler Pacifica allowed both front and back seat passengers to communicate with the system. For drivers, the system allows easy, voice-control access to maps and directions, but it also offers more personalized information via the systems AI-driven Contextual Reasoning Framework. During the demonstration, the driver asked for directions to find parking near an address in San Francisco. The system listed several choices on the display, but the driver can pare that down by stating preferences like no valet and cheapest. Over time the system learns your preferences and those appear first, said Lior Ben Gigi, Senior Product Manager at Nuance. The system can also be proactive, alerting the driver when it's time to get gas and recommending where to get it, selecting the cheapest option or perhaps one that honors a loyalty card if that has been a preferred choice. Dragon Drive can recognize the voice of other individuals in the car based on where they are seated. A passenger in the back seat for example can say Im cold and Dragon Drive will activate the seat warmer if thats available. Dragon Drive is always listening, explained Eric Montague, Senior Director for Product Marketing and Strategy at Nuance. But to get the systems attention, individuals need to preface their request with Hello Dragon. Near the end of the demonstration, Nuance showed a feature still in development called Just Talk where the system was able to discern a request without specifically being addressed. For example, the driver might say in conversation with a passenger Were going to need directions and Dragon Drive will speak up to offer directions. Already the system uses interference cancellation to recognize individual speakers so the driver, for example, can converse with Dragon Drive uninterrupted even if other passengers are talking. Other features for passengers are mainly in the area of infotainment, but Montague said Dragon Drive could potentially help facilitate conference calls and other communication of interest to business travelers. In the demonstration, eWEEK joined three passengers in the car in a game of Dragon Tunes where you try to name popular songs based on brief music clips. Any passenger could say Got it and the system correctly recognized whether or not you had the right answer. Because features like Dragon Tunes are embedded in the system, they wont be interrupted by inconsistent cellular reception. Montague said Nuance has been working with car makers for 15 years with various voice recognition products and expects to eventually get Dragon Drive pre-installed in a range of new vehicles. Companies such as Google (Android Auto) and Apple (CarPlay) also have deals with car makers to get their software pre-installed, offering navigation, music and other infotainment features. He said Nuance is open to working with other providers to make sure their systems can interoperate where its appropriate. I think theres an opportunity where eventually well be in a world where virtual assistants work with each other, he said. ",http://www.eweek.com/mobile/nuance-in-car-speech-recognition-lets-passengers-control-infotainment.html,"Cars are getting smarter for drivers, but new technology from Nuance Communications enables multi-passenger communications in connected ...",Nuance in-Car Speech Recognition Lets Passengers Control ...,Nuance Communications PALO ALTO Calif.The Nuance Communications AI Nuance Chrysler Pacifica Nuance Automotive Assistant Dragon Drive Nuance Dragon Drive BMW BMW Chrysler Pacifica AI-driven Contextual Reasoning Framework San Francisco Lior Ben Gigi Senior Product Manager Nuance Dragon Drive Im Dragon Drive Dragon Drive Eric Montague Senior Director Product Marketing Strategy Nuance Hello Dragon Nuance Just Were Dragon Drive Dragon Drive Montague Dragon Drive Dragon Tunes Any Got Dragon Tunes Montague Nuance Dragon Drive Google Android Auto Apple CarPlay Nuance,1
213,"Legal-Specific Speech Recognition Offers Simplified Management, Custom Vocabularies, and Time-Saving Transcription SYDNEY, Australia, 27 July 2016  Nuance Communications, today announced Dragon Legal New Zealand  enterprise-ready speech recognition software for legal professionals. Built with a specialised New Zealand legal vocabulary, Dragon Legal New Zealand enables fast, efficient and accurate dictation of legal documentation, robust transcription and powerful customisation features that can be easily shared and deployed across a practice or legal department. Dragon Legal New Zealand, coupled with the Nuance User Management Center, simplifies how speech recognition can be deployed and managed across groups of users for significant time and cost savings. For instance, a single administrator can centrally oversee deployments of Dragon to multiple employees, including assigning Dragon licenses, monitoring usage, managing settings or updates, and sharing custom terminology. Dragon Legal New Zealand  part of Nuances suite of Dragon professional products, including Dragon Professional Group  includes features that help legal professionals reduce time spent on administrative tasks and documentation, minimise or eliminate reliance on costly transcription services, and improve productivity, including:  Fast and accurate dictation, made more accurate by a New Zealand legal vocabulary trained using millions of words from legal documents.  Improved transcription capabilities for tasks like transcribing recorded notes, which can reduce transcription time and costs or eliminate transcription services completely.  Customisation capabilities that further improve accuracy of dictation and transcription and speed up repetitive tasks, including adding and sharing additional custom words and auto-texts (a frequently-used text passage such as a client or work order description).  Support for Citrix virtualised environments.  An improved What can I say feature, which provides helpful context-sensitive suggestions for words and commands to use as a person is dictating. Dragon is already a proven solution for many legal professionals who rely on the written word for their livelihood, said Derek Austin, Director, Dragon Solutions Asia Pacific. With Dragon Legal New Zealand, it is simple for law firms and legal departments to extend the benefits of Dragon, including fast and accurate dictation and transcription, the ability to share custom legal terminology, and integration with existing workflows, to employees to ultimately increase time spent with clients and on billable work. Availability Dragon Legal New Zealand is now available for Windows through Nuances partner, Sound Business Systems. Nuances partners provide customised solutions including software, hardware, deployment management and training. Suggested recommended pricing for Dragon Legal New Zealand starts from NZ$995 with volume discounts for five or more licenses. Upgrades from other Dragon editions are available. For more information, contact Sound Business Solutions on 09 375 9391 or visit www.soundbusiness.co.nz http://soundbusiness.co.nz . Join the conversation by liking Nuance on Facebook, following Nuance on Twitter at @DragonTweets and @NuanceInc, and subscribing to the Whats next blog. About Nuance Communications, Inc. Nuance Communications, Inc. is a leading provider of voice and language solutions for businesses and consumers around the world. Its technologies, applications and services make the user experience more compelling by transforming the way people interact with devices and systems. Every day, millions of users and thousands of businesses experience Nuances proven applications. For more information, please visit www.nuance.com Nuance, Dragon and the Nuance logo are trademarks or registered trademarks of Nuance Communications, Inc. or its subsidiaries in the United States of America and/or other countries. All other company names or product names may be the trademarks of their respective owners. ",http://www.scoop.co.nz/stories/BU1607/S00820/nuance-brings-speech-recognition-software-to-nz-lawyers.htm,"Dragon Legal New Zealand, coupled with the Nuance User Management Center, simplifies how speech recognition can be deployed and ...",Nuance Brings Speech Recognition Software to NZ Lawyers,Offers Simplified Management Custom Vocabularies Time-Saving Transcription SYDNEY Australia July Nuance Communications Dragon Legal New Zealand New Zealand Dragon Legal New Zealand Dragon Legal New Zealand Nuance User Management Center Dragon Dragon Dragon Legal New Zealand Nuances Dragon Dragon Professional Group New Zealand Customisation Citrix Dragon Derek Austin Director Dragon Solutions Asia Pacific Dragon Legal New Zealand Dragon Availability Dragon Legal New Zealand Windows Nuances Sound Business Systems Nuances Dragon Legal New Zealand NZ Dragon Sound Business Solutions Nuance Facebook Twitter @ DragonTweets @ NuanceInc Whats Nuance Communications Inc. Nuance Communications Inc. Nuances Nuance Dragon Nuance Nuance Communications Inc. United States America,1
214,"Symantec issued a security response on Wednesday (Feb. 22) concerning a new variant of Android ransomware that uses speech recognition APIs and forces victims to vocalizean unlock code rather than typing it in. The malware threat, known as Android.Lockdroid.E ,locks an infected device and then displays a ransom note in Chinese that gives instructions to contact the cybercriminals directly for further instructions on how to pay the ransom and unlock the phone. Victims are directed to press a button to initiatethe speech recognition functionality and the malware-using third-party APIs to compare the spoken words to the expected code. While analyzing these latest Android.Lockdroid.E variants, I observed several implementation bugs such as improper speech recognition intent firing and copy/paste errors. Its clear that the malware authors are continually experimenting with new methods to achieve their goal of extorting money from their victims. We can be certain this isnt the last trick well see from this threat family, Venkatesan continued. Its long been known that Android devices arent as secure as Apples iOS, but a report released in November revealed that some Android devices could get infiltrated with software that tracks a users behavior through their mobile device, including phone calls and text messages, and sends the data to China. According to the report , the infections were discovered by security firm Kryptowire, which said it could be a potentially serious security risk . The report claimedthat China-based Shanghai Adups Technology developed the software, which is installed on an unknown amount of Android-based devices. The information stolen, which includes contact lists, call logs and other sensitive personal information, is sent automatically to Adups every 72 hours, noted the report, citing Kryptowire. The report also statedthe software could be used to remotely install additional software on the infected devices without the owner even knowing it. In the report, Kryptowire said Adups software is running on 700 million devices around the world, with most of its clients being small Chinese device markers. ",http://www.pymnts.com/news/security-and-risk/2017/voice-is-now-malwares-secret-weapon/,Symantec issued a security response on Wednesday (Feb. 22) concerning a new variant of Android ransomware that uses speech recognition ...,Voice Is Now Malware's Secret Weapon,Symantec Wednesday Feb. Android APIs Android.Lockdroid.E APIs Venkatesan Android Apples November Android China Kryptowire Shanghai Adups Technology Adups Kryptowire Kryptowire Adups,4
215,"Sit on it, Robi: Robi sitting on his charging base. Its too bad that the design of the base is such that Robi cannot set himself down to charge.  TAN KIT HOONG/The Star Judging from the number of people that gathered around our desk when we started Robi the robot up, everyone seems to have a fascination with automatons, especially those that can speak or sing. In fact, Robi does more than that  it recognises commands and can perform a number of tricks, like doing push-ups and answering questions in a variety of languages. However, while a robot that does all this is nothing new, the Robi is a little unusual in that you actually build it from parts that come bundled with a weekly magazine. Yes, thats right  Robi is an interactive robot that you can build yourself. As far as we can remember, Robi is the most advanced product that weve seen sold as a part-by-part kit in magazines. The kit comes in parts spread over 70 magazine issues, with detailed assembly instructions. Robi can speak, sing, dance and do pushups when asked to do so. The first issue is RM24.90 but depending on the part you get in each issue, some issues (such as the one with the CPU or the speech recognition circuit board) can cost you RM137. However, those prices are not the norm and most issues of the magazine will cost you RM74.90 each. Subscription prices will be lower, however  theres a 10% discount off each issue when you subscribe. The magazine is released weekly, which means that its going to take well over a year to complete building the robot  thats quite a long time to wait to get the complete robot. We got our prototype Robi already pre-assembled so we didnt have the opportunity to experience the fun of putting the robot together. However, we did get to test the product quite a bit in the week or so that we had Robi to try out. Aesthetically, Robi is a pretty cute robot  some people we showed it to have observed that it looks vaguely like Osamu Tezukas Astro Boy. The robot does feel hefty and pretty high-quality in the hand. Our only concern is the docking base for the Robot  were not sure if its our prototype sample, but it does take a bit of force to dock Robi into its base to charge. The base looks like a tiny stool which Robi sits on, but strangely enough because of the design of the power connector, the robot cant go back and sit down to charge itself. Robis scarf houses the power switch and also doubles as a handle for carrying the robot around. The power switch is located on Robis scarf, which also serves as a carrying handle  the instructions recommend that you carry the robot either using this carrying handle or hold it around the waist to ensure that you dont damage the servomotors. Once you turn it on, the eyes will flash to tell you that Robi is ready to go. However, the ones that will get Robi to perform the most interesting actions are Do push-ups and Come on, lets dance, which of course gets Robi to perform those actions. This isnt Apples Siri or Google Assistant however  not only are you limited to specific phrases, you also have to speak slowly, fairly loudly and clearly to get Robi to respond. We tried it a few times and at first Robi wouldnt respond until we increased the volume of our voice and spoke clearly. Robi is supposed to respond to over 250 phrases, but the prototype model we got responded to only about 20 or 30 phrases. Interestingly, these phrases include some questions in Mandarin as well  when asked questions in Mandarin, Robi will respond in Mandarin as well. The localisation also includes some Malaysian English phrases, so Robi can respond to Whats your name with Just call me Robi lah! Battery life is not particularly long  if you ask Robi to do push-ups or dance a lot, youre looking at perhaps about 15 to 20 minutes of battery life at most before Robi will tell you that he needs to be charged again. Oh yes, one thing that Robi can also do is that it can be used as an infrared remote control for your TV, although we couldnt test this function since we had no way of setting the built-up robot to learn the infrared signals from our brand of TV. From what weve gathered, though, you set the robot to learn the infrared signal of your particular TV brand during the assembly process  Robi simply captures the TV remotes ON/OFF signal when the robots infrared circuit board is put into learning mode. Whether you think Robi is a great product or not depends on your expectations  if you think youre getting a personal assistant or pet that will follow you around and carry stuff for you then youre going to be disappointed. However, as a project to put together with your kids which will ultimately get you a walking, talking, dancing robot, then Robi is really fun  I know that if I were a kid, Id have been ecstatic to build such a thing. However, do be aware that although spread across 70 issues, youre ultimately paying about RM5,300 if you buy it at the local newsstand, or almost RM5,000 if you subscribe. Pros: Surprisingly sophisticated for a kit robot. Cons: Battery life is relatively short; Robi only responds well if you speak clearly and slowly. ",http://www.thestar.com.my/tech/tech-news/2017/03/20/ride-with-robi-the-robot/,"... on the part you get in each issue, some issues (such as the one with the CPU or the speech recognition circuit board) can cost you RM137.",Ride with Robi the robot,"Robi Robi Robi TAN KIT HOONG/The Star Judging Robi Robi Robi Robi Robi Robi RM24.90 CPU RM137 RM74.90 Robi Robi Robi Osamu Tezukas Astro Boy Robot Robi Robi Robis Robis Robi Robi Do Come Robi Apples Siri Google Assistant Robi Robi Robi Mandarin Mandarin Robi Mandarin Robi Just Robi Battery Robi Robi Robi Robi ON/OFF Robi Robi Id RM5,300 RM5,000 Battery Robi",-1
216,"Intelligent Virtual Assistant Market (2016-2022): By Type (Speech        Recognition, Text-to-Speech Recognition) & Application (BFSI,        Automotive, IT & Telecom, Retail, Healthcare) - Research and Markets DUBLIN--( BUSINESS WIRE )--Research and Markets has announced the addition of the ""Intelligent        Virtual Assistant Market - Forecast (2016-2022)"" report to        their offering. An intelligent virtual assistant can be defined as a programmed        engineered entity which interfaces with humans in a human way. IVA        technology incorporates elements of interactive voice response and other        modern artificial intelligence projects to deliver full-fledged virtual        identities that converse with users. Development in IVA technologies        including speech and voice recognition technology such as Apple's Siri        and Amazons' Alexa has been the driving factor behind the growth of IVA        industry. Globally, an emerging trend of outsourced assistance is        anticipated to boost the market demand and is expected to remain the key        growth driver during the period of study. Geographically, North America dominated the intelligent virtual        assistant market owing to the increased penetration of smartphones and        emerging concept of working remotely coupled with technological        advancements which have resulted in increased usage of cloud-based tools        such as Skype and MS Office Online. North America was followed by Europe        and Asia-Pacific as second and third largest market for the intelligent        virtual assistant market. Asia Pacific is projected to have the fastest growth due to demand for        self-reliance which has boosted virtual agent acceptance and adoption.        In recognition of these changing dynamics, service providers have        developed multi-language platforms to serve the need of varied users. ",http://www.businesswire.com/news/home/20170125005922/en/Intelligent-Virtual-Assistant-Market-2016-2022-Type-Speech,"Intelligent Virtual Assistant Market (2016-2022): By Type (Speech Recognition, Text-to-Speech Recognition) & Application (BFSI, Automotive, ...",Intelligent Virtual Assistant Market (2016-2022): By Type (Speech ...,Virtual Assistant Market Type Speech Recognition Text-to- Application BFSI Automotive IT Telecom Retail Healthcare Markets DUBLIN BUSINESS WIRE Research Markets Intelligent Virtual Assistant Market Forecast IVA Development IVA Apple Siri Amazons Alexa IVA Globally Geographically North America Skype MS Office Online North America Europe Asia-Pacific Asia Pacific,3
217,"Speech Recognition Market Players (7 Global, 10 Chinese) Review and Analysis Global and China Speech Recognition Industry Report, 2015-2020 market research highlights 7 foreign and 10 Chinese speech recognition technology-related companies (voice recognition business and application in automotive field). Propelled by big data, mobile Internet, cloud computing, and other technologies, global intelligent voice industry has entered the stage of rapid application. Global intelligent voice market size was USD4.75 billion in 2014 and is predicted to grow 30.7% from a year ago to hit USD6.21 billion in 2015. With intense involvement of Internet giants including Google, Microsoft, and Apple around 2010, global intelligent voice industry has gradually evolved from oligopoly to monopolistic competition. In 2015, speech recognition leader Nuance still took the first place with a market share of 31.1% but suffered a significant decline; Google, Microsoft, Apple, and IFLYTEK witnessed rapid share growth, standing at 20.7%, 13.4%, 12.9%, and 6.7%, respectively. Complete report of 102 pages is available at http://www.reportsnreports.com/reports/527625-global-and-china-speech-recognition-industry-report-2015-2020.html . Thanks to national policy support and demand growth from downstream sectors, China's intelligent voice industry also flourishes with an ever-expanding market size. In 2015, the Chinese intelligent voice market scale was estimated at RMB4.68 billion, a year-on-year surge of 53.1%, making up around 12% of the global market. Traditional Chinese speech recognition companies mostly take a place in intelligent voice market by relying on domestic scientific research institutions, while new firms largely accelerate their presence in intelligent voice industry via financing. The majority of the Chinese intelligent voice market is held by iFLYTEK, Baidu, and Apple (a combined 79% share in 2015). To gain an advantage in market competition, Chinese speech recognition players have flooded into market segments such as intelligent in-vehicle, smart home, and wearable devices. Major Foreign Speech Recognition Players are Nuance, Apple, Google, Microsoft, IBM, MindMeld and Speaktoit. Major Chinese Voice Recognition Companies are iFlyTek, Baidu, AI Speech, Unisound, ThinkIT, SinoVoice, Pattek, Mobvoi (Chumenwenwen), d-Ear and Vcyber. Order a copy of Global and China Speech Recognition Industry Report, 2015-2020 at http://www.reportsnreports.com/purchase.aspxname=527625 . In intelligent in-vehicle field, speech recognition giants Nuance, Apple, Google, Microsoft, iFLYTEK, and Baidu have launched Dragon Drive in-vehicle speech development platform, CarPlay, AndroidAuto, Windows in the Car, Auto Speech System, and CarLife, respectively, and cooperated with carmakers to grab emerging intelligent in-vehicle market. As the application of speech recognition technology in intelligent in-vehicle, smart home, and wearable devices goes deeper, global and Chinese intelligent voice market will maintain the momentum of rapid growth, reaching estimated USD19.17 billion and RMB25.14 billion in 2020, respectively. Another research titled Global Voice Recognition Industry 2015 Market Research Report is a professional and in-depth study on the current state of the Voice Recognition industry. The report provides a basic overview of the industry including definitions, classifications, applications and industry chain structure. The Voice Recognition market analysis is provided for the international markets including development trends, competitive landscape analysis, and key regions development status. Development policies and plans are discussed as well as manufacturing processes and cost structures are also analyzed. This report also states import/export consumption, supply and demand Figures, cost, price, revenue and gross margins. The report focuses on global major leading industry players providing information such as company profiles, product picture and specification, capacity, production, price, cost, revenue and contact information. Upstream raw materials and equipment and downstream demand analysis is also carried out. The Voice Recognition industry development trends and marketing channels are analyzed. Finally the feasibility of new investment projects are assessed and overall research conclusions offered. With 68 tables and figures the report provides key statistics on the state of the industry and is a valuable source of guidance and direction for companies and individuals interested in the market. Key Manufacturers are Apple, Google, Microsoft and Nuance Communications. Place a direct purchase order of this report at http://www.reportsnreports.com/purchase.aspxname=443637 . ReportsnReports.com is an online market research reports library of 500,000+ in-depth studies of over 5000 micro markets. Not limited to any one industry, ReportsnReports.com offers research studies on agriculture, energy and power, chemicals, environment, medical devices, healthcare, food and beverages, water, advanced materials and much more. ",http://www.prnewswire.com/news-releases/speech-recognition-market-players-7-global-10-chinese-review-and-analysis-576030541.html,"Global and China Speech Recognition Industry Report, 2015-2020 market research highlights 7 foreign and 10 Chinese speech recognition ...","Speech Recognition Market Players (7 Global, 10 Chinese) Review ...",Players Global Chinese Review Analysis Global China Industry Report Internet Global USD4.75 USD6.21 Internet Google Microsoft Apple Nuance Google Microsoft Apple IFLYTEK China RMB4.68 Baidu Apple Foreign Players Nuance Apple Google Microsoft IBM MindMeld Speaktoit Major Voice Recognition Baidu AI Speech Unisound ThinkIT SinoVoice Pattek Mobvoi Chumenwenwen Vcyber Global China Industry Report Nuance Apple Google Microsoft Baidu Dragon Drive CarPlay AndroidAuto Windows Car Auto Speech System CarLife USD19.17 RMB25.14 Global Voice Recognition Industry Market Research Report Voice Recognition Voice Recognition Development Upstream Voice Recognition Key Manufacturers Apple Google Microsoft Nuance Communications ReportsnReports.com ReportsnReports.com,3
218,"Home entertainment speakers maker Sonos has announced new initiatives that will take advantage of users increasingly connected homes as the Internet of Things continues to bloom. One of the developments is the integration of Sonos controls into Spotify, which will allow users to control whats playing on their entire Sonos home speaker systems through the popular music streaming services mobile app. But perhaps more striking is Sonos move to pair up its speakers with devices featuring Amazons Alexa virtual AI assistant. That will allow owners of products like the Amazon Echo, Amazon Tap, and Amazon Fire TV to control their speakersincluding whats playing via Spotify and Amazon Musicby voice command. The latter move illustrates voice commands continuing ascent as a key user interface of the Internet of Things and the smart home, with major IT and electronics companies focusing R&D efforts on speech and even voice recognition. Its also a testament to Amazons wisdom in allowing third party companies to develop custom capabilitiescalled skills in Amazons parlancefor the Alexa interface, an approach that Apple has recently sought to emulate . Sonos Spotify integration will arrive via a software update in October, and the Alexa compatibility will arrive in 2017 after a beta test later this year. Meanwhile, Sonos has announced that it has also partnered up with smart home brandsincluding Control4, Crestron, iPort, Lutron, QIVICON, and Savant, all of which will integrate Sonos speakers into their platforms. ",http://findbiometrics.com/speech-recognition-speakers-309122/,Speech Recognition AI Helps Bring Sonos Speakers into the Smart Home One of the developments is the integration of Sonos controls into ...,Speech Recognition Helps Bring Sonos Speakers into the Smart ...,Home Sonos Internet Things Sonos Spotify Sonos Amazons Alexa AI Amazon Echo Amazon Tap Amazon Fire Spotify Amazon Musicby Internet Things IT R D Amazons Amazons Alexa Apple Sonos Spotify October Alexa Sonos Control4 Crestron Lutron QIVICON Savant Sonos,-1
219,"On April 4, 1967, a year to the day before he was assassinated, Martin Luther King Jr. delivered one of the most powerful, controversial speeches of his life: Beyond Vietnam: Time to Break the Silence. The legendary orator and organizer, this young Nobel Peace laureate, laid out a bold condemnation of the U.S. war in Vietnam, encouraging collaboration between the civil-rights and the anti-war movements. Fifty years later, as the Trump administration attempts to radically increase the Pentagon budget by $54 billion and gut social programs and the State Department budget  crucial for achieving diplomatic solutions to conflict  Kings Beyond Vietnam speech remains chillingly relevant. More than 3,000 people had gathered on that spring day in New York Citys Riverside Church. In his speech, King called the United States the greatest purveyor of violence in the world today and committed to oppose the giant triplets of racism, extreme materialism, and militarism. He detailed the history of how the U.S. role escalated in Vietnam, then linked the expense of war to poverty at home, saying, A few years ago  there was a real promise of hope for the poor  both black and white  through the poverty program. Then came the buildup in Vietnam, he continued, and I watched this program broken and eviscerated, as if it were some idle political plaything of a society gone mad on war, and I knew that America would never invest the necessary funds or energies in rehabilitation of its poor so long as adventures like Vietnam continued to draw men and skills and money.  I was increasingly compelled to see the war as an enemy of the poor and to attack it as such. The mainstream backlash against Kings speech was immediate. Life magazine accused King of betraying the cause for which he worked so long, adding that much of his speech was a demagogic slander that sounded like a script for Radio Hanoi. The Washington Post editorialized: Dr. King has done a grave injury to those who are his natural allies.  He has diminished his usefulness to his cause, his country, and his people. But King would not relent, continuing to pursue what people now call intersectional organizing. When he was murdered one year after that speech, he was in Memphis, supporting striking sanitation workers who were seeking union recognition. On April 3, 1968, in Memphis, he gave his last speech. Ive been to the mountaintop, he declared. Living with continual death threats and FBI harassment, he went on: Like anybody, I would like to live a long life. Longevity has its place. But Im not concerned about that now. Less than a day later, he was dead. The nations largely African-American inner cities erupted into riots, and the history of the country was forever changed. Today, the Rev. Dr. William Barber, head of the North Carolina NAACP, calls Kings Beyond Vietnam speech a prophetic sermon, and is carrying Kings strategy forward into the 21st century. Where are we on racism, when we see 22 states in this country passing systemic race-based voter-suppression laws and we have less voting-rights protection today than we had in 1965 with the gutting of the Voting Rights Act he said on the Democracy Now! news hour. Where are we, when we dont use the word poor in our public and political conversation When we just a few weeks ago saw an out-of-control military strike kill 200 innocent citizens and some 400,000 citizens were killed during the Iraq War, that we should have never gone into When were talking about expanding an already-bloated military budget and spending some $54 billion, that if we use that same money in a modern-day war against poverty and a modern call for health care and education, we could do so much more If Fox News Channel were to pre-empt just one episode of the program hosted by the accused serial sexual harasser Bill OReilly and play Beyond Vietnam, or if CNN or MSNBC would air the speech in its entirety, there is a chance President Donald Trump, a voracious cable-news consumer, might see it. Perhaps then he might think twice before escalating the war in Iraq and Yemen, or provoking one against North Korea. As the world reels in horror at the latest poison-gas attack in Syria, most likely launched by the Assad regime against its own citizens, Trump might consider leading the world, now momentarily united in outrage, in a global, diplomatic response that could lead to a political solution there. With a strong leader committed to peace, the United States could do this. Most likely, though, that difficult work remains for those in whom Martin Luther King Jr. had the most hope: the people, organizing grass-roots power for peace. ",https://www.abqjournal.com/984776/mlks-words-on-war-should-awaken-us.html,"In his speech, King called the United States the greatest purveyor of ... striking sanitation workers who were seeking union recognition.",MLK's words on war should awaken us,April Martin Luther King Jr. Vietnam Silence Nobel Peace U.S. Vietnam Trump Pentagon State Department Kings Beyond Vietnam New York Citys Riverside Church King United States U.S. Vietnam A Vietnam America Vietnam Kings Life King Radio Hanoi Washington Post Dr. King King Memphis April Memphis Ive FBI Longevity Im Rev Dr. William Barber North Carolina NAACP Kings Beyond Vietnam Kings Voting Rights Act Iraq War Fox News Channel Bill OReilly Beyond Vietnam CNN MSNBC President Donald Trump Iraq Yemen North Korea Syria Assad Trump United States Martin Luther King Jr.,12
220,"Administrators at St. Thomas University said in a letter to the field director of Turning Point USA, Driena Sixto, that the clubs use of foul language does not align with the schools Roman Catholic values and that they are prohibited from promoting their cause on campus. In reviewing your organization, including its website, we found that your organizations use of foul language is offensive to the very principle of what we stand for in our Catholic core values as an institution, Carmen Brown, the St. Thomas administrator, wrote in a March 16 letter obtained by College Fix . Therefore, we regret to inform you that we are not approving your organizations presence on our campus. Turning Point USA is a conservative nonprofit and student movement whose stated mission is to ""identify, educate, train, and organize students to promote fiscal responsibility, free markets and limited government."" While administrators at St. Thomas did respond to Sixtos request for clarification, she said that she believes that school officials probably took offense to the clubs slogans and signs reading Big Government Sucks and Socialism Sucks. Sixto, who is not a student at St. Thomas but works with Turning Point USA to start club chapters at Miami-area universities, sent a pointed message to students at the school who signed up for the organization. In an ironic turn of events, the school that was founded in 1961 by Augustinians that were expelled from Cuba by the communist Castro dictatorship is now a school where conservative values and freedom of expression are undesired, thanks to complaints of left-wing, and self-declared communist/socialist faculty members, Sixto wrote. The administrations move to shut down the group has even drawn the ire of the top brass at Turning Point USA, who called the incident troubling. The school invokes a broad opposition to foul language to keep a free-market, conservative group off of campus, Matt Lamb, director of campus integrity for Turning Point USA, said. The higher-ups in the school administration frequently dodged requests to speak with us, and then failed to keep their facts straight. ",http://www.foxnews.com/us/2017/04/10/college-started-by-priests-fleeing-castros-communist-cuba-shut-down-pro-capitalist-club.html,... of these clubs have overcome these obstacles with the help of free-speech ... leaders' February decision to deny the club official recognition.,College started by priests fleeing Castro's communist Cuba shut ...,St. Thomas University Point USA Driena Sixto Roman Catholic Catholic Carmen Brown St. Thomas March College Fix Point USA St. Thomas Sixtos Sucks Socialism Sucks Sixto St. Thomas Point USA Miami-area Cuba Castro Sixto Point USA Matt Lamb Point USA,5
221,"Speech recognition applications have been around for decades. Until recently however, outside of specialist areas such as warehousing, voice recognition hasn't seen a huge uptake by the small business community as a whole. With high accuracy rates and professional apps available for mobile devices, is voice recognition a service your business could make great use of today Many business users may have already experienced what voice recognition can offer. Apple's Siri may grab the headlines but Windows 10 users have Cortana, which moves the digital assistant to a whole new level of functionality. More importantly for today's businesses is the fact that Cortana is available on phones, tablets and desktop PCs offering a level of integration and familiarity across a number of devices being used across your company. The power of voice recognition applications is that they can turn us all into high-speed typists. Being able to dictate at normal speaking speed and have your words accurately transcribed is a huge productivity bonus all businesses can benefit from. And if your business has to endlessly type the same blocks of text into numerous documents, the leading speech recognition applications can have special voice commands defined that will enter these blocks of text for you. Think standard clauses in contracts or email signatures. However, voice recognition can go further than being used as an accurate transcription application. Today's voice recognition systems can be set up to operate multiple applications. So, if you are a sales rep for instance and have just dictated an email to a new customer, you can use your voice to enter that customer's contact details onto your business' CRM (Customer Relationship Management) system. This seamless integration of voice control is where these systems come into their own. Your business is now run on the move. Voice recognition systems from Nuance have a cloud component that makes these systems infinitely more flexible especially in a business setting. You can use your phone to dictate an email when you're travelling and see this document on any device connected to the cloud. Being able to sync your dictated documents can have a profound impact on your firm's overall efficiency. Businesses in highly specialised sectors such as the legal profession are in an ideal position to take advantage of what voice recognition can offer. All of the systems are learning machines  the more you use them, the more accurate they become. If your business often creates documents that include jargon and other specialised terminology, voice recognition systems will learn these terms to ensure high levels of accuracy. Ultimately it's the time-saving factor that attracts businesses to voice recognition. Being able to spend less time typing and more time working on more important aspects of running a business such as customer service, is why more businesses are adopting voice recognition into their business processes. Even if your company only uses these applications for simple transcription, you'll be surprised at the efficiency gains that can be obtained from relatively low-cost software. To decide whether voice recognition would be useful for your business, ask yourself these questions: Does your business generate high levels of written documents Does your business trade with overseas companies Does your workforce often generate documents outside of your office Does your business frequently create documents with similar content Does your business operate in a sector using high levels of jargon or technical language If you answered yes to any of these questions your business could be a candidate for voice recognition applications. The best course of action is to test the systems in your organisation. Even a short trial will reveal whether these systems can be seamlessly integrated into your company's working patterns. Always consult with your staff, as they will be using these applications, which can take some getting used to after a lifetime of typing with a keyboard. What many businesses usually find is that they augment their day-to-day practices with a level of voice recognition. And over time, this increases and expands, as they gain experience of its advantages and of course its limitations. ",http://www.techradar.com/news/software/applications/how-voice-recognition-can-be-a-major-asset-for-your-business-1321534,"Speech recognition applications have been around for decades. Until recently however, outside of specialist areas such as warehousing, voice ...",How voice recognition can be a major asset for your business,Many Apple Siri Cortana Cortana CRM Customer Relationship Management Voice Nuance Does Does Does,-1
222,"(AP Photo/Jean-Francois Badias). Far-right candidate for the presidential election Marine Le Pen speaks during a campaign meeting in Monswiller near Strasbourg, eastern France, Wednesday, April 5, 2017. A self-described patriot, Le Pen hopes to extract... (Lionel Bonaventure/Pool Photo via AP). Independent centrist presidential candidate for the presidential election Emmanuel Macron attends a television debate at French private TV channels BFM TV and CNews, in La Plaine-Saint-Denis, outside Paris, Franc... (AP Photo/Christophe Ena, Francois Mori, Michel Euler, Jean-Francois Badias, Files). This combination of file photos show the eleven French official presidential candidates: Top from left: Emmanuel Macron, Benoit Hamon, Francois Fillon, Marine le Pen. ... PARIS (AP) - The Latest on France's two-round, April 23-May 7 presidential election (all times local): Far-right presidential candidate Marine Le Pen wants to fight terrorism by tackling the ""root of evil"" in French society. She said in a speech on Monday she would oversee a tough repression of criminals who risk turning to jihad and a cleanup within the Muslim population to rid it of fundamentalists who try to intimidate peaceful believers. There are more jihadis in Syria from France than any other European country. Le Pen vowed to ""reconquer the lost territory of the Republic"" by ensuring French law, not Muslim Sharia law, prevails. Among measures she listed to reduce the risk of terrorism are ensuring sermons are delivered in French and dissolving the Union of Islamic Organizations of France. She also wants a minimum defense budget of 2 percent of GNP written into the Constitution and a single agency to fight the terror threat. French presidential candidate Emmanuel Macron wants to hold discussions with U.S. social media groups about how to halt the spread of ""Islamist propaganda."" Macron, a centrist considered the front-runner in the upcoming election, told a press conference he wants to ""have a frank discussion"" with the internet giants, such as Apple, Facebook, Google and Twitter. He also pledged to work with other European countries and members of the NATO alliance to force internet firms to provide data encryption keys to police or access to some encrypted contents when needed in a counterterrorism investigation. He said only an ""international coordinated initiative"" can put pressure on the internet giants. Macron also said he will maintain French operations in Iraq, Syria and Africa's Sahel regions against Islamic extremists. France has been under a state of emergency since the 2015 attacks in Paris. Israel's Foreign Ministry has condemned comments by France's far-right presidential candidate Marine Le Pen denying that the French State was responsible for rounding up Jews in World War II. The ministry says in a statement that her comments are ""contrary to historical truth, as expressed in the statements of successive French presidents who recognized France's responsibility for the fate of the French Jews who perished in the Holocaust."" Former president Jacques Chirac formally acknowledged the state's role in Jewish persecution in 1995, a position maintained and approved by his successors Nicolas Sarkozy and Francois Hollande. The Israeli Foreign Ministry also says France's recognition of its responsibility ""underpins the annual events marking the anniversary of the expulsion of the Jews from France and the study of the Holocaust in the education system, both of which are important elements in the battle against anti-Semitism, which unfortunately is once again raising its head."" Emmanuel Macron, an independent French presidential candidate, says his far-right rival Marine Le Pen made ""a serious mistake"" by denying that France State was responsible for the roundup of Jews in World War II. He was among many presidential candidates criticizing Le Pen's comments Monday. Macron, the front-runner in the April 23-May 7 two-round election, told BFM TV that ""some had forgotten that Marine Le Pen is the daughter of Jean-Marie Le Pen."" Le Pen's father repeatedly has been convicted for anti-Semitism and racism. Le Pen said Sunday on RTL radio ""I don't think France is responsible for the Vel d'Hiv,"" a reference to the stadium where thousands of Jews were rounded up in July 1942 before being sent to Nazi death camps. In all, about 75,000 Jews were deported from France to Nazi concentration camps during World War II. Only 2,500 survived. Copyright 2017 The Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed. ",http://www.ksby.com/story/35109753/the-latest-le-pen-vows-to-tackle-root-of-evil-in-france,She said in a speech on Monday she would oversee a tough repression of ... The Israeli Foreign Ministry also says France's recognition of its ...,The Latest: Le Pen vows to tackle 'root of evil' in France,AP Photo/Jean-Francois Badias Marine Le Pen Monswiller Strasbourg France Wednesday April Le Pen Lionel Bonaventure/Pool Photo AP Emmanuel Macron BFM CNews La Plaine-Saint-Denis Paris Franc AP Photo/Christophe Ena Francois Mori Michel Euler Jean-Francois Badias Files Emmanuel Macron Benoit Hamon Francois Fillon Marine Pen PARIS AP Latest France April Marine Le Pen Monday Muslim Syria France Le Pen Republic Muslim Sharia French Union Islamic Organizations France GNP Constitution Emmanuel Macron U.S. Macron Apple Facebook Google Twitter NATO Macron Iraq Syria Africa Sahel Islamic France Paris Israel Foreign Ministry France Marine Le Pen State World War II France Jews Holocaust Former Jacques Chirac Nicolas Sarkozy Francois Hollande Israeli Foreign Ministry France Jews France Holocaust Emmanuel Macron Marine Le Pen France State Jews World War II Le Pen Monday Macron April BFM Marine Le Pen Jean-Marie Le Pen Le Pen Le Pen Sunday RTL France Vel Jews July Nazi France Nazi World War II Copyright Associated Press All,10
223,"Navy veteran Angela Smith will be coaching veterans at 1 p.m. March 28 at the Veterans Memorial Building on how to prepare resumes for civilian jobs. A job fair with 20 employers will take place at the veterans building 1-4 p.m. April 6.  (Dan Coyro -- Santa Cruz Sentinel) What: Job fair for military veterans, their family members and friends. Recruiting: UC Santa Cruz, county of Santa Cruz, Santa Cruz Seaside Co., city of Santa Cruz, Santa Cruz Nutritionals, Fox, Santa Cruz Bicycles, S. Martinelli & Co.,  Graniterock, Bright Vision Solar, Comcast, Allied Universal Security, United Brotherhood of Carpenters, International Order of Electrical Workers, Operating Engineers Local 3, Dignity Health, New Leaf Community Markets, Farmer Veteran Coalition and California Conservation Corps. Job prep: Resume workshop 1 p.m. March 28, follow-up workshop 2-4 p.m. April 4, mock interviews 3:30-5:30 p.m. April 5, all at Veterans Memorial Building. Presented by: Veterans board of trustees, United Veterans Council, Veterans Services Office, California Employment Development Department, Workforce Santa Cruz County and Vets4Vets. SANTA CRUZ >> Its possible to turn military experience into skills in demand by the private sector. Ask Angela Smith, a civilian who had a 20-year career in the Navy and Marine Corps. You take life experience, skills and accomplishments to highlight to the employer what they bring to the table, she said. We make them think like the employer. Smith, 62, of Lompico, will give a free class on resume building for veterans from 1-3 p.m. March 28 at the Veterans Memorial Building, 846 Front St. The workshop is scheduled in advance of a job fair for veterans 1-4 p.m. April 6 at the Veterans Memorial Building, where 20 employers will be on hand with openings to fill. Quantifying accomplishments is important, she said, giving an example from her civilian life when her inquiries on insurance premiums paid by her employer, a Jewish synagogue, resulted in a $140,000 reimbursement for overpayment. To help veterans see the possibilities, she will use Crosswalk, a program that produces a list of potential civilian job titles that match with the military occupation classifications. Smith said an Army or Air Force medic could become a paramedic, emergency medical technician, a surgical technologist, a radiology technician, a respiratory therapist, or work in insurance billing, payroll records or finance. A computer lab will be set up in the Veterans Memorial Building for attendees to use the program and revise their resumes. Smith previously was in San Diego where her son was in the hospital after being injured in Afghanistan. When she got there, she spoke up, surprised there were no rooms for female veterans. A year later, there were 14 beds for women veterans and a halfway house for post traumatic stress disorder. I dont take no for an answer, she said. In San Diego, she worked for Hire Our Heroes, a nonprofit founded by veterans to help returning veterans transition into civilian jobs. One of her assignments was a group of 15 Iraq veterans who had lost their sight. Smith said they got tech jobs where they used the Dragon Naturally Speaking speech recognition software along with the paperwork skills they developed in the military. The first employer to sign up was NBC News, she said, and Comerica, Bank of America and Wells Fargo followed. By the end of her assignment, 75 veterans who had lost their sight had transferred to train for civilian jobs in San Diego, where the Metro trolley lines offer accessibility. One of Smiths friends worked with 450 veterans who lost a limb during their service, helping them find jobs in security and at concessions at Petco Park where the San Diego Padres play. The loss of a limb was not an obstacle to their employment. That military mindset helped them overcome it, Smith said. I saw these young men and women competing on the basketball court and racing on bicycles. What: Job fair for military veterans, their family members and friends. Recruiting: UC Santa Cruz, county of Santa Cruz, Santa Cruz Seaside Co., city of Santa Cruz, Santa Cruz Nutritionals, Fox, Santa Cruz Bicycles, S. Martinelli & Co., Graniterock, Bright Vision Solar, Comcast, Allied Universal Security, United Brotherhood of Carpenters, International Order of Electrical Workers, Operating Engineers Local 3, Dignity Health, New Leaf Community Markets, Farmer Veteran Coalition and California Conservation Corps. Job prep: Resume workshop 1 p.m. March 28, follow-up workshop 2-4 p.m. April 4, mock interviews 3:30-5:30 p.m. April 5, all at Veterans Memorial Building. Presented by: Veterans board of trustees, United Veterans Council, Veterans Services Office, California Employment Development Department, Workforce Santa Cruz County and Vets4Vets. Subscribe to Home Delivery and SAVE! Jondi Gumz is an award-winning reporter covering Santa Cruz business, housing, healthcare and Santa Cruz County government. Reach the author at jgumz@santacruzsentinel.com or follow Jondi on Twitter: @jondigumz . ",http://www.santacruzsentinel.com/business/20170322/in-santa-cruz-a-job-fair-and-resume-workshop-for-veterans,Smith said they got tech jobs where they used the Dragon Naturally Speaking speech recognition software along with the paperwork skills they ...,"In Santa Cruz, a job fair and resume workshop for veterans",Navy Angela Smith March Veterans Memorial Building April Dan Coyro Santa Cruz Sentinel Job UC Santa Cruz Santa Cruz Santa Cruz Seaside Co. Santa Cruz Santa Cruz Nutritionals Fox Santa Cruz Bicycles S. Martinelli Co. Graniterock Bright Vision Solar Comcast Allied Universal Security United Brotherhood Carpenters International Order Electrical Workers Operating Engineers Local Dignity Health New Leaf Community Markets Farmer Veteran Coalition California Conservation Corps Job Resume March April April Veterans Memorial Building Veterans United Veterans Council Veterans Services Office California Employment Development Department Workforce Santa Cruz County Vets4Vets SANTA CRUZ > > Ask Angela Smith Navy Marine Corps Smith Lompico March Veterans Memorial Building Front St April Veterans Memorial Building Crosswalk Smith Army Air Force Veterans Memorial Building Smith San Diego Afghanistan San Diego Hire Heroes Iraq Smith Dragon Naturally Speaking NBC News Comerica Bank America Wells Fargo San Diego Metro Smiths Petco Park San Diego Padres Smith Job UC Santa Cruz Santa Cruz Santa Cruz Seaside Co. Santa Cruz Santa Cruz Nutritionals Fox Santa Cruz Bicycles S. Martinelli Co. Graniterock Bright Vision Solar Comcast Allied Universal Security United Brotherhood Carpenters International Order Electrical Workers Operating Engineers Local Dignity Health New Leaf Community Markets Farmer Veteran Coalition California Conservation Corps Job Resume March April April Veterans Memorial Building Veterans United Veterans Council Veterans Services Office California Employment Development Department Workforce Santa Cruz County Vets4Vets Subscribe Home Delivery SAVE Jondi Gumz Santa Cruz Santa Cruz County @ Jondi,4
224,"There is no denying that speech or voice-based data is one of the most important assets when it comes to Artificial Intelligence (AI) development or taking predictive analytics to the next level. It can be termed as a trend that has the potential to revolutionize the industry and the current rate of adoption across industries substantiates this. Sales enablement industry in particular is vying to jump the queue in more than one aspect in leveraging speech recognition, especially, localization. The sales-enablement industry will be able to take major leaps in bridging the gap between strategy and execution when speech recognition truly becomes language and accent agnostic. Owing to the huge demographic dividend, India, as a country, can easily be termed as a gold mine of workforce. 22 major languages, 13 different scripts and 720 dialects definitely makes our country rich in diversity, however, at times, it even acts as a handicap when we look for a synchronized workforce, which is an essential requirement when it comes to sales. All these different dialects, which belong to newer and emerging markets, cannot be ignored just because of language and literacy level becoming an information transfer barrier. This is precisely where localized speech recognition will be able to add value to the entire process of sales enablement. Reporting is one of the most critical ingredients of sales functions as it not only enables accurate and timely flow of feedback which in turn helps organizations and brands to strategize future roadmap based on market feedback analysis. Speech recognition technology localization takes away the need to type, and with its apt implementation across requisite set of workforce, the literacy roadblock can be surpassed. While India has a composite literacy rate of approx. 74% (as per 2011 census), the percentage varies when we go deeper into the rural regions. Talking about rural India, while everyone can speak but not everyone is proficient in written skills and therefore when speech recognition becomes language, accent and dialect agnostic, it will enable native-language speaking work force to capture market information effectively through speech. With this advancement taking away the need to type, which in turn means a substantial amount of time-saving, even training of the workforce will become an easier task. As per a recent report from IAMAI, total internet users in India touched 462 million mark in June 2016 while the number of mobile internet users was around 436 million by end of June 2016. And although 71% (approx.) of this mobile internet user population belongs to urban India, rural India is quick to follow the trend with its user base getting almost doubled in the last two years. These figures hold extreme importance for the sales enablement industry as a major chunk of the sectoral workforce (field persons) is spread across rural geography. And streamlining the sales process implementation across diverse geographies, with multiple local language resources is what this new-age technology will seamlessly enable. Though the connectivity and internet penetration is progressing exponentially in our country, the reception, storage and transmission of speech-to-text data is not as contingent on the strength of connectivity signals. Therefore, usage of this technology will be preventing any loss of information or real time market feedback data on account of connectivity strength. Faster transfer of information will in-turn provision the ability to act and react faster and thus, will aid in fast decision-making. This is particularly a decisive point for perishable goods industry where quick transfer of accurate data is all-the-more crucial. Considering the apt implementation of localized speech recognition across required set of workforce in near future, the divide between English speaking and non-English speaking workforce will diminish and organizations will be equipped with real-time, complete and authentic information from the baseline workforce.This will eventually enable building up of effective sales strategies and formalizing growth based roadmaps based on the market sentiments. More so, this advantage will be two-pronged since the organization will be getting access to newer and unpenetrated markets while the ground-level sales person will also be able to expand his/her reach without wasting time in reporting the progress rate to the next level or even taking a journey cycle plan brief from them. Specifically, from the purview of sales enablement industry, effective implementation of localized speech recognition will ensure better output without compromising on the quantity or quality of inputs. Effective leverage of this technology as it evolves will help in automating transactional quality check in a telesales environment thereby increasing throughput and effectiveness of a QA team. If leveraged well, speech recognition technology has the power to enable sales with optimised returns. (The author, Snehashish Bhattacharjee is Global CEO & Co-Founder of Denave, a global sales-enablement company) ",http://www.voicendata.com/localization-of-speech-recognition-can-re-define-sales-enablement/,"Sales enablement industry in particular is vying to jump the queue in more than one aspect in leveraging speech recognition, especially, ...",Localization of 'speech recognition' can re-define 'sales enablement',Intelligence AI India India India IAMAI India June June India India Faster English QA Bhattacharjee Global CEO Co-Founder Denave,3
225,"Its estimated that some 1.2 billion people, roughly equivalent to 16 percent of the worlds population, speak some form of Chinese. Obviously, a lot of developers would like to tap into that market without necessarily having to translate their application into Chinese. To help accelerate that process, Baidu, the most widely used search engine in China, announced today that it is making available to developers four speech application programming interfaces (APIs). The APIs specifically address Long Utterance Speech Recognition, Far-Field Speech Recognition, Expressive Speech Synthesis and Wake Word. Long Utterance Speech Recognition enables the transcription of long audio clips such as interviews, speeches and lectures. Far-Field Speech Recognition enables the recognition of speech from audio sources that are up to 16 feet away. Expressive Speech Synthesis provides a collection of realistic voices that can be used to read aloud. Wake Word allows developers to create customized short words or phrases that can be spoken to turn devices on. Sanjeev Satheesh, a research scientist at Baidu who specializes in machine learning, says Baidu is trying to drive globalization of applications by making speech APIs available along with other services such as facial recognition, optical character recognition and natural language processing. Were not just talking about speech to text, says Satheesh. To drive that effort, Satheesh says that in contrast to other providers of APIs, Baidu has opted to make the four speech APIs available to developers for free. Longer term, Satheesh says Baidu envisions speech replacing traditional keyboards as the primary interface users interact with to access applications. In fact, Satheesh says Baidu expects natural language APIs to be used in concert with speech APIs and language translation services enabled by a deep learning framework it developed to translate audio. Baidu early this year made that deep learning framework, dubbed PaddlePaddle, available as an open source project. Satheesh says free access to APIs coupled with open source software will eventually force other providers of these types of technologies to follow suit in terms of making advanced technologies more pervasively available to all developers. In the meantime, developers might want to start considering what access to a billion more potential users might mean for their applications. ",http://www.itbusinessedge.com/blogs/it-unmasked/baidu-makes-speech-apis-available-for-free.html,"The APIs specifically address Long Utterance Speech Recognition, Far-Field Speech Recognition, Expressive Speech Synthesis and Wake ...",Baidu Makes Speech APIs Available for Free,Chinese Chinese Baidu China APIs APIs Long Utterance Far-Field Expressive Speech Synthesis Wake Word Long Utterance Speech Synthesis Wake Word Sanjeev Satheesh Baidu Baidu APIs Were Satheesh Satheesh APIs Baidu APIs Satheesh Baidu Satheesh Baidu APIs APIs Baidu PaddlePaddle Satheesh APIs,-1
226,"PUPILS have been praised after raising more than 1,000 to support a Wrexham mum who overcame life-threatening complications when giving birth. Children of St Pauls Voluntary Aided Primary School in Isycoed organised the event in aid of 35-year-old Rachel Harry. Rachel is still undergoing intensive physiotherapy to learn to walk again and receiving speech therapy in order to communicate again after suffering a hypoxic brain injury during the birth of her daughter Freya in July 2012. Rachels injury came as the result of an undetected blood clot which caused a heart attack. Her daughter Freya, who was delivered by a Caesarean section operation, now attends St Pauls Voluntary Aided Primary School. Emma Jones, key stage two teacher at the school and Rachel Harrys sister, said she was delighted at the success of the event, which was devised and led by school council members 10-year-old Charlotte Thomas, 11-year-old Paige Thomas, 10-year-old Callum Wittenbrink and nine-year-old Megan Jones, who is also Rachel Harrys niece. She said: The children were given a project to do where they had to research charities to present to the class and the school to decide on one to support. The children chose the Rachel Harry Fund and decided on a football tournament, then a coffee and cake session where they invited parents, presented medals and gave speeches. It was brilliant  the childrens teamwork was fantastic. They organised the whole school into teams of mixed ages and then had a nursery and reception football match at the end. We raised 1,001 for the Rachel Harry Fund, it is going to make such a massive difference to Rachel. The therapy sessions she has are 75, three-times-a-week. She is doing so much better now and she has just started to walk with the aid of sticks in her sessions. She does need a lot more support but she is doing remarkably well. Rachel came to the event  she and her daughter were over the moon. The event, in particular the football tournament, also had the support of Wrexham Football Club, which has been working with the school on topics such as health and well being. ",http://www.newsnorthwales.co.uk/news/174261/pupils-score-a-winner-for-heart-attack-victim-mum-from-wrexham.aspx,"She has a new speech recognition software and she gave a speech to say thank you. The event, in particular the football tournament, also ...",Pupils score a winner for heart attack victim mum from Wrexham,Wrexham Children St Pauls Voluntary Aided Primary School Isycoed Rachel Harry Rachel Freya July Rachels Freya St Pauls Voluntary Aided Primary School Emma Jones Rachel Harrys Charlotte Thomas Paige Thomas Callum Wittenbrink Megan Jones Rachel Harrys Rachel Harry Fund Rachel Harry Fund Rachel Rachel Wrexham Football Club,-1
227,"This product was designed to allow officers to focus on things that are hazardous to them, and leave the nonhazardous stuff to automation At IACP this year, I got a chance to talk to Eric Guinazzo, of Nuance Communications. As you know, Nuance is the company that produces Dragon NaturallySpeaking . You probably also know that I am a power user of their products . What they introduced to me at IACP was a natural fit for their product line, but very unique in the law enforcement world. Nuance has launched Dragon Law Enforcemen t. It's not just a dictation product specific to law enforcement users  you know, with the unique vocabulary, etc. This product was designed to allow officers to focus on things that are hazardous to them, and leave the nonhazardous stuff to automation. As you know, Dragon NaturallySpeaking products have two different components to the speech recognition. The components are dictation and command. The former takes human speech and turns it into text, using complex algorithms. The latter takes human speech and turns it into commands to tell the computer what to do. You should also know that I am dictating this article while occasionally telling the mouse to go somewhere else, click on something, or turn the dictation on or off. The P1 staff hits the showroom floor to bring you the latest in police products and innovations. Nuance took this capability a little further. Since reports already integrate into Records Management Systems (RMS), Nuance took the commands and created a pathway for reports to be integrated as certain activities happen. This is done using Dragon Law Enforcement open-ended custom commands, DragonTemplates, and a macro recorder, which saves a series of actions (keystrokes and mouse clicks) under a single voice command. This can allow an officer to open a pre-loaded agency specific template for reporting. You see, it isn't just custom vocabularies, its industry specific customization. In fact, the best part of Dragon Law Enforcement are other in patrol car commands like text to speech license plate lookup and commands that drive in car documentation. Dragon Law Enforcement is easily deployed to any size agency. It is scalable and easily managed. Who would've known that a dictation product could aid in officer safety Lindsey Bertomen is a retired police officer and retired military small arms trainer.  He teaches criminal justice at Hartnell College in Salinas, California.  He has a BS in Criminal Justice and an MS in Online Teaching and Learning.  Lindsey has taught shooting techniques for over a decade.  His articles on firearms tactics have appeared in print for over a decade.  Lindsey enjoys competing in shooting sports, running, and cycling events. ",https://www.policeone.com/police-products/police-technology/software/rms/articles/232915006-IACP-2016-Nuance-Communications-launches-LEO-specific-speech-recognition-solution/,"As you know, Dragon NaturallySpeaking products have two different components to the speech recognition. The components are dictation and ...",IACP 2016: Nuance Communications launches LEO-specific speech ...,IACP Eric Guinazzo Nuance Communications Nuance Dragon NaturallySpeaking IACP Dragon Law Enforcemen Dragon NaturallySpeaking P1 Records Management Systems RMS Nuance Dragon Law Enforcement DragonTemplates Dragon Law Enforcement Dragon Law Enforcement Lindsey Bertomen Hartnell College Salinas California BS Criminal Justice MS Online Teaching Learning Lindsey Lindsey,1
228,"Get today's popular DigitalTrends articles in your inbox: Google Now, search giant Googles eponymous voice assistant, has a surprisingly good grasp on the nuances of human speech. Thanks to a killer combination of machine learning and crowdsourced data, it can parse mumbles, murmurs, and even the most garbled of phrases.In August of last year, as an example, Google said it cutvoice transcription errors by up to 49 percent. But if theres one element of linguistic diversity thats tended to trip it up, its accents  only recently did Now gainofficial support for Indian and Australian dialects. Reportedly, though, Google has aplan to improve things: recruiting users of Reddit. Reddit, a social network perhaps as well known for its internet activism as its controversial upper management, is reportedly serving as a recruitment pool for Google voice volunteers. The Mountain View, California-based company has retained the services of a third-party firm, Appen, that has begun hiring Reddit users  or Redditors, as theyre colloquially known  with specific accents for the purpose of improving Googles voice recognition engine. Gig listings by Appen began appearing this week on a number of subreddits  Reddits term for the individual communities that live under the broader networks umbrella. The ads are equitably directed at users searching for part-time work  i.e., Redditors of /r/slavelabour, /r/WorkOnline /r/beermoney and those who live in cities with high concentrations of distinctive inflections, like /r/Edinburgh. Theyre all seeking the same: users with particular linguistic cadences who will submit to the [collection of] speech data. Im currently recruiting to collect  data for Google, read one request, since removed, on /r/slavelabour.It requires you to use an Android to complete the task. The task is recording voice prompts like Indy now, [and] Google whats the time. Each phrase takes around 3-5 seconds. The work in whole is fairly involved, apparently  participants are required to recite 2,000 individual phrases over the course of three hours  but rewarded generously in cold, hard cash. Adults earn 27 pounds ($36), and kids under 16 earn slightly less  20 pounds ($26)  but they read from a shorter, 45-minute script of 500 phrases. Google appears to be focusing on one accent in particular: that of the Scottish variety. Its a relatively tough inflection to nail, according to Quartz  its peculiar cadence frequently trips up voice assistants from Now to Apples Siri on the iPhone and iPad. The training sessions are relatively straightforward. Participants who spoke to The Verge  a diverse bunch with accents from the U.K. and America in addition to more exotic dialects, including Indian and Chinese-accented English  reported being directed to a mobile onboarding webpage. After tapping a record icon on thatpage, phrases appeared in sequence. Some snippets referenced Google, apparently  OK Google, and Hey, Google  while others included brand names, toys, video games, movie titles, and YouTube channel names. And still others ran the gamut: queries from Google searches like How to make a birthday cake; idioms like Hey Google, get cold feet, and even trivia questions (Presidents in order). Samples, once collected, are processed by Aspens in-house team.Company chief Mark Brayan, who spoke to The Verge, broke down the workflow:employeesanalyze recordings from around the world in 130 languages, distilling sentences down into their grammatical fundamentals. In a subsequent process Aspen calls decoration, the linguists make contextual annotations, noting such details as the environment in which the recordings were made  outdoors, for instance, or in a crowded hallway  and the device used to conduct them. Its an arduous undertaking, according to Brayan. Minor improvements require massive quantities of data and analysis. To go from understanding 95 percent of words to 99 percent, the recognizer has to digest infrequently used words, of which there are millions,Brayan told The Verge. And unusual terms like esoteric product names are even more problematic  Appen must account not only for familiar pronunciations of suchwords, but unique pronunciations of them, too. One of the big challenges is what we call named entity recognition, Brayan said. Thats brand names, product names, individual names, and so on. So if youre launching in Canada, for example, you need not only the French language but also French-accented Canadian English. Related: Want to talk to your PC Heres how to enable speech to text in Windows 10 The ideal endresult Leaps and bounds in voice recognition. Marsal Gavalda, headof machine intelligence at Yik Yak, said that historically, thecapabilities of speech recognition systems have been limited by the homogeny of the data ingested. [Such systems] have been trained from data collected mostly in universities, and mostly from the student population, he told the Verge. He has a term for it: electronic imperialism. The [diversity of voices] reflect the student population 30 years ago, Gavalda said. Already, the situation is improving albeit marginally. Google misinterprets words in tier 2 languages  the less popular languages to which companies like Google and Apple devote less attention much less frequentlythan it once did. Over the past two years alone, the word errorrate for Indonesian has decreased from 40 percent to 18 percent, Googles chief of speech recognition Johan Schalkwyk told Fusion . But companies like Google have a long way to go  Schalkwyk said the companys voice recognition engine needs at least 5,000 hours of voice data to understand a language well. Google, it seems, is going to need a lot more accented Redditors. ",http://www.digitaltrends.com/mobile/google-reddit-voice-recognition/,"Google Now, search giant Google's eponymous voice assistant, has a surprisingly good grasp on the nuances of human speech. Thanks to a ...",Google is recruiting Reddit users to improve speech recognition,Get DigitalTrends Googles August Google Google Reddit Reddit Google Mountain View Appen Reddit Googles Gig Appen Reddits Redditors /r/slavelabour /r/WorkOnline /r/beermoney /r/Edinburgh Theyre [ ] Im Google Android Indy [ ] Google Adults Google Quartz Apples Siri Verge U.K. America English Google Google Hey Google YouTube Google How Hey Google Samples Aspens Mark Brayan Verge Aspen Brayan Brayan Verge Appen Brayan Thats Canada French-accented Canadian English Windows Leaps Marsal Gavalda Yik Yak Verge Gavalda Already Google Google Apple Indonesian Googles Johan Schalkwyk Fusion Google Schalkwyk Google Redditors,4
229,"Voice and Speech Recognition Technology 2015-2022 - Global Strategic        Business Report 2016 - Rapid Penetration of Mobile Devices: A Major        Growth Driver - Research and Markets DUBLIN--( BUSINESS WIRE )--Research and Markets has announced the addition of the ""Voice        and Speech Recognition Technology - Global Strategic Business Report"" report to their offering. This report analyzes the worldwide markets for Voice and Speech        Recognition Technology in US$ Thousand. The report provides separate        comprehensive analytics for the US, Canada, Japan, Europe, Asia-Pacific,        Latin America, and Rest of World. Annual estimates and forecasts are provided for the period 2015 through        2022. Also, a six-year historic analysis is provided for these markets.        Market data and analytics are derived from primary and secondary        research. Company profiles are primarily based on public domain        information including company URLs. The Global market is additionally analyzed by the following        Segments: Applied Voice & Speech Technologies, Inc. (US) ",http://www.businesswire.com/news/home/20161027005737/en/Voice-Speech-Recognition-Technology-2015-2022---Global,This report analyzes the worldwide markets for Voice and Speech Recognition Technology in US$ Thousand. The report provides separate ...,Voice and Speech Recognition Technology 2015-2022 - Global ...,Voice Technology Strategic Business Report Penetration Mobile Devices Growth Driver Markets DUBLIN BUSINESS WIRE Research Markets Voice Technology Global Strategic Business Report Voice Speech Recognition Technology US Thousand US Canada Japan Europe Asia-Pacific Latin America Rest World Market URLs Applied Voice Speech Technologies Inc. US,3
230,"Chloe has deep technological innovation at her core and works with organizations to solve real business problems. We are proud to work with Avaya to create the next generation of customer support Artificial Intelligence"" Fabio Cardenas, CEO, Sundown AI New York City, New York (PRWEB) March 27, 2017 Sundown AI, a company that builds self-learning Artificial Intelligence applications, today announced it has been selected by Avaya for membership as a Technology Partner in the Avaya DevConnect program. Avaya is a leading global provider of business communications software, systems and services. Avaya solutions enable customer and team engagement across multiple channels and devices for better customer experience, increased productivity and enhanced financial performance. Sundown AI is the developer of Chloe, a self-learning Artificial Intelligence application that is powered by NLP (Natural Language Processing), graph algorithms and machine learning to answer repetitive questions from emails, SMS, social media or chats. As a DevConnect Technology Partner, Sundown AI expects to deliver proven interoperability of its solutions with the Avaya customer engagement, mobility and small business portfolios, helping companies automate customer service and reduce labor costs. The Avaya DevConnect program promotes the development, compliance-testing and co-marketing of innovative third-party solutions that are compatible with standards-based Avaya solutions. Member organizations have expertise in a broad range of capabilities  spanning collaboration, management, analytics, reporting and communications-enabled business process applications  helping joint customers extend the value of their collaboration and contact center investments and accelerate the speed at which their organization delivers true value to the bottom line. Chloe has deep technological innovation at her core and works with organizations to solve real business problems.  We are proud to work with Avaya, a global leader in information communication technology, to create the next generation of customer support artificial intelligence said Fabio Cardenas, President of Sundown AI The Avaya DevConnect program currently includes thousands of software and hardware developer companies, integrators, service providers and customers. Members have created a broad array of innovative solutions tested for Avaya compliance, including natural language speech recognition applications, mobile and emergency notification services, specialized computer telephony integration and reporting capabilities, and applications tailored for specific vertical industries. Through the DevConnect program, Avaya provides companies with a wide range of technical education, access and support for many Avaya platforms and interfaces, often at no cost to Registered members. Technology Partners receive additional benefits in terms of in-depth, joint compliance testing activities and co-marketing support, based on their alignment with Avaya strategy and value offered to Avaya customers.  DevConnect Technology Partners like Sundown AI must meet rigorous criteria for customer satisfaction, product support, business operations, marketing and sales. Technology Partners like Sundown AI are helping Avaya meet customer demand for leading-edge engagement methods such as advanced AI. said Eric Rossman, vice president, Developer Relations, Avaya. Avaya enables the mission critical, real-time communication applications of the worlds most important operations. As the global leader in delivering superior communications experiences, Avaya provides the most complete portfolio of software and services for contact center and unified communications with integrated, secure networking offered on premises, in the cloud, or a hybrid. Todays digital world requires some form of communications enablement, and no other company is better positioned to do this than Avaya. For more information, please visit http://www.avaya.com . Sundown AI builds self-learning Artificial Intelligence applications that are powered by NLP, graph algorithms and machine learning. They automate customer service and sales via existing business suites to help companies grow. Chloe, an AI system, provides companies with business solutions to reduce costs, boost productivity and increase customer satisfaction. Custom integrations are available via an API. Learn more at http://www.sundown.ai . Share article on social media or email: ",http://www.prweb.com/releases/2017/03/prweb14184240.htm,"Members have created a broad array of innovative solutions tested for Avaya compliance, including natural language speech recognition ...",Sundown AI Selected for Membership in Avaya DevConnect Program,Chloe Avaya Artificial Intelligence Fabio Cardenas CEO Sundown AI New York City New York PRWEB March Sundown AI Artificial Intelligence Avaya Technology Partner Avaya DevConnect Avaya Avaya Sundown AI Chloe Artificial Intelligence NLP Natural Language Processing SMS DevConnect Technology Partner Sundown AI Avaya Avaya DevConnect Avaya Member Chloe Avaya Fabio Cardenas President Sundown AI Avaya DevConnect Avaya DevConnect Avaya Avaya Registered Technology Partners Avaya Avaya DevConnect Technology Partners Sundown AI Technology Partners Sundown AI Avaya AI Eric Rossman Developer Relations Avaya Avaya Avaya Todays Avaya Sundown AI Artificial Intelligence NLP Chloe AI Custom API Learn Share,1
231,"A former Nasa chief has spoken for the first time about a new form of speech recognition technology developed in secret by his stealth startup which is claimed to eclipse current services offered by Apple, Google & Microsoft. Machine learning firm KnuEdge, established under the radars by Dan Goldin, has lifted the lid on its first product, KnuVerse, a voice recognition tool designed to be used as a form of biometric audio fingerprint to allow people to access banking and other services without the need for a PIN or password. Goldin said: We are not about incremental technology. Our mission is fundamental transformation. We were swinging for the fences from the very beginning, with intent to create next-generation technologies that will in essence alter how humans interact with machines, and enable next-generation computing capabilities ranging from signal processing to machine learning. At the heart of the new technology is a new computer chip dubbed Knupath which, it is claimed, is superior at processing machine learning and AI tasks than existing chipsets. ",http://www.thedrum.com/news/2016/06/06/former-nasa-boss-gives-voice-fundamental-transformation-speech-recognition,A former Nasa chief has spoken for the first time about a new form of speech recognition technology developed in secret by his 'stealth startup' ...,Former Nasa boss gives voice to 'fundamental transformation' of ...,Nasa Apple Google Microsoft Machine KnuEdge Dan Goldin KnuVerse PIN Goldin Knupath AI,9
232,"OCBC Singapore launches voice biometrics, speech recognition in contact center OCBC Bank in Singapore has launched voice metrics and speech recognition in its contact center to improve the retail customer experience. According to the banks head of consumer financial services Dennis Tan, the solutions launched reduce the time taken for customer verification, giving customers quicker access to services required. The voice biometric authentication replaces PINs, one-time passwords and security questions at the banks contact centre. Customers can use their voices as vocal passwords for authentication. Voice biometrics was launched by OCBC in September last year to a targeted group of retail customers. With the technology, customers could use their voiceprints to authenticate requests for account balances, latest transactions and the status of deposited cheques. Voice biometrics will be available to the banks retail customers in the fourth quarter of this year, with customers expected to be able to use their voiceprints to authenticate a majority of banking transactions. To enrol their voiceprint, customers are asked to say a specific phrase, called a passphrase, three times. A passphrase is an explicit sentence crafted by OCBC Bank to be spoken by the customer into the system to capture the customers voice. The voiceprint is created using the spoken passphrase and stored in the systems database. A voiceprint is not a recording of a voice but a digital representation of a persons vocal characteristics, so it cannot be disguised and is not affected by emotion or a blocked nose. To authenticate a banking transaction, the customer will be asked to say the passphrase that was used to enroll his or her voiceprint. If further verification is needed to confirm the customers initial vocal password is valid and is not a voice recording, the system will then ask the customer to say a different sentence from the enrolled passphrase. The customers voice is captured and is compared with the relevant stored voiceprint on the database. A verification result is then provided by the system. The authenticationprocess is hassle-free and can be done in 15 seconds. OCBC Bank launched speech recognition at its contact center in April this year to all personal banking customers. While voice biometrics enhances customer experience by replacing PINs, passwords and security questions, speech recognition replaces the need to select service options via the phone keypad. The deployment also reduces the number of steps needed to enter the options sequentially on the keypad to access a particular service. Speech recognition technology recognizes and understands a customers spoken request, thereby enabling the customer to access the required service faster and more accurately. According to OCBC Bank, the top customer enquiries received via speech recognition are checking recent transactions and account balances, requests for fee waivers, and Internet Banking and statement enquiries. These requests amount to 30.4 per cent of all requests to the Contact Centre. The success rate of the speech recognition service has been extremely high, says OCBC Bank, with 90 per cent of customers having their spoken requests recognised by the system. ",http://www.enterpriseinnovation.net/article/ocbc-singapore-launches-voice-biometrics-speech-recognition-contact-center-754920513,OCBC Bank in Singapore has launched voice metrics and speech recognition in its contact center to improve the retail customer experience.,"OCBC Singapore launches voice biometrics, speech recognition in ...",OCBC Singapore OCBC Bank Singapore Dennis Tan PINs Voice OCBC September Voice OCBC Bank OCBC Bank April PINs OCBC Bank Internet Banking Contact Centre OCBC Bank,-1
233,"Ahead of the 102nd anniversary of the Armenian Genocide, the Parliament of Cyprus has called on the international community to recognize the Armenian Genocide, Ermenihaber reports citing Kibrispostasi news agency. At the beginning of the Parliament session, Speaker Demetris Syllouris delivered a speech noting that Turkey has implemented a policy of ethnic cleansing against the Armenian people, and although 100 years have passed since the committal of the Armenian Genocide, Turkey continues to deny it. Syllouris noted that Cyprus was among the first states in the world to recognize and condemn the Armenian Genocide, and in 1990, the Parliament of Cyprus declared April 24 as Armenian Genocide Remembrance Day. The Parliament has also adopted the law on the criminalization of the genocide denial and the war crimes against the humanity. Armenian MP Vartkes Mahdessian also delivered a speech at the Parliament, noting that the perpetrators of the Armenian Genocide still remain unpunished. More than one hundred year has passed. We do not seek vengeance, we seek justice. We want the historical facts over this tragedy to be unanimously accepted, he added. ",http://www.panorama.am/en/news/2017/04/08/Cyprus-Parliament-Armenian-Genocide/1758303,"At the beginning of the Parliament session, Speaker Demetris Syllouris delivered a speech noting that Turkey has implemented a policy of ...",Cyprus Parliament calls for Armenian Genocide recognition,Genocide Parliament Cyprus Genocide Ermenihaber Kibrispostasi Parliament Speaker Demetris Syllouris Turkey Genocide Turkey Syllouris Cyprus Genocide Parliament Cyprus April Genocide Remembrance Day Parliament MP Vartkes Mahdessian Parliament Genocide,10
234,"Is battery consumption of your devices a serious concern for you So heres something you should know. A research team at MIT has recently designed a low-power special-purpose chip that claims to decrease the power utilization of electronic devices and as a result, helps them save battery. The chip is dedicated to automatic speech recognition and has been asserted to need 0.210 mW in comparison with 1 W needed by a standard smartphone operating the speech recognition software. The research team states that such a significant variation in power necessity efficiently by the new chip may lead to power saving up to 9099%. Thus, such an extreme decrease in power utilization can enable the use of speech recognition in more number of devices, specifically in those where power restriction was hindering the application of this technology. Moreover, the devices size is getting smaller day by day, and hence more power elements are used, the battery life problem solved by this chip can open new avenues for the hardware developers. If you are thinking how the chip handles saving power, then the reason behind it is the well-organized execution of speech recognition networks. In addition, the chip consists of a voice activity recognition circuit that parts the surrounding noise to verify whether it may be a speech or not. Its after the confirmation of being a speech by this circuit, a bigger and more compound voice recognition circuit is given a green signal to get functional. What do you think how much helpful this chip would be for a user Feel free to share your opinion. ",http://www.technologynewsextra.com/new-speech-recognition-chip-saves-power-99/6331.html,The chip is dedicated to automatic speech recognition and has been asserted to need 0.2–10 mW in comparison with 1 W needed by a ...,New speech recognition chip saves power up to 99%,MIT W Feel,-1
235,"Is battery consumption of your devices a serious concern for you So heres something you should know. A research team at MIT has recently designed a low-power special-purpose chip that claims to decrease the power utilization of electronic devices and as a result, helps them save battery. The chip is dedicated to automatic speech recognition and has been asserted to need 0.210 mW in comparison with 1 W needed by a standard smartphone operating the speech recognition software. The research team states that such a significant variation in power necessity efficiently by the new chip may lead to power saving up to 9099%. Thus, such an extreme decrease in power utilization can enable the use of speech recognition in more number of devices, specifically in those where power restriction was hindering the application of this technology. Moreover, the devices size is getting smaller day by day, and hence more power elements are used, the battery life problem solved by this chip can open new avenues for the hardware developers. If you are thinking how the chip handles saving power, then the reason behind it is the well-organized execution of speech recognition networks. In addition, the chip consists of a voice activity recognition circuit that parts the surrounding noise to verify whether it may be a speech or not. Its after the confirmation of being a speech by this circuit, a bigger and more compound voice recognition circuit is given a green signal to get functional. What do you think how much helpful this chip would be for a user Feel free to share your opinion. ",http://www.technologynewsextra.com/new-speech-recognition-chip-saves-power-99/6331.html,The chip is dedicated to automatic speech recognition and has been asserted to need 0.2–10 mW in comparison with 1 W needed by a ...,New speech recognition chip saves power up to 99%,MIT W Feel,-1
236,"The festivities kicked offwith an all-star tribute to Chuck Berry, recognized as the first musician the Rock Hall ever inducted. Following a video presentation remembering Berry's life, ELOs Jeff Lynne took the stage minutes before his own band was inducted into the Rock Hall, honoring Berry with his own take on Roll Over Beethoven. When Joan Baezs folk-music peer Jackson Browne took the stage to induct the legendary protest singer into the Rock Hall, he acknowledged that the recognition was long, long overdue. In a touching speech, where he admitted Baezs was the first album he ever bought as a young music fan, Browne spoke about how her musics dedication to social justice is still relevant in todays political climate. When I hear (her recordings) now, I feel a deep sadness that the songs are as needed now as then, now more than ever, he said. The changes that began happening in the 60s are still happening ... we need to be as empowered now as we were then. Baezs speech touched on the same political themes as Brownes, as she spoke about her history of representing disempowered parties in both her music and her activism. She also dedicated her speech to her granddaughter, who she said had no idea who I was until I took her backstage at a Taylor Swift concert, only to have the younger pop star greet her warmly. I want my granddaughter to know I fought against an evil tide and had the masses on my side, Baez said. In a candid and touching remembrance for his former labelmate and close friend, Snoop Dogg tackled the duties of inducting Tupac Shakur into the Rock Hall, recalling the pairs rise through 90s hip hop together in a six-minute speech. Snoop remembered how Pac gave him his first blunt, accompanied him on an unexpected parasailing trip with rap mogul Suge Knight, and outfitted him in a matching designer suit. Calling Shakur the greatest rapper alive, Snoop recalled visiting the rappers mother after his death, comforted by her strength. Following his emotional speech was a stacked tribute performance. Led by Alicia Keys on piano, Snoop returned to the stage, accompanied by fellow rappers YG and T.I. to deliver a mashup of Pacs greatest hits. In honor ofJourney's Rock Hall induction,former frontman Steve Perry reunited with the band for the first time in 12 years. While he didn't perform with Journey, he joined his former bandmates onstagefor a heartfelt speech thanking his fans and family. ""You put us here, we would not be here had it not been for you,"" Perry said, thanking his fans. ""Your tireless love and consistent devotion, you never have stopped. But from my heart I must tell you  I've been gone a long time, but you've always been in my heart."" While Nile Rodgers wasn't the most famous face to take the Barclays stage Friday, the legendary producer likely has the longest rsum, producing hits for icons including Madonna, David Bowie and Diana Ross. After an introduction from Pharrell Williams, who collaborated with Rodgers on Daft Punk's 2013 hit Get Lucky, Rodgers dedicated his speech to praising the many names he's worked with over his career. As he bragged, hes worked with a large chunk of the people in the Rock Hall already. ""When people work with me they think I'm the boss but for every record, I join their band,"" he said. ""I want to make every artist know I have their interest at heart. My name doesn't mean (expletive). ""This award is reallybecause of all the people who allowed me to come into their lives and join their band, be it Madonna, Mick Jagger, Bowie, Pharrell, Diana Ross ... it just goes on and on and on,"" he added. Nile Rodgers greets his fans. While many awards shows have featured Prince tributes since the singer's death one year ago this month, not every performance did justice to the pop icon. Lenny Kravitz attempted to avoid the self-indulgent pitfalls of past Prince remembrances by limiting his Rock Hall tribute to two tracks,When Doves Cry and The Cross. Instead of attempting to imitate the Purple One in his singing, dancing and guitar skills, Kravitz wisely found strength in numbers during his performance, inviting a powerful choir onstage to bolster his vocals.  Maeve McDermott (@maeve_mcdermott) April 8, 2017 Neil Young's loss was theceremony's gain. After the rocker dropped out of the ceremony due to illness, former Late Show host David Letterman filled his spot to deliver the night's most freewheeling speech, where he shared his many memories with Pearl Jam, thebeloved Seattle rockers he was inducting into the Rock Hall. Pearl Jam frontman Eddie Vedder shared his own poignant memories of Letterman's hosting days. ""He doesn't know, but I used to work the midnight shift .. and there was a small TV and Dave was my co-pilot,"" he said. ""To have him up here, it's an honor."" Before his band closed out the night, Vedder also seized the opportunity to make a political statement of his own. ""Climate change is real, it's not fake news,"" he said. ""If the Chicago Cubs can win the World Series, humans can solve climate change,"" although ""we don't have 108 years to wait."" Read or Share this story: http://usat.ly/2obLFDV ",https://www.usatoday.com/story/life/music/2017/04/08/rock-and-roll-hall-of-fame-recap-best-speeches-tribute-performances/100201254/,"... induct the legendary protest singer into the Rock Hall, he acknowledged that the recognition was long, long overdue. In a touching speech, ...",Rock Hall recap: The ceremony's 8 big moments,Chuck Berry Rock Hall Berry ELOs Jeff Lynne Rock Hall Berry Roll Over Beethoven Joan Baezs Jackson Browne Rock Hall Baezs Browne Baezs Brownes Taylor Swift Baez Snoop Dogg Tupac Shakur Rock Hall Snoop Pac Suge Knight Shakur Snoop Alicia Keys Snoop YG T.I Pacs Rock Hall Steve Perry Journey Perry Nile Rodgers Barclays Friday Madonna David Bowie Diana Ross Pharrell Williams Rodgers Daft Punk Get Lucky Rodgers Rock Hall Madonna Mick Jagger Bowie Pharrell Diana Ross Nile Rodgers Prince Lenny Kravitz Prince Rock Hall Cry Cross Purple One Kravitz Maeve McDermott @ April Neil Young Late Show David Letterman Pearl Jam Seattle Rock Hall Pearl Jam Eddie Vedder Letterman .. Dave Vedder Climate Chicago Cubs World Series Share,-1
237,"Now that weve all become comfortable with Siri or Cortana, there are no longer any barriers when it comes to talking to our tech -- whether were dictating something or asking where the nearest Italian restaurant is because were craving pizza. Gone are the days of awkwardly making commands, stupid-looking headsets or monotone dictation. With that in mind, why wouldnt you use the same convenience to get more done at work or home Nuances Dragon software is the world leader in the speech recognition space and is trusted by over a million users worldwide. Furthermore, recent improvements to the brands latest speech recognition software, Dragon Professional Individual 15 for the PC and Dragon Professional Individual for Mac v6, means that speech recognition is no longer awkward or time-consuming and could actually make a huge difference to your productivity levels and how much you get done each day. The newest version of the software uses the latest in Deep Learning tech to adapt to your voice, as well as take into account any environmental variations, so youre getting a result that delivers to a really high standard. In fact, it boasts more than 99 percent recognition accuracy. And were not experts here, but we imagine thats more accurate than most of your hurried typing. Designed for those working in all kinds of fields, Dragon Professional Individual provides you with robust transcription, lots of customisation options and the chance to sync everything up with the brands Dragon Anywhere mobile dictation solution as well. So youve got unrivalled dictation whether youre on-the-go or at your PC. The great thing about Dragon Professional is its a professional solution thats robust and comprehensive, but it can be up-and-running really quickly. Its also easy-to-install, easy-to-use and gives you the competitive edge whether youve got a PC or a Mac. Try Dragon Professional Individual speech recognition for yourself today. It comes with a 30 day money back guarantee if youre not happy. ",http://www.lifehacker.co.uk/2016/10/04/dump-your-keyboard-and-use-your-voice-instead-with-dragon-speech-recognition-from-nuance,"Furthermore, recent improvements to the brand's latest speech recognition software, Dragon Professional Individual 15 for the PC and Dragon ...",Dump Your Keyboard and Use Your Voice Instead With Dragon ...,Siri Cortana Dragon Furthermore Dragon Professional Individual Dragon Professional Individual Mac Deep Learning Dragon Professional Individual Dragon Anywhere Dragon Professional Mac Try Dragon Professional Individual,1
238,"Chennai-based speech recognition startup Uniphore Software Systems has raised $2 million (Rs 13.4 crore) in a bridge round led by IDG Ventures with participation of existing investors, including Indian Angel Network (IAN) and YourNest Angel Fund. We are happy with the kind of progress the company has made and how it has migrated from an enterprise model to a SaaS model. It has also got marquee brands as clients, said Girish Shivani, partner, YourNest Angel Fund told Techcircle.in. Uniphore was started in 2008 by Umesh Sachdev (CEO) and Ravi Saraogi (COO), both alumni of Jaypee University of Information Technology, Himachal Pradesh. Earlier, Sachdev also co-founded Singularis Technologies, and prior to that he worked at TeNet. Saraogi has experience in directing technology teams in initiatives spanning mobile theft security, wireless network development and mobile learning applications. The company has worked with over 70 enterprise customers and served over 4 million users. It has offices in India, Philippines and the UAE. Last year, the firm raised $3.5 million (Rs 20.3 crore) in Series A funding from Infosys co-founder Kris Gopalakrishnan along with IAN, IDG ventures and YourNest . In 2014, it raised an undisclosed amount from a group of investors led by Nagaraja Prakasam and MV Subramanian, both members of IAN. Uniphores other investors include Ray Stata, co-founder and chairman of Analog Devices Inc; IIT Madras Rural Technology and Business Incubator (RTBI), Villgro Innovations Foundation and National Research Development Corporation. ",http://techcircle.vccircle.com/2016/10/03/exclusive-speech-recognition-startup-uniphore-raises-2-mn-led-by-idg-ventures/,Uniphore1 Chennai-based speech recognition startup Uniphore Software Systems has raised $2 million (Rs 13.4 crore) in a bridge round led ...,Exclusive: Speech recognition startup Uniphore raises $2 mn led by ...,Uniphore Software Systems Rs IDG Ventures Angel Network IAN YourNest Angel Fund SaaS Shivani YourNest Angel Fund Techcircle.in Uniphore Umesh Sachdev CEO Ravi Saraogi COO Jaypee University Information Technology Himachal Pradesh Earlier Sachdev Singularis Technologies TeNet Saraogi India Philippines UAE Rs Series A Infosys Kris Gopalakrishnan IAN IDG YourNest Nagaraja Prakasam MV Subramanian IAN Ray Stata Analog Devices Inc IIT Madras Rural Technology Business Incubator RTBI Villgro Innovations Foundation National Research Development Corporation,-1
239,"Speech & Voice Recognition Market by Technology, Application, Vertical and Geography - Global Forecast to 2022 NEW YORK, June 21, 2016 /PRNewswire/ -- The speech recognition market is expected to grow from USD 3.73 billion in 2015 to reach USD 9.97 billion by 2022, at a CAGR of 15.78% during the forecast period. The voice recognition market is expected to grow from USD 440.3 million in 2015 to reach USD 1.99 Billion by 2022, at a CAGR of 23.66% between 2016 and 2022. The factors driving the speech recognition market are the increasing acceptance of speech recognition solutions in the healthcare and banking, financial services, and insurance sectors for time and attendance monitoring. On the other hand, the growth of the voice recognition market is driven by factors such as the increasing acceptance of biometric technologies such as voice verification for remote authentication applications. The installation of biometric security for mobile banking is driving the demand for voice recognition technology in the banking & finance sector. The governments in various countries are among the major end users of voice recognition technology. The growing adoption of automated and smart applications in consumer and healthcare industries is the major contributor for the growth of the speech recognition market. The speech recognition technology holds the major market size because of its increasing adoption by major players such as Google, Apple, and Microsoft. Self-service applications in Asia-Pacific emerged in 2008 as the developing economies witnessed strong organic growth in sectors such as telecom and banking. The growth in the customer care services in various verticals such as banks, telecom, travel, and hospitality contributes to the growth of the speech & voice recognition market to combat the increasing occurrence of fraudulent activities in these sectors. China, Japan, and India are the fastest-growing economies in Asia-Pacific. Their financial strength enables them to make significant investments in the biometric sector for security-related applications. Several government projects are increasingly using voice biometrics for the purpose of security and authentication. The presence of major consumer electronics device manufacturers such as Samsung (South Korea) and Sony (Japan) creates a lucrative market for speech & voice recognition technology in this region. In the process of determining and verifying the market size for several segments and subsegments gathered through secondary research, extensive primary interviews have been conducted with people holding key positions across several regions. The breakup of the profile of primary participants is given below: Major players in the speech & voice recognition market are Nuance Communications (U.S.), Microsoft Inc. (U.S), Agnitio SL (Spain), Biotrust (Netherland), VoiceVault (U.S.), VoiceBox Technologies Corp. (U.S.), LumenVox LLC. (U.S.), M2Sys LLC (U.S.), Raytheon BBN Technologies (U.S.), M2SyS LLC (U.S.) ValidSoft UK Limited (U.K.), Advanced Voice Recognition Systems (U.S.), Sensory Inc. (U.S.), and MMODAL Inc. (U.S.). Reasons to Buy the Report: The report would help the market leaders/new entrants in this market in the following ways: 1. This report segments the speech & voice recognition market comprehensively and provides the closest approximations of the market sizes for the overall market and sub-segments across the different verticals and regions. 2. The report helps stakeholders to understand the pulse of the market and provides them information on key market drivers, restraints, challenges, and opportunities. ReportLinker is an award-winning market research solution. Reportlinker finds and organizes the latest industry data so you get all the market research you need - instantly, in one place. ",http://www.prnewswire.com/news-releases/speech--voice-recognition-market-by-technology-application-vertical-and-geography---global-forecast-to-2022-300288175.html,"NEW YORK, June 21, 2016 /PRNewswire/ -- The speech recognition market is expected to grow from USD 3.73 billion in 2015 to reach USD ...","Speech & Voice Recognition Market by Technology, Application ...",Speech Voice Recognition Market Technology Application Vertical Geography Global Forecast NEW YORK June USD USD CAGR USD USD Billion CAGR Google Apple Microsoft Self-service Asia-Pacific China Japan India Asia-Pacific Samsung South Korea Sony Japan Nuance Communications U.S. Microsoft Inc. U.S Agnitio SL Spain Biotrust Netherland VoiceVault U.S. VoiceBox Technologies Corp. U.S. LumenVox LLC U.S. M2Sys LLC U.S. Raytheon BBN Technologies U.S. M2SyS LLC U.S. ValidSoft UK Limited U.K. Advanced Voice Recognition Systems U.S. Sensory Inc. U.S. MMODAL Inc. U.S. ReportLinker Reportlinker,3
240,"Google researchers have developed a speech recognition system that runs  faster than real-time on a Nexus 5 Android smartphone. Moreover, it can sustain highspeed and accuracy while running offline. The researchers accomplished their aims partly by integrating contextual data directly into the app; for example, in order to process the voice command, Text Kendrick, On my way,' the system would need access to the users Contacts informationso they imported that data directly into the app. Meanwhile, they helped to compress the apps size by combining voice command and dictation algorithms into a single language model, ultimately attaining a memory footprintof only 20.3 MB. According to the researchers, that small size doesnt compromise the systems speed or accuracyand those metrics are impressive, too. Performing a natural speech dictation task, the system has a word error rate of only 13.5 percent, and its median speed is seven times faster than real-time, the researchers note. Its a further demonstration of Googles continuingand perhaps growinginterest in speech recognition technology , with the company having invested heavily in speech recognition R&D and recently introduced voice dictation for its Google Docs app. This technology could have growing applications in the emerging Internet of Things , and Google is likely betting that these investments will have considerable pay-offs down the line. ",http://mobileidworld.com/android-speech-recognition-operates-offline-103151/,Google researchers have developed a speech recognition system that runs faster than real-time on a Nexus 5 Android smartphone. Moreover ...,Android Speech Recognition Operates Offline and 'Faster Than Real ...,Google Nexus Android Text Kendrick MB Googles R D Google Docs Internet Things Google,4
241,"Get today's popular DigitalTrends articles in your inbox: Whether theyre something as serious as bomb threats or an apparent joke like a fake call to the Coast Guard, every year law enforcement agencies spend billions of dollars on hoax calls. Because of the potential seriousness of these calls, agencies are compelled to respond  which often means deploying personnel and wasting valuable resources. This also has a negative impact on the general public, sinceit means taking away resources from genuine emergencies which could otherwise have been attended to. Up until now law enforcement agencies have had few ways of knowing whether a potential hoax call was legitimate or not, or ofidentifying an anonymous caller. Thats changing thanks to the work of researchers like Rita Singh , a speech scientistat Carnegie Mellon. Singhs research focuses on using speech recognition algorithms to extract seemingly impossible amounts of detail from callers . According to Singh (and the law enforcement agencies she works with) this data can include everything from a persons gender and age to their height, weight, place of birth, ethnicity, level of intoxication, emotional state, and possible drug taking. Going further than that, other algorithms can reveal the possible facial structure of callers, and even precise details about their physical environment  such as the material of the walls and ceiling, any carpet, surrounding objects, and more. Not everything is present in all voice samples, but a smattering of [them] usually are, and we are able to find them accurately enough to send the law enforcement looking in the right directions, Singh told Digital Trends. While we hear a lot about biometrics like fingerprints, Singh explained that voice is perhaps the most important biomarker of all  since it can reveal so many details of aa person. When people commit crimes through voice, they dont realize this kind of technology exists, she continued. People will try to disguise their voice, particularly if theyre a repeat offender. What were working on is coming up with the right parameters so that we can tell law enforcement which aspects of peoples voice they are able to disguise and which aspects cannot be changed because theyre not under the speakers voluntary control. Its these second parts that we focus on because its what gives us the most useful, accurate information. The work was originally developed by the Coast Guard Investigative Agency, and Singh explained that it is funded by the Department of Homeland Security. Not all of the applications are about criminal cases, however. For instance, earlier this year Singh lent her analysis of vocal micro-features to determining whether Donald Trump had posed as public-relations man John Miller in a 1991 phone interview, which he publicly denied. (According to Singh, based on recordings of Trump at the time, it was indeed him .) As impressive as this work is, however, Singh told Digital Trends that it is not yet at the point of being fully automated. When I get a voice print from a crime scene, it takes me more than a week to extract all of the information I can squeeze out of it, she said. There are plenty of algorithms that we have at our disposal, but you have to choose the right one. There isnt a process in place that will choose the right ones for you. Its going to take a few years still. But its already a very powerful tool. ",http://www.digitaltrends.com/cool-tech/vocal-algorithm-profiling/,Singh's research focuses on using speech recognition algorithms to extract seemingly impossible amounts of detail from callers. According to ...,Criminal profiling tech can pin down your background based entirely ...,Get DigitalTrends Whether Coast Guard Thats Rita Singh Carnegie Mellon Singhs Singh [ ] Singh Digital Trends Singh Coast Guard Investigative Agency Singh Department Homeland Security Singh Donald Trump John Miller Singh Trump Singh Digital Trends,12
242,"Cambridge speech recognition technology startup Speechmatics is seeking up to 20 new recruits in a rapid scale-up. It recently launched Universal Time Alignment  a language-independent forced-alignment service to match words in text files to their counterparts in audio files. The technology accurately and automatically delivers improved content discoverability, in any language, and the company is leveraging an Arabic version. Speechmatics boasts that it provides some of the worlds fastest and most accurate speech recognition technologies, based in the cloud and leveraging the latest advances in deep learning neural networks. It already supports a number of large companies as well as SMEs. The technology was pioneered by Dr Tony Robinson 30 years ago at the University of Cambridge and is now pushing the boundaries in speaker-independent automatic speech recognition  claiming world-beating accuracy levels. Cambridge has become a world-leader in speech recognition technology  evidenced by Amazons swoop for Evi and Apples more recent acquisition of VocalIQ. Dr Robinson started his first company (SoftSound) in 1995, which was acquired by Autonomy in 2000; he subsequently led the speech recognition group and was a key architect of Autonomys success. Since leaving Autonomy in 2006, he has founded or been involved in a large number of speech-based startups  most notably SpinVox, where he built a fully automated voicemail-to-text system. Speechmatics says it can can extract audio from almost any common file format. Alongside generating a transcript, Speechmatics also provides speaker recognition (diarisation), punctuation and capitalisation. The company has begun to employ world-leading technology developed by the Qatar Computing Research Institute to help transcribe Arabic broadcasts and audio files into text and subtitles. It can transcribe modern standard Arabic as well as four major Arabic dialects  Egyptian, Levantine, North African and Gulf Arabic. Speechmatics recently recruited Neil MacDonald as chief revenue officer. He has enjoyed a long and successful career in the speech-tech industry having served at senior management levels within major players including Nuance, SVOX, iSpeech and HP. More recently he led the B2B sales team at SwiftKey, the latest UK tech-innovator to be acquired by one of the US mega-techs (Microsoft paid $250 million). MacDonald led the sales effort on some of the most significant advances in speech-enabled product to come to market, from the likes of Google, Nokia, Samsung, Motorola and Sony  and through his work with the Android platform added the voice to over 600,000,000 Android based devices. ",http://www.businessweekly.co.uk/news/hi-tech/cambridge-speech-recognition-firm-scales,Cambridge speech recognition technology startup Speechmatics is seeking up to 20 new recruits in a rapid scale-up. It recently launched ...,Cambridge speech recognition firm scales up,Cambridge Speechmatics Universal Time Alignment Arabic SMEs Dr Tony Robinson University Cambridge Cambridge Amazons Evi Apples VocalIQ Dr Robinson SoftSound Autonomy Autonomys Autonomy SpinVox Alongside Speechmatics Qatar Computing Research Institute Arabic Arabic Arabic Levantine North African Gulf Arabic Neil MacDonald Nuance SVOX HP B2B SwiftKey US Microsoft MacDonald Google Nokia Samsung Motorola Sony Android Android,-1
243,"50 years of Star Trek inspires innovations in mobile computers, speech recognition and tricorders Commander Spock and Captain James T. Kirk, played by Leonard Nimoy and William Shatner, pictured here in The Original Series by NBC Television. Public domain. On September 8th, 1966, the American broadcast network NBC aired the first episode of Star Trek , a science fiction television series conceived by Gene Roddenberry . In the decades which have followed, the original series which only lasted three seasons inspired a growing list of additional series and feature films, making a huge impact on both science fiction and pop culture. As the 50th anniversary of the first Star Trek episode comes around this September, fans around the world continue to be fascinated by the adventures of the crew of the USS Enterprise and the rest of the Starfleet as they explore space, the final frontier. Early August saw the 15th annual Star Trek Las Vegas convention where stars from the original series and cosplaying fans express their admiration for the show and its futuristic vision of a world where differences are explored and not feared. In Scotland, a free September 10th screening of Star Trek II: The Wrath of Khan will bring many fans to a 600-year old castle in Linlithgow , the Scottish town from which Enterprise crew member Scotty hails. This summer saw the release Star Trek Beyond , the latest addition to the franchises canon of movies. In January 2017, Star Trek: Discovery will become the newest TV series in the Star Trek franchise when it becomes available on CBS All Access, so the Star Trek universe will continue to expand in the future. One of the more interesting aspects of Star Trek is just how much of the shows science fiction has become technological reality. Earlier this year, we profiled the discovery of spinel, a material which closely resembles the transparent aluminum which played a large role in the 1986 movie Star Trek IV: The Voyage Home . With 50 years of scientific progress haven taken place since the first Star Trek episode aired, many of the technological wonders created for television and movies have either become commonplace or are being seen on the horizon of research and development. Star Trek: The Next Generation began its seven-year run in 1987, showing the adventures of the USS Enterprise under the direction of Jean-Luc Picard who was portrayed by British actor Patrick Stewart . It was the series which introduced viewers to the Personal Access Display Device (PADD), a handheld computing device used by Starfleet and others. Crew manifests, diagnostic reports and other data could be input through a touch-sensitive screen powered by printed circuit boards and were generally rectangular in shape. Most people watching The Next Generation or any of the following series in which PADDs were used would very quickly recognize that the devices closely resemble the tablet computers which have been commercialized over the past decade, most notably the iPad developed by consumer tech company Apple Inc. ( NASDAQ:AAPL ) of Cupertino, CA. Personal digital assistants (PDAs) had already become a popular piece of consumer gadgetry through the 1990s but they lack the touchscreen interfaces which have been realized both by tablet computers as well as smartphones. Interestingly, budget constraints appear to be a major reason why PADDs incorporated a large touchscreen and mainly utilized software-definable functions. Ars Technica reports that, compared to the Star Trek movies, the television series did not have large art budgets . The use of a large screen without control knobs and on which colored sheets could be displayed as the computer interface was a cheap option for creating a believable mobile computing platform. The Next Generation also introduced Star Trek fans to the replicator , a piece of technology which could produce food for consumption, mechanical parts for repairs and just about any other object imaginable, except for fatal poisons . Replicators used transporter technology which improved upon the performance of food synthesizers from the original series by dematerializing matter to get rid of waste and then rematerializing matter in its requested form. 3D printers may be a step behind replicators in terms of transportation technology but we do live in a world in which programmable printing devices can be sent instructions to create objects from toys to electronic components to food. This February, scientists at Wake Forest University announced that they had created a transplantable organ which was 3D printed . Edible printing filaments are used by the Foodini, developed by Natural Machines of Barcelona, Spain, which connects to cloud-based recipe sites so users can select their dinner. The waste reductions realized by 3D printing is also having an impact on the manufacturing sector, where the technology is known as additive manufacturing. American automaker Ford Motors ( NYSE:F ) and Chicago-based Boeing Company ( NYSE:BA ) were recently announced as partners with 3D printing developer Stratasys ( NASDAQ:SSYS ) of Eden Prairie, MI, and will incorporate Stratasyss 3D printing platforms into their manufacturing processes. Global spending on 3D printing tech is expected to reach $26.7 billion in 2019 according to the International Data Corporation. Tricorders , handheld devices which help Starfleet crew members detect atmospheric and biological data in unknown locales, appeared as early as the original Star Trek series in the 1960s. Star Trek tricorders are multipurpose and some versions have been finely tuned to detect diseases in the human body or to aid in starship engineering. Although tricorders may have different purposes, every tricorder achieves three functions: sensing conditions, computing the data and then recording that data. The first manmade device marketed as a tricorder was the TR-107 Mark 1 , a device first released in 1996 by the now-defunct Vital Technologies Corporation. It was capable of measuring electromagnetic fields and offered other components including a clock, a timer, a thermometer and a barometer. Outside of this, there have been some limited developments on the road towards a true functioning tricorder. Late last year, media outlets were reporting on research out of Stanford University which could result in a medical tricorder which can detect tumors within a human body from up to a foot away . Verily, the life sciences arm of tech conglomerate Alphabet Inc. ( NASDAQ:GOOGL ), has also announced its work on a cancer-detecting tricorder, although development of that project seems to have stalled in recent months . From the earliest days of Star Trek, Captain Kirk and other crew members of the USS Enterprise could communicate with the ships computer through voice command. The technology conceived by Roddenberry and other Star Trek writers was very sophisticated and handled natural language speech interactions with ease . Those with a great deal of Star Trek trivia knowledge are probably already aware that the voice of the computer in most of the series and movies was provided by Majel Barrett-Roddenberry , the wife of Gene Roddenberry. Research into speech recognition technologies extends all the way back to Bell Labs in the 1950s but its only in recent years that commercially successful software systems for voice commands have been developed. In October 2011, Apple added the voice recognition technology known as Siri to its iOS mobile devices. Some reports indicate that an iOS update this September will be the first to connect Siri to a deep neural network to improve Siris sentence recognition and make her speech sound more human. Natural language processing is also achieved by the Watson cognitive computing system developed by American tech powerhouse IBM ( NYSE:IBM ). Although Watson cannot hold conversations like Siri or the USS Enterprises computer, its ability to process language to perform computing tasks is impossible to ignore here. This April, officials from Watsons development team reportedly announced that Watson was capable of processing languages in the English language with a 6.9 percent word error rate , down from a previous 8 percent word error rate. Steve Brachmann is a writer located in Buffalo, New York. He has worked professionally as a freelancer for more than a decade. He has become a regular contributor to IPWatchdog.com, writing about technology, innovation and is the primary author of the Companies We Follow series. His work has been published by The Buffalo News, The Hamburg Sun, USAToday.com, Chron.com, Motley Fool and OpenLettersMonthly.com. Steve also provides website copy and documents for various business clients. Warning & Disclaimer: The pages, articles and comments on IPWatchdog.com do not constitute legal advice, nor do they create any attorney-client relationship. The articles published express the personal opinion and views of the author and should not be attributed to the authors employer, clients or the sponsors of IPWatchdog.com. Read more . There are currently 1 Comment comments. ",http://www.ipwatchdog.com/2016/09/07/50-years-star-trek-inspires-innovations/id=72311/,"Commander Spock and Captain James T. Kirk, played by Leonard Nimoy and William Shatner, pictured here in The Original Series by NBC ...",50 years of Star Trek inspires innovations in mobile computers ...,Star Trek Commander Spock Captain James T. Kirk Leonard Nimoy William Shatner Original Series NBC Television Public September American NBC Star Trek Gene Roddenberry Star Trek September USS Enterprise Starfleet August Star Trek Las Vegas Scotland September Star Trek II Wrath Khan Linlithgow Enterprise Scotty Star Trek Beyond January Star Trek Star Trek CBS All Access Star Trek Star Trek Star Trek IV Voyage Home Star Trek Star Trek Generation USS Enterprise Jean-Luc Picard Patrick Stewart Personal Access Display Device PADD Starfleet Crew Generation PADDs Apple Inc. NASDAQ AAPL Cupertino CA Personal PDAs PADDs Ars Technica Star Trek Generation Star Trek February Wake Forest University Foodini Natural Machines Barcelona Spain Ford Motors NYSE Boeing Company NYSE Stratasys NASDAQ SSYS Eden Prairie MI Stratasyss International Data Corporation Starfleet Star Trek Star Trek TR-107 Mark Vital Technologies Corporation Late Stanford University Alphabet Inc. NASDAQ GOOGL Star Trek Captain Kirk USS Enterprise Roddenberry Star Trek Star Trek Majel Barrett-Roddenberry Gene Roddenberry Research Bell Labs October Apple Siri September Siri Siris Watson IBM NYSE IBM Watson Siri USS Enterprises April Watsons Watson Steve Brachmann Buffalo New York IPWatchdog.com Companies Follow Buffalo News Hamburg Sun USAToday.com Chron.com Motley Fool OpenLettersMonthly.com Steve Warning Disclaimer IPWatchdog.com IPWatchdog.com Read,-1
244,"was keen to emphasize the impressive facial recognition technology of UWP, its Windows Apps Teams latest and final developers blog post on FamilyNotes emphasizes its voice command and speech recognition functionality. Voice command is enabled through Cortana, Microsofts flagship AI assistant platform. It lets users launch the app simply by saying, Hey Cortana, start FamilyNotes; and more custom voice commands can be added by developers. But Cortana can also recognize natural language, enabling users to dictate text; and that dictated text can even be played back to the user. While the Windows Apps Team is also careful to highlight the stylus-based scribbling enabled by its Ink feature, its clear that voice interaction is expected to play a crucial role as a user interface for apps and products based on UWP. That reflects a broader arms race playing out in the IT and consumer electronics industries, as giants like Amazon and Apple strive to improve their voice-based interaction systems , anticipating growing importance for the technology as the Internet of Things and its many deviceslacking traditional keyboard interfaces continues to emerge. ",http://findbiometrics.com/microsoft-speech-recognition-uwp-307066/,"While Microsoft Microsoft Highlights Speech Recognition in UWP was keen to emphasize the impressive facial recognition technology of UWP, ...",Microsoft Highlights Speech Recognition in UWP,UWP Windows Apps Teams FamilyNotes Voice Cortana Microsofts AI Hey Cortana FamilyNotes Cortana Windows Apps Team Ink UWP IT Amazon Apple Internet Things,8
245,"Deep Speech 2, a speech recognition network developed by China's answer to Google, is so stunningly accurate it can transcribe Chinese better than a person, writes Will Knight Stroll through Sanlitun, a bustling neighbourhood in Beijing filled with tourists, karaoke bars and luxury shops, and you'll see plenty of people using the latest smartphones from Apple, Samsung and Xiaomi. Look closely, however, and you might notice some of them ignoring the touch screens on these devices in favour of something much more efficient and intuitive: their voice. A growing number of China's 691 million smartphone users now regularly dispense with swipes, taps and tiny keyboards when looking things up on the country's most popular search engine, Baidu. China is an ideal place for voice interfaces to take off, because Chinese characters were hardly designed with tiny touch screens in mind. But people everywhere should benefit as Baidu advances speech technology and makes voice interfaces more practical and useful. That could make it easier for anyone to communicate with the machines around us. ""I see speech approaching a point where it could become so reliable that you can just use it and not even think about it,"" says Andrew Ng Yan-tak, Baidu's chief scientist and an associate professor at Stanford University, in the United States. ""The best technology is often invisible and, as speech recognition becomes more reliable, I hope it will disappear into the background."" Voice interfaces have been a dream of technologists (not to mention science-fiction writers) for many decades. But in recent years, thanks to some impressive advances in machine learning, voice control has become a lot more practical. No longer limited to just a small set of predetermined commands, it now works even in a noisy environment, such as the streets of Beijing or when you're speaking across a room. Voice-operated virtual assistants such as Apple's Siri, Microsoft's Cortana and Google Now come bundled with most smartphones, and newer devices, such as Amazon's Alexa, offer a simple way to look up information, cue up songs and build shopping lists with your voice. These systems are not perfect, sometimes mishearing and misinterpreting commands in comedic fashion, but they are improving steadily, and they offer a glimpse of a graceful future in which there's less need to learn a new interface for every new device. Baidu is making particularly impressive progress, especially with the accuracy of its voice recognition, and it has the scale to advance conversational interfaces even further. The company - founded in 2000 as China's answer to Google, which is currently blocked in China- dominates the domestic search market, with 70 per cent of all queries. And it has evolved into a purveyor of many services, from music and movie streaming to banking and insurance. A more efficient mobile interface would come as a big help in China. Smartphones are far more common than desktops or laptops, and yet browsing the web, sending messages and doing other tasks can be painfully slow and frustrating. There are thousands of Chinese characters, and although Pinyin allows them to be generated phonetically from Latin ones, many people (especially those aged over 50) do not know the system. It's also common in China to use messaging apps such as WeChat to perform all sorts of tasks, such as paying restaurant bills. And yet in many of the poorer regions, where there is perhaps more opportunity for the internet to have big social and economic effects, literacy levels are still low. ""It is a challenge and an opportunity,"" says Ng, who has been honoured for his work in artificial intelligence and robotics at Stanford University in the United States. ""Rather than having to train people used to desktop computers in new behaviours appropriate for cellphones, many of them can learn the best ways to use a mobile device from the start."" Ng believes that voice may soon be reliable enough to be used for interacting with all sorts of devices. Robots or home appliances, for example, could be easier to deal with if you could simply talk to them. The company has research teams at its headquarters in Beijing and in California's Silicon Valley that are dedicated to advancing the accuracy of speech recognition and working to make computers better at parsing the meaning of sentences. Jim Glass, a senior research scientist at the Massachusetts Institute of Technology who has been working on voice technology for the past few decades, agrees that the timing may finally be right for voice control. ""Speech has reached a tipping point in our society,"" he says. ""In my experience, when people can talk to a device rather than via a remote control, they want to do that."" Last November, Baidu reached an important landmark with its voice technology, announcing that its Silicon Valley lab had developed a powerful speech recognition engine called Deep Speech 2. It consists of a very large, or ""deep"", neural network that learns to associate sounds with words and phrases as it is fed millions of examples of transcribed speech. Deep Speech 2 can recognise spoken words with stunning accuracy. In fact, the researchers found that it can sometimes transcribe snippets of Putonghua more accurately than a person. Baidu's progress is all the more impressive because Putonghua is phonetically complex and uses tones that transform the meaning of a word. Deep Speech 2 is also striking because few of the researchers in the California lab where the technology was developed speak Putonghua, Cantonese or any other variant of Chinese. The engine essentially works as a universal speech system, learning English just as well when fed enough examples. Most of the voice commands that Baidu's search engine hears today are simple queries - concerning tomorrow's weather or pollution levels, for example. For these, the system is usually impressively accurate. Increasingly, however, users are asking more complicated questions. To take them on, last year the company launched its own voice assistant, called Duer, as part of its main mobile app. Duer can help users find cinema screening times or book a table at a restaurant. The big challenge for Baidu will be teaching its AI systems to understand and respond intelligently to more complicated spoken phrases. Eventually, Baidu would like for Duer to take part in a meaningful back-and-forth conversation, incorporating changing information into the discussion. To get there, a research group at Baidu's Beijing offices is devoted to improving the system that interprets users' queries. This involves using the kind of neural-network technology that Baidu has applied in voice recognition, but it also requires other tricks. And Baidu has hired a team to analyse the queries fed to Duer and correct mistakes, thus gradually training the system to perform better. ""In the future, I would love for us to be able to talk to all of our devices and have them understand us,"" Ng says. ""I hope to someday have grandchildren who are mystified at how, back in 2016, if you were to say 'Hi' to your microwave oven, it would rudely sit there and ignore you."" The MIT Technology Review magazine recently identified 10 breakthrough technologies - those that are most likely to solve a big problem and open up new opportunities - for 2016. One is conversational interfaces, as discussed in the Baidu article. Below are the other nine: Immune engineering: the technology to produce and edit T cells, which play a crucial role in determining how effective a person's immune system is, is coming on in leaps and bounds. Precise gene editing in plants: a new gene-editing method is providing a precise way to modify crops in the hope of increasing yields and making them more drought- and disease-resistant. The technology is known as CRISPR and already a lab in China has used it to create a fungus-resistant wheat; several groups in the mainland are using the technique on rice in efforts to boost yields; and a group in Britain has used it to tweak a gene in barley that helps govern seed germination, which could aid efforts to produce drought-resistant varieties. Reusable rockets: rockets typically are destroyed on their maiden voyage, but now they can make an upright landing and be refuelled for another trip, setting the stage for a new era in spaceflight. Robots that teach each other: this will be an important step in preparing robots to perform many of those arduous or dangerous tasks we'd like not to do ourselves, such as packing items in warehouses, assisting bedridden patients or aiding soldiers on the front lines. DNA App store: United States company Helix is attempting to create the first ""app store"" for genetic information. Helix's idea is to collect a spit sample from anyone who buys a DNA app, sequence and analyse the customers' genes, and then digitise the findings. This will make genetic information available to consumers at an unprecedentedly low price. Improved solar panels: at a time when conventional silicon-based solar panels from China have never been cheaper, the US is fighting back. American company SolarCity will soon begin producing solar panels with a technology that combines a standard crystalline-silicon solar cell with elements of a thin-film cell, along with a layer of a semiconductor oxide, making affordable panels that could have more than 22 per cent efficiency. Today's commodity silicon-based solar panels have efficiencies of between 16 and 18 per cent. Slack: the intra-office messaging system, often described as the fastest-growing workplace software the world has ever seen, gives you a centralised place to communicate with colleagues through instant messages and in chat rooms. Slack funnels messages into streams that everyone who works together can see, allowing you to ""overhear"" what is going on in an organisation or group. The Tesla autopilot: an ""autopilot""-enabled Tesla car can manage its speed, steer within and even change lanes, and park itself. Power from the air: technology that lets gadgets work and communicate using only energy harvested from nearby television, radio, cellphone or Wi-fi signals is fast becoming a commercial reality. Your weekly dose of Post Magazine direct to your inbox. A valid E-mail address is required. The email address is already in use. Please login to subscribe. Your captcha answer is not correct. THANK YOU! Your submission has been received. ",http://www.scmp.com/magazines/post-magazine/article/1925784/why-baidus-breakthrough-speech-recognition-may-be-game,"But people everywhere should benefit as Baidu advances speech ... The best technology is often invisible and, as speech recognition ...",Why Baidu's breakthrough on speech recognition may be a game ...,Deep Speech China Google Chinese Will Knight Stroll Sanlitun Beijing Apple Samsung Xiaomi Look China Baidu China Baidu Andrew Ng Yan-tak Baidu Stanford University United States Beijing Apple Siri Microsoft Cortana Google Amazon Alexa Baidu China Google China- China Pinyin Latin China Ng Stanford University United States Ng Robots Beijing California Silicon Valley Jim Glass Massachusetts Institute Technology Speech November Baidu Silicon Valley Deep Speech Deep Speech Putonghua Baidu Putonghua Deep Speech California Putonghua Cantonese Chinese Baidu Duer Duer Baidu AI Baidu Duer Baidu Beijing Baidu Baidu Duer Ng 'Hi MIT Technology Review Baidu T Precise CRISPR China Britain App United States Helix Helix Improved China US SolarCity Slack Tesla Tesla Post Magazine E-mail Please,3
246,"Courtesy photo District librarian, Deb Logan sharing Laura Numeroffs If You Take a Mouse to Schools. On Thursday, April 6, 20 preschoolers along with their families came to the Park Avenue Elementary Indians in Training: Ready and Set for School event. The preschoolers started their evening with snacks and coloring murals that launched the theme for the evening. The theme was based on Laura Numeroffs If You Give a Mouse a Cookie book series and the snacks included If You Give a Moose a Muffin muffins and If You Take a Mouse to the Movie popcorn. After being treated to a reading of Numeroffs If You Take a Mouse to School by Mount Gilead Schools district librarian, Deb Logan, the preschools participated in four learning through play activities. District psychologist, Tab Walls, provided a fine motor skills activity based on If You Take a Mouse to the Movies. Objects found in If You Take a Mouse to School and a plush version of Mouse were used by district speech therapist, Amy Miley to help the preschoolers understand spatial relationships. Preschoolers practiced tool skills as they played with playdough If You Give a Mouse a Muffin muffins provided by preschool teacher, Leah Demain. Intervention specialists, Stacey Prothman and Barb Gudas letter recognition and memory activity was inspired by If You Give a Pig a Pancake. The evening ended with a prize drawing conducted by principal, Tabatha Wilburn. Parents and preschoolers took home fun learning games and information. Information included flyers about community services, a booklet on preparing preschoolers for school and a flyer with tips on raising readers and ideas to use when reading with a child. If parents and preschoolers were unable to attend, copies of flyers and information may be picked up from Park Avenue Elementary. The materials will also be available during kindergarten screenings which will be held on May 18-19. This event was organized by a team of Mount Gilead staff led by kindergarten teacher, Mrs. Jennifer Williams. In addition to the educators who helped with the activities throughout the evening, the team also include Melody Church, Kim Porter, Shannon Ruhl, Tammy Eicher and Christina Kottenstette. Courtesy photo District librarian, Deb Logan sharing Laura Numeroffs If You Take a Mouse to Schools. Courtesy photo District librarian, Deb Logan sharing Laura Numeroffs If You Take a Mouse to Schools. ",http://morrowcountysentinel.com/news/13242/park-avenue-hosts-preschool-education-event,"... of Mouse were used by district speech therapist, Amy Miley to help the ... Stacey Prothman and Barb Guda's letter recognition and memory ...",Park Avenue hosts preschool education event,Courtesy District Deb Logan Laura Numeroffs Mouse Schools Thursday April Park Avenue Elementary Indians Set School Laura Numeroffs Mouse Cookie Moose Muffin Mouse Movie Numeroffs Mouse School Mount Gilead Schools Deb Logan District Tab Walls Mouse Movies Objects Mouse School Mouse Amy Miley Mouse Muffin Leah Demain Stacey Prothman Barb Gudas Pig Pancake Tabatha Wilburn Park Avenue Elementary May Mount Gilead Mrs. Jennifer Williams Melody Church Kim Porter Shannon Ruhl Tammy Eicher Christina Kottenstette Courtesy District Deb Logan Laura Numeroffs Mouse Schools Courtesy District Deb Logan Laura Numeroffs Mouse Schools,2
247,"A top legal team in the United States, the Alliance Defending Freedom, whose lawyers regularly appear before the U.S. Supreme Court, is warning Congress that the First Amendment is not well on college campuses. At California State University-Los Angeles, faculty members actually linked arms to prevent students from entering an auditorium to hear a speech from nationally known speaker Ben Shapiro on  ironically  freedom of speech, the group said this week. The testimony from ADF was delivered to the House Judiciary Committee Subcommittee on the Constitution and Civil Justice, which was addressingFirst Amendment protections on university and college campuses. The issue, which has been in the news for several years already, erupted again this week. It was the Young Americans for Liberty who were refused recognition by the student government association at Wichita State University. There, the student senate rejected a resolution to recognize the student chapter. Those opposed to the acknowledgment of a Young Americans for Liberty chapter claim to respect free speech, but by blocking YAL, these students are proving that they do not truly believe in the First Amendment,said YAL President Cliff Maloney Jr. of the dispute. Rather this small group of power hungry students only approve of speech that aligns with their political agendas. Scott Greer has released No Campus for White Men: The Transformation of Higher Education into Hateful Indoctrination, now at the WND Superstore. Colleges and universities used to act as a beacon of intellectual discussion and open discourse. It is a shame that a small group of irrational students have prohibited the free speech of their liberty-minded peers. We will continue to seek recognition and challenge their opposition to open, political discourse. The issue of snowflakes on campus, those who easily are offended by ordinary conversation and the like, became the focal point of a popular television show with actor Tim Allen, whose solution to addressing a crowd recently was, Good afternoon ladies and gentlemen, and all of you on the fence: ADF noted the subcommitteehearing was on First Amendment violations on university and college campuses. Although public universities are supposed to be marketplaces of ideas, they are far too often anything but, said ADF Legal Counsel Caleb Dalton, one of the authors of the submitted testimony. A robust exchange of competing ideas and philosophies is essential to liberty and progress, but todays campuses have transformed the marketplace of ideas into the intellectual vacuum of government intolerance. In the last decade, ADF has assisted hundreds of students and student groups of varying religious and political beliefs who face violations of their First Amendment freedoms on campus. Our current and recent cases illustrate the breadth of the constitutional crisis students face. The organization pointed out the fight already has been in the courts in Michigan, California, Georgia, New York, North Carolina, Wisconsin and Iowa. The testimony identifies four top problems with unconstitutional protections ofsnowflakescampuses are offering: So-called speech zones, which restrict constitutionally protected free speech to tiny portions of a campus that are frequently not well traveled. Vague harassment and non-discrimination policies that are so ambiguous they censor expression that the First Amendment protects. Granting unlimited power to administrators to suppress or favor speech through, for example, the assessment of security fees for controversial speakers, the denial of recognition to student organizations with viewpoints that administrators dont prefer or distributing mandatory student fees primarily to favored groups. Limiting equal access and free association by preventing some student groups from associating around shared beliefs and choosing only leaders and spokespersons who share those convictions. Todays college students will be tomorrows legislators, judges, commissioners and voters, said ADF Senior Counsel Casey Mattox, director of the ADF Center for Academic Freedom. Thats why its so important that public universities model the First Amendment values they are supposed to be teaching to students, and why it should disturb everyone when many colleges and universities fail to defend these values. We commend the subcommittee for the attention it is paying to this issue. The testimony to Congress said: A significant majority of public universities are restricting the First Amendment rights of speech and association of their students and faculty through a vast array of onerous policies and restrictions that not only violate students rights now, but teach them false lessons about how they should think about their own and others constitutional rights. ADF said that in the last decade, it has assisted hundreds of students and student groups of varying religious and political beliefs facing violations. While the Center for Academic Freedom has achieved a 100 percent success rate in challenging the all-too-common speech zones  universities nevertheless persist in applying such unconstitutional polices. The state of the First Amendment on public universities and colleges is not well, the organization said. The status quo at most institutions substantially restricts free speech and association, and teaches students that government censorship is the norm, not the exception. ",http://www.wnd.com/2017/04/4-unconstitutional-safe-spaces-for-campus-snowflakes/,We will continue to seek recognition and challenge their opposition to ... are restricting the First Amendment rights of speech and association of ...,4 unconstitutional safe spaces for campus 'snowflakes',United States Alliance Defending Freedom U.S. Supreme Court Congress First Amendment California State Angeles Ben Shapiro ADF House Judiciary Committee Subcommittee Constitution Civil Justice Amendment Young Americans Liberty Wichita State University Young Americans Liberty YAL First Amendment YAL President Cliff Maloney Jr. Scott Greer No Campus White Hateful Indoctrination WND Superstore Colleges Tim Allen ADF First Amendment ADF Legal Counsel Caleb Dalton A ADF First Amendment Michigan California Georgia New York North Carolina Wisconsin Iowa Vague First Amendment Todays ADF Senior Counsel Casey Mattox ADF Center Academic Freedom Thats university First Amendment Congress First Amendment ADF Center Academic Freedom First Amendment,2
248,"Announced in October, Nuances SpeechKit for Fabric was in beta mode until today. Its available for both iOS and Android , and supports over 40 languages. Developers already using Nuance can still take advantage of SpeechKit, and Twitter will provision your keys for you. ",https://thenextweb.com/dd/2016/03/31/twitter-fabric-speech-recognition/,"SpeechKit is Nuance's plugin for Fabric apps, offering cloud-based speech recognition and natural text-to-voice interactions for users. It can be ...",Twitter Fabric now has speech recognition courtesy of Nuance's ...,October Nuances SpeechKit Fabric Android Developers Nuance SpeechKit Twitter,-1
249,"Chinas dominant Internet company, Baidu, is developing powerful speech recognition for its voice interfaces. Accurate voice recognition will be vital for making voice interfaces more useful and pervasive. Chinas leading Internet-search company, Baidu, has developed a voice system that can recognize English and Mandarin speech better than people, in some cases. The new system, called Deep Speech 2 , is especially significant in how it relies entirely on machine learning for translation. Whereas older voice-recognition systems include many handcrafted components to aid audio processing and transcription, the Baidu system learned to recognize words from scratch, simply by listening to thousands of hours of transcribed audio. The technology relies on a powerful technique known as deep learning, which involves training a very large multilayered virtual network of neurons to recognize patterns in vast quantities of data. The Baidu app for smartphones lets users search by voice, and also includes a voice-controlled personal assistant called Duer (see  Baidus Duer Joins the Personal Assistant Party ). Voice queries are more popular in China because it is more time-consuming to input text, and because some people do not know how to use Pinyin, the phonetic system for transcribing Mandarin using Latin characters. Historically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features, says Andrew Ng, a former Stanford professor and Google researcher, and now chief scientist for the Chinese company. The learning algorithms are now so general that you can just learn. Deep learning has its roots in ideas first developed more than 50 years ago, but in the past few years new mathematical techniques, combined with greater computer power and huge quantities of training data, have led to remarkable progress, especially in tasks that require some sort of visual or auditory perception. The technique has already improved the performance of voice recognition and image processing, and large companies including Google, Facebook, and Baidu are applying it to the massive data sets they own. Deep learning is also being adopted for ever-more tasks. Facebook, for example, uses deep learning to find faces in the images that its users upload. And more recently it has made progress in using deep learning to parse written text (see  Teaching Machines to Understand Us ). Google now uses deep learning in more than 100 different projects, from search to self-driving cars. In 2013, Baidu opened its own effort to harness this new technology, the Deep Learning Institute , co-located at the companys Beijing headquarters and in Silicon Valley. Deep Speech 2 was primarily developed by a team in California. In developing Deep Speech 2, Baidu also created new hardware architecture for deep learning that runs seven times faster than the previous version. Deep learning usually relies on graphics processors, because these are good for the intensive parallel computations involved. The speed achieved allowed us to do experimentation on a much larger scale than people had achieved previously, says Jesse Engel , a research scientist at Baidu and one of more than 30 researchers named on a paper describing Deep Speech 2. We were able to search over a lot of [neural network] architectures, and reduce the word error rate by 40 percent. Ng adds that this has recently produced some impressive results. For short phrases, out of context, we seem to be surpassing human levels of recognition, he says. He adds: In Mandarin, there are a lot of regional dialects that are spoken by much smaller populations, so theres much less data. This could help us recognize the dialects better. Weren't able to make it to EmTech Digital ",https://www.technologyreview.com/s/544651/baidus-deep-learning-system-rivals-people-at-speech-recognition/,"China's leading Internet-search company, Baidu, has developed a voice system that can recognize English and Mandarin speech better than ...",Baidu's Deep-Learning System Rivals People at Speech Recognition,Chinas Internet Baidu Accurate Baidu Mandarin Deep Speech Baidu Baidu Duer Baidus Duer Personal Assistant Party Voice China Pinyin Mandarin Andrew Ng Stanford Google Deep Google Facebook Baidu Deep Facebook Us Google Baidu Deep Learning Institute Beijing Silicon Valley Deep Speech California Deep Speech Baidu Jesse Engel Baidu Deep Speech [ ] Ng Mandarin EmTech Digital,3
250,"According to the new market research report ""Speech & Voice Recognition Market by Technology (Speech Recognition, Voice Recognition), Application (AI Based, Non AI Based), Vertical (Automotive, Consumer, Finance, Retail, Military, Healthcare & Government) and Geography - Global Forecast to 2022"", published by MarketsandMarkets, the speech recognition market is expected to grow from USD 3.73 Billion in 2015 to USD 9.97 Billion by 2022, at a CAGR of 15.78% during the forecast period, while the voice recognition market is expected to grow from USD 440.3 Million in 2015 to USD 1.99 Billion by 2022, at a CAGR of 23.66% between 2016 and 2022. Early buyers will receive 10% customization on this report. The factors driving the speech & voice recognition market are the growing instances of fraud in several end-user industry segments such as enterprise, healthcare, and so on and the adoption of mobile banking by several national and international banks and e-commerce retailers. The speech recognition technology is being used in an ever-increasing array of applications as a result of the continuing advancement of computing power and widespread adoption of mobile and cloud-based computer technology. The growth has been particularly noticeable in the healthcare sector where speech recognition is used to produce health data records. PDAs and other handheld devices are becoming increasingly powerful and affordable and it has become possible to run multimedia on these devices. Speech recognition systems emerge as efficient alternatives for such devices where typing is difficult due to their small screen. The large market size of the consumer vertical is attributed to the higher penetration rate of speech & voice recognition technologies in consumer products such as smartphone, laptops, and tablets. Major companies such as Google, Apple, and Microsoft are leveraging their large customer base and neural networks to process, understand, and take decisive actions based on real-time voice inputs from the user. In the long term, speech & voice recognition abilities are expected to be integrated with other consumer devices such as refrigerators, ovens, mixers, thermostats, and so on with the growth of IoT. This growth in other consumer devices is expected to be fueled by the trend among OEMs of integrating speech & voice recognition technology in their devices. North America dominated the global Speech & Voice Recognition Market owing to the large-scale deployment of biometric systems compared to other regions. The demand for a high level of security leads to high adoption of biometric systems. The initiatives taken by the governments in the Americas to deploy biometric systems across various agencies are driving the biometrics market for government applications in North America. Moreover, the strong economy of the U.S. is the major driving factor for the growth of the speech & voice recognition market in the region. The major players in the speech & voice recognition market are Nuance Communications (U.S.), Microsoft Inc. (U.S), Agnitio SL (Spain), Biotrust (Netherland), VoiceVault (U.S.), VoiceBox Technologies Corp. (U.S.), LumenVox LLC. (U.S.), M2Sys LLC (U.S.), Raytheon BBN Technologies (U.S.), M2SyS LLC (U.S.) ValidSoft UK Limited (U.K.), Advanced Voice Recognition Systems (U.S.), Sensory Inc. (U.S.), and MMODAL Inc. (U.S.). Biometric System Market by Application (Government, Military & Defense, Healthcare, Banking & Finance, Consumer Electronics, Travel & Immigration, and Security), Technology (Face Recognition, Fingerprint Recognition, Iris Recognition, Palm Recognition, Voice Recognition, Signature Recognition, Vein Recognition, and Others), Function (Contact and Non-Contact) & Geography (North America, Europe, APAC, Row) - Global Forecast to 2020 ",http://www.prnewswire.com/news-releases/speech-and-voice-recognition-market-worth-1196-billion-usd-by-2022-582118741.html,"According to the new market research report Speech & Voice Recognition Market by Technology (Speech Recognition, Voice Recognition), ...",Speech and Voice Recognition Market Worth 11.96 Billion USD by ...,Speech Voice Recognition Market Technology Voice Recognition Application AI Non AI Vertical Automotive Consumer Finance Retail Military Healthcare Government Geography Global Forecast USD Billion Billion CAGR USD Million Billion CAGR PDAs Google Apple Microsoft IoT OEMs North America Speech Voice Recognition Market Americas North America U.S. Nuance Communications U.S. Microsoft Inc. U.S Agnitio SL Spain Biotrust Netherland VoiceVault U.S. VoiceBox Technologies Corp. U.S. LumenVox LLC U.S. M2Sys LLC U.S. Raytheon BBN Technologies U.S. M2SyS LLC U.S. ValidSoft UK Limited U.K. Advanced Voice Recognition Systems U.S. Sensory Inc. U.S. MMODAL Inc. U.S. Market Application Government Military Defense Healthcare Banking Finance Consumer Electronics Travel Immigration Security Technology Face Recognition Fingerprint Recognition Iris Recognition Palm Recognition Voice Recognition Signature Recognition Vein Recognition Others Function Contact Non-Contact Geography North America Europe APAC Row,3
251,"Baidu has been developing its own AI system for four years, and its latest achievement shows it is capable of learning vast amounts of information with no human involvement. Baidu  commonly referred to as the Chinese Google  has been following its Silicon Valley counterparts in developing AI with an emphasis on deep learning and, seemingly, beatingancient parlour games. Now, after four years of development within its research labs, the Chinese company has revealed that its AI can now handle speech synthesis with unprecedented accuracy. According to MIT Technology Review , the breakthrough was made by developing its deep learning machine to reduce the amount of time its speech synthesis needs to switch gears when putting emphasis on words. Under many existing text-to-speech systems, the voice comes from a vast database of different words from a single source, whichare then combined into one phrase or sentence. To facilitatea change of emphasis or a new speaker, however, it would require the need for an entirely new database, which would be both costly and time-consuming. Baidus Deep Voice AI has counteracted this bycreating speech in real time, thereby learning how to talk to itself  with no human involvement  in the space of a few hours. The Deep Voice AI is a development from Baidus previous research using WaveNet, whichwas similar in design but required some human involvement to tidy up its speech patterns. To achievethis latest advancement with Deep Voice, Baidu researchers said in a scientific journal that it broke down text into phenomes, or units of sound. By usingsmaller sound units, it can recreate words more accurately, and sound more natural. The only problem is that, in doing so, the processing power required increases dramatically, as having only 20 microseconds to generate a sample word means that each of these phenomes must be generated within 1.5 microseconds. Our system is trainable without any human involvement, dramatically simplifying the process of creating text-to-speech systems, said the research team. To showhow human-like the phrases sound, Baidu has begun uploading audio samples to the crowdsourcing Amazon site Mechanical Turk to see if the wider public can tell the difference. ",https://www.siliconrepublic.com/machines/baidu-ai-deep-voice,"Now, after four years of development within its research labs, the Chinese company has revealed that its AI can now handle speech synthesis ...","Baidu AI learns to talk by itself in hours, with no human involvement",Baidu AI Baidu Google Silicon Valley AI AI MIT Technology Review Baidus Deep Voice AI Voice AI Baidus WaveNet Deep Voice Baidu Baidu Amazon Mechanical Turk,3
252,"M*Modal Selected by BJC HealthCare for Speech Recognition and Clinical Documentation Improvement (CDI) System Marketwired Wednesday, January 18, 2017 FRANKLIN, TN--(Marketwired - January 18, 2017) - M*Modal , a leading provider of clinical documentation and Speech Understanding solutions, today announced that it has been selected by BJC HealthCare for implementation across the organization's 15 hospitals, health services and medical group practices. The system will enable BJC HealthCare and physician partners at Washington University School of Medicine to streamline clinical documentation workflows across the enterprise and enable clinicians to document directly and accurately in their current NextGen and Allscripts EHR systems from any location, using any device. Importantly, as BJC HealthCare consolidates EHR systems and migrates to Epic , physicians will be able to easily continue creating complete and compliant notes using the M*Modal advanced documentation system for a smooth transition as well as optimized physician efficiency and satisfaction. M*Modal was chosen after a competitive evaluation for its proposed extensive and robust EHR integrations , virtual deployment capabilities, and real-time clinical intelligence delivered through its built-in Computer-Assisted Physician Documentation (CAPD) functionality. Additionally, M*Modal's shared-risk, performance-based business model will help drive the adoption and utilization of its front-end speech recognition solution, M*Modal Fluency Direct , for real-time documentation, increased physician satisfaction and the lowest total cost of ownership. With this multi-facility deployment, BJC HealthCare hopes to improve not only the speed and cost of clinical documentation but also quality and compliance using M*Modal CDI Engage for better clinical and financial outcomes. ""We are proud to collaborate with BJC HealthCare, one of the nation's leading health care systems, and deliver innovative solutions that support its larger goal of delivering higher-quality care more cost effectively while reducing the documentation burden on clinicians,"" said Scott MacKenzie, CEO of M*Modal. ""Our unique, physician-centric approach brings CDI to the front-end documentation workflow, helps free up back-end resources, supports more appropriate reimbursement and delivers a significant return on investment."" The M*Modal documentation solutions utilize proprietary Speech and Natural Language Understanding technologies to provide clinicians at BJC HealthCare and Washington University: High-Performing Front-End Speech Recognition: M*Modal Fluency Direct is a top-ranking, closed-loop clinical documentation system that enables physicians of any medical specialty to create, edit and sign reports directly in templates of over 120 EHR systems. Built to scale, Fluency Direct can be readily deployed in virtualized environments and provides the platform for M*Modal CAPD. A single, cloud-hosted user voice profile ensures that physicians can easily and conveniently document from any location, using multiple devices (including iOS and Android mobile devices) for unmatched portability and security.Unique, Real-Time CDI: By utilizing the CAPD functionality, M*Modal CDI Engage ensures that physicians create a complete and accurate patient record right from the start with regard to CDI, ICD-10 compliance and risk-adjustment documentation using Hierarchical Condition Categories (HCCs). M*Modal CDI Engage automates the identification and correction of the most common documentation deficiencies and specificity queries in notes created via speech, templates and/or typing -- even before they are saved in the EHR. This considerably reduces time-consuming, retrospective physician queries that are very disruptive to clinicians and labor intensive for nurses and CDI specialists. To see first-hand how these matchless M*Modal solutions can help you meet your goals and capitalize on opportunities, please visit the M*Modal booth (# 1043) at the 2017 HIMSS conference being held in Orlando from February 19 to 23. M*Modal is a leading healthcare technology provider of advanced clinical documentation solutions, enabling hospitals and physicians to enrich the content of patient electronic health records (EHR) for improved healthcare and comprehensive billing integrity. As one of the largest clinical transcription service providers in the U.S., with a global network of medical editors, M*Modal also provides advanced cloud-based Speech Understanding technology and data analytics that enable physicians and clinicians to include the context of their patient narratives into electronic health records in a single step, further enhancing their productivity and the cost-saving efficiency and quality of patient care at the point of care. For more information, please visit www.mmodal.com , Twitter , Facebook and YouTube . BJC HealthCare is one of the largest nonprofit health care organizations in the United States, delivering services to residents primarily in the greater St. Louis, southern Illinois and mid-Missouri regions. Serving the health care needs of urban, suburban and rural communities, BJC includes 15 hospitals and multiple health service organizations. Services include inpatient and outpatient care, primary care, community health and wellness, workplace health, home health, community mental health, rehabilitation, long-term care and hospice. BJC's nationally recognized academic hospitals, Barnes-Jewish and St. Louis Children's hospitals, are affiliated with Washington University School of Medicine. Washington University School of Medicine 's 2,100 employed and volunteer faculty physicians also are the medical staff of Barnes-Jewish and St. Louis Children's hospitals. The School of Medicine is one of the leading medical research, teaching and patient-care institutions in the nation, currently ranked sixth in the nation by U.S. News & World Report. Through its affiliations with Barnes-Jewish and St. Louis Children's hospitals, the School of Medicine is linked to BJC HealthCare . ",http://finance.yahoo.com/news/m-modal-selected-bjc-healthcare-150000864.html,"FRANKLIN, TN--(Marketwired - January 18, 2017) - M*Modal, a leading provider of clinical documentation and Speech Understanding™ ...",M*Modal Selected by BJC HealthCare for Speech Recognition and ...,M*Modal BJC HealthCare Clinical Documentation Improvement CDI System Marketwired Wednesday January FRANKLIN TN Marketwired January M*Modal Speech Understanding BJC HealthCare BJC HealthCare Washington University School Medicine NextGen Allscripts EHR BJC HealthCare EHR Epic M*Modal M*Modal EHR Documentation CAPD M*Modal M*Modal Fluency Direct BJC HealthCare M*Modal CDI Engage BJC HealthCare Scott MacKenzie CEO M*Modal CDI M*Modal Speech Natural Language Understanding BJC HealthCare Washington University M*Modal Fluency Direct Fluency Direct M*Modal CAPD A Android mobile CAPD M*Modal CDI Engage CDI ICD-10 Hierarchical Condition Categories HCCs M*Modal CDI Engage EHR CDI M*Modal M*Modal HIMSS Orlando February M*Modal EHR U.S. M*Modal Speech Understanding Twitter Facebook YouTube BJC HealthCare United States St. Louis Illinois BJC BJC St. Louis Children Washington University School Medicine Washington University School Medicine St. Louis Children School Medicine U.S. News World Report St. Louis Children School Medicine BJC HealthCare,-1
253,"Baidu has been developing its own AI system for four years, and its latest achievement shows it is capable of learning vast amounts of information with no human involvement. Baidu  commonly referred to as the Chinese Google  has been following its Silicon Valley counterparts in developing AI with an emphasis on deep learning and, seemingly, beatingancient parlour games. Now, after four years of development within its research labs, the Chinese company has revealed that its AI can now handle speech synthesis with unprecedented accuracy. According to MIT Technology Review , the breakthrough was made by developing its deep learning machine to reduce the amount of time its speech synthesis needs to switch gears when putting emphasis on words. Under many existing text-to-speech systems, the voice comes from a vast database of different words from a single source, whichare then combined into one phrase or sentence. To facilitatea change of emphasis or a new speaker, however, it would require the need for an entirely new database, which would be both costly and time-consuming. Baidus Deep Voice AI has counteracted this bycreating speech in real time, thereby learning how to talk to itself  with no human involvement  in the space of a few hours. The Deep Voice AI is a development from Baidus previous research using WaveNet, whichwas similar in design but required some human involvement to tidy up its speech patterns. To achievethis latest advancement with Deep Voice, Baidu researchers said in a scientific journal that it broke down text into phenomes, or units of sound. By usingsmaller sound units, it can recreate words more accurately, and sound more natural. The only problem is that, in doing so, the processing power required increases dramatically, as having only 20 microseconds to generate a sample word means that each of these phenomes must be generated within 1.5 microseconds. Our system is trainable without any human involvement, dramatically simplifying the process of creating text-to-speech systems, said the research team. To showhow human-like the phrases sound, Baidu has begun uploading audio samples to the crowdsourcing Amazon site Mechanical Turk to see if the wider public can tell the difference. ",https://www.siliconrepublic.com/machines/baidu-ai-deep-voice,"Now, after four years of development within its research labs, the Chinese company has revealed that its AI can now handle speech synthesis ...","Baidu AI learns to talk by itself in hours, with no human involvement",Baidu AI Baidu Google Silicon Valley AI AI MIT Technology Review Baidus Deep Voice AI Voice AI Baidus WaveNet Deep Voice Baidu Baidu Amazon Mechanical Turk,3
254,"M*Modal Selected by BJC HealthCare for Speech Recognition and Clinical Documentation Improvement (CDI) System Marketwired Wednesday, January 18, 2017 FRANKLIN, TN--(Marketwired - January 18, 2017) - M*Modal , a leading provider of clinical documentation and Speech Understanding solutions, today announced that it has been selected by BJC HealthCare for implementation across the organization's 15 hospitals, health services and medical group practices. The system will enable BJC HealthCare and physician partners at Washington University School of Medicine to streamline clinical documentation workflows across the enterprise and enable clinicians to document directly and accurately in their current NextGen and Allscripts EHR systems from any location, using any device. Importantly, as BJC HealthCare consolidates EHR systems and migrates to Epic , physicians will be able to easily continue creating complete and compliant notes using the M*Modal advanced documentation system for a smooth transition as well as optimized physician efficiency and satisfaction. M*Modal was chosen after a competitive evaluation for its proposed extensive and robust EHR integrations , virtual deployment capabilities, and real-time clinical intelligence delivered through its built-in Computer-Assisted Physician Documentation (CAPD) functionality. Additionally, M*Modal's shared-risk, performance-based business model will help drive the adoption and utilization of its front-end speech recognition solution, M*Modal Fluency Direct , for real-time documentation, increased physician satisfaction and the lowest total cost of ownership. With this multi-facility deployment, BJC HealthCare hopes to improve not only the speed and cost of clinical documentation but also quality and compliance using M*Modal CDI Engage for better clinical and financial outcomes. ""We are proud to collaborate with BJC HealthCare, one of the nation's leading health care systems, and deliver innovative solutions that support its larger goal of delivering higher-quality care more cost effectively while reducing the documentation burden on clinicians,"" said Scott MacKenzie, CEO of M*Modal. ""Our unique, physician-centric approach brings CDI to the front-end documentation workflow, helps free up back-end resources, supports more appropriate reimbursement and delivers a significant return on investment."" The M*Modal documentation solutions utilize proprietary Speech and Natural Language Understanding technologies to provide clinicians at BJC HealthCare and Washington University: High-Performing Front-End Speech Recognition: M*Modal Fluency Direct is a top-ranking, closed-loop clinical documentation system that enables physicians of any medical specialty to create, edit and sign reports directly in templates of over 120 EHR systems. Built to scale, Fluency Direct can be readily deployed in virtualized environments and provides the platform for M*Modal CAPD. A single, cloud-hosted user voice profile ensures that physicians can easily and conveniently document from any location, using multiple devices (including iOS and Android mobile devices) for unmatched portability and security.Unique, Real-Time CDI: By utilizing the CAPD functionality, M*Modal CDI Engage ensures that physicians create a complete and accurate patient record right from the start with regard to CDI, ICD-10 compliance and risk-adjustment documentation using Hierarchical Condition Categories (HCCs). M*Modal CDI Engage automates the identification and correction of the most common documentation deficiencies and specificity queries in notes created via speech, templates and/or typing -- even before they are saved in the EHR. This considerably reduces time-consuming, retrospective physician queries that are very disruptive to clinicians and labor intensive for nurses and CDI specialists. To see first-hand how these matchless M*Modal solutions can help you meet your goals and capitalize on opportunities, please visit the M*Modal booth (# 1043) at the 2017 HIMSS conference being held in Orlando from February 19 to 23. M*Modal is a leading healthcare technology provider of advanced clinical documentation solutions, enabling hospitals and physicians to enrich the content of patient electronic health records (EHR) for improved healthcare and comprehensive billing integrity. As one of the largest clinical transcription service providers in the U.S., with a global network of medical editors, M*Modal also provides advanced cloud-based Speech Understanding technology and data analytics that enable physicians and clinicians to include the context of their patient narratives into electronic health records in a single step, further enhancing their productivity and the cost-saving efficiency and quality of patient care at the point of care. For more information, please visit www.mmodal.com , Twitter , Facebook and YouTube . BJC HealthCare is one of the largest nonprofit health care organizations in the United States, delivering services to residents primarily in the greater St. Louis, southern Illinois and mid-Missouri regions. Serving the health care needs of urban, suburban and rural communities, BJC includes 15 hospitals and multiple health service organizations. Services include inpatient and outpatient care, primary care, community health and wellness, workplace health, home health, community mental health, rehabilitation, long-term care and hospice. BJC's nationally recognized academic hospitals, Barnes-Jewish and St. Louis Children's hospitals, are affiliated with Washington University School of Medicine. Washington University School of Medicine 's 2,100 employed and volunteer faculty physicians also are the medical staff of Barnes-Jewish and St. Louis Children's hospitals. The School of Medicine is one of the leading medical research, teaching and patient-care institutions in the nation, currently ranked sixth in the nation by U.S. News & World Report. Through its affiliations with Barnes-Jewish and St. Louis Children's hospitals, the School of Medicine is linked to BJC HealthCare . ",http://finance.yahoo.com/news/m-modal-selected-bjc-healthcare-150000864.html,"FRANKLIN, TN--(Marketwired - January 18, 2017) - M*Modal, a leading provider of clinical documentation and Speech Understanding™ ...",M*Modal Selected by BJC HealthCare for Speech Recognition and ...,M*Modal BJC HealthCare Clinical Documentation Improvement CDI System Marketwired Wednesday January FRANKLIN TN Marketwired January M*Modal Speech Understanding BJC HealthCare BJC HealthCare Washington University School Medicine NextGen Allscripts EHR BJC HealthCare EHR Epic M*Modal M*Modal EHR Documentation CAPD M*Modal M*Modal Fluency Direct BJC HealthCare M*Modal CDI Engage BJC HealthCare Scott MacKenzie CEO M*Modal CDI M*Modal Speech Natural Language Understanding BJC HealthCare Washington University M*Modal Fluency Direct Fluency Direct M*Modal CAPD A Android mobile CAPD M*Modal CDI Engage CDI ICD-10 Hierarchical Condition Categories HCCs M*Modal CDI Engage EHR CDI M*Modal M*Modal HIMSS Orlando February M*Modal EHR U.S. M*Modal Speech Understanding Twitter Facebook YouTube BJC HealthCare United States St. Louis Illinois BJC BJC St. Louis Children Washington University School Medicine Washington University School Medicine St. Louis Children School Medicine U.S. News World Report St. Louis Children School Medicine BJC HealthCare,-1
255,"Black Book Names Nuance #1 End-to-End Coding, CDI, Transcription & Speech Recognition Vendor I read this article and found it very interesting, thought it might be something for you. The article is called Black Book Names Nuance #1 End-to-End Coding, CDI, Transcription & Speech Recognition Vendorand is located athttp://hitconsultant.net/2016/11/09/black-book-names-nuance-1-end-end-coding-cdi-transcription-speech-recognition-vendor/. Nuance Communicationshas been named the leading vendor for End-to-End Healthcare Coding, Clinical Documentation Improvement (CDI), Transcription and Speech Recognition Technology vendor, according to a recent Black Book survey from 313hospitaland 2,005 physician practices. Conducted between April and October 2016, 3,107revenue cycle and coding management professionals submitted user satisfaction responses. The findings reveal that seventy-nine percent of hospitals surveyed and 94% of physician practices report cutting transcription costs in half or more while improving the transparency of dictation and transcription processes within one year of implementing end-to-end coding, CDI and transcription software tools. Eighty-seven percent of providers realized operational efficiencies without impacting clinician workflows. Eighty-one percent of hospital chief financial officers participating in the Black Book survey reported that comprehensive coding, CDI and transcription programs delivered among the most rapid return on all technology investments in 2016. Black Book forecasts the medical transcription, clinical documentation and coding and speech recognition IT spending market to grow at a CAGR of 12.9% during the period 2016-2020. Compared to otherprovider initiatives to execute organizational improvements in case mix index, physician engagement and reimbursement, end-to-end technologyresulted in the most appropriate reimbursement increases at the fastest measured rates, said Doug Brown, Managing Partner of Black Book in a statement. I read this article and found it very interesting, thought it might be something for you. The article is called Black Book Names Nuance #1 End-to-End Coding, CDI, Transcription & Speech Recognition Vendorand is located athttp://hitconsultant.net/2016/11/09/black-book-names-nuance-1-end-end-coding-cdi-transcription-speech-recognition-vendor/. ",http://hitconsultant.net/2016/11/09/black-book-names-nuance-1-end-end-coding-cdi-transcription-speech-recognition-vendor/,"Black Book forecasts the medical transcription, clinical documentation and coding and speech recognition IT spending market to grow at a ...","Black Book Names Nuance #1 End-to-End Coding, CDI ...",Black Book Names Nuance Coding CDI Transcription Vendor Black Book Names Nuance Coding CDI Transcription Vendorand Communicationshas End-to-End Healthcare Coding Clinical Documentation Improvement CDI Transcription Technology Black Book April October CDI Black Book CDI Black Book IT CAGR Doug Brown Managing Partner Black Book Black Book Names Nuance Coding CDI Transcription Vendorand,-1
256,"Both Vint Cerf, known as a ""father of the Internet,"" and his wife have hearing disabilities. This is part of CNET's "" Tech Enabled "" series about the role technology plays in helping the disability community. Vint Cerf is often called the ""father of the internet."" Consider him a pretty stern papa. Cerf, who is hearing-impaired, played an integral part in the invention of some of the most crucial technologies of the last half century, including the internet and email. But as quickly as he'll extol how tech can advance society, he won't mince words about its track record accommodating people with disabilities. Accessibility shouldn't be a ""pixie dust"" designers sprinkle on as an afterthought, he said. ""It's a crime that the most versatile device on the planet, the computer, has not adapted well to people who need help, who need assistive technology,"" he said in an interview last month. ""It's almost criminal that programmers have not had their feet held to the fire to build interfaces that are accommodating for people with vision problems or hearing problems or motor problems."" Plenty of guidelines for designing accessible technology exist, but their implementation too often has been subordinate to other design goals, he said. Cerf is best known as one of the designers of the architecture for the internet in the early 1970s, helping to shape the rules that dictate where internet traffic goes and, about a decade later, helping to deliver the first commercial email system. Today he is Google 's ""chief internet evangelist"" and contributes to the People Centered Internet , a group he cofounded to advance connectivity worldwide. His own disability, and the disabilities of people close to him, shaped his approach to tech, he said. Email, for one, brought Cerf more than the typical benefit of posting and interacting on your own timeline. ""Because I'm hearing-impaired, emails are a tremendously valuable tool because of the precision that you get,"" he said, sitting on a hotel couch in his trademark three-piece suit before a SXSW keynote organized by engineering trade organization IEEE . (On this occasion it was grey pinstripe with a blue shirt.) ""I can read what's typed as opposed to straining to hear what's being said."" He's not alone in needing an assist from technology. About 360 million people worldwide have a hearing disability, roughly 5 percent of all the people on Earth, according to the World Health Organization . Then factor in those with vision, motor or other impairments. In the US alone, more than one in three households has a member who identifies as having a disability, according to panel research by Nielsen last year. President George W. Bush presented Cerf with the Presidential Medal of Freedom, the nation's highest civil award, in 2005. Email and the internet were also crucial to his wife's adaptation to her own disability, even though Cerf teases her for being uninterested in email for more than two decades after he began playing with network mail in the early '70s. Sigrid Cerf, who became deaf as a 3-year-old because of spinal meningitis, finally took the plunge onto the net in the mid '90s to learn about cochlear implants: surgically embedded devices that bypass the ear and send the brain signals it interprets as sound. She learned about the technology -- and the doctors specializing in it at Johns Hopkins Hospital -- by surfing the web. ""She couldn't get anybody's attention at Johns Hopkins until somebody in Israel put her in touch by an email exchange,"" he said. Even as an inventor of the internet, Cerf said he was amazed by the role email and the net played in so fundamentally changing his wife's relationship with her disability. Cerf's awareness of disability also sharpens his criticism of tech's shortcomings. ""It can't be a pixie dust that you sprinkle on top of the program and suddenly make it accessible, which is the behavior pattern in the past,"" he said. Accessibility should be a design choice that is  rewarded, ""something a lot of companies have not stepped up to,"" he added. But he believes awareness among engineers and designers is improving. For people with hearing impairments, speech-to-text products are growing more sophisticated, like automatic closed captioning on YouTube . Voice-command technologies, like those in Amazon's Alexa, Apple 's Siri and Google Assistant, are more commonplace. And most recently, neural networks -- a programming technique based loosely on how the human brain learns -- are advancing speech synthesis, to make it more natural for people with vision or physical disabilities to interact with technology. Perhaps most encouraging, he said, is a growing recognition in the tech community that accessibility is important. ""We need to build in these things from the beginning,"" he said. ""That's very powerful stuff."" Special Reports : CNET's in-depth features in one place. Life, disrupted : In Europe, millions of refugees are still searching for a safe place to settle. Tech should be part of the solution. But is it CNET investigates. ",https://www.cnet.com/news/internet-inventor-vint-cerf-accessibility-disability-deaf-hearing/,"For people with hearing impairments, speech-to-text products are ... Perhaps most encouraging, he said, is a growing recognition in the tech ...",Internet inventor: Make tech accessibility better already,Vint Cerf Internet CNET Tech Vint Cerf Consider Cerf Accessibility Cerf Google People Centered Internet Email Cerf SXSW IEEE World Health Organization US Nielsen President George W. Bush Cerf Presidential Medal Freedom Email Cerf Sigrid Cerf Johns Hopkins Hospital Johns Hopkins Israel Cerf Cerf Accessibility YouTube Voice-command technology Amazon Alexa Apple Siri Google Assistant CNET Life Europe Tech CNET,7
257,"Get today's popular DigitalTrends articles in your inbox: IBM just passed two milestones in artificial intelligence . From a distance, the accomplishmentsseem insignificant but IBM Research Audio Analytics Jason Pelecanos called themnecessarily small steps toward increasingly intelligent machines. The first milestone involved demonstrating higher sensitivity in IBMs automatic speaker recognition software, which is tasked with recognizing the identity of a speaker based solely on his or her voice patterns. Back in 2000, the best speaker verification software had an error rate of about 10 percent. Todays industry standard has an error rate of less than one percent. IBMs software has set a new record of just 0.59 percent. Pelecanos acknowledges that the milestone is slight, and in most cases wouldnt be obviousto active users trying to gain access to their smartphones. However, he told Digital Trends, if a system was stricter and had a higher accept threshold, the user would notice that they are rejected on several occasions, by both a system with a 0.6 percent and with a one percent error rate. [But] the better-performing system may incorrectly deny access to you for about half of the occasions of the poorer performing system. This difference will be very noticeable. The IBM team also also developed a system to estimate the age of someone who is speaking, and the company says its published the most accurate results of any system yet with an average error rate of 4.7 years. So  youre wondering  what couldthis be good for First of all, the age estimate software may enable more personalization while tailoring conversations to age groupsby considering things like vocabulary and syntax. Besides better voice activation and security, Pelecanos thinks these highly sensitive speakerrecognition systems will soon be able to multitask. One key focus for us is to have systems that can interact with more than one person at a time, he says. Current technologies, for example speech applications and personal assistants on smartphones or home units, have dialogue, which is established for a one-to-one interaction. Systems that can interact with multiple people at once bring about exciting opportunities for group collaborations with systems. ",http://www.digitaltrends.com/computing/ibm-milestones/,"... higher sensitivity in IBM's automatic speaker recognition software, which is ... Current technologies, for example speech applications and ...",Just say something: IBM's AI software can estimate a person's age ...,Get DigitalTrends IBM IBM Research Audio Analytics Jason Pelecanos IBMs IBMs Digital Trends IBM First Pelecanos Current Systems,-1
258,"Its no coincidence that the list of 100 greatest robot movies of all time consists mainly of movies during or after the 1950sthe start of speech recognition. Before the invention of Audrey , the first speech recognition system that only understood numbers, robots were mainly portrayed in films and comics as villains who hardly ever spoke (mostly due to the silent film era). Films like Flash Gordon (1936), The Phantom Creeps (1939), and Mysterious Doctor Satan (1940) all played upon the classic motif of evil robots programmed to kill or destroy. After speech recognition took off in the 60s-70s, both businesses and the creative media jumped aboard the ever-moving train of this state-of-the-art technology. The invention of Audrey, Harpy , and other quickly-evolving speech recognition hardware sparked a paradigm shift in the way robots were portrayed in TV shows, literature, films and comics. Almost immediately, artificial intelligence became widely viewed as helpers and allies of the human race. Perhaps the most well-known of such robots was C-3PO from Star Wars (1977). Not only could his character talk with fluctuations that emulate human speech, he also understood conversational speech and carried out commands as they were giveneven with a bit of humor. He existed to assist humans, as were the other robots in the Star Wars universe. C-3PO created a lasting impression of the assistant robot that is reminiscent of Siri and other humorous speech-recognizing artificial intelligence today. C-3POs antics and ability to learn truly reflected the growth in speech recognition in the late 70s and early 80s. Once tied down by a limited vocabulary bank, speech recognition hit a gold mine when developers switched to a different algorithm, known as the hidden Markov model , which allowed the speech recognition technology to accept and record new sounds as words. The hidden Markov model opened up new doorways for primitive speech technology, making speech recognition a limitless stride. This advancement greatly impacted the way writers and artists portrayed artificial intelligence thereafter. Similar to C-3PO was J.A.R.V.I.S. (or JARVIS) from the Iron Man (1968) comics. Jarvis was Tony Starks artificially intelligent sidekick, with impeccable speech recognition and critical thinking capabilities. Jarvis came about during the very age when speech recognition gained traction from the media due to its strong influence on the imagination. Jarvis took what little speech recognition could do in the 60s and annihilated its shortcomings. The invention of JARVIS tells us that although the technology was quite limited at the time, creative minds were still able to fill the gaps because they understood the potentials of speech recognition. With each real-life advancement in speech technology, media portrayals of artificial intelligence became a thousand times more exaggerated. One can say that as fast as technology was evolving, the imagination moved much faster. The fear of evil robots came back with a vengeanceeven JARVIS couldnt escape this tired motif! In Avengers: Age of Ultron, the AI that Stark created (known as Ultron) became its own sentient being, consuming JARVIS in its wake because it possessed its own evil willseparate from its masters. This motif continued for quite some time, although it had a different spin. Now, theres a juxtaposition between sentient robots who are friendly and helpfula tribute to our new perspective on technologyand sentient robots who are destructivea remnant of our xenophobic past. This good vs. evil motif fueled inspiration for AI comics like Transformers (1984) and films like The Terminator (1984), I, Robot (2004) and WALL-E (2008). As voice recognition technology becomes more advanced and commonplace, we can expect to see at least one droid or AI system in every film that is set in modern day. Its clear that voice recognition plays an integral part in our imaginative future. In fact, we believe that it is ingrained in our vision for tomorrow. ",http://www.alleywatch.com/2017/01/famous-robots-rise-speech-technology/,"Before the invention of Audrey, the first speech recognition system that only understood numbers, robots were mainly portrayed in films and ...",Famous Robots and the Rise of Speech Technology,Audrey Flash Gordon Phantom Creeps Mysterious Doctor Satan Audrey Harpy Almost Star Wars Star Wars C-3PO Siri Markov Markov C-3PO J.A.R.V.I.S JARVIS Iron Man Jarvis Tony Starks Jarvis Jarvis JARVIS JARVIS Age Ultron AI Stark Ultron JARVIS AI Transformers Terminator Robot WALL-E AI,6
259,"New variant of Android ransomware comes with a bizarre twist. First there was a ransomware attack that spoke to its victims via a voice message, and now there's one in the wild that requires the victim read aloud - via voice recognition - the code to free his or her infected mobile device. Symantec researchers recently spotted a new variant of the so-called Android.Lockdroid.E mobile ransomware that now employs speech recognition APIs for the victim to input the unlock code rather than type it after paying the ransom. This bizarre yet creative twist to raises more questions than it answers about the attackers' intent, given the obvious inefficiency and potential fallibility of the voice-recognition step. The attack thus far has been targeting Chinese-speaking victims, and a ransom note written in Mandarin appears on the infected device's window with instructions to contract the attackers via QQ instant messaging to receive payment instructions and the unlock code. Since the victim's device is locked up with the ransomware, he or she must use a separate device to contact the attackers, which in and of itself could discourage or preclude payment if the victim doesn't obtain another mobile device to finish the transaction. That bulky and inefficient feature of the attack has researchers baffled. The attackers may well just be ""live-testing"" this as another payment approach, says Kevin Haley, director of Symantec Security Response. But Haley says it's likely this new voice recognition feature could backfire on the Lockdroid attackers. ""My guess is this isn't going to work as well,"" he says. ""If the victim can't figure out how to pay the ransom, [the campaign] isn't going to do so well,"" Haley says, adding that the researchers were unable to discern how many victims had fallen for the attack. Android.Lockdroid.E's new voice-recognition step follows its previous version's similarly odd step of requiring the victim to scan a barcode in order to log into the QQ messaging app: via a separate, second device. Symantec a year ago first detailed the barcode feature, noting that the malware posed as a porn app and gave the attackers admin rights on the infected device. The newest version harbors a few implementation bugs, according to Symantec, including improper speech recognition intent-firing and copy/paste flaws. The researchers say the authors likely are experimenting with new features to shake down their victims. Lockdroid.E is similar but not related to another mobile ransomware variant dubbed Android/LockScreen.Jisut by ESET, whose number of detections doubled in 2016 over the previous year, according to new ESET data. Lukas Stefanko, a malware researcher at ESET, says his firm calls LockDroid.E Android/LockerPin or Android/Locker. Symantec pointed out similarities between the two Android ransomware variants: ""The usage of QQ messenger as the communication platform is common across this wave of ransomware, and almost all of the Lockdroid and LockScreen variantsthat use Mandarin instructions share similar properties,"" says Dinesh Venkatesan, principal threat analysis engineer at Symantec. ""In short, we can say that they may be from similar groups, but we don't have solid proof that the two ransomware variants are related."" An earlier variant of Android/LockScreen.Jisut actually spoke to the victims via a voice message. ""After infecting the device, a female voice speaking Chinese 'congratulated' the victim and asked for 40 Yuans (approx. 6 dollars) to unlock it,"" ESET said in a mobile ransomware report published this month. That was likely the handiwork of young Chinese attackerspossibly teenagers, according to ESET. Unlike most ransomware that requires payment via Bitcoin or pre-paid cash vouchers to keep the money and recipient hidden, the LockScreen attackers don't seem to be trying to hide. ""If the information in the QQ profiles is valid, the malware operators are Chinese youths between 17 and 22 years old,"" ESET said in its report. Symantec's Haley notes that other ransomware attackers are providing more ""customer service"" such as instant messaging assistance to help their victims learn about Bitcoin and how to obtain it, for example. ""They're just out there trying to get their percentage of [victim] customers up,"" he says. Ransomware overall is exploding: new data this week from Check Point found that ransomware attacks doubled around the globe in the second half of 2016, from 5.5% to 10.5% of all attacks. Desktop ransomware families Locky (41%), Cryptowall (27%), and Cerber (23%) are the biggest culprits. The Hummingbad family of malware rules the mobile ransomware world for now, at 60%, according to Check Point. Meantime, other less pervasive but more bizarre forms of ransomware such as Lockdroid. E are popping up on mobile devices as ransomware authors toy with new ways to shake down their victims. Lockdroid is going through ""an evolution,"" Symantec's Haley notes. ",http://www.darkreading.com/endpoint/speak-up-ransomware-attack-uses-voice-recognition-/d/d-id/1328235,E mobile ransomware that now employs speech recognition APIs for the victim to input the unlock code rather than type it after paying the ...,Speak Up: Ransomware Attack Uses Voice Recognition,New Android First Android.Lockdroid.E APIs Mandarin QQ Kevin Haley Symantec Security Response Haley Lockdroid ] Haley Android.Lockdroid.E QQ Symantec Lockdroid.E Android/LockScreen.Jisut ESET ESET Lukas Stefanko ESET LockDroid.E Android/LockerPin Android/Locker Symantec Android QQ Lockdroid LockScreen Mandarin Dinesh Venkatesan Symantec Android/LockScreen.Jisut Chinese ESET Chinese ESET Bitcoin LockScreen QQ ESET Symantec Haley Bitcoin [ Ransomware Check Point Desktop Locky Cryptowall Cerber Hummingbad Check Point Meantime Lockdroid Lockdroid Symantec Haley,4
260,"Google's ""Cloud Speech API"" enters limited preview in 80 languages. If you want to build a productwith speech recognition capabilities, Nuancehas been the default choice for some time. The company's technology powers Apple's Siri and Samsung's S-Voice as well as car computing interfaces fromBMW, Chrysler, Ford, and many other automakers. Google has had its own voice recognition service for some time, but previously it was only used in Google-branded products like the Google app, Google keyboard,or Google.com. Now that voice recognition technology is being opened up to developers. At its NEXT cloud platform conference , Google announced the Cloud Speech API . The new API will bring Google's voice technology to the masses, and it seems to work pretty much the way it does in Google products today.Speech is streamedup to the cloud and back in real-time, including partial ""type-as-you-speak"" results. The transcribedtext can be dumped into an input field for voice transcription or used for a ""command and control"" feature, like bossing around a robot.Google's speech API can handle 80 languages and variants, while its now-rival Nuance only seems to support 38 . As part of the Google Cloud Platform, we'd imagine it needs a constant Internet connection to work. For now, the cost of the Cloud Speech API, which is only in a ""limited preview,"" is free. Google says it ""will introduce pricing in future phases."" The preview seems to be invite-only, but interested developers can fill out this page and hope they get accepted. Ron Amadeo Ron is the Reviews Editor at Ars Technica, where he specializes in Android OS and Google products. He is always on the hunt for a new gadget and loves to rip things apart to see how they work. ",https://arstechnica.com/gadgets/2016/03/google-to-take-on-nuance-with-speech-recognition-api/,"If you want to build a product with speech recognition capabilities, Nuance has been the default choice for some time. The company's ...",Google to take on Nuance with speech recognition API,Google Cloud Speech API Nuancehas Apple Siri Samsung Chrysler Ford Google Google Google Google.com NEXT Google Cloud Speech API API Google Google API Nuance Google Cloud Platform Internet Cloud Speech API Google Ron Amadeo Ron Reviews Editor Ars Technica Android OS Google,4
261,Markets data delayed by at least 15 minutes.  THE FINANCIAL TIMES LTD 2017. FT and Financial Times are trademarks of The Financial Times Ltd. ,https://www.ft.com/content/9059e394-6b95-11e6-a0b1-d87a9fea034f,"In the esoteric speech technology field, voice synthesis, or TTS (text-to-speech), is the country cousin of automatic speech recognition (ASR), ...",Synthesised speech deserves recognition,FINANCIAL TIMES LTD FT Financial Times Financial Times Ltd,-1
262,"Speech Recognition Better Than a Human's Exists. You Just Can't Use It Yet Chatting with a digital assistant is about as much fun as trying to reason with a stubborn child. If you've ever found yourself yelling at your Xbox or swearing at Siri , you may have already lost hope. But researchers say recent breakthroughs in speech recognition and artificial intelligence will soon make gadgets dramatically better at understanding people. This new breed of highly competent machines, which are able to not only hear us but to understand context and nuance, is just a year or two away, says Johan Schalkwyk, a distinguished engineer at Google. Schalkwyk (pronounced ""skulk-vick,"" though Siri calls him ""shaw-quick"") is working on an ambitious research project at Google to create speech systems that plug into the company's vast amounts of data. A project currently being tested in the lab allows computers to hear and essentially ""think"" about what people say into Google's digital ear, Schalkwyk says. Recent inventions in the field of speech and machine learning should lead to major changes in how we murmur, shout, question and interrogate our devices. One of the brains behind Siri says engineers are feverishly working toward speech recognition that's smart enough to engage in authentic conversations with users. ""All areas of spoken language understanding have made a lot of progress,"" says William Mark, a vice president at SRI International, which developed the fundamental technology behind Siri before it was acquired by Apple. ""This kind of conversational interaction is where the leading edge is right now."" Tim Tuttle has been waiting a long time for this. He earned a Ph.D from the Massachusetts Institute of Technology in 1997 and worked at its A.I. Lab. He spent the last decade making the rounds at various companies in Silicon Valley before founding his startup Expect Labs in 2010. Tuttle's company began working last year on a system to add complex voice commands to mobile apps, which might allow users to walk into a store and ask their phone which aisle brooms are located in. ""A year ago, we were doing a benchmarking, and our conclusion was it was not yet possible to do that. That's all changed, and our company has doubled down entirely around voice, primarily because of these improvements we've seen,"" Tuttle says. ""You're going to see speech recognition systems that have human or better-than-human accuracy become commercialized."" But first, a quick history lesson: Two and a half years ago, researchers from Google and the University of Toronto published an influential paper about using ""deep neural networks"" to model speech in computers, and followed this up several months later with another paper resulting from a collaboration with Microsoft and IBM . This led to what Google engineer Jeff Dean describes as the ""biggest single improvement in 20 years of speech research."" The findings resurrected a decades-old invention around digital neural networks. The technology tested well in the 1980s at predicting and analyzing large fields of data, but performance was hindered by the wimpy speed of computers at the time. Neural networks only became a viable option recently, following a massive speed-up in computer processing and in the development of new software approaches. Google's lab project builds off of this research. Six months ago, the team moved on from an older method, called feed-forward neural networks, in favor of recurrent neural networks. The switch allows the system to store more information, and process longer and more complex sequences. Google's breakthrough results from a simplification of the underlying code that will let its software hold more ideas and concepts within the same system, making it easier to ask complicated questions and get sensible answers. ""System complexity can hurt your long-term growth,"" Schalkwyk says. Google's system currently uses context, physical location and certain other things it knows about the speaker to make assumptions on where a conversation is going and what it all meansjust like humans do. Google's new network technology should do this so efficiently that it can process larger amounts of data than ever before, allowing it to answer more complex requests. To explain how the future of voice recognition should work, Schalkwyk likes to reference a high-end Vietnamese eatery located a few miles from Google's headquarters in Mountain View, California. Xanh Restaurant poses a challenge to typical speech recognition systems because its namepronounced ""zahn""is ""very difficult to recognize,"" says Schalkwyk. ""If I can map it and say, 'It's a restaurant, and this restaurant is in California,' then the list of restaurants suddenly gets a lot smaller,"" he says. ""Using that semantic knowledge, we can significantly improve quality."" It sounds trivial, but to a computer, hearing a word, recognizing the context from the sentence and then layering that information over geography is extremely difficult and takes time. Today, Google voice search can recognize the restaurant correctlyperhaps because its creators are regular patrons, we wonder. In the future, Google will be able to handle many other equally ambiguous questions, Schalkwyk says. Inside Google, there's been an ""unprecedented"" number of technological evolutions in speech recognition, says Schalkwyk. While Google's big step forward is still another year or two away from showing up on your phone, the project is already yielding techniques that are making their way into other appendages of Google's mammoth brain. ""You build something to go to the moon, and in the meantime, you develop a hundred other technologies that are useful,"" Schalkwyk says. Three years ago, Google's voice recognition could recognize just three out of four words coming out of your mouth, Schalkwyk says. Thanks to an accelerated pace of innovation, the Google apps on your phone right now can correctly guess 12 out of every 13 words. Pretty soon, according to Tuttle, ""We're going to live in a world where devices dont have keyboards."" Press spacebar to pause and continue. Press esc to stop. ",http://www.bloomberg.com/news/2014-12-23/speech-recognition-better-than-a-human-s-exists-you-just-can-t-use-it-yet.html,But researchers say recent breakthroughs in speech recognition and artificial intelligence will soon make gadgets dramatically better at ...,Speech Recognition Better Than a Human's Exists. You Just Can't ...,Human Siri Johan Schalkwyk Google Schalkwyk Siri Google Google Schalkwyk Siri William Mark SRI International Siri Apple Tim Tuttle Massachusetts Institute Technology A.I Lab Silicon Valley Expect Labs Tuttle Tuttle Google University Toronto Microsoft IBM Google Jeff Dean Google Google System Schalkwyk Google Google Schalkwyk Google Mountain View California Restaurant Schalkwyk California Google Google Schalkwyk Inside Google Schalkwyk Google Google Schalkwyk Google Schalkwyk Google Pretty Tuttle Press Press,4
263,"Amazon says more than 5,000 people a day profess their love for Alexa. On the other hand, Alexa devotees also know that unless you speak to her very clearly ... and ... slowly, she's likely to say: Sorry, I don't have the answer to that question. Amazon's Echo has made tangible the promise of an artificially intelligent personal assistant in every home. Those who own the voice-activated gadget (known colloquially as Alexa, after its female interlocutor) are prone to proselytizing ""her"" charms, applauding Alexa's ability to call an Uber, order pizza or check a 10th-grader's math homework. The company says more than 5,000 people a day profess their love for Alexa. On the other hand, Alexa devotees also know that unless you speak to her very clearly ... and ... slowly, she's likely to say: Sorry, I don't have the answer to that question. ""I love her. I hate her, I love her,"" one customer wrote on Amazon's website, while still awarding Alexa five stars. ""You will very quickly learn how to talk to her in a way that she will understand and it's not unlike speaking to a small frustrating toddler."" Voice recognition has come a long way in the past few years. But it's still not good enough to popularize the technology for everyday use and usher in a new era of human-machine interaction, allowing us to talk with all our gadgets-cars, washing machines, televisions. Despite advances in speech recognition, most people continue to swipe, tap and click. And probably will for the foreseeable future. What's holding back progress The artificial intelligence that powers the technology has room to improve. There's also a serious deficit of data-specifically audio of human voices, speaking in multiple languages, accents and dialects in often noisy circumstances that can defeat the code. So Amazon, Apple, Microsoft and China's Baidu have embarked on a worldwide hunt for terabytes of human speech. Microsoft has set up mock apartments in cities around the globe to record volunteers speaking in a home setting. Every hour, Amazon uploads Alexa queries to a vast digital warehouse. Baidu is busily collecting every dialect in China. Then they take all that data and use it to teach their computers how to parse, understand and respond to commands and queries. The challenge is finding a way to capture natural, real-world conversations. Even 95 percent accuracy isn't enough, says Adam Coates, who runs Baidu's artificial intelligence lab in Sunnyvale, California. ""Our goal is to push the error rate down to 1 percent,"" he says. ""That's where you can really trust the device to understand what you're saying, and that will be transformative."" Not so long ago, voice recognition was comically rudimentary. An early version of Microsoft's technology running in Windows transcribed ""mom"" as ""aunt"" during a 2006 demo before an auditorium of analysts and investors. When Apple debuted Siri five years back, the personal assistant's gaffes were widely mocked because it, too, routinely spat out incorrect results or didn't hear the question correctly. When asked if Gillian Anderson is British, Siri provided a list of English restaurants. Now Microsoft says its speech engine makes the same number or fewer errors than professional transcribers, Siri is winning grudging respect, and Alexa has given us a tantalizing glimpse of the future. Much of that progress owes a debt to the magic of neural networks, a form of artificial intelligence based loosely on the architecture of the human brain. Neural networks learn without being explicitly programmed but generally require an enormous breadth and diversity of data. The more a speech recognition engine consumes, the better it gets at understanding different voices and the closer it gets to the eventual goal of having a natural conversation in many languages and situations. Hence the global scramble to capture a multitude of voices. ""The more data we shove in our systems the better it performs,"" says Andrew Ng, Baidu's chief scientist. ""This is why speech is such a capital-intensive exercise; not a lot of organizations have this much data."" When the industry began working seriously on voice recognition in the 1990s, companies like Microsoft relied on publicly available data from research institutes such as the Linguistics Data Consortium, a storehouse of voice and text data founded in 1992 with backing from the U.S. government and located at the University of Pennsylvania. Then tech companies started collecting their own voice data, some of it garnered from volunteers who came in to read and be recorded. Now, with the popularity of speech-controlled software gaining ground, they harvest much of the data from their own products and services. When you tell your phone to search for something, play a song or guide you to a destination, chances are a company is recording it. (Apple, Google, Microsoft and Amazon emphasize that they anonymize user data to protect customer privacy.) When you ask Alexa what the weather is or the latest football score, the gadget uses the queries to improve its understanding of natural language (although ""she"" isn't listening to your conversations unless you say her name). ""By design, Alexa gets smarter as you use her,"" says Nikko Strom, senior principal scientist for the program. A key challenge is getting the technology conversant with multiple languages, accents and dialects. Nowhere, perhaps, is this more crucial than in China. Seeking to harvest dialects from all over the country, Baidu launched a marketing campaign during Chinese New Year earlier this year. Calling the push a ""dialect conservation initiative,"" the search giant promised people that if they contributed they would help usher in a future when they would talk to Baidu using their dialect. In two weeks, the company recorded more than 1,000 hours of speech to plug into its computers. Many people did it for free simply because they were proud of their hometown dialects. A high school teacher in Sichuan was so excited about the program, he asked a class of students to record more than 1,000 ancient poems in Sichuanese. Another challenge: teaching voice recognition technology to pick up commands over background noise-the clamor of happy hour, say, or the cacophony of a sports stadium. Microsoft has deployed an Xbox app called Voice Studio to harvest conversation over the din of users shooting villains or watching movies. The company offered rewards including points and digital apparel for avatars and lured hundreds of subjects willing to contribute their game chatter to Microsoft's speech efforts. The program worked gangbusters in Brazil, where the local subsidiary promoted the app heavily on the main Xbox page. The data was used to create the Brazilian Portuguese version of Cortana, released earlier this year. Companies are also designing voice recognition systems for specific situations. Microsoft has been testing technology that can answer travelers' queries without being distracted by the constant barrage of flight announcements at airports. The company's technology is also being used in an automated ordering system for McDonald's drive-thrus. Trained to ignore scratchy audio, screaming kids and ""ums,"" it can spit out a complicated order, getting even the condiments right. Amazon is conducting tests in automobiles, challenging Alexa to work well with road noise and open windows. Even as companies scour the world for data, they're figuring out ways to improve voice recognition with less of it. The technology being tested at McDonald's is more accurate than other systems that use much more data, says Xuedong Huang, Microsoft's chief speech scientist, who has been working on voice recognition at the company for more than two decades. ""You can always have breakthroughs even without using the most data."" Google generally subscribes to a less-is-more philosophy, deploying a piecemeal approach that uses unintelligible units of sound to build words and phrases. With its speech recognition system, the company aims to solve multiple problems with just one change. For its data sets, Google strings together tens of thousands of audio snippets that are typically two to five seconds long. The process requires less computing power and can be more easily tested and tweaked, says Google researcher Franoise Beaufays. For its part, Baidu is working on more efficient algorithms where learning one language makes it easier to learn the next twelve. That's particularly important for those spoken by tens of thousands of people rather than millions, where there just won't be huge swaths of data no matter what, says Ng, the company's chief scientist. Ask researchers like Ng when it will be possible to speak naturally to your digital assistant and they get wistful. No one really knows. Neural networks remain mysterious even to those who understand them best. And much of the work is trial and error; make a tweak here and you're never quite sure what will happen there. Based on the current technology and methods, the process will probably take years. But Ng, Huang, Beaufays and other scientists say you never know when a breakthrough will arrive, catapulting research forward and turning Alexa and Siri into true conversationalists. ",http://www.dailyherald.com/article/20161217/business/161219157/,"Despite advances in speech recognition, most people continue to swipe, tap and click. And probably will for the foreseeable future.","Why Google, Microsoft and Amazon want to hear your voice",Amazon Alexa Alexa Sorry Amazon Echo Alexa Alexa Uber Alexa Alexa Sorry Amazon Alexa Voice Amazon Apple Microsoft China Baidu Microsoft Amazon Alexa Baidu China Adam Coates Baidu Sunnyvale California Microsoft Windows Apple Siri Anderson Siri Microsoft Siri Alexa Hence Andrew Ng Baidu Microsoft Linguistics Data Consortium U.S. University Pennsylvania Apple Google Microsoft Amazon Alexa Nikko Strom China Baidu New Year Baidu Sichuan Sichuanese Microsoft Voice Studio Microsoft Brazil Xbox Portuguese Cortana Microsoft McDonald Amazon Alexa McDonald Xuedong Huang Microsoft Google Google Google Franoise Beaufays Baidu Ng Ask Ng Ng Huang Beaufays Alexa Siri,7
264,"The futuristic vision of humans having conversations with machines was promised long before Michael Knights KITT burst onto TV screens. This reality is now closer than ever So whilst VUI will be a key piece in the authentication puzzle, enterprises cannot look to voice as a direct and sole replacement for authentication. No matter how good the VUI is today, it is still always better to have multi-factor authentication Voice-user interface (VUI) technology such as Siri, Cortana and Amazons Echo has advanced to a point where voice recognition can be used as an authentication alternative to passwords. High profile technology investor Mary Meeker recently dedicated a large section of her annual report on the state of the internet to lift-off voice-user interface technology. This tech makes human interaction with computers possible through speech. While VUIhas been around for decades, the technology has made massive strides over the years and its improving accuracy continues to raise its profile. In the 1970s machines could recognise words with just 10% accuracy, by 2010 that had reached around 70%, and today it stands at approximately 90%. Now its only a matter of time before accuracy is no longer an issue  for instance Google has announced that its working to ensure its speech recognition software will work with even the thickest of accents. As VUIaccuracy continues to improve, it will naturally become a form of authentication as all voices have subtle differences, similar to fingerprints. Consumer facing organisations are already starting to trial voice recognition as a method of authentication. >See also: Think before you speak: voice recognition replacing the password A number of UK banks are seeing the need for this and have introduced new voice related security measures. Findings from a survey carried out by Pindrop showed that over half of respondents felt that no bank was fully secure, and 59% said they would leave their bank if they thought another one was more secure. Voice recognition is also finding its way into the workplace, with employees being able to access work profiles and systems by simply speaking. However, using voice authentication presents some clear concerns. Chief among these are the security implications: how will voice authentication be added, changed and shared organisation-wide Today 95% of employees use a typed password to access email, and devices such as phones and laptops. As a result, amongst the most common tickets raised to IT help desks is forgotten password requests. The emergence of voice authentication should offer reprieve to the IT department, freeing them up to address more pressing tasks. The most astute and forward-thinking IT leaders will be paying close attention to developments in VUI. However, adoption should be cautious and staged in order to limit the security risks to the business  if there are speed bumps during implementation, new security operations can be tested, re-tested and contained if necessary. Gradual adoption is how enterprises have traditionally met the introduction of new technology such as the rise of the smartphone. >See also: Are the Brits too trusting of biometric security IT teams have traditionally beennervous about allowing employees to access work email on their personal devices. In the same way, as organisations recognise the advantages of voice authentication and race to gain competitive advantage they should slow down and find the solution that best fits. Two-factor authentication adds a second level of verification to an account log-in. In traditional password authentication, the additional credential can be a further piece of information known by the user such as a phone number, or owned by the user like a biometric. The added layer of security can trump the action of a hacker with access to the first authentication information. But there is a dilemma with passwords in that hackers can pretend to be users and request to recover sign-in credentials  which is much harder with voice recognition. Two-factor authentication is part of the larger movement of maturing multi-factor authentication. For instance, biometrics are one way to solve the credential recovery issue but it cannot detect fraud on its own. So whilst VUI will be a key piece in the authentication puzzle, enterprises cannot look to voice as a direct and sole replacement for authentication. No matter how good the VUI is today, it is still always better to have multi-factor authentication. It may not stop every attacker, but creating multiple layers certainly creates a roadblock for many of them. Even KITT got hacked once. ",http://www.information-age.com/voice-recognition-enterprise-123462469/,Now it's only a matter of time before accuracy is no longer an issue – for instance Google has announced that it's working to ensure its speech ...,Is voice recognition to become part of enterprise authentication?,Michael Knights KITT So VUI VUI Voice-user VUI Siri Cortana Amazons Echo High Mary Meeker VUIhas Google VUIaccuracy Consumer See A UK Pindrop Voice Chief IT IT IT VUI See Brits IT VUI VUI KITT,7
265,"Everyone familiar with Siri, Google Now, Cortana, S-Voice, and/or Echo is familiar with the progress and improvement in speech recognition over the past decade. Much of this improvement comes from cloud -based recognizers deploying deep learning on big data . Although its often out of the spotlight, theres been lots of progress in speech recognition for embedded systems. In fact, most of the major speech engines deploy a combination of embedded plus cloud-based recognition. This is most noticeable in commands like Hey Siri, OK Google, Hey Cortana, Hi Galaxy, and Alexa. All of these cloud-based recognition systems use embedded trigger phrases to open the cloud connection to ready itself for the speech recognition. Embedded trigger phrases allow a few improvements and practicalities over cloud-based approaches. For one, having an embedded recognizer always on is a lot less creepy than having your conversations going up to the clouds for Google and others to analyze any way they want. Since its on-device, theres no speech recording or transmitting until the trigger phrase is spoken, and the trigger listening is done in real time without your speech being sent off. There are also practical reasons for an embedded wake-up trigger, and a leading one is power consumption. Running exclusively on in the cloud would require lots of data transfer and analysis, making a battery-operated or green product impractical. Many major DSP companies have solutions for always on DSPs that run Sensorys TrulyHandsfree wake-up trigger options at 2 mA or less. With sound activity detection schemes, the average battery drain can be under 1 mA, placing it in the realm of battery leakage. Other popular uses of embedded speech recognition are in devices that want fast and accurate responses to limited commands. One of my favorite examples is in the Samsung Galaxy smartphones where, in camera mode, users can enable voice commands to take pictures. This works for me from up to 20 feet away in a quiet setting or 5 feet in a noisier location. Its an awesome alternative to carrying around a selfie stick, and whenever I show this feature to people they quickly get it and love it. Embedded speaker verification is also being deployed more frequently and is often incorporated into a wake-up trigger to decrease the probability that others can wake up your device. With speech recognition and speaker verification, theres always a trade-off between false accepts (accepting the wrong user) and false rejects (rejecting the right user). The preferred wake-up trigger setting is often to keep false rejects extremely low at the cost of occasionally letting the right person in. In systems requiring more sophisticated speaker verification for security, its possible to deploy more complex algorithms that dont require the lowest power consumption, to gain better accuracy at the cost of increasing current consumption. As consumer products and mobile phones use more sophisticated processors, I expect a higher percentage of speech recognition use will move to the embedded devices, and a layered speech-recognition approach will emerge, whereby a fast initial analysis is done on device and responded to if the device has a high confidence of success (self-perception), but passed to the cloud if its less sure of its response or if a cloud-based search is required. Todd Mozer is the CEO of Sensory. He holds over a dozen patents in speech technology and has been involved in previous startups that reached IPO or were acquired by public companies. Todd holds an MBA from Stanford University, and has technical experience in machine learning, semiconductors, speech recognition, computer vision, and embedded software. ",http://embedded-computing.com/guest-blogs/deploying-speech-recognition-locally-versus-the-cloud/,"Everyone familiar with Siri, Google Now, Cortana, S-Voice, and/or Echo is familiar with the progress and improvement in speech recognition ...",Deploying speech recognition locally versus the cloud,Siri Google Cortana S-Voice Echo Hey Siri OK Google Hey Cortana Hi Galaxy Alexa Embedded Google DSPs Sensorys TrulyHandsfree Samsung Galaxy Todd Mozer Sensory IPO Todd Stanford University,-1
266,"LONDON: The global automotive voice and speech recognition (VR) system market is expected to grow at a CAGR of over 11 percent during 2016-2020, according to latest report by market research firm Technavio . Siddhartha Jaiswal , a lead analyst from Technavio, specializing in research on automotive electronics sector, says, ""The industry is in a state of technological flux, and the players that come up with the best possible solution within an acceptable timescale are expected to take up the mantle of being the breakthrough companies, which can help them carve out a massive market share for themselves."" The report further talks about the top three emerging trends driving the global automotive voice and speech recognition system market. Automakers supply fully integrated suites like Ford's SYNC and Fiat's Blue&Me, which not only provide a common interface to most of the car's electronic functions but also incorporate VR to control these connected systems through an elaborate set of commands or simply real-time natural language processing (NLP) in some cases. These suites of overarching electronic control systems were developed much earlier compared than the VR components. So when VR did come to the market, it was swiftly incorporated by the respective automakers into their existing platforms. VR systems thus, are a part of the much wider suite of functionalities in the automakers' electronic platforms. VR systems are of three types: on-device, off-board, and hybrid. Of these three, the off-board systems, which primarily utilize cloud facilities to provide VR services to the users, are expected to assume prominence in the development plans of automakers. However, for such a system to work in an automobile, it becomes necessary to have uninterrupted connectivity to the Internet. This is also because connectivity to its cloud services for fast execution of user queries would be the unique selling proposition of these systems. Currently, telematics in automobiles largely depends on OEM-driven subscription-based systems like OnStar, or carrier-based systems based on 3G/4G packages. ""Dedicated on-board and off-board telematics development is expected to broaden the telematics universe,"" says Siddharth. One of the reasons why the current VR system misinterprets commands is that the driver's voice is usually drowned out by in-car noise. In order to fix this issue, it has been suggested that automakers could layer on noise suppression systems in their vehicles. While this may be counterproductive in the case of chatting between those seated in the front and rear seats, Harman International has been involved in an effort to cancel noise for the benefit of automotive VR systems. This effort has led to the invention of an innovative system, which creates a noise-blocking sonic curtain between the front and rear seats. We expect such sonic curtains to become standard accessory in cars, with effective VR systems. It must be noted that sonic curtains do have limitations, and therefore, R&D effort should be concentrated in the direction of creating localized sonic curtain technology, which can block unwanted in-car noise from interfering with the VR system's work, as well as allow the passengers to naturally talk to each other. Another innovative method of noise cancellation is the creation of a so-called virtual cube in space, wherein the VR system is able to sense sound originating only from within this space. ",http://auto.economictimes.indiatimes.com/news/auto-technology/top-3-trends-impacting-global-automotive-voice-speech-recognition-market/53378071,LONDON: The global automotive voice and speech recognition (VR) system market is expected to grow at a CAGR of over 11 percent during ...,Top 3 trends impacting global automotive Voice-speech Recognition ...,VR CAGR Technavio Siddhartha Jaiswal Technavio Ford SYNC Fiat Blue Me VR NLP VR VR VR VR VR Internet OnStar Siddharth VR Harman International VR VR R D VR VR,-1
267,"Microsofts artificial intelligence (AI) system can now understand conversational human speech better than a trained transcriptionist, and is less prone to error. The AIs error rate in understanding speech moved down to about 5.9% from 6.3%, which puts it slightly below the human error rate. Error rate refers to the number of times a human, or machine, mishears words. We [improved] on our recently reported conversational speech recognition system by about 0.4%, and now exceed human performance by a small margin, the report stated. >See also: Think before you speak: voice recognition replacing the password This news comes only a month after Microsoft achieved the 6.3% error rate. It is learning fast. The research found, however, that the error rates of human transcribers can vary between 4.1% to 9.6%, depending on how well they concentrate on the transcription. The near-perfect accuracy, regardless, is somewhat of a breakthrough and should have significant impacts on Microsofts AI tools, including its virtual personal assistant Cortana. Although it is unclear exactly how the real-world applications, where background noise and multiple speakers are significant issues, will take form. Perhaps, simply, less speech-enabled error when interacting with smartphones or, in the future, autonomous cars. As reported by The Verge citing a statement from the company, Microsofts chief speech scientist Xuedong Huang said that they had reached human parity, and called the improvement in speech recognition an historic achievement. This human parity was achieved by optimising convolutional and recurrent neural networks, using 2,000 hours of voice recorded data. Microsofts AI voice recognition announcement reflects a the focus the company has placed on the technology. Indeed, last month Microsoft CEO Satya Nadella laid out the organisations 4 pillar plan for democratizing AI, and said that its cloud platform Azure is becoming the first AI supercomputer. ",http://www.information-age.com/voice-recognition-ai-just-beaten-human-123462851/,"Microsoft's artificial intelligence (AI) system can now understand conversational human speech better than a trained transcriptionist, and is less ...",Voice recognition: has AI just beaten a human?,Microsofts AI AIs See Microsoft Microsofts AI Cortana Verge Microsofts Xuedong Huang Microsofts AI Microsoft CEO Satya Nadella AI Azure AI,2
268,"Speech-recognition systems promise the world. But for more than nine million people with voice ailments, that world is out of reach By Emily Mullin on May 27, 2016 Emma Mattes has given up on Siri. No matter how clearly or slowly Mattes speaks, the Apple iPhones iconic voice-recognition technology has been no help to the 69-year-old woman from Seminole, Fla. She struggles with spasmodic dysphonia, a rare neurological voice disorder that causes involuntary spasms in the vocal cords, producing shaky and unstable speech. Her cars Bluetooth voice system does not understand her either. Voice interfaces like Siri have now been sold in millions of products ranging from smartphones and Ford vehicles to smart TVs and the Amazon Echo. These systems promise to let people check the weather, lock their house doors, place a hands-free call while driving, record a TV show and buy the latest Beyonc album with simple voice commands. They tout freedom from buttons and keyboards and promise nearly endless possibilities. But the glittering new technology cannot be used by more than nine million people in the U.S. with voice disabilities like Mattes nor by stutterers or those afflicted with cerebral palsy and other disorders. Speech recognizers are targeted at the vast majority of people at that center point on a bell curve. Everyone else is on the edges, explained Todd Mozer, CEO of the Silicon Valleybased company Sensory, which has voice-recognition chips in a variety of consumer products like Samsung Galaxy phones and Bluetooth headsets. Worse, help for people like Mattes may be a long way off. Although voice recognition is getting more accurate, experts say it is still not very good at recognizing many atypical voices or speech patterns. Researchers are trying to develop more inclusive voice recognizers, but that technology has serious hurdles to overcome. People on Mozers edges include approximately 4 percent of the U.S. population that had trouble using their voices for one week or longer during the past 12 months because of a speech, language or vocal problem, according to the National Institute on Deafness and Other Communication Disorders. Dysarthria, which is slow or slurred speech that can be caused by cerebral palsy, muscular dystrophy, multiple sclerosis, stroke and a variety of other medical conditions, are part of this spectrum of problems. And the trouble extends worldwide. Cerebral palsy, for instance, affects the speech of Mike Hamill, of Invercargill, New Zealand, who was born with the disease and developed swallowing and throat control difficulties in his 30s. As a result, his speech is often strained and erratic. People who stutter also have trouble using voice-recognition technology, like automated phone menus, because these systems do not recognize their disjointed speech, says Jane Fraser, president of The Stuttering Foundation of America. There are other problems, such as vocal cord paralysis or vocal cysts, which tend to be less severe and are usually temporary. But these disorders can still reduce accuracy in speech recognition. For example, in a 2011 study that appeared in Biomedical Engineering Online researchers used a conventional automatic speech-recognition system to compare the accuracy of normal voices and those with six different vocal disorders. The technology was 100 percent correct at recognizing the speech of normal subjects but accuracy varied between 56 and 82.5 percent for patients with different types of voice ailments. For individuals with severe speech disorders like dysarthria, this technologys word-recognition rates can be between 26.2 percent and 81.8 percent lower than for the general population, according to research published in Speech Communication by Frank Rudzicz, a computer scientist at the Toronto Rehabilitation Institute and assistant professor at the University of Toronto. Theres a lot of variation among people with these disorders, so its hard to narrow down one model that would work for all of them, Rudzicz says. This vocal variation is exactly why systems like Siri and Bluetooth have such a hard time understanding people with speech and voice disorders. Around 2012 companies started using neural networks to power voice-recognition products. Neural networks learn from a variety of speech samples and predictable patterns. Intelligent personal assistants like Siri and Google Now were not that robust when they first came out in 2011 and 2012, respectively. But they got better as they acquired more data from many different speakers, Mozer says. Now, these systems can do a lot more. Many companies boast an 8 percent or less word error rate, says Shawn DuBravac, chief economist and senior director of research at the Consumer Technology Association. Amazon Echo, which became widely available in June 2015, has a voice recognizer called Alexa that is targeted to perform specific functions such as fetching news from local radio stations, accessing music streaming services and ordering merchandise on Amazon. The device also has voice controls for alarms and timers as well as shopping and to-do lists. Over time Amazon has been adding more functions. But the nature of speech and vocal disabilities is that they produce random and unpredictable voices, and voice-recognition systems cannot identify patterns to train on. Apple and Amazon declined to address this problem directly when asked to comment, but said via email that, in general, they intend to improve their technology. Microsoft, which developed the speech-recognition personal assistant Cortana, said via a spokesperson that the company strives to be intentionally inclusive of everyone from the beginning when designing and building products and services. To find solutions, companies and researchers have looked to lip-reading, which has been used by some deaf and hard of hearing people for years. Lip-reading technology could provide additional data to make voice recognizers more accurate, but these systems are still in their early stages. At the University of East Anglia in England, computer scientist Richard Harvey and his colleagues are working on lip-reading technology that spells out speech when voice recognition is not enough to determine what a person is saying. Lip-reading alone will not make you able to deal with speech disability any better. But it helps because you get more information, Harvey says. Some products and systems might be more amenable to learning unusual voices, researchers say. A banks voice-automated customer service phone system or a cars hands-free phone system have limited vocabulariesso hypothetically, Harvey says it would be easier to build a set of algorithms that recognize different versions and pronunciations for a fixed set of words. But these systems still use some unique words like the users name, which have to be learned. Another possibility is that devices could have the ability to ask clarifying questions to users when their voice-recognition systems do not immediately understand them, DuBravac says. Better-designed neural networks could eventually be part of the solution for people with speech disabilitiesit is just a matter of having enough data. The more data that becomes available, the better this technology is going to get, Mozer says. That is starting to happen already with different languages and accented speech. According to Apple, Siri has so far learned 39 languages and language variants. But as this technology in its current state becomes more embedded in our daily lives, researchers such as Rudzicz warn that multitudes of people with speech and vocal problems will be excluded from connected smart homes with voice-activated security systems, light switches and thermostats, and they might not be able to use driverless cars. These individuals need to be able to participate in our modern society, he says. So far, attempts by tech companies to include them are little more than talk. ",https://www.scientificamerican.com/article/why-siri-won-t-listen-to-millions-of-people-with-disabilities/,"Although voice recognition is getting more accurate, experts say it is still not very good at recognizing many atypical voices or speech patterns.",Why Siri Won't Listen to Millions of People with Disabilities,Emily Mullin May Emma Mattes Siri Mattes Apple Seminole Fla. Bluetooth Voice Siri Ford TVs Amazon Echo Beyonc U.S. Todd Mozer CEO Silicon Sensory Samsung Galaxy Bluetooth U.S. National Institute Deafness Communication Disorders Dysarthria Cerebral Mike Hamill Invercargill New Zealand Jane Fraser Stuttering Foundation America Biomedical Engineering Online Speech Communication Frank Rudzicz Toronto Rehabilitation Institute University Toronto Rudzicz Siri Bluetooth Intelligent Siri Google Mozer Shawn DuBravac Consumer Technology Association Amazon Echo June Alexa Amazon Amazon Apple Amazon Microsoft Cortana Lip-reading University East Anglia England Richard Harvey Harvey Harvey DuBravac Mozer Apple Siri Rudzicz So,-1
269,"Tiny electronic device can monitor heart, recognize speech Researchers from the University of Colorado Boulder and Northwestern University have developed a tiny, soft and wearable acoustic sensor that measures vibrations in the human body, allowing them to monitor human heart health and recognize spoken words. The stretchable device captures physiological sound signals from the body, has physical properties well-matched with human skin and can be mounted on nearly any surface of the body, said CU Boulder Assistant Professor Jae-Woong Jeong, one of three lead study authors. The sensor, which resembles a small Band-Aid, weighs less than one-hundredth of an ounce and can gather continuous physiological data. This device has a very low mass density and can be used for cardiovascular monitoring, speech recognition and human-machine interfaces in daily life, said Jeong of the Department of Electrical, Computer and Energy Engineering. It is very comfortable and convenient  you can think of it as a tiny, wearable stethoscope. A paper on the subject was published Nov. 16 in Science Advances, a sister journal of Science. The other two co-corresponding authors are Professors Yonggang Huang and John Rogers of Northwestern. The thin, soft, skin-like characteristics of these advanced wearable devices provide unique capabilities for listening in to the intrinsic sounds of vital organs of the body, including the lungs and heart, with important consequences in continuous monitoring of physiological health, said Rogers, the Simpson Querrey Professor of Materials Science and Engineering, Biomedical Engineering and Neurological Surgery. Rogers also is director of Northwesterns Center for Bio-Integrated Electronics. The researchers say the new device can pick up mechanical waves that propagate through tissues and fluids in the human body due to natural physiological activity, revealing characteristic acoustical signatures of individual events. They include the opening and closing of heart valves, vibrations of the vocal cords and even movements in gastrointestinal tracts. The sensor can also integrate electrodes that can record electrocardiogram (ECG) signals that measure the electrical activity of the heart as well electromyogram (EMG) signals that measure the electrical activity of muscles at rest and during contraction. While the sensor was wired to an external data acquisition system for the tests, it can easily be converted into a wireless device, said Jeong. Such sensors could be of use in remote, noisy places  including battlefields  producing quiet, high-quality cardiology or speech signals that can be read in real time at distant medical facilities. Using the data from these sensors, a doctor at a hospital far away from a patient would be able to make a fast, accurate diagnosis, said Jeong. Vocal cord vibration signals also could be used by the military personnel or civilians to control robots, vehicles or drones. The speech recognition capabilities of the sensor also have implications for improving communication for people suffering from speech impairments, he said. As part of the study, the team used the device to measure cardiac acoustic responses and ECG activity including the detection of heart murmurs  in a group of elderly volunteers at Camp Lowell Cardiology, a private medical clinic in Tucson, Arizona collaborating with the University of Arizona, a project partner. The researchers also were able to detect the acoustical signals of blood clots in a related lab experiment, said Jeong. Other CU Boulder study co-authors on the Science Advances paper include Assistant Professor Jianliang Xiao and doctoral student Zhanan Zou of mechanical engineering and doctoral student Raza Qazi of electrical engineering. The sticky, flexible polymer encapsulating the tiny device is stretchable enough to follow skin deformation, said study first author Yuhao Liu, who earned his doctorate and the University of Illinois-Urbana Champaign and now works at Lam Research, headquartered in Fremont, California. The device contains a tiny commercial accelerometer to measure the vibration of the body acoustics and allows for the evaporation of human sweat. The researchers also showed vocal cord vibrations gathered when the device is on ones throat can be used to control video games and other machines. As part of the study a test subject was able to control a Pac-Man game using vocal cord vibrations for the words up, down, left and right. While other skin electronics devices have been developed by researchers, what has not been demonstrated before is the mechanical-acoustic coupling of our device to the body through the skin, Jeong said. Our goal is to make this device practical enough to use in our daily lives. The study also included the Eulji University College of Medicine in Korea. ",http://www.colorado.edu/today/2016/11/16/tiny-electronic-device-can-monitor-heart-recognize-speech,"This device has a very low mass density and can be used for cardiovascular monitoring, speech recognition and human-machine interfaces in ...","Tiny electronic device can monitor heart, recognize speech",Tiny Researchers University Colorado Boulder Northwestern University CU Boulder Assistant Professor Jae-Woong Jeong Band-Aid Jeong Department Electrical Computer Energy Engineering Nov. Science Advances Science Professors Yonggang Huang John Rogers Northwestern Rogers Simpson Querrey Professor Materials Science Engineering Biomedical Engineering Neurological Surgery Rogers Northwesterns Center Bio-Integrated Electronics ECG EMG Jeong Jeong Vocal ECG Camp Lowell Cardiology Tucson Arizona University Arizona Jeong CU Boulder Science Advances Assistant Professor Jianliang Xiao Zhanan Zou Raza Qazi Yuhao Liu University Illinois-Urbana Champaign Lam Research Fremont California Jeong Eulji University College Medicine Korea,4
270,"For those not comfortable in English, using messaging platforms can be a tedious task. There are phones these days that integrate regional language keyboards in the operating system, but ever tried stringing together a five-line sentence in Punjabi Hunting the correct maatras on the cumbersome keyboard can easily set you back by multiple minutes. Hence, an app called Gappi tried to change how people converse by converting speech to text, which was made possible by using Liv.Ais technology. Only 5% in India are comfortable with English, and if you ask how many can express in English, those numbers are even less. So, we use Liv.Ais API at the backend, and do the handholding for the user from our side, said Nikhil Jha, founder of Gappi, a chat app available in multiple Indian languages that converts speech into text. This is just one of the many use-cases that Liv.Ais technology offers. Liv Artificial Intelligence Pvt Ltd was founded in May 2015 by three friends from IIT Kharagpur, Subodh Kumar, Sanjeev Kumar and Kishore Mudra. They all believe that the next big catalyst is the disruption the Internet is bringing to industries such as healthcare, mobile-tech, defence, transportation and deep learning. One thing we realised was that our machines should be able to talk to us, and we should be working on speech recognition, said Subodh. The first few months were spent in research, as it is a deeply technical field with scarce resources. People in India are not proficient in English, but the ecosystem has been built around English. Content creation to build this system is a hassle. Speech recognition in regional languages eliminates the need to speak or type in English, helping machines to talk to you in your language, added Subodh. For example, one company that Liv.Ai is working with is in e-commerce, which uses its automated speech system to call customers for feedback. Using Liv.Ais application programming interface (API) reduces time and labour costs as the answers get transcribed into text and are ready for analysis by the company, without any gestation in between. Similarly, the government is also using this technology to get feedback from citizens and call centre companies are using it for voice search. Liv.Ais technology can be used in any company with a consumer interface, or one with a large mobile base that wants to let its customers talk to them in their own language. Currently, this business-to-business (B2B) startup is working with six to seven companies in e-commerce and the government sectors. But there is more in store for this startup. Liv.Ai is developing its technology to go beyond providing transcripts of conversations and to draw inferences from text, enabling companies to carry out tasks or data analysis the moment they receive input from the customer. For example, for large hospitals, their priority is to identify customers that opt for high-value items. Liv.ais technology processes the customer conversations and converts them into leads for the sales team. Another use-case scenario for Liv.Ais technology is with telecom service providers. Usually, customers have to spend considerable time on interactive voice response before getting their queries addressed. Liv.Ai automates the entire process. A customer request such as I want to return my order is either directed to the correct person or the machine itself can ask for details about replacing or returning the article. Liv.Ai is currently doing a proof-of-concept with banks for the full-stack call centre automation process. Banks spend a lot of money on customer care. But with us, you just have to say that I want to transfer Rs 500 to Sanjeev and it will be done, and then you just need to confirm, explains Subodh. By widening the scope for the technologys application, Liv.Ai wants at least 80% of the Indian population to able to talk to apps in any language. More than half of the clients our customers receive are first-time Internet users, and they would like to talk in their own language. If you want to talk in your regional language, then speech is the default input. It is much more comfortable as compared to text. Our target is the whole Internet mass of India, claimed Subodh. The company, which comprises seven people, has raised angel funding, but the founders did not divulge more details. While investors believe that disrupting speech with machine learning has its merits, it is a capital-intensive business to scale. The challenge is that we do not have enough research here imparting this expertise. So, the technology itself is new, plus we have limited pools of resources. The knowledge base here needs to improve, but companies like Liv.Ai are still trying to do something innovative in this space, says Kunal Khattar, partner at early-stage venture capital firm advantEdge. While Khattar believes that resources pose a hindrance for scaling up such a business, those pioneering in this field will definitely have a first-mover advantage, he adds. This can be monetised only from a B2B perspective, and for that, the technology has to evolve significantly more in order to decipher what has been inputted. So the level of interactivity goes up in order to be useful, and it requires significant capital, he concludes. ",http://techcircle.vccircle.com/2017/03/07/liv-ai-wants-to-make-voice-recognition-in-local-languages-easier/,"One thing we realised was that our machines should be able to talk to us, and we should be working on speech recognition, said Subodh.",How Liv.Ai is helping machines talk to Indians in their mother tongue,English Punjabi Hence Gappi Liv.Ais India English English API Nikhil Jha Gappi Liv.Ais Liv Artificial Intelligence Pvt Ltd May IIT Kharagpur Subodh Kumar Sanjeev Kumar Kishore Mudra Internet Subodh India English English Content English Subodh Liv.Ai Liv.Ais API Liv.Ais B2B Liv.Ai Liv.ais Liv.Ais Liv.Ai Liv.Ai Rs Sanjeev Subodh Liv.Ai Internet Internet India Subodh Liv.Ai Kunal Khattar Khattar B2B,-1
271,"The President of Georgia, Giorgi Margvelashvili delivered his annual address to the nation at the parliament building in Kutaisi on Friday, covering the most acute and important issues of domestic and foreign policy. At the beginning of his speech, the President remembered his address to the new parliament in November 2016, when the ruling party Georgian Dream (GD) obtained constitutional majority with its 116 MPs in a 160 seat legislative body. I stated that holding a constitutional majority by a single party within Parliament was a threat of concentration of power, but at the same time, it was an opportunity for bold reforms and initiatives. What have we actually received after five months the president asked. Margvelashvili stressed that the alleged political threat has turned into a serious problem, while the dynamics of reforms is not promising. Unfortunately, the ruling party has rejected the dialogue process and isolated itself, he said. I look forward to the time when the winning political power starts seeking allies, not enemies; as I believe that unification of the country and society must be the greatest ambition a politician has, the president said, calling on the GD to get engaged in a dialogue and institutional cooperation. The President also talked about his campaign Constitution for All, which aims at making amendments to the constitution of the country. People need more democracy, not weakening but strengthening self-governance Society wants more involvement in the decision-making process and clearer social protection guarantees to be reflected in the Constitution, he said. The President referred to the alleged plans of the GD to change the rule of the election of President from a direct one to indirect. We cannot deprive people of the right to elect the President. The presidential election need to be adjusted to the interests of society, nor to the wish of current or former presidents or prime-Ministers, for the creation of an effective balance of power, he stressed. On economy, the President said that the most acute problems remain unchanged. Jobs, poverty, increasing prices, miserable pensions and national currency instability - these are the actual problems of our society, which is obvious without any research, he noted. Margvelashvili believes that state and private sectors need to work jointly for mutual reinforcement of the countrys economy. It is decisive for the country to further stimulate local production. With this aim, I have initiated the campaign Ask for Georgian, which contributes to economic growth, job creation and strengthening the national currency. In the frame of this campaign, in parallel to popularization of local production, we have created a loyalty card which allows consumers to get bonuses when purchasing Georgian products, the President explained, and had distributed said special cards to the MPs to get them involved in his campaign. While delivering his speech, the President also mentioned Georgias relationship with the European Union (EU) and NATO, saying entry of visa-free travel for Georgian citizens to the EUs Schengen Area was a significant achievement. Georgias current goal is to return to the European family and become an EU member with a strong democracy, stable institutions, human rights protection, rule of law, economic development and a European legal system, he said. Margvelashvili called on the countrys foreign partners to raise their voices on Georgias behalf at the upcoming NATO Summit this May in Brussels, which will gather only the member states of the Alliance. The President noted that Georgia actively contributed to the global security by sending its soldiers to various peacekeeping missions. He said Georgia is also participating in the strategic Black Sea security dialogue. The security of the Black Sea starts from Georgia. A secure Black Sea is a precondition of economic cooperation and long-term peace in the region, he added. One of the main painful issues raised by the President was the annexation Georgias breakaway regions by the Russian Federation. In response to our success, the Russian Federation takes actions towards the factual annexation of Georgias occupied territories. Therefore, as in the case of the non-recognition policy, it is necessary to create the anti-annexation policy of Georgias occupied territories with the active engagement of our allies, and to take coordinated steps to deter the annexation, he said. According to Margvelashvili, the State needs to react promptly on the new dividing lines and installation policy of barbed wires, cases on the prohibition of education in Georgian language in Gali schools, as well as kidnapping of Georgian citizens and destruction of cultural heritage. I hope that one day artificially built walls will be destroyedThat the time will come when the divided society of Abkhazia and Tskhinvali regions will be united, when Georgians, Abkhazians and Ossetians will be able to find one who will unite us, and when the refugees will return home safely, the President said, adding that it was crucial to update the countrys National Security Concept, elaborate a national security strategy and further enhance the national security system. The proper military reserve and mobilization system, together with a strong professional army, is the main pillar of our defense capability, he stressed. Margvelashvili went on to lament that recent developments have demonstrated a continuing crisis in the judiciary system of the country. Margvelashvili added that the judges themselves should become the main actors in improving the state of the judiciary system. The struggle for independence of the judiciary system should happen in every court, every courtroom, every office and every conference room; everyone should assume this burden. Regretfully, the judges keep silent over these important issues, he underlined. To conclude, the President underlined that he had decided to support the appeal of citizens on the so called Law on Wiretapping, which has created a lot of concern in the recent period. I will address the Constitutional Court with a different status - the status of a friend of the court, that will give me an opportunity to support the court in making a better decision, he said. The Presidents speech was criticized by the GD majority. From my point of view, the address does not reflect the reality our country is experiencing, Parliament Speaker, Irakli Kobakhidze said, while majority member Gia Volsky said the President should have been more prepared for his presentation. The opposition believes the Presidents report was adequate and covered all necessary issues and problems of the country. The President voiced all problems facing our society, opposition United National Movement (UNM) member Nika Melia said. Parliamentary minority Movement for Freedom-European Georgia leader, Davit Bakradze also approved the Presidents address, however, noted that it should have contained more details and analyses. The Head of the Presidents Administration, Giorgi Abashishvili, said that the Presidents report was fully based on peoples thoughts and needs and did not reflect the visions of any political forces. ",http://georgiatoday.ge/news/6296/President%E2%80%99s-Annual-Report-Covers-Most-Important-Issues-of-the-Year,"At the beginning of his speech, the President remembered his address to the new ... Therefore, as in the case of the non-recognition policy, it is ...",President's Annual Report Covers Most Important Issues of the Year,President Georgia Giorgi Margvelashvili Kutaisi Friday President November Dream GD MPs Parliament Margvelashvili GD President Constitution All Society Constitution President GD President President President Jobs Margvelashvili Ask Georgian President MPs President Georgias European Union EU NATO EUs Schengen Area Georgias European Margvelashvili Georgias NATO Summit May Brussels Alliance President Georgia Georgia Black Sea Black Sea Georgia Black Sea President Georgias Federation Federation Georgias Georgias Margvelashvili State Gali Abkhazia Tskhinvali Abkhazians Ossetians President National Security Concept Margvelashvili Margvelashvili President Wiretapping Constitutional Court Presidents GD Parliament Speaker Irakli Kobakhidze Gia Volsky President Presidents President United National Movement UNM Nika Melia Movement Freedom-European Georgia Davit Bakradze Presidents Head Presidents Administration Giorgi Abashishvili Presidents,11
272,"As other major IT companies step up their efforts in speech recognition and AI , Microsoft is following suit with respect to its Universal Windows Platform (UWP). The company is offering a new set of APIs for UWP developers called Microsoft Cognitive Services. The cloud-based APIs revolve around speech recognition, but also take things a step further with voice recognition, enabling speaker identification and authentication functionality. With respect to the latter, Microsoft Cognitive Services lets developers build systems that would, for example, enroll an individual by having her recite a couple of phrases while scanning a smart card in order to link her voice to that card. And for the formerspeaker identificationMicrosofts APIs let developers offer identification through natural speech,establishing a profile with about a minutes worth of speaking. Microsoft says this speaker identification functionality is currently only suitable for devices or applications dealing with a relatively small set of users, but it points to the growing sophistication of voice-based interaction. While Apple and Google are currently focused on refining their speech recognition technologies , Microsofts advancements into voice identification point to a future in which devices will not only respond to users, but will know who they are responding to. ",http://findbiometrics.com/microsoft-speech-voice-306063/,"As other major IT companies step up their efforts in speech recognition and AI, Microsoft is following suit with respect to its Universal Windows ...",Microsoft Cognitive Services Enable Speech and Voice Recognition,IT AI Microsoft Universal Windows Platform UWP APIs UWP Microsoft Cognitive Services APIs Microsoft Cognitive Services APIs Microsoft Apple Google Microsofts,2
273,"Learn English 6000 Words, a Fun Easy Learn e-learning app that has made English language learning easy, now comes with two new features. Learn English 6000 Words, a Fun Easy Learn language learning app that has received recognition for its simple interface and host of user-friendly features, recently underwent few major changes. The updated app now comes with speech recognition and slow play mode, two features that the developers think will help users to learn English vocabulary words . The existing users can now auto-update their downloaded and installed Learn English 6000 Words app for free, while the new users can experience the changes right from the beginning. According to the developers, the speech recognition feature, which has simple AI structure, will help users in correcting their pronunciations. On the other hand, the Slow Mode will help them in recognizing the foreign words more effortlessly and without playing the same audio time and again. The developers at Fun Easy Learn claimed that the updated Learn English 6000 Words app will bring a revolution to the English language learning process. They said that the app can make learning English language a no-brainer even for the uninitiated, as the app was primarily designed and developed for the children, but can be exploited by adults as well. Learn English 6000 Words is a free language learning app, though there are in-app purchases which the users can opt for. The developers of Fun Easy Learn added the app is perfectly suitable for people who are visually challenged, as the app contains pronunciation of the English words and have small built-in audio files. With the addition of the new features, the developers believe that the app will receive even greater recognition in the coming days. They said that the app has already started to receive rave reviews and ratings from their users. Undoubtedly, the speech recognition and the slow mode are two great additions to the app, as these two features will make learning English an even easier task for the learners, and even for the ones who have no familiarity with the language. Our app is designed in a way so it supports the learning process for all and sundry, said the chief developer of Fun Easy Learn. Share article on social media or email: ",http://www.prweb.com/releases/FunEasyLearn/Learn-English-6000-Words/prweb13925350.htm,"The updated app now comes with speech recognition and slow play mode, two features that the developers think will help users to learn ...",Learn English 6000 Words Now Comes with Speech Recognition ...,Learn English Words Fun Easy Learn Learn Words Fun Easy Learn English Learn Words AI Slow Mode Fun Easy Learn Learn Words Learn Words Fun Easy Learn English English Fun Easy Learn Share,1
274,"AWS chief Andy Jassy speaks about AI on Wednesday at re:Invent. Photo via AWS livestream. Amazon today announced three new artificial intelligence-related toolkits for developers building apps on Amazon Web Services. At thecompanys AWS re:invent conference in Las Vegas, Amazon showed how developers can use three new services  Amazon Lex, Amazon Polly, Amazon Rekognition  to build artificial intelligence features into apps for platforms like Slack, Facebook Messenger, ZenDesk, and others. The idea is to let developers utilize the machine learning algorithms and technology that Amazon has already created for its own processes and serviceslike Alexa. Instead of developing their own AI software, AWS customers can simply use an API call or the AWS Management Console to incorporate AI features into their own apps. AWS CEO Andy Jassy noted that Amazon has been building AI and machine learning technology for 20 years and said that there are now thousands of people dedicated to AI in our business. Now the company is opening up the back-end infrastructure to third-party developers. Three services were announced today and more are coming next year, Jassy said. Amazon AI services are fully managed services so there are no deep learning algorithms to build, no machine learning models to train, and no up-front commitments or infrastructure investments required, Amazon said in its press release . This frees developers to focus on defining and building an entirely new generation of apps that can see, hear, speak, understand, and interact with the world around them. Amazon Polly is available today in US East (N. Virginia), US East (Ohio), US West (Oregon), and EU (Dublin) Regions, and will expand to other regions in the coming months. Amazon Rekognition is available in US East (N. Virginia), US West (Oregon), and EU (Dublin) Regions, and will also expand. Developers can sign up for a preview of Amazon Lex today. Amazon CTO Werner Vogels detailed the new services in a blog post published Wednesday. Heres a quick rundown ofAmazon Lex, Amazon Polly, Amazon Rekognition. Amazon Lex  which refers to whats in between Alexa  lets developers build conversational interfaces into apps by using voice and text. The same conversational engine that powers Alexa is now available to any developer, making it easy to bring sophisticated, natural language chatbots to new and existing applications, Vogels wrote. The power of Alexa in the hands of every developer, without having to know deep learning technologies like speech recognition, has the potential of sparking innovation in entirely new categories of products and services. Jassy, speaking at the keynote, said that Lex will allow developers to build conversational applications. He used an example of an app that lets people order pizza, with AWSartificial intelligence technology, natural language processing, knowledge graphs, and morehelpingto understand context and user intent. You can build multi-step conversations, Jassy noted. Dr. Matt Wood, general manager of product strategy for AWS, talks about Lex. Photo via AWS livestream. Dr. Matt Wood, general manager of product strategy for AWS, also spoke during the keynote and showed howLex can be used to build conversational travel planning apps. The process went like this: Wood: Book a flight to London. AI: Whendo you want to travel AI:Theres a flight leaving at 5 p.m. for $500. Book it This is a very simple example of the type of fluid conversations you canhave with services running on Amazon Lex, Wood said. Lex can be integrated into Salesforce, Microsoft Dynamics, Hubspot, Twilio, Facebook Messenger, and other platforms. Amazon Rekognition is an image recognition service that can quickly detect whats in an image, like the number of people, the gender of those people, their emotions, different items, etc. It uses the same technology that Amazon has builtto analyze images on Amazon Prime Photos. Amazon Rekognition democratizes the application of deep learning technique for detecting objects, scenes, concepts, and faces in your images, comparing faces between two images, and performing search functionality across millions of facial feature vectors that your business can store with Amazon Rekognition, Vogels wrote. Amazon Rekognitions easy-to-use API, which is integrated with Amazon S3 and AWS Lambda, brings deep learning to your object store. Photo via Werner Vogels weblog. During his travel planning demo, Wood showed how Rekognition can be used in conjunction with Lex. For example, using the Rekognition image technology, the AI can show a user photos of a lake or forest thathelp guide themto where they want to travel. Although each of these individualservices  Polly, Lex, Rekognition  can be used independently, pulling them together allows you to build some really novel, sophisticated, category-defining applications, Wood said. Polly is a text-to-speech deep learning service. With Amazon Polly, we are making the same text-to-speechtechnology used to build Alexas voice to AWS customers, Vogels wrote. It is now available to any developer aiming to power their apps with high-quality spoken output. Jassy explained how developerscanuse Polly to input a text like the temperature in WA is 75 F, and thenspin out an MP3stream. There is intelligence built into Polly so that theaudio comes out like the temperature in Washington is 75 degrees Fahrenheit. Polly comes with 47 different voices and 24 languages. You can see pricing info for Polly here . Taylor Soper is a GeekWire staff reporter who covers a wide variety of tech assignments, including emerging startups in Seattle and Portland, the sharing economy and the intersection of technology and sports. Follow him @taylor_soper and email taylor@geekwire.com . Join us as wecelebrate the iconoclasts, innovators and disruptors of the Pacific Northwest technology community at the GeekWire Awards , presented by Wave Business , onMay 4that the Museum of Pop Culture in Seattle. Continuing this great tradition, well reveal the winners of the Pacific Northwests annual startup and technology awards, as nominated and chosen by GeekWire readers. ",http://www.geekwire.com/2016/amazon-launches-new-artificial-intelligence-services-developers-image-recognition-text-speech-alexa-nlp/,"The power of Alexa in the hands of every developer, without having to know deep learning technologies like speech recognition, has the ...",Amazon launches new artificial intelligence services for developers ...,AWS Andy Jassy AI Wednesday Photo AWS Amazon Amazon Web Services AWS Las Vegas Amazon Amazon Lex Amazon Polly Amazon Rekognition Slack Facebook Messenger ZenDesk Amazon Alexa AI AWS API AWS Management Console AI AWS CEO Andy Jassy Amazon AI AI Jassy Amazon AI Amazon Amazon Polly US East N. Virginia US East Ohio US West Oregon EU Dublin Regions Amazon Rekognition US East N. Virginia US West Oregon EU Dublin Regions Amazon Lex Amazon CTO Werner Vogels Wednesday Lex Amazon Polly Amazon Rekognition Amazon Lex Alexa Alexa Vogels Alexa Jassy Lex Jassy Dr. Matt Wood AWS Lex Photo AWS Dr. Matt Wood AWS London AI Whendo AI Amazon Lex Wood Lex Salesforce Microsoft Dynamics Hubspot Twilio Facebook Messenger Amazon Rekognition Amazon Amazon Prime Photos Amazon Rekognition Amazon Rekognition Vogels Amazon Rekognitions API Amazon S3 AWS Lambda Photo Werner Vogels Wood Rekognition Lex Rekognition AI Lex Rekognition Wood Polly Amazon Polly Alexas AWS Vogels Jassy Polly WA F Polly Washington Fahrenheit Polly Polly Taylor Soper GeekWire Seattle Portland Follow @ Pacific Northwest GeekWire Awards Wave Business Pop Culture Seattle Pacific Northwests GeekWire,6
275,"New footprint scanning technology, however, is making it easier than ever for investigators to accurately create 3D models of footwear and potentially pinpoint a criminal as being at a crime scene at a particular point in time. Matthew Bennett, a professor at Bournemouth University has adapted software used to scan ancient fossil imprints into a tool that can be used by crime scene investigators. South Korea's bribery scandal takes another turn. Could it bring Samsung to its knees DigTrace turns digital photos taken of a footprint into a 3D model of a shoe sole. That model can then be compared with existing footwear databases, making it cheaper and quicker for CSIs to pair up a footprint with its matching shoe. The software tool, developed with help from the Home Office and the National Crime Agency is freely available for use by police forces in the UK and overseas. The software uses a technique called photogrammetry where specific pixels in an image are manipulated using an algorithm so the footprint can be moved around digitally and mapped onto different surfaces. In the future, the researchers hope to be able to recreate 3D models from video footage. Potentially much more useful than a footprint, of course, is CCTV footage of a criminal during a crime. Researchers at the University of East Anglia are trialling visual speech recognition technology that could reconstruct conversations captured on video even where there is no sound. Lip-reading is one of the most challenging problems in artificial intelligence, said Professor Richard Harvey, of UEAs School of Computing Sciences. The research has concentrated on training machines to recognise the appearance and shape of humans lips as they form words and sentences. Potentially, a robust lip-reading system could be applied in a number of situations, from criminal investigations to entertainment, said Helen Bear, the lead author of the study into visual speech recognition. Deciphering the difference between p and b sounds on film has always proved tricky, but the teams technology has made it much simpler to identify the differences between the two sounds. Understanding what is being said on CCTV footage could give vital insights into how a crime was committed, or point to further suspects outside of those known to police. New innovations in crime scene photography, though, might actually allow juries to virtually visit the crime scenes themselves. At Staffordshire University, Dr Caroline Colls has trialled technology that uses 3D imaging and virtual reality headsets to let jurors walk through the crime scene in virtual reality. Barristers currently rely on photographs and sketches to familiarise the jury with crime scenes, and only in very exceptional cases are juries allowed to visit the scene itself. Scene degradation starts from the moment the crime is committed, Mezheb Chowdhury, a PhD researcher in Forensic Science and Criminal Investigations, told WIRED. Inspired by Nasas Curiosity Mars rover, Chowdhury and his colleagues created a crime scene rover which takes 360-degree video that can be played back on a smartphone using Google Cardboard . The rover is set free, it takes its photos and its video, it comes back and you don't edit the video at all. You export it to smartphones, put it into a Google Cardboard and just have a look, Chowdhury said. The rover could either follow a path set by a crime scene investigator, be controlled via a smartphone app or navigate its own way around a crime scene using sensors. According to Chowdhury, who has already demonstrated his creation to 50 police forces in the UK and USA, letting juries explore crime scenes virtually will introduce more objectivity into criminal cases. In the current system, barristers a can pick and choose which crime scene photos to show the jury. ""It's just storytelling. That just takes out all the objectivity out of it, he said. If jurors could explore high-resolution, 360 images of the crime scene, they could decide for themselves what was worth checking out in more detail. Crime scene investigators would put the rover in the scene at the earliest possible opportunity, to capture the space as it was immediately after the crime was committed. It would allow investigators to categorically say we were there, this is how the crime scene looked, Chowdhury said. ",http://www.wired.co.uk/article/crime-scene-technology,"Sir Bernard Spilsbury was – by modern standards – an unlikely celebrity. As a forensic pathologist, he delved inside dead bodies to discover ...","Crimes of the future will be solved by robo-CSIs, smell and VR",New Matthew Bennett Bournemouth University South Korea Samsung DigTrace CSIs Home Office National Crime Agency UK CCTV University East Anglia Professor Richard Harvey UEAs School Computing Sciences Helen Bear CCTV New Staffordshire University Dr Caroline Colls Scene Mezheb Chowdhury PhD Forensic Science Criminal Investigations WIRED Nasas Curiosity Mars Chowdhury Google Cardboard Google Cardboard Chowdhury Chowdhury UK USA Crime Chowdhury,4
276,"Speech Recognition Software Market 2016 - 2021 Analysis With IBM, AT&T, Microsoft and Other Players ReportsnReports.com adds 2016 Global Speech Recognition Software Market Research Report to the growing collection of software, Information Technology & telecommunication category in its online business intelligence library. Complete report on the Speech Recognition Software market spread across 155 pages, profiling 12 companies and supported with 234 tables and figures is now available at The Global Speech Recognition Software Industry 2016 Market Research Report is a professional and in-depth study on the current state of the Speech Recognition Software industry. The report provides a basic overview of the industry including definitions, classifications, applications and industry chain structure. The Speech Recognition Software market analysis is provided for the international market including development history, competitive landscape analysis, and major regions' development status. Development policies and plans are discussed as well as manufacturing processes and cost structures. This report also states import/export, supply and consumption figures as well as cost, price, revenue and gross margin by regions (US, EU, China and Japan), and other regions can be added. Few key manufacturers included in this report are IBM, AT&T, Microsoft, Nuance Communications, Dolby Fusion Speech, LumenVox, OnMobile Global, Raytheon BBN Technologies, Sensory Tellme Networks and Voxeo. The report focuses on global major leading industry players with information such as company profiles, product picture and specification, capacity, production, price, cost, revenue and contact information. Upstream raw materials, equipment and downstream consumers analysis is also carried out. The feasibility of new investment projects is assessed, and overall research conclusions are offered. With 234 tables and figures to support the Speech Recognition Software market analysis, this research provides key statistics on the state of the industry and is a valuable source of guidance and direction for companies and individuals interested in the market. 2016-2021 forecasts for Speech Recognition Software market provided in this report include 2016-2021 Speech Recognition Software capacity production overview, production market share, sales overview, supply sales and shortage, import export consumption and cost price production value gross margin by regions (United States, EU, China and Japan), and other regions can be added. Order a copy of Global Speech Recognition Software Market Report 2016 research report at http://www.reportsnreports.com/Purchase.aspxname=576227 . Some of the tables and figures provided in Global Speech Recognition Software Market Report 2016 research report include: Another research titled ""Global Voice Recognition Industry 2015 Market Research Report"" provides voice recognition market analysis for major regions including USA, Europe, China and Japan, and other regions can be added. For each region, market size and end users are analyzed as well as segment markets by types, applications and companies. Analysis of Voice Recognition Industry Key Manufacturers profiled in this report is Industry Apple, Google, Microsoft and Nuance Communications. Read more at http://www.reportsnreports.com/reports/443637-global-voice-recognition-industry-2015-market-research-report.html . ReportsnReports.com is an online market research reports library of 500,000+ in-depth studies of over 5000 micro markets. Not limited to any one industry, ReportsnReports.com offers research studies on agriculture, energy and power, chemicals, environment, medical devices, healthcare, food and beverages, water, advanced materials and much more. ",http://www.prnewswire.com/news-releases/speech-recognition-software-market-2016---2021-analysis-with-ibm-att-microsoft-and-other-players-581666331.html,"ReportsnReports.com adds 2016 Global Speech Recognition Software Market Research Report to the growing collection of software, ...",Speech Recognition Software Market 2016 - 2021 Analysis With IBM ...,IBM AT T Microsoft Players ReportsnReports.com Global Software Market Research Report Information Technology Software Global Software Industry Market Research Report Software Software Development US EU China Japan IBM AT T Microsoft Nuance Communications Dolby Fusion Speech LumenVox OnMobile Global Raytheon BBN Technologies Sensory Tellme Networks Voxeo Upstream Software Software Software United States EU China Japan Global Software Market Report Global Software Market Report Voice Recognition Industry Market Research Report USA Europe China Japan Voice Recognition Industry Key Manufacturers Industry Apple Google Microsoft Nuance Communications ReportsnReports.com ReportsnReports.com,3
277,"Prescott Area Leadership honors two youth, two adults for empowering others Prescott Area Leadership presented its annual awards during a banquet Wednesday, April 5 at the Prescott Resort. From left, Man of the Year Dan Streeter, Male Youth Leader of the Year Alex Daniels, Female Youth Leader of the Year Anne Snyder and Woman of the Year Marnie Uhl. When it comes to the best examples of community stewardship, Prescott Area Leadership Program President Alexandra Piacenza is confident that not only is this area in good stead, but Americas future is strong based on the cadre of both young and adult trail blazers forging paths for others to follow. The annual Leadership awards ceremony held on Wednesday, April 5, just uplifts me so much as it honors four youth and adults who are truly making a difference for others, Piacenza said.  Each one, selected from a group of 12 finalists, embraces the notion of powering others rather than someone who has power over others, Piacenza said. The award recipients are: Female Youth Leader of the Year Annie Snyder, a Prescott High School senior; Male Youth Leader of the Year Alex Daniels; Woman Leader of the Year Marnie Uhl, president and chief executive officer of the Prescott Valley Chamber of Commerce; and Male Leader of the Year Dan Streeter, superintendent of the Humboldt Unified School District. The other PAL finalists were: male youth leaders Mako Bennett and Thaddeus Martin; female youth leaders Amber Glisson and Sarah Nguyen; women leaders Rosemary Agneessens and Sherri Hanna; and male leaders Tom Benson and Jeramy Plumb. The youth leader winners each received a $2,000 college scholarship; the male and female winners each received $500. Though each of the award winners was selected for their particular gifts, Piacenza said a common denominator between them is they are people who dont beat their own drums but rather are driven by their determination to do good for others in every way they can. They serve as an example that people may be inclined to follow, Piacenza said. All of them are highly accomplished, and highly committed to the community. Annie Snyder, Female Youth Leader of the Year, Prescott High School senior; member of the National Honor Society, Key Club, Academic Honors Club, Political Activism Club, president of the Leo Club, vice-president of Future Business Leaders of America and selected for Arizona Girls State and the We The People competition: Honestly, I was really surprised  I hadnt prepared an acceptance speech because I was so sure I wouldnt win. Now that I have won, I have a lot more confidence and feel like I have been doing everything right. Being so involved in my community was a privilege; I love seeing how much the projects I have worked on have helped other people. Im really honored and humbled to have won this award and Im very happy that all my hard work paid off. Alex Daniels, Male Youth Leader of the Year, Chino Valley High School senior: Alex has been accepted into the honors program at Grand Canyon University and Arizona State University. He is a member of the Student Council and the National Honor Society and was the state winner of the National History Day event. Alex has taken career and technical courses and is a four-year cross country star athlete. As a volunteer, Alex is active with the Lions Club, the Chamber of Commerce and trains senior citizens in the use of technology and doing repairs on their cellphones and tablets. Efforts to reach Daniels for comment were not successful. Marnie Uhl, Woman of the Year, president and chief executive officer of the Prescott Valley Chamber of Commerce, incoming president of the Prescott Center for the Arts, board member for the Coalition for Compassion and Justice and a member of electors for the Yavapai Regional Medical Center as well as volunteer with several other area nonprofit organizations: In her role as the chamber leader since 2006, Uhl said she has had the opportunity to make many connections and contacts that engage me in the community. I run a business organization, but its about the whole community that makes it work for us  our education system, our nonprofits and our businesses. The more we work together the better it is for all of us. She said she feels blessed to do work that feeds a personal passion to bring people together for good. To be honored for it is just the icing on the cake, Uhl said. Dan Streeter, Man of the Year, superintendent of the Humboldt Unified School District; program chairman for Yavapai Big Brothers Big Sisters; Prescott Valley Chamber of Commerce Board of Directors; Prescott Valley Economic Development Foundation, Prescott Little League and supporter of the Hungry Kids Project: Im incredibly honored by this recognition. I am very proud to be part of this community, and to contribute in any way I can. One thing he said he has always told students is, Show me your friends, and Ill show you your future. ",https://www.dcourier.com/news/2017/apr/10/leaders-making-difference-prescott/,I hadn't prepared an acceptance speech because I was so sure I wouldn't win. Now that ... I'm incredibly honored by this recognition. I am very ...,Leaders making a difference in Prescott,Prescott Area Leadership Prescott Area Leadership Wednesday April Prescott Resort Man Year Dan Streeter Male Youth Leader Year Alex Daniels Female Youth Leader Year Anne Snyder Woman Year Marnie Uhl Prescott Area Leadership Program President Alexandra Piacenza Americas Leadership Wednesday April Piacenza Piacenza Youth Leader Year Annie Snyder Prescott High School Male Youth Leader Year Alex Daniels Woman Leader Year Marnie Uhl Prescott Valley Chamber Commerce Male Leader Year Dan Streeter Humboldt Unified School District PAL Mako Bennett Thaddeus Martin Amber Glisson Sarah Nguyen Agneessens Sherri Hanna Tom Benson Jeramy Plumb Piacenza Piacenza Annie Snyder Female Youth Leader Year Prescott High School National Honor Society Key Club Academic Honors Club Political Activism Club Leo Club Future Business Leaders America Arizona Girls State People Im Im Alex Daniels Male Youth Leader Year Chino Valley High School Alex Grand Canyon University Arizona State University Student Council National Honor Society National History Day Alex Alex Lions Club Chamber Commerce Daniels Marnie Uhl Woman Year Prescott Valley Chamber Commerce Prescott Center Arts Coalition Compassion Justice Yavapai Regional Medical Center Uhl Uhl Dan Streeter Man Year Humboldt Unified School District Yavapai Big Brothers Big Sisters Prescott Valley Chamber Commerce Board Directors Prescott Valley Economic Development Foundation Prescott Little League Hungry Kids Project Im Show Ill,2
278,"IBM has made a big leap forward in the ability of its Watson artificial intelligence computer to recognize conversational speech. Last year, IBM was able to hold conversations in which the AI recognized English conversational speech with an 8 percent word error rate. Now IBMs Watson team has been able to knock the word error rate down to 6.9 percent. The achievement shows that AI is getting smarter and smarter  and that were all going to be replaced by robots some day. IBM Watsongeneral manager David Kenny announced thebreakthrough in Watsons conversational capabilities for developers at the Samsung Developer Conference in San Francisco today. The Watson team included Kenny,Tom Sercu, Steven Rennie, and Jeff Kuo. Watson had its finest moment in 2011 when it beat the reigning human champion on the Jeopardy television quiz show. To put this result in perspective, back in 1995, a high-performance IBM recognizer achieved a 43 percent error rate. Spurred by a series of Defense Advanced Research Projects Agency evaluations in the past couple of decades, IBMs system improved steadily. Most recently, the advent of deep neural networks was critical in helping achieve the 8 percent and 6.9 percent results, said George Saon, lead scientist in the IBM Watson Group, in a post . The ultimate goal is to reach or exceed human accuracy, which is estimated to be around 4 percent on this task, dubbed the Switchboard. IBM said it made improvements inboth acoustic and language modeling. On the acoustic side, we use a fusion of two powerful, deep neural networks that predict context-dependent phones from the input audio, Saonsaid. The models were trained on 2000 hours of publicly available transcribed audio from the Switchboard, Fisher, and CallHome corpora. Saon added, We are currently working on integrating these technologies into IBM Watsons state-of-the-art speech-to-text service. By exposing our acoustic and language models to increasing amounts of real-world data, we expect to bridge the gap in performance between the lab setting and the deployed service. Its time to learn how AI can transform your business at the 2017 VB Summit, June 5th  6th in Berkeley. Meet AI ecosystem leaders who are shaping a new digital economy. Request an Invite - Save 50%! ",https://venturebeat.com/2016/04/28/samsung-and-ibm-show-how-watson-has-improved-conversational-speech-recognition/,Samsung and IBM show how Watson has improved conversational speech recognition ... English conversational speech with an 8 percent word error rate. ... these technologies into IBM Watson's state-of-the-art speech-to-text ...,Samsung and IBM show how Watson has improved conversational ...,IBM Watson IBM AI English IBMs Watson AI IBM Watsongeneral David Kenny Watsons Samsung Developer Conference San Francisco Watson Kenny Tom Sercu Steven Rennie Jeff Kuo Watson Jeopardy IBM Defense Advanced Research Projects Agency IBMs George Saon IBM Watson Group Switchboard IBM Saonsaid Switchboard Fisher CallHome Saon IBM Watsons AI VB Summit June Berkeley Meet AI Request Invite,0
279,"Global Markets and Technologies for Voice Recognition Reaching Double-Digit Growth Rates, Reports BCC Research Wellesley, Mass, Feb.  09, 2017  (GLOBE NEWSWIRE) -- New developments in speech and machine learning, lower costs, and greater processing power are driving growth in the global markets for voice recognition and technologies. BCC Research reveals in its new report that productivity gains from voice-enabled applications, along with new lower cost implementations, are making these systems easier to develop and deploy. The global market for voice recognition technologies should increase from $104.4 billion in 2016 to $184.9 billion in 2021, growing at a five-year compound annual growth rate (CAGR) of 12.1%. The consumer market is expected to reach $54.4 billion and $95.9 billion in 2016 and 2021, respectively, demonstrating a five-year CAGR of 12.0%. The enterprise market is expected to grow from $44.0 billion in 2016 to $79.0 billion in 2021 at a five-year CAGR of 12.4%. The adoption of voice-enabled applications for mobile devices and other smart voice-enabled consumer products is increasing the scale of the consumer/mobile category, challenging the traditional voice technology engine of growth, namely contact centers. Legal compliancy issues will continue to ensure the healthcare sectors dependence on voice recognition solutions that can help the industry meet electronic medical record (EMR) standards while providing efficient healthcare. In the enterprise and healthcare sectors, voice recognition technologies are allowing physicians, soldiers, and other users to accomplish more by reducing the time consumed by written communications. The military sector, the industry's largest investor, uses the technology to increase operational efficiency and for precision driving. North America remains the leading market in that sector, but Asia-Pacific countries are rapidly increasing their market shares. Among the underlying technology areas, the nascent market for speaker variation segment in speech analytics should demonstrate the highest five-year CAGR, at 17.6% from 2016-2021. ""The implementation of analytics across voice recognition systems is critical to support greater responsiveness and enable automation, says BCC Research analyst Michael Sullivan. ""The largest markets are voice-activated mobile search and advertising services, which have overtaken interactive response systems and automatic speech recognition. Mobile search is growing faster than all other areas at a CAGR of 21.1%, which reflects the powerful combination of a richer than ever storehouse of data."" Global Markets and Technologies for Voice Recognition (IFT039D) analyzes top supplier market shares, new technologies, and the unique challenges faced by each category. The report also examines the importance of traditional and emerging voice-enabled devices, as these media will promote and extend voice recognitions reach. Global market drivers and trends, with data from 2015, estimates for 2016, and projections of CAGRs through 2021 also are provided. BCC Research is a publisher of market research reports that provide organizations with intelligence to drive smart business decisions. By partnering with industry experts worldwide, BCC Research provides unbiased measurements and assessments of global markets covering major industrial and technology sectors, including emerging markets. For more information about BCC Research, please visit bccresearch.com . Follow BCC Research on Twitter at @BCCResearch . Editors and reporters who wish to speak with the analyst should contact Steven Cumming at steven.cumming@bccresearch.com.   ",https://globenewswire.com/news-release/2017/02/09/915498/0/en/Global-Markets-and-Technologies-for-Voice-Recognition-Reaching-Double-Digit-Growth-Rates-Reports-BCC-Research.html,"09, 2017 (GLOBE NEWSWIRE) -- New developments in speech and ... growth in the global markets for voice recognition and technologies.",Global Markets and Technologies for Voice Recognition Reaching ...,Markets Technologies Voice Recognition Growth Rates Reports BCC Research Wellesley Mass Feb. GLOBE NEWSWIRE New BCC Research CAGR CAGR CAGR EMR North America CAGR BCC Research Michael Sullivan CAGR Markets Technologies Voice Recognition IFT039D CAGRs BCC Research BCC Research BCC Research Follow BCC Research Twitter @ BCCResearch Steven Cumming @,3
280,"In an age when technology companies routinely introduce new forms of everyday magic, one problem that remains seemingly unsolved is that of long-form transcription. Sure, voice dictation for documents has been conquered by Nuances Dragon software. Our phones and smart home devices can understand fairly complex commands, thanks to self-teaching recurrent neural nets and other 21st century wonders. However, the task of providing accurate transcriptions of long blocks of actual human conversation remains beyond the abilities of even todays most advanced software. When solved on a broad scale, it is a problem that might unlock vast archives of oral histories, make podcasts easier to consume for speed-readers (tl;dl), and be a world-changing boon for journalists everywhere, liberating precious hours of sweet life. It could make YouTube text-searchable. It would be a fantasy come true for researchers. It would usher a dystopia for others, providing a new form of textual panopticon . (Though with Mattels voice-recognition-driven Hello Barbie that listens to the children playing with it, the dystopia might already be here.) Researchers say that functional transcription is only a matter of time, though the amount of time remains a very open question. The task of providing accurate transcriptions of long blocks of actual human conversation remains beyond the abilities of even todays most advanced software. We used to joke that, depending who you ask, speech recognition is either solved or impossible, says Gerald Friedland, the director of the Audio and Multimedia lab at the International Computer Science Institute, affiliated with UC Berkeley. The truth is somewhere in between. The range of answers about the future of speaker-independent transcription of spontaneous human speech suggests that the joke falls into the category its funny cuz its true. If you have people transcribe conversational speech over the telephone, the error rate is around 4 percent, says Xuedong Huang, a senior scientist at Microsoft, whose Project Oxford has provided a public API for budding voice recognition entrepreneurs to play with. If you put all the systems togetherIBM and Google and Microsoft and all the best combinedamazingly the error rate will be around 8 percent. Huang also estimates commercially available systems are probably closer to 12 percent. This is not as good as humans, Huang admits, but its the best the speech community can do. Its about as twice as bad as humans. However, Huang is quick to add that this error rate is phenomenal when compared to where the field was just five years ago. And its here where he begins to get audibly excited. XD Huang has been researching the problem of voice recognition for over 30 years, first at Tsinghua University in Beijing in the early 80s. We had this dream of having a natural conversation with a computer, Huang says, recounting a long series of magic moments and benchmarks, at Raj Reddy s pioneering lab at Carnegie Mellon, and beginning at Microsoft in 1995. Huang covered the progress, co-authoring a paper with Reddy and Dragon Systems Jim Baker in a the January 2014 issue of Communications of ACM titled  A Historical Perspective on Speech Recognition . Ten years ago, it was probably an 80 percent [error] rate! he says. To have an error reduction from 80 percent [down to] 10 percent and now were approaching 8 percent! If we can keep the trend for the next two or three years, something magic is absolutely going to happen. Predictions are always hard, but based on historical data, tracking records of the community, not one person in the next two or three years, I think we will be approaching human parity in transcribing speech over a typical mobile phone setting. Carl Case, a research scientist on the Machine Learning team at Baidu, works on the Chinese web giants own speech recognition system, Deep Speech . Weve made some very good progress in Deep Speech with state-of-the-art speech systems in English and Chinese, Case says. But I still think theres work to do to go from works for some people in some contexts to actually just works the same way you and I can have this conversation, having never met, over a relatively noisy phone line and have no problem understanding one another. Case and his associates have been testing their technology in windy cars, with music playing in the background, and under other adverse conditions. Like their colleagues at Microsoft, they have released their API to the public, partly in the name of science, and partly because the more users it has, the better it gets. For freelancers and other types who want transcriptions and cant afford the $1 minute rate of traditional transcriptionists, solutions exist. However, none of them are exactly perfect. Programmer (and occasional WIRED contributor) Andy Baio wrote a script to slice an audio interview into one-minute chunks, upload the pieces to Amazons Mechanical Turk, and outsource the job of transcribing those one-minute chunks to a platoon of humans. It saves money, but theres a not-insignificant amount of prep and clean-up required. ( Casting Words seems to have built a business model on the same technique, though it lands right back at the $1 per minute rate.) For an easier to operate crowdsourced interface, there is also the sharing-economy-era site TranscribeMe , transcriptions provided by a small army of manual transcribers, heeding the companys call to monetize your downtime. A freely available voice transcription tool is likewise built-in to Google Docs for those who would care to experiment. You can play recorded audio on your computer, and the system will do its best to make the proper text appear in a Google Doc. For the five phone interviews conducted for this article, recorded via Skype, only one subject spoke slowly and clearly enough to even register as recognizably transcribed text, with an error rate of roughly 15 percent. Those who only want to transcribe podcasts might have better luck. Where currently available transcription technology cant handle multiple voices or background chaos, reliable software like Nuances Dragon NaturallySpeaking (also an outgrowth of Reddys lab at Carnegie Mellon) has become quite capable at trained single voices. David Byron, editorial director of Speech Technology magazine suggests a technique called parroting: listening to a recording in real-time and repeating its text back into the microphone for the software to transcribe. It saves some typing, but is far from instantaneousand still forces interviewers to relive their most awkward interview moments. One person who has doubts about the imminent arrival of long-form transcription technology is Roger Zimmerman, Chief of Research and Development at 3Play Media , perhaps the only company currently offering a commercial application for automated long-form transcription. Using a combination of APIs provided by vendors Zimmerman said he could not disclose, 3Plays initial transcriptions average around 80 percent accuracysometimes much more, sometimes much lessand are corrected by human transcribers before being sent to customers. Speech recognition technology is not anywhere near human capability, Zimmerman says, and wont be for many, many years, my guess is decades still. Humans do not speak like text, says Zimmerman, who has been working with speech technology since the 1980s, when he landed a job at the Voice Processing Corporation, an offshoot of MIT. Ive hesitated, Ive corrected, Ive gone back and repeated, and to the extent that you have disorganized spontaneous speech, the language model is unsuited for that. Its the weak component. Its the component of the system now thats dependent on fundamental artificial intelligence. What theyve done with acoustic modeling is signal processing-oriented, and it is well framed, these new deep neural networks, they understand what theyre doing when they decode an acoustic signal, but they dont really understand what a language model needs to do to mimic human languaging process. Theyre using number-crunching to address a much higher artificial intelligence problem which has really not been solved yet. But its not thaaat hard, suggests Jim Glass, a Senior Research Scientist at MIT who leads the Spoken Language Systems Group and who serves as an advisor to 3Play. Glass says, in fact, that the technology is already here. The way to think of this problem is [to ask] what error rate is tolerable for your needs, so if youre skimming through the transcript and could jump back to the audio to verify it, you might be willing to tolerate a certain amount of errors. The technology is good enough today to do that. It would take somebody to decide that they want to make that capability available. Part of the problem historically with speech technology is companies figuring out how to make money off of it, and I dont know if theyve figured out how to do that yet, Glass says. He points out that there are toolkits available for developers who would like to play with the nascent technology. The piece that has yet to be combined into commercially available transcription like Google Voice is known as two party diarization, a speaker-independent system that can determine who is speaking and what they are saying. One person speaking clearly is one thing, but two people engaging in lively discourse is another entirely. And it is a problem that has been solved, in part, at least in the bounds of scientific research. There is a whole field devoted to it, rich transcription. In 2012, the Institute of Electrical and Electronics dedicated a whole issue of their journal, Transactions on Audio, Speech, and Language Processing, to  New Frontiers in Rich Transcription . Part of the problem historically with speech technology is companies figuring out how to make money off of it, and I dont know if theyve figured out how to do that yet. Jim Glass, Senior Research Scientist at MIT Over a relatively clean phone line, technology could identify the speaker about 98 percent of the time, says Gerald Friedland, who headed the diarization project at the nonprofit ICSI, as the group participated in trials run by the National Institute of Standards and Technology. Running the Meeting Recorder Project to test group recording situations, ICSI confirmed that once the microphone is no longer the close-range type provided by phones, the error rate shoots up to anywhere between 15 percent and 100 percent. Friedland points out the range of problems that have to be addressed once one goes past the relatively clean speech of broadcast news into the type of long-form speech that many researchers work with today. He says, If you put your cell phone on the table and try to record everything thats being said and then try to transcribe it, you have a combination of many of these problems: new vocabulary [words], the cocktail party noise problem, regular noise, people overlapping, and people never speak perfectly. Its got coughs and laughs and there might be yelling and there might be whispering. It becomes very diverse. Two voice spectrums that often cause chaos in diarization studies fail tests are children and the elderly. You can combine these scenarios, he says. I think all of this guarantees that a perfect speech recognizer that just listens in like a human will not be achieved in a reasonable time. You and I will probably not see that. Which shouldnt be interpreted to mean that were not living in the golden age of speech technology. This month, Friedland helped launch MOVI, a Kickstarted speech recognizer/voice synthesizer for Arduino that operates without the use of the cloud. It doesnt use the Internet, Friedland says. You dont have to use the cloud to do recognition. It can work with a couple hundred sentences and it adapts. He laughs at Sony, Apple, Google, Microsoft, and other companies that send speech into the cloud for processing. All of this is exploiting the fact that people think [voice recognition] is so hard that it has to get done in the cloud. If you have one speaker speaking into a computer, we should consider this problem solved. For now, Friedland says, most transcription start-ups seem to be mainly licensing Googles API and going from there. But the field and the market are wide open for innovation at every level, with bizarre kinds of unforeseen societal change coming as soon as a project succeeds. ",https://www.wired.com/2016/04/long-form-voice-transcription/,"We used to joke that, depending who you ask, speech recognition is either solved or impossible, says Gerald Friedland, the director of the ...",Why Our Crazy-Smart AI Still Sucks at Transcribing Speech,Nuances Dragon Mattels Hello Barbie Gerald Friedland Audio Multimedia International Computer Science Institute UC Berkeley Xuedong Huang Microsoft Oxford API Google Microsoft Huang Huang Huang XD Huang Tsinghua University Beijing Huang Raj Reddy Carnegie Mellon Microsoft Huang Reddy Dragon Systems Jim Baker January Communications ACM Historical Perspective [ Carl Case Machine Learning Baidu Deep Speech Weve Deep Speech English Chinese Case Microsoft Programmer WIRED Andy Baio Amazons Mechanical Turk Words TranscribeMe Google Docs Google Doc Skype Nuances Dragon NaturallySpeaking Reddys Carnegie Mellon David Byron Speech Technology Roger Zimmerman Chief Research Development Media APIs Zimmerman Zimmerman Zimmerman Voice Processing Corporation MIT Ive Ive Ive Theyre Jim Glass Senior Research Scientist MIT Spoken Language Systems Group Glass ] Glass Google Voice Institute Electrical Electronics Transactions Audio Speech Language Processing New Frontiers Rich Transcription Jim Glass Senior Research Scientist MIT Gerald Friedland ICSI National Institute Standards Technology Recorder Project ICSI Friedland ] Which Friedland MOVI Kickstarted Arduino Internet Friedland Sony Apple Google Microsoft Friedland Googles API,3
281,"In an age when technology companies routinely introduce new forms of everyday magic, one problem that remains seemingly unsolved is that of long-form transcription. Sure, voice dictation for documents has been conquered by Nuances Dragon software. Our phones and smart home devices can understand fairly complex commands, thanks to self-teaching recurrent neural nets and other 21st century wonders. However, the task of providing accurate transcriptions of long blocks of actual human conversation remains beyond the abilities of even todays most advanced software. When solved on a broad scale, it is a problem that might unlock vast archives of oral histories, make podcasts easier to consume for speed-readers (tl;dl), and be a world-changing boon for journalists everywhere, liberating precious hours of sweet life. It could make YouTube text-searchable. It would be a fantasy come true for researchers. It would usher a dystopia for others, providing a new form of textual panopticon . (Though with Mattels voice-recognition-driven Hello Barbie that listens to the children playing with it, the dystopia might already be here.) Researchers say that functional transcription is only a matter of time, though the amount of time remains a very open question. The task of providing accurate transcriptions of long blocks of actual human conversation remains beyond the abilities of even todays most advanced software. We used to joke that, depending who you ask, speech recognition is either solved or impossible, says Gerald Friedland, the director of the Audio and Multimedia lab at the International Computer Science Institute, affiliated with UC Berkeley. The truth is somewhere in between. The range of answers about the future of speaker-independent transcription of spontaneous human speech suggests that the joke falls into the category its funny cuz its true. If you have people transcribe conversational speech over the telephone, the error rate is around 4 percent, says Xuedong Huang, a senior scientist at Microsoft, whose Project Oxford has provided a public API for budding voice recognition entrepreneurs to play with. If you put all the systems togetherIBM and Google and Microsoft and all the best combinedamazingly the error rate will be around 8 percent. Huang also estimates commercially available systems are probably closer to 12 percent. This is not as good as humans, Huang admits, but its the best the speech community can do. Its about as twice as bad as humans. However, Huang is quick to add that this error rate is phenomenal when compared to where the field was just five years ago. And its here where he begins to get audibly excited. XD Huang has been researching the problem of voice recognition for over 30 years, first at Tsinghua University in Beijing in the early 80s. We had this dream of having a natural conversation with a computer, Huang says, recounting a long series of magic moments and benchmarks, at Raj Reddy s pioneering lab at Carnegie Mellon, and beginning at Microsoft in 1995. Huang covered the progress, co-authoring a paper with Reddy and Dragon Systems Jim Baker in a the January 2014 issue of Communications of ACM titled  A Historical Perspective on Speech Recognition . Ten years ago, it was probably an 80 percent [error] rate! he says. To have an error reduction from 80 percent [down to] 10 percent and now were approaching 8 percent! If we can keep the trend for the next two or three years, something magic is absolutely going to happen. Predictions are always hard, but based on historical data, tracking records of the community, not one person in the next two or three years, I think we will be approaching human parity in transcribing speech over a typical mobile phone setting. Carl Case, a research scientist on the Machine Learning team at Baidu, works on the Chinese web giants own speech recognition system, Deep Speech . Weve made some very good progress in Deep Speech with state-of-the-art speech systems in English and Chinese, Case says. But I still think theres work to do to go from works for some people in some contexts to actually just works the same way you and I can have this conversation, having never met, over a relatively noisy phone line and have no problem understanding one another. Case and his associates have been testing their technology in windy cars, with music playing in the background, and under other adverse conditions. Like their colleagues at Microsoft, they have released their API to the public, partly in the name of science, and partly because the more users it has, the better it gets. For freelancers and other types who want transcriptions and cant afford the $1 minute rate of traditional transcriptionists, solutions exist. However, none of them are exactly perfect. Programmer (and occasional WIRED contributor) Andy Baio wrote a script to slice an audio interview into one-minute chunks, upload the pieces to Amazons Mechanical Turk, and outsource the job of transcribing those one-minute chunks to a platoon of humans. It saves money, but theres a not-insignificant amount of prep and clean-up required. ( Casting Words seems to have built a business model on the same technique, though it lands right back at the $1 per minute rate.) For an easier to operate crowdsourced interface, there is also the sharing-economy-era site TranscribeMe , transcriptions provided by a small army of manual transcribers, heeding the companys call to monetize your downtime. A freely available voice transcription tool is likewise built-in to Google Docs for those who would care to experiment. You can play recorded audio on your computer, and the system will do its best to make the proper text appear in a Google Doc. For the five phone interviews conducted for this article, recorded via Skype, only one subject spoke slowly and clearly enough to even register as recognizably transcribed text, with an error rate of roughly 15 percent. Those who only want to transcribe podcasts might have better luck. Where currently available transcription technology cant handle multiple voices or background chaos, reliable software like Nuances Dragon NaturallySpeaking (also an outgrowth of Reddys lab at Carnegie Mellon) has become quite capable at trained single voices. David Byron, editorial director of Speech Technology magazine suggests a technique called parroting: listening to a recording in real-time and repeating its text back into the microphone for the software to transcribe. It saves some typing, but is far from instantaneousand still forces interviewers to relive their most awkward interview moments. One person who has doubts about the imminent arrival of long-form transcription technology is Roger Zimmerman, Chief of Research and Development at 3Play Media , perhaps the only company currently offering a commercial application for automated long-form transcription. Using a combination of APIs provided by vendors Zimmerman said he could not disclose, 3Plays initial transcriptions average around 80 percent accuracysometimes much more, sometimes much lessand are corrected by human transcribers before being sent to customers. Speech recognition technology is not anywhere near human capability, Zimmerman says, and wont be for many, many years, my guess is decades still. Humans do not speak like text, says Zimmerman, who has been working with speech technology since the 1980s, when he landed a job at the Voice Processing Corporation, an offshoot of MIT. Ive hesitated, Ive corrected, Ive gone back and repeated, and to the extent that you have disorganized spontaneous speech, the language model is unsuited for that. Its the weak component. Its the component of the system now thats dependent on fundamental artificial intelligence. What theyve done with acoustic modeling is signal processing-oriented, and it is well framed, these new deep neural networks, they understand what theyre doing when they decode an acoustic signal, but they dont really understand what a language model needs to do to mimic human languaging process. Theyre using number-crunching to address a much higher artificial intelligence problem which has really not been solved yet. But its not thaaat hard, suggests Jim Glass, a Senior Research Scientist at MIT who leads the Spoken Language Systems Group and who serves as an advisor to 3Play. Glass says, in fact, that the technology is already here. The way to think of this problem is [to ask] what error rate is tolerable for your needs, so if youre skimming through the transcript and could jump back to the audio to verify it, you might be willing to tolerate a certain amount of errors. The technology is good enough today to do that. It would take somebody to decide that they want to make that capability available. Part of the problem historically with speech technology is companies figuring out how to make money off of it, and I dont know if theyve figured out how to do that yet, Glass says. He points out that there are toolkits available for developers who would like to play with the nascent technology. The piece that has yet to be combined into commercially available transcription like Google Voice is known as two party diarization, a speaker-independent system that can determine who is speaking and what they are saying. One person speaking clearly is one thing, but two people engaging in lively discourse is another entirely. And it is a problem that has been solved, in part, at least in the bounds of scientific research. There is a whole field devoted to it, rich transcription. In 2012, the Institute of Electrical and Electronics dedicated a whole issue of their journal, Transactions on Audio, Speech, and Language Processing, to  New Frontiers in Rich Transcription . Part of the problem historically with speech technology is companies figuring out how to make money off of it, and I dont know if theyve figured out how to do that yet. Jim Glass, Senior Research Scientist at MIT Over a relatively clean phone line, technology could identify the speaker about 98 percent of the time, says Gerald Friedland, who headed the diarization project at the nonprofit ICSI, as the group participated in trials run by the National Institute of Standards and Technology. Running the Meeting Recorder Project to test group recording situations, ICSI confirmed that once the microphone is no longer the close-range type provided by phones, the error rate shoots up to anywhere between 15 percent and 100 percent. Friedland points out the range of problems that have to be addressed once one goes past the relatively clean speech of broadcast news into the type of long-form speech that many researchers work with today. He says, If you put your cell phone on the table and try to record everything thats being said and then try to transcribe it, you have a combination of many of these problems: new vocabulary [words], the cocktail party noise problem, regular noise, people overlapping, and people never speak perfectly. Its got coughs and laughs and there might be yelling and there might be whispering. It becomes very diverse. Two voice spectrums that often cause chaos in diarization studies fail tests are children and the elderly. You can combine these scenarios, he says. I think all of this guarantees that a perfect speech recognizer that just listens in like a human will not be achieved in a reasonable time. You and I will probably not see that. Which shouldnt be interpreted to mean that were not living in the golden age of speech technology. This month, Friedland helped launch MOVI, a Kickstarted speech recognizer/voice synthesizer for Arduino that operates without the use of the cloud. It doesnt use the Internet, Friedland says. You dont have to use the cloud to do recognition. It can work with a couple hundred sentences and it adapts. He laughs at Sony, Apple, Google, Microsoft, and other companies that send speech into the cloud for processing. All of this is exploiting the fact that people think [voice recognition] is so hard that it has to get done in the cloud. If you have one speaker speaking into a computer, we should consider this problem solved. For now, Friedland says, most transcription start-ups seem to be mainly licensing Googles API and going from there. But the field and the market are wide open for innovation at every level, with bizarre kinds of unforeseen societal change coming as soon as a project succeeds. ",https://www.wired.com/2016/04/long-form-voice-transcription/,"We used to joke that, depending who you ask, speech recognition is either solved or impossible, says Gerald Friedland, the director of the ...",Why Our Crazy-Smart AI Still Sucks at Transcribing Speech,Nuances Dragon Mattels Hello Barbie Gerald Friedland Audio Multimedia International Computer Science Institute UC Berkeley Xuedong Huang Microsoft Oxford API Google Microsoft Huang Huang Huang XD Huang Tsinghua University Beijing Huang Raj Reddy Carnegie Mellon Microsoft Huang Reddy Dragon Systems Jim Baker January Communications ACM Historical Perspective [ Carl Case Machine Learning Baidu Deep Speech Weve Deep Speech English Chinese Case Microsoft Programmer WIRED Andy Baio Amazons Mechanical Turk Words TranscribeMe Google Docs Google Doc Skype Nuances Dragon NaturallySpeaking Reddys Carnegie Mellon David Byron Speech Technology Roger Zimmerman Chief Research Development Media APIs Zimmerman Zimmerman Zimmerman Voice Processing Corporation MIT Ive Ive Ive Theyre Jim Glass Senior Research Scientist MIT Spoken Language Systems Group Glass ] Glass Google Voice Institute Electrical Electronics Transactions Audio Speech Language Processing New Frontiers Rich Transcription Jim Glass Senior Research Scientist MIT Gerald Friedland ICSI National Institute Standards Technology Recorder Project ICSI Friedland ] Which Friedland MOVI Kickstarted Arduino Internet Friedland Sony Apple Google Microsoft Friedland Googles API,3
282,"Meeami Technologies, together with Qualcomm Technologies and XMOS, Enable Enhanced Voice Recognition for Amazon Alexa Voice Services (AVS) and Internet of Things (IoT) Applications LAS VEGAS and SANTA CLARA, Calif., Jan.  05, 2017  (GLOBE NEWSWIRE) -- Consumer Electronics Show 2017 -- Meeami Tech , the leader in IP Communications and voice IoT, today announced integration of Meeamis industry leading ClearVoice voice enhancement software solution on the Wi-Fi router platform (IPQ40x8) from Qualcomm Technologies, a subsidiary of Qualcomm Incorporated, and the XMOS xCORE-200 XUF216. ClearVoice vastly improves automatic speech recognition (ASR) for voice command and control user interfaces (UI), and artificial intelligence (AI) driven natural language processing (NLP) found in services such as AVS, Cortana and Google Assistant. The joint demonstration shows ClearVoice-powered AVS integration for a range of home IoT applications.  ClearVoice is an advanced software solution for voice IoT applications that enables superior voice recognition in far-field and noisy environments via patented voice signal processing algorithms for multi-microphone beamforming, noise cancellation (NC), acoustic echo cancellation (AEC) and automatic gain control (AGC). ClearVoice offers broad hardware platform support and highly optimized CPU and memory utilization for easy system integration with reduced cost and complexity. It supports single microphone and arrays from 2 up to 8 or more microphones. With the popularity of Amazons Alexa, Voice IoT has gained significant momentum in the smart home market and we now see wide adoption of voice-based UIs across wearables, connected car, enterprise, and vertical and industrial applications, noted Doug Makishima, vice president of sales and marketing for Meeami Tech. This solution, integrating Qualcomm Technologies solutions and XMOS, will enable OEMs to add high-quality ASR quickly and cost effectively to their products. Even as the Qualcomm Network continues to drive simplicity and seamlessness into the modern network, we never cease in our pursuit of the next great consumer feature, said Gopi Sirineni, vice president, product management, Qualcomm Technologies, Inc. Our demonstration at CES 2017, showcases just how compelling a combination of voice and audio capabilities can be when integrated into a distributed or mesh network. XMOS is delighted to support Meeami to bring their ClearVoice voice enhancement software solution to market, said Mark Lippett, XMOS CEO. The triumvirate of high performance voice capture systems, advanced automatic speech recognition systems, and the seamless integration with distributed networking for the IoT will undoubtedly drive the adoption of next generation human-machine interfacing. Meeami Technologies, a spin-off of the former media processing and real-time communications group of Imagination Technologies, is the recognized leader in IP Communications and Voice IoT technology platforms for voice, video and messaging applications. To see how Meeami is helping top-tier OEM, IC and carrier customers, with embedded software, mobile apps and end-to-end communications solutions, see www.meeamitech.com . Qualcomm is a trademark of Qualcomm Incorporated, registered in the United States and other countries. Qualcomm Network features are products of Qualcomm Technologies, Inc. Contact  Doug Makishima VP Sales and Marketing Meeami Technologies Tel: +1.925.271.5820 Email: doug@meeamitech.com other press releases by Meeami Technologies, Inc. ",https://globenewswire.com/news-release/2017/01/05/903804/0/en/Meeami-Technologies-together-with-Qualcomm-Technologies-and-XMOS-Enable-Enhanced-Voice-Recognition-for-Amazon-Alexa-Voice-Services-AVS-and-Internet-of-Things-IoT-Applications.html,"ClearVoice vastly improves automatic speech recognition (ASR) for voice command and control user interfaces (UI), and artificial intelligence ...","Meeami Technologies, together with Qualcomm Technologies and ...",Meeami Technologies Qualcomm Technologies XMOS Enable Enhanced Voice Recognition Amazon Alexa Voice Services AVS Internet Things IoT Applications LAS VEGAS SANTA CLARA Calif. Jan. GLOBE NEWSWIRE Consumer Electronics Show Meeami Tech IP Communications IoT Meeamis ClearVoice IPQ40x8 Qualcomm Technologies Qualcomm Incorporated XMOS XUF216 ClearVoice ASR UI AI NLP AVS Cortana Google Assistant ClearVoice-powered AVS IoT ClearVoice IoT NC AEC AGC ClearVoice CPU Amazons Alexa Voice IoT UIs Doug Makishima Meeami Tech Qualcomm Technologies XMOS OEMs ASR Qualcomm Network Gopi Sirineni Qualcomm Technologies Inc. CES Meeami ClearVoice Mark Lippett XMOS CEO IoT Meeami Technologies Imagination Technologies IP Communications Voice IoT Meeami OEM IC Qualcomm Qualcomm Incorporated United States Qualcomm Network Qualcomm Technologies Inc Contact Doug Makishima VP Marketing Meeami Technologies Tel @ Meeami Technologies Inc,3
283,"There are millions of these in households, and theyre not collecting dust, Nikko Strom , a speech-recognition expert and founding member of the team at Amazon that built Alexa and Echo, said at the AI Frontiers conference in Santa Clara, California, last week. We get an insane amount of data coming in that we can work on. Strom said that data had already helped the company make progress on a longstanding challenge in speech recognition known as the cocktail party problem, where the challenge is to pick out a single voice from a hubbub of many people talking. Initially Alexa could easily tell that someone had called out its name, butlike other voice-recognition systemsit struggled to know which words being said around it were the request being issued. Then Stroms team developed a system that notes characteristics of a voice that calls out Alexa and uses them to home in on the words of the person asking for help. The data Amazon is amassing to take on problems like that could be unique. Standard datasets available for training and testingspeech recognition systems dont usually include audio captured in home environments, or using microphone arrays like that the Echo uses to focus on speech from a particular direction, says Abeer Alwan , a professor at University of California, Los Angeles, who works on speech recognition. People have been toying with microphone arrays for a long time but I dont think there has been a deployment at the scale Amazon is talking about, says Alwan. More data on a particular scenario or type of speech usually translates into better performance, she says. Strom said he also hopes that his teams data trove could eventually help upgrade Alexa to being able to follow two people speaking simultaneously. Its hard, but theres been some progress, he said. Its super interesting for us if we could solve that problem. Strom didnt say what Alexa might be able to do once that problem is solved. But it might make it more natural for multiple people to interact with an Echo or other device at once, whether thats kids peppering Alexa with questions or their parents rattling off a shopping list. The data piling up from Alexa could also help Amazon fend off Googles Echo competitor, Google Home, which launched late last year . Google can draw on years of work in Web search and voice search, and sizeable investments in artificial intelligence. But its previous products and businesses dont naturally collect speech like that of a person calling out to a device in the home, or on the same type of requests people ask home assistants to serve. Amazon is probably hoping that this contest turns out like the Web search market. Research has suggested that one reason Googles dominance couldnt be shaken by startups or well-funded competitors such as Microsoft was that Google had piles more data on what people search for and click on. Early reviews of Google Home have generally said that it and Amazons products are broadly similar, each with their own strong points. And Google is presumably working hard to learn all it can from the data coming in from its new product. But it will take some time for that flow of information to rival what Amazon is getting. Analysts estimated last November that over five million Echo devices had been sold since its launch two years prior, and Amazon said last month that Echo devices were the top seller over the holiday season. Alexa is also set to start appearing in products, such as speakers, cars, and fridges , from other companies. Weren't able to make it to EmTech Digital ",https://www.technologyreview.com/s/603380/alexa-gives-amazon-a-powerful-data-advantage/,"There are millions of these in households, and they're not collecting dust, Nikko Strom, a speech-recognition expert and founding member of ...",Alexa Gives Amazon a Powerful Data Advantage,Nikko Strom Amazon Alexa Echo AI Frontiers Santa Clara California Strom Initially Alexa Stroms Alexa Amazon Standard Echo Abeer Alwan University California Los Angeles Amazon Alwan Strom Alexa Strom Alexa Echo Alexa Alexa Amazon Googles Echo Google Home Google Web Amazon Web Research Googles Microsoft Google Google Home Amazons Google Amazon November Echo Amazon Echo Alexa EmTech Digital,4
284,"Opus Research recently released a report titled Executive Survey on Speech Analytics, which found that 72% of companies believe that speech analytics can lead to improved customer experience and more than 52% think that its deployment can lead to higher revenue. Additionally, Opus Research analysts found that 68% of respondents consider speech analytics as a cost-saving mechanism. The most salient meaning of these data points is that experienced customer care professionals have gained confidence in real-time capabilities of the latest speech analytics platforms, said Dan Miller, lead analyst at Opus Research. The study found that 68% of respondents expect rapid and easy recognition of customer intent, while 67% believe that it helps monitor calls for quality purposes and agent training. The research also pointed to speech analytics ability to detect and prevent fraud, especially in financial services, banking and retail industries. Most respondents said they expect to increase spending and investment on speech analytics for their business operations. The research is based on 500 surveys of executive decision makers at companies with revenue of more than $50 million. The survey was conducted in the first quarter of 2016 across the United States, Canada, Southeast Asia, India and Australia, representing diverse sectors such as banking, financial markets, healthcare services, retail, telecommunications, travel and hospitality. In total, 247 out of the 500 respondents had speech analytics deployed at their enterprises and organizations. The report confirmed cost savings and improved customer experience as the significant reasons for deployment of speech analytics. Markets and Markets has also introduced a new report titled  Speech & Voice Recognition Market by Technology (Speech Recognition, Voice Recognition), Application (AI Based, Non AI Based), Vertical (Automotive, Consumer, Finance, Retail, Military, Healthcare & Government) and Geography  Global Forecast to 2022 . The study found that the speech recognition market is expected to grow from $3.73 billion in 2015 to $9.97 billion by 2022, at a CAGR of 15.78% during the forecast period. Meanwhile, the voice recognition market is expected to grow from $440.3 million in 2015 to $1.99 billion by 2022, at a CAGR of 23.66% between 2016 and 2022. The key factors driving the speech and voice recognition market are the growing instances of fraud in several end-user industry segments such as enterprise and healthcare, and the adoption of mobile banking by several national and international banks and e-commerce retailers. The report also emphasizes that major companies such as Google, Apple, and Microsoft are leveraging their large customer base and neural networks to process, understand, and take decisive actions based on real-time voice inputs from the user. In the long term, speech and voice recognition abilities are expected to be integrated with other consumer devices such as refrigerators, ovens, mixers and thermostats, with the growth of IoT. integrating speech and voice recognition technology in their devices. North America dominated the global speech and voice recognition market due to the large-scale deployment of biometric systems compared to other regions. The report also highlights major players in the speech and voice recognition market, including Nuance Communications, Microsoft Inc., Agnitio SL, Biotrust, VoiceVault, VoiceBox Technologies Corp., LumenVox LLC, M2Sys LLC, Raytheon BBN Technologies, ValidSoft UK Limited, Advanced Voice Recognition Systems, Sensory Inc., and MMODAL Inc. ",https://www.biometricupdate.com/201606/reports-forecast-significant-growth-in-speech-and-voice-recognition-market,"Opus Research recently released a report titled Executive Survey on Speech Analytics, which found that 72% of companies believe that ...",Reports forecast significant growth in speech and voice recognition ...,Opus Research Executive Survey Speech Analytics Opus Research Dan Miller Opus Research United States Canada Southeast Asia India Australia Markets Speech Voice Recognition Market Technology Voice Recognition Application AI Non AI Vertical Automotive Consumer Finance Retail Military Healthcare Government Geography Global Forecast CAGR CAGR Google Apple Microsoft IoT North America Nuance Communications Microsoft Inc. Agnitio SL Biotrust VoiceVault VoiceBox Technologies Corp. LumenVox LLC M2Sys LLC Raytheon BBN Technologies ValidSoft UK Limited Advanced Voice Recognition Systems Sensory Inc. MMODAL Inc,3
285,"An international coalition of consumer watchdogs says Nuance Communications is violating the privacy of children who play with two toys that use the Burlington companys speech-recognition software. The toys, My Friend Cayla and i-Que Intelligent Robot, use Nuance software to answer questions posed by children. But in complaints they expect to file with federal regulators on Tuesday, the consumer groups allege that Nuance is saving recordings of those interactions with children for future use without providing adequate warning to parents, in violation of a 1998 federal law to protect the online privacy of minors. Both toys are made by Genesis Toys of Los Angeles, which will also be named in a complaint to the Federal Trade Commission. Neither Genesis Toys nor Nuance could be reached for comment Monday. My Friend Cayla and i-Que toys are spying on children, and sharing their intimate conversations with Genesis, Nuance, and unknown third parties, said Josh Golin, executive director of the Boston-based Campaign for a Commercial Free Childhood , one of the organizations that plans to file the complaint with the FTC. Get Talking Points in your inbox: An afternoon recap of the days most important business news, delivered weekdays. Thank you for signing up! Sign up for more newsletters here Other groups backing the complaint include Consumers Union , which publishes Consumer Reports magazine, and the Electronic Privacy Information Center (EPIC). The coalition also includes several groups based in Europe, such as the Norwegian Consumer Council, that are expected to file similar complaints Tuesday with regulators for the European Union, France, the Netherlands, Belgium, Ireland, and Norway. My Friend Cayla carries a starting price of about $40 but more costly versions are available. The I-Que Intelligent Robot costs about $130. Both are available through Amazon.com; My Friend Cayla is also sold at other retailers including Walmart, Sears, and Toys R Us. Both toys use Bluetooth wireless radio to connect to a software app to run a nearby mobile device, such as a smartphone or a tablet. The mobile device relays data between the toy and Nuance over the Internet. A child can ask questions of the toy, such as Who wrote Alice in Wonderland Her voice is then transmitted to a Nuance data center, which translates the question into text and sends a reply to the toy, in the form of a synthesized voice. In this way, a child can talk to the toy about an endless variety of topics. But what happens to the digital recording of the childs voice According to the consumer groups, Nuances service contract with Genesis Toys stipulates the Massachusetts company will store the recordings and analyze them to improve the performance of the toys and of other products and services. That, the groups said, violates a federal law that sets strict limits on the collection of data from anyone under age 13. Moreover, they point to Nuances own privacy policy, in which the company states that anyone under age 18 should not send any information about yourself to us and promises to delete any information that is inadvertently acquired from a child under age 13. The Genesis toys are not the first to fall under suspicion because of the way they connect children to the Internet. Mattel Corp.s Hello Barbie, a talking doll with Internet-based speech recognition, came under fire last year for security flaws that might allow hackers to access a childs recorded words. Mattel said it has fixed security flaws in its servers, and moreover, the doll did not collect any sensitive information from children. Nuance was peripherally involved in a case involving Samsung Electronics, whose Smart TV used the companys speech software to recognize verbal commands of watchers. In 2015, EPIC filed a complaint with the FTC alleging that the TVs could pick up unrelated conversations  and transmit those recordings to Nuance. Samsung subsequently clarified its policy that the software collected only the commands that watchers directed at the TV. Claire Gartland, the director of EPICs consumer privacy project, said federal law requires that any data collected from a child be used only to provide specific services for that child, and that it must then be deleted. Children arent turning their information over so that Nuance can spiff up its voice- recognition technology, Gartland said. Both Genesis toys are also vulnerable to hacking because Genesis has not developed adequate security features, the consumer groups allege. Researchers at the Norwegian Consumer Council found that they could use a Bluetooth-equipped mobile device to take control of the toys and eavesdrop on a childs conversation  and even talk back to the child. Finn Myrstad, director of digital policy for the Norwegian Consumer Council, said that Genesis was informed of the problem a year and a half ago, but has not fixed it. Myrstads group also found that the My Friend Cayla doll appears to be preprogrammed to deliver oral advertisements for products from Walt Disney Co. For instance, the doll says her favorite movie is Disneys The Little Mermaid and that she loves to visit Disneyland. The complaint alleges that including such advertising in a toy constitutes a deceptive business practice under federal law. ",https://www.bostonglobe.com/business/2016/12/06/nuance-under-fire-over-toy-privacy/WryFiVdq6zIVTxLuRImdVI/story.html,"s Hello Barbie, a talking doll with Internet-based speech recognition, came under fire last year for security flaws that might allow hackers to ...",Could your children's toys be violating their privacy?,Nuance Communications Burlington My Friend Cayla Intelligent Robot Nuance Tuesday Nuance Genesis Toys Los Angeles Federal Trade Commission Genesis Toys Nuance Monday Cayla Genesis Nuance Josh Golin Campaign Commercial Free Childhood FTC Get Thank Sign Consumers Union Consumer Reports Electronic Privacy Information Center EPIC Europe Consumer Council Tuesday European Union France Netherlands Belgium Ireland Norway Cayla Intelligent Robot Amazon.com Friend Cayla Walmart Sears Toys R Us Both Bluetooth Nuance Internet Who Alice Wonderland Her Nuance Nuances Genesis Toys Massachusetts Nuances Genesis Internet Mattel Corp.s Hello Barbie Mattel Samsung Electronics Smart EPIC FTC TVs Nuance Samsung Claire Gartland EPICs Children Nuance Gartland Genesis Genesis Consumer Council Bluetooth-equipped Finn Myrstad Consumer Council Genesis Myrstads My Friend Cayla Walt Disney Co. Disneys Little Mermaid Disneyland,-1
286,"Nuance Exec on status on speech recognition, where the field is going Founded in 1994, Nuance Communications provides speech recognition technology sophysicians can dictateinformation into EMRs, freeing them to spend more time with patients. The Burlington, MA-based company currently does $1 billion business in the health space and has more than half of all U.S. physicians on its speech products. Joe Petro, senior vice president of healthcare R&D, recently spoke with Healthcare Dive about how far speech recognition technology has come in the last decade, where it is going and how that affects physician practice and healthcare generally. Healthcare Dive:How far has speech recognition changedas a tool for physicians Joe Petro:Speech recognition has changed a lot just in the seven years that Ive been at Nuance. And the way we sort of measure the goodness of speech recognition is through accuracy or word error rate. If you look where things started 15 years ago, speech recognition was in the 75% accuracy range, which meant one in four words were incorrect. That was a real challenge, because if you think about how a physician might use something, if a product was producing a one in four WER, that would be very difficult to have a sustainable workflow and produce something that was actually usable. What ended up happening was that speech was applied in such a way that the problem was very, very narrow. For example, if there were only three or four words you were selecting, the words sounded quite different from one another and you could ratchet the accuracy way up to 90%. But if you were generally narrating, the error rate was actually quite high, in the 25% range, so only 75% accuracy. Over the years, speech recognition has become a lot more accurate. Right out of the box, were now in the 95% range. The performance is very high fidelity with real time, so you can put the speech into the system and the system can write the words on the paper as fast as you can actually speak them. Not only does it do speech recognition, but also command and control interfaces. So with an EMR, you can input the patients information and actually control the interface as well. What are some of the key ways its being used to improve physicians relationship with technology Petro:As you know, EMRs have sort of a mixed level of reception among physicians. Its putting a lot of demands and regulations on physicians to capture information inside the EMR, and for many physicians it is getting in the way of patient care. We feel, and we have measured these results, that speech recognition gives the physician more time to get back to the bedside. So speech recognition has made the data entry and documentation experience much more natural. The other thing thats happened over the last several years is physicians have become mobile and they want the same kind of documentation experience that they have on their desktop on their laptop or iPad or iPhone. And because we can deliver speech thru the cloud now and stream that experience down to the device, weve created something called form factor agnostic speech. That essentially meansthe physician can use our technology anytime, anywhere on any device and get the same experience.We feel like thats been a real game changer and thats really just come into play over the last few years. How does this change or enhance the physician-patient relationship Petro:This is all about getting the physician back to the bedside, giving them a little more time in the day to actually treat patients. So in terms of documentation, something that would take the physician 15 minutes can now be done in five. Its really easy to see that speech recognition is creating more bandwidth between the physician and patient and creating a more natural experience. It also improves the doctor-patient experience because the patient can listen to what the physician is actually saying and entering into the record versus the physician privately entering the documentation. How are the interoperability and functionality currently Petro:From an interoperability point of view, we work with all of the major EMR companies  Epic, Allscripts, eClinical Works, you name it. The experience is a deeply integrated experience. The physician basically pulls up the EMR, automatically log into Dragon, click into the field, pick up the microphone or use theiriPhone and enter speech directly to the point of documentation. All of that happens in real time if its a front-end experience. As for functionality, what weve added lately has really been focused around this form factor agnostic notion of where were enabling speech on more devices and making the experience on those devices more powerful. And over the last year and a half to two years, weve crossed that line where theres no functionality gap between the desktop experience and the mobile experience. Where do you see the integration going in the future Petro:Whats going to happen is understanding natural speech, or clinical language understanding. Were marrying that into the speech workflow, so as youre speaking, were mining the things that you narrate  patient complaint, active medication, allergies, social habits, etc. By harvesting that out of the narrative, we make that information actionable. So you can leverage this information for things like population health, clinical documentation improvement, quality metrics, clinical outcomes. Whats happening is the functionality is extending not only from an experiential point of view  the quality and accuracy experience  but also extending into new workflows and new use models that are basically driven by natural language processing. So speech and natural language processing, or language understanding, are becoming inseparable. How fast are hospitals and healthcare practices moving to adopt speech recognition technology Petro:We have about 500,000 physicians on our products, between 50% and 66% of the market. From a transcription point of view, we do about 5 billion lines a year, which is about half the market. And especially whats happening with transcription, as EMRs get installed, the transcription market is converting over to front-end speech recognition. I would say that at any account that installs Dragon, anywhere between 50% and 100% of doctors actually adopt it. We dont really see any slowdown relative to the adoption rate for speech technology. Speech is pretty much mainstreamed into the IT infrastructure at this point. Are there other companies working on speech recognition in the medical space, and do you see this niche heating up as the technology becomes more widespread Petro:There are a lot of players out there. Google has it. Microsoft and Cortana. There are a couple of small players in our space as well. Were the big gorilla in speech with a $1 billion business now. We feel like weve been working in a heated up market for a period of time. When I got here, the division was about $220 million, so in seven years weve more than quadrupled. The technology is becoming widespread, and I think on the consumer side, Nuance, as well as companies like Google and Microsoft, are making it more and more natural for us to talk to our iPhone. So I think this is all working for us and its helping advance the technology, because the more comfortable everybody feels using it, speech is one of the best ways to communicate. What do you see as the major challenges in speech recognition going forward From an accuracy point of view, were on the last mile. Right out of the box now, just about anybody can expect a 95% accuracy rate. What were working on, and were spending tens of millions of dollars per year from a research point of view, is closing the 5% gap so it turns into 1%. Some radiologists already have a 99% accuracy rate, and were working on their 1%. A lot of the functionality that weve been adding new workflows and capabilities around the mobile physician and making sure that the mobile experience is on par with the experience that physicians have gotten used to on their desktop. And were pretty much there at this point. There are a few things were adding here and there. Youll see extensions from a technology point of view. The cloud enables a lot of very, very interesting things like I can pick my phone up and dictate, put my phone down and move to my desktop, and my dictation is already there. I can have the exact same experience if Im using a partners mobile application like Epics Haiku or. Epic Power Chart Touch, because my ID sort of follows me around. So these things are all pointed in the direction of making speech so incredibly natural that it doesnt feel like a tool anymore; it just feels like part of a very natural experience in terms of working with the electronic medical record. How sensitive is speech recognition today when youre dealing with different accents and speech patterns We have accent-based models, and within the first four seconds of speaking, the technology switches you to the appropriate model for the accent. Weve got several accent models just within the United States, and when our technology hears youve got a southwestern accent, it plugs you into the southwestern model and that model becomes part of your experience moving forward. The same thing with American Irish, Indian, different Asian accents  those are all different models that we support. We also support about 75 different languages. How is user experience measured in terms of accuracy and satisfaction We measure a variety of things, but one of the things we track on the cloud, for example, is accumulated change rate, or ACR. That basically means that when a physician is interacting with a document  they speak, text gets delivered, they review and modify the text  we watch how much they fiddle around with the document and the amount of changes they make. So if it misses a word like not, thats a big deal. And we try to make sure that we screen for that when were looking at ACR. One of the things that we try to do is pool physicians into a cohort group, so if weve got a number of physicians who are in the 98% to 99% range and their experience is really, really positive, well compare and contrast that to a cohort that is in the 92% to 93% range. Well ask ourselves, is there anything we can derive from the data that tells us that theyre doing something differently, and if we have somebody on site, we might interview those physicians and try to figure out what theyre doing differently. The cloud has really changed the game and everything is de-identified, so its all HIPAA-compliant and physicians opt into the program. So its all part of what we do. ",http://www.healthcaredive.com/news/nuance-exec-on-status-on-speech-recognition-where-the-field-is-going/413273/,"Founded in 1994, Nuance Communications provides speech recognition technology so physicians can dictate information into EMRs, freeing ...","Nuance Exec on status on speech recognition, where the field is going",Nuance Exec Nuance Communications EMRs Burlington U.S. Joe Petro R D Healthcare Dive Healthcare Dive How Joe Petro Ive Nuance WER Right EMR Petro EMRs EMR So Petro Petro EMR Epic Allscripts Works EMR Dragon Petro Petro EMRs Dragon Speech IT Petro Google Microsoft Cortana Nuance Google Microsoft Right Youll Im Epics Haiku Epic Power Chart Touch ID Weve United States Irish Indian ACR ACR Well HIPAA-compliant,1
287,"Sensory Inc. announced this past week that Samsung continues to utilize TrulyHandsfree technology to provide speech trigger and speech recognition experience on the Samsung Galaxy S7 and Galaxy S7 Edge smartphones, even when no internet connection is present. Samsung has been integrating Sensory technology on Samsung Galaxy smartphone products since the introduction of the S2 back in 2011 and TrulyHandsfree technology has been on every Samsung Galaxy smartphone product since the S2, including the recently announced Galaxy S7 and S7 Edge. TrulyHandsfree also enables a touch-free experience on Samsungs Galaxy Note tablets, beginning with the original Note launched in 2011, to the current Note 5, as well as on the Galaxy Gear and Galaxy Gear S2 smart watches. According to the company, Sensory has ported TrulyHandsfree to more platforms than any other speech recognition company, with ultra-low-power deeply embedded ports available for leading DSP/MCU IP cores from ARM, Cadence, CEVA, NXP CoolFlux, Synopsys and Verisilicon, as well as for integrated circuits from Audience, Avnera, Cirrus Logic, Conexant, DSPG, Fortemedia, Intel, Invensense, Microsemi, NXP, Qualcomm, QuickLogic, Realtek, STMicroelectronics, TI and Yamaha. TrulyHandsfree supports US English, UK English, French, German, Italian, Japanese, Korean, Mandarin Chinese, Portuguese, Russian, and Spanish. ",https://www.biometricupdate.com/201604/samsung-continues-to-utilize-speech-recognition-technology-by-sensory,Sensory Inc. announced this past week that Samsung continues to utilize TrulyHandsfree technology to provide speech trigger and speech ...,Samsung continues to utilize speech recognition technology by ...,Sensory Inc. Samsung TrulyHandsfree Samsung Galaxy S7 Galaxy S7 Edge Samsung Sensory Samsung Galaxy S2 TrulyHandsfree Samsung Galaxy S2 Galaxy S7 S7 Edge TrulyHandsfree Samsungs Galaxy Note Galaxy Gear Galaxy Gear S2 Sensory TrulyHandsfree DSP/MCU IP ARM Cadence CEVA NXP CoolFlux Synopsys Verisilicon Audience Avnera Cirrus Logic Conexant DSPG Fortemedia Intel Invensense Microsemi NXP Qualcomm QuickLogic Realtek STMicroelectronics TI Yamaha TrulyHandsfree US English UK English French German Italian Japanese Korean Mandarin Chinese Portuguese Russian Spanish,3
288,"A Dublin-based startup has raised 7m to help develop its voice recognition technology. A Dublin-based startup has raised 7m to help develop its voice recognition technology. Voysis, founded by Peter Cahill, has built a voice recognition system that specialises in natural language processing and text to speech capabilities. The funding has come from Polaris Partners, which has also backed Irish tech startups Boxever and Profitero. Polaris was an investor in Logentries, which sold to Rapid7 for 64m in 2015. Noel Ruane, European venture partner with Polaris Partners, has become executive chairman of Voysis. Cahill said that Voysis will use the round of financing to expand its US team, opening a new office in Boston and to develop its technology for a wider range of partners and customer use cases. It's been a busy period for Irish startups dabbling in voice technology. Last month, another Dublin voice technology company, Soapbox Labs, raised 1.2m. Founded by Dr Patricia Scanlon, Soapbox Labs uses deep neural net speech recognition technology to analyse children's speech in noisy 'real world' environments such as kitchens and cafs. ""Voice is finally breaking through as the next interface and companies across all industries are eager to leverage these capabilities so users can speak naturally with their favourite brands from their phones, cars, or home appliances,"" said Cahill. ""Apple, Amazon and Google have built general purpose voice assistants that do an excellent job of understanding simple commands, but can sometimes lack real utility and purpose for third party applications."" The news comes as Ireland undergoes a venture capital boom. Recent figures from the Irish Venture Capital Association show that 888m in funding was recorded here last year, mostly for technology firms. This represents a 70pc increase on the previous 12 months. The figures also say that 'seed' funding for early-stage startups exceeded 70m. A Dublin-based startup has raised 7m to help develop its voice recognition technology. Voysis, founded by Peter Cahill, has built a voice-recognition system that specialises in natural language processing and text to speech capabilities. The funding has come from Polaris Partners, which has also backed Irish tech startups Boxever and Profitero. Polaris was an investor in Logentries, which sold to Rapid7 for 64m in 2015. Noel Ruane, European venture partner with Polaris Partners, has become executive chairman of Voysis. Mr Cahill said that Voysis will use the round of financing to expand its US team, opening a new office in Boston and to develop its technology for a wider range of partners and customer use cases. It's been a busy period for Irish startups dabbling in voice technology. Last month, another Dublin voice technology company, Soapbox Labs, raised 1.2m. Founded by Dr Patricia Scanlon, Soapbox Labs uses deep neural net speech recognition technology to analyse children's speech in noisy 'real world' environments such as kitchens and cafs. ""Voice is finally breaking through as the next interface and companies across all industries are eager to leverage these capabilities so users can speak naturally with their favourite brands from their phones, cars, or home appliances,"" said Mr Cahill. ""Apple, Amazon and Google have built general-purpose voice assistants that do an excellent job of understanding simple commands, but can sometimes lack real utility and purpose for third-party applications."" The news comes as Ireland undergoes a venture capital boom. Recent figures from the Irish Venture Capital Association show that 888m in funding was recorded here last year, mostly for technology firms. This represents a 70pc increase on the previous 12 months. The figures also say that 'seed' funding for early-stage startups exceeded 70m. ",http://www.independent.ie/business/jobs/voicerecognition-startup-voysis-raises-7m-35473221.html,"Voysis, founded by Peter Cahill, has built a voice recognition system that specialises in natural language processing and text to speech ...",Voice-recognition startup Voysis raises €7m,Voysis Peter Cahill Polaris Partners Boxever Profitero Polaris Logentries Rapid7 Noel Ruane Polaris Partners Voysis Cahill Voysis US Boston Dublin Soapbox Labs Dr Patricia Scanlon Soapbox Labs Voice Cahill Apple Amazon Google Ireland Venture Capital Association Voysis Peter Cahill Polaris Partners Boxever Profitero Polaris Logentries Rapid7 Noel Ruane Polaris Partners Voysis Mr Cahill Voysis US Boston Dublin Soapbox Labs Dr Patricia Scanlon Soapbox Labs Voice Mr Cahill Apple Amazon Google Ireland Venture Capital Association,3
289,"Google researchers have developed a speech recognition system that runs  faster than real-time on a Nexus 5 Android smartphone. Moreover, it can sustain highspeed and accuracy while running offline. The researchers accomplished their aims partly by integrating contextual data directly into the app; for example, in order to process the voice command, Text Kendrick, On my way,' the system would need access to the users Contacts informationso they imported that data directly into the app. Meanwhile, they helped to compress the apps size by combining voice command and dictation algorithms into a single language model, ultimately attaining a memory footprintof only 20.3 MB. According to the researchers, that small size doesnt compromise the systems speed or accuracyand those metrics are impressive, too. Performing a natural speech dictation task, the system has a word error rate of only 13.5 percent, and its median speed is seven times faster than real-time, the researchers note. Its a further demonstration of Googles continuingand perhaps growinginterest in speech recognition technology , with the company having invested heavily in speech recognition R&D and recently introduced voice dictation for its Google Docs app. This technology could have growing applications in the emerging Internet of Things , and Google is likely betting that these investments will have considerable pay-offs down the line. ",http://findbiometrics.com/google-offline-speech-recognition-303176/,Google researchers have developed a speech recognition system that runs faster than real-time on a Nexus 5 Android smartphone. Moreover ...,Google Researchers Develop High Speed Offline Speech Recognition,Google Nexus Android Text Kendrick MB Googles R D Google Docs Internet Things Google,4
290,"Computers are very close to understanding what you're saying as well as another human could, even if they don't yet know what you're talking about. ""Speech recognition is really close to reaching parity with humans, in the next three years,"" Xuedong Huang, Microsoft's Chief Speech Scientist, told techradar pro. ""If we can achieve this goal it will be a major landmark for civilisation. Language is only something we humans understand and master. The moment a computer can transcribe your conversation over the phone almost as accurately as humans is a major landmark for AI."" And for the typical conversation over the phone, he believes we'll get there in three years  at least in terms of recognising what's being said. ""Transcription is different from understanding; understanding is a different story,"" he cautions. ""To understand the message, the subtlety of what's being said  that's a long way off. To understand intent and meaning, we still have a long way to go."" He's been working on speech recognition for over 30 years, and every year, he says, he's seen consistent improvements. The benchmark researchers use to measure accuracy is making a transcription of two people talking on the telephone, and every year, he's seen the error rate go down 20% from the previous year. Thanks to deep learning, the best systems, like Cortana , are now making only twice as many errors as humans do. ""The transcription error is around 8% now; that's about twice as high as human error, which is around 4%. If we can maintain a 25% reduction every year  well, you do the math! I hope the last 4% is not too hard, and in the next three years we can achieve this."" The recent advances in speech recognition are down to a relatively new machine learning technique, deep learning . ""Machine learning as a whole is important, but deep learning has been critical to these improvements,"" Huang explains. Now Microsoft is making the Computational Network Toolkit (CNTK) it uses to build systems like Cortana's speech recognition available, free, as open source on GitHub. ""We believe the work we're doing internally can benefit the whole community. If you have better tools and better recipes, better dishes will be prepared. We believe the tools we're sharing can accelerate the progress of AI."" CNTK has previously been available to academic researchers, for non-commercial projects through the Codeplex site  now anyone can use it to build commercial systems. ""We did it in a quiet way, to get feedback,"" he says. ""Now we're trying to broaden the audience. This is one of our best kept secrets. We're moving forward and making it more open."" ",http://www.techradar.com/news/world-of-tech/machine-language-computers-are-on-the-brink-of-mastering-speech-recognition-1313691,"Speech recognition is really close to reaching parity with humans, in the next three years, Xuedong Huang, Microsoft's Chief Speech Scientist, ...",Machine language: Computers are on the brink of mastering speech ...,Xuedong Huang Microsoft Chief Speech Scientist AI Transcription Cortana Machine Huang Microsoft Computational Network Toolkit CNTK Cortana GitHub AI CNTK Codeplex,2
291,"This story was delivered to BI Intelligence Apps and Platforms Briefing subscribers. To learn more and subscribe, please click here . Microsoft Research developed a voice recognition system as accurate as humans, with an error rate of just 5.9%, according to Microsoft's Chief Speech Scientist Xuedong Huang. The historic breakthrough comes just one month after the company surpassed IBM Watson's previous record of 6.9%. Microsoft will likely incorporate the improved system into its various products including voice assistant Cortana, Xbox, and other productivity tools. These advancements will also spur competition among tech companies, which will likely boost overall voice recognition capabilities.  Rapid advancements in voice recognition accuracy is going to help the technology emerge as a dominant computing interface. That's because it's much faster and easier to talk to your device than type on it. A recent Stanford University study showed thatvoice input is three times faster than keyboard input, and with a 20.4% lower error rate. And in an attempt to move beyond the hype, tech companies including Amazon, Apple, Google, and IBM, are already deploying voice assistance to consumers. Apple, for instance, recently added third-party integration to Siri, which will enable it to communicate with other apps. This means that users can order an Uber through Siri. And Google is slowly rolling out Google Assistant to its users. But voice assistants need to overcome a number of hurdles before mass adoption occurs: ""As close as humanly possible"" is not good enough for voice assistants. Despite the impressive results Microsoft's research yielded, speech recognition needs to reach roughly 99% for voice to become the most efficient form of computing input, according to Kleiner Perkins analyst Mary Meeker. This is likely because expectations for automated services are much less forgiving than human error allows. In fact, when asked what voice assistants could do better, ""understand the words I am saying"" received 44% of votes, according to MindMeld. Consumer behavior needs to change.While the use of voice assistants is increasing, many users are still uncomfortable speaking to their assistant. Thirty-four percent of millennial respondents in an exclusive BI Intelligence survey were either unaware of voice assistants, or unwilling to use them freely.And 18% said they would use Amazon's Echo only if they weren't around other people. Millennials surveyed by BI Intelligence skew toward the tech-savvy, which means that the level of discomfort among millennials in the general population is likely higher.For voice to truly replace text or touch as the primary interface, consumers need to be more willing to use the technology in all situations. Voice assistants need to be more helpful.Opening up third-party apps to voice assistants will be key in providing consumers with a use case more in line with future expectations of a truly helpful assistant. Voice assistants like Siri, Google Assistant, and Echo, are only just beginning to gain access to these apps, allowing users to carry out more actions like ordering a car.  ",http://www.businessinsider.com/microsoft-hits-a-milestone-in-voice-recognition-2016-10,"Despite the impressive results Microsoft's research yielded, speech recognition needs to reach roughly 99% for voice to become the most ...",Microsoft has hit a milestone in voice recognition,BI Intelligence Apps Platforms Briefing Microsoft Research Microsoft Chief Speech Scientist Xuedong Huang IBM Watson Microsoft Cortana Xbox Stanford University Amazon Apple Google IBM Apple Siri Uber Siri Google Google Assistant Microsoft Kleiner Perkins Mary Meeker MindMeld Consumer BI Intelligence Amazon Echo BI Intelligence Voice Voice Siri Google Assistant Echo,-1
292,"For a long time, speech recognition was going to be the next big thing. People would dictate documents naturally and flawlessly in half the time that they could type them, freeing up time for other activities. Eventually, however, people became tired of waiting for speech recognition to reach an adequate standard for regular use, and the concept became derided as a novelty and unusable for day-to-day work. Now, however, speech recognition has come of age and it's sneaked back into people's lives via a side entrance. Google, Apple and Microsoft all have speech recognition functionality built into their mobile operating systems, and you don't have to go out and buy a CD from which to install it as was the case during the first wave of interest. Speech recognition software has followed the classic Hype Cycle: an initial burst of inflated expectations followed by a trough of disillusionment, a gradual enlightenment about the technology's actual usefulness, and an eventual levelling out of productivity. There is logic to using speech recognition software on mobile devices, because often it can be quicker and easier than fiddling around with an on-screen keyboard. Mainstream adoption of speech recognition software on desktop PCs, however, has never happened. That may be partly due to people recognising some of its limitations, its perceived lack of suitability for an office environment, or simply because people moved on and just forgot about it. Whatever the reason, speech recognition software is now at the level for which people had initially hoped. It is accurate, useful and gradually beginning to bleed into our lives. There are a variety of different speech recognition tools and software packages, each offering specific functionality. Some are more fully loaded and advanced than others, but generally, they can be used for controlling a user's computer and dictating to a document. This article provides an overview of some of the most popular speech recognition tools and software packages available. Nuance's Dragon NaturallySpeaking software is regarded as the market leader where speech recognition is concerned. NaturallySpeaking was launched in 1997 and is now up to its thirteenth iteration. There are a host of versions available depending on the user's requirements and the software offers a huge amount of functionality. According to Nuance, NaturallySpeaking is the world's best-selling speech recognition software for the PC. NaturallySpeaking Professional Edition aims to provide business users with a means of controlling their computers and dictating documents, and Nuance claims it is three times faster than typing. As a result, it says, productivity can be improved and cost savings made. Amongst the functionalities provided are the capabilities to manage email, search the web and automate business processes. NaturallySpeaking allows users to dictate into Microsoft Office applications and OpenOffice, create emails, tasks and meetings in Microsoft Outlook, search the web using any major browser, and post to social media services such as Facebook and Twitter. The software recognises a number of standard commands, such as creating files, scheduling calendar entries and searching a user's computer. It's also possible to set up custom commands. Beyond its desktop functionality, Nuance can automatically transcribe user dictations into approved voice recorders, and mobile apps are available for iOS or Android. iOS users can record audio files whilst Android devices can be used as a wireless microphone. NaturallySpeaking Professional Edition costs 549. Prices vary for other editions, depending on the functionality. For more information on the package, check out our full Dragon NaturallySpeaking Premium 13 review . If you only need basic speech-to-text dictation functionality, then TalkTyper may well be adequate. TalkTyper is a simple, free-to-use website that captures user speech and renders it in plain text ready for being copied and then pasted elsewhere. It's not possible to sign up for an account, meaning that the website is designed simply for immediate and straightforward use. TalkTyper was created with the aim of making voice dictation freely available to anyone who needed it. According to TalkTyper, it first became possible when Google added speech input functionality to its Chrome browser. Once a user has loaded the TalkTyper website, they can click the microphone button and begin dictating. In addition to basic dictation, users can add basic punctuation by using commands like ""period"", ""question mark"" and ""new paragraph"". If they are happy with the resulting text, they can add it to their saved text pad. Having finished their dictation, the user is able to add symbols, copy the text, print it, send it to Twitter, send it via an email or translate it into a different language. ",http://www.techradar.com/news/software/business-software/speech-recognition-software-top-six-on-the-market-1259815,"For a long time, speech recognition was going to be the next big thing. People would dictate documents naturally and flawlessly in half the time ...",Speech recognition software: top six on the market,Google Apple Microsoft Hype Mainstream Nuance Dragon NaturallySpeaking NaturallySpeaking Nuance NaturallySpeaking NaturallySpeaking Professional Edition Nuance Microsoft Office OpenOffice Microsoft Outlook Facebook Twitter Nuance Android Android NaturallySpeaking Dragon NaturallySpeaking Premium TalkTyper TalkTyper TalkTyper TalkTyper Google Chrome TalkTyper Twitter,-1
293,"Voximplant has announced that they will leverage the Google Cloud Speech API as part of the Voximplant platform. As Voximplant expands, the cloud communications company has steadily implemented platform additions that meet new developer needs and improve the quality of the service. Here is a rundown on what you can expect from their new product from a developer's perspective. - Call recording transcription. Voximplant users can produce high-fidelity transcripts of their calls. - Real-time voice recognition for smart interactive voice response (IVR) scenarios. Customers can now make use of IVR systems offering computerized voice interactions through the Voximplant platform. - Rapid speech recognition and processing. Fast recognition reduces delays between when a person stops speaking and when the software receives the speech as text in order to act upon it. - Superior end-of-speech detection. Accurate end-of-speech detection enables more effective parsing of conversations for both transcription and software processing purposes. - Robust language support. Over 80 languages are now supported. - Comprehensive testing. Voximplants comprehensive internal testing found Google to offer high quality speech recognition. ",https://appdevelopermagazine.com/4561/2016/10/28/voximplant-to-use-the-google-cloud-speech-api-in-their-platform/,Rapid speech recognition and processing. Fast recognition reduces delays between when a person stops speaking and when the software ...,Voximplant to use the Google Cloud Speech API in their platform,Voximplant Google Cloud Speech API Voximplant Voximplant IVR IVR Voximplant Fast Accurate Google,-1
294,"The latest smartphones can recognise you by your voice. What happens when technology can pick us out from the crowd just by listening NOW your phone knows you better than ever. The latest version of Apples mobile operating system learns what your voice sounds like, and can identify you when you speak to Siri, ignoring other voices that try to butt in. Siri, the intelligent personal assistant, is not the only one who knows your voice. As learning software improves, voice-identification systems have started to creep into everyday life, from smartphones to police stations to bank call centres. More are probably on the way. In a paper published at the end of September, researchers at Google unveiled an artificial neural network that could verify the identity of a speaker saying OK Google with an error rate of 2 per cent. Voice is a  physiological phenomenon  shaped by your physical characteristics and the languages you speak, says Roger Moore at the University of Sheffield in the UK. A passphrase such as Hey Siri or  OK Google  is a powerful way to verify that you are who you say you are, he adds. My voice is different from your voice, which is different from your mothers voice, which is different from someone on the far side of the world, Moore says. The latest machine-learning techniques can tease apart the tiny differences. For machines, recognising individual voices is different from understanding what they are saying . The recognition software has been fuelled by massive sets of vocal data built into a huge model of how people speak. This allows measurements of how much a persons voice deviates from that of the overall population, which is the key to verifying a persons identity. Changes to someones voice due to sickness or stress can throw off the software. The technology is already being used in criminal investigations. Last year, when journalist James Foley was beheaded, apparently by ISIS, police used it to compare the killers voice with that of a list of possible suspects. And the banks JP Morgan and Wells Fargo have reportedly started using voice biometrics to figure out whether people calling their helplines are scam artists. Your voice doesnt just give away who you are, but what youre like and what youre doing, says Rita Singh at Carnegie Mellon University in Pittsburgh, Pennsylvania. Your speech is like your fingerprints or your DNA. Your voice doesnt just give away who you are, but what youre like and what youre doing Singh is figuring out how to build profiles of a stranger from audio recordings. A voiceprint gives insight into the speakers height and weight, their demographic background, and even what their environment is like. She is working with doctors in Massachusetts and Ohio to detect a persons likely diseases or psychological state through voice analysis. Having devices in the home that recognise voices does raise security concerns, especially if they understand what youre saying. Speech and voice algorithms often arent embedded in the device itself; instead, what you say is sent to a server somewhere else for analysis, and then ported back quickly. For example, Samsung fell into hot water this year with the revelation that its smart TVs could record private conversations. There are privacy concerns everywhere, says Singh. There is no device out there that ensures privacy. This article appeared in print under the headline OK computer, who am I ",https://www.newscientist.com/article/mg22830423-100-speech-recognition-ai-identifies-you-by-voice-wherever-you-are/,The latest smartphones can recognise you by your voice. What happens when technology can pick us out from the crowd just by listening?,Speech recognition AI identifies you by voice wherever you are,NOW Apples Siri Siri September Google Google Voice Roger Moore University Sheffield UK Hey Siri OK Google Moore James Foley ISIS JP Morgan Wells Fargo Rita Singh Carnegie Mellon University Pittsburgh Pennsylvania Singh Massachusetts Ohio Speech Samsung TVs Singh OK,-1
295,"Veritone Adds to Speech Recognition Capability, Expands Ecosystem with Growing Suite of New Cognitive Engine Providers Veritone partners with VoiceBase and adds new speech recognition capabilities to Veritones growing list of cognitive engines. Veritone, Inc., a leading cloud-based Artificial Intelligence (AI) technology company pioneering the first Cognitive Media Platform (CMP), announced today that it has partnered with VoiceBase, Inc., a highly scalable speech recognition and speech analytics API platform. This collaboration is expected to add new speech recognition capabilities to Veritones growing list of cognitive engines. Speech recognition allows users of Veritones CMP to accurately, quickly and affordably identify words and phrases in customer interactions to build custom predictive solutions found in historical and transactional data. Identifying these patterns enables users of VoiceBases cognitive engine to identify and optimize opportunities and risks. VoiceBases contribution to Veritones CMP is central to our machine learning capabilities, allowing us to analyze recorded audio with unprecedented granularity, said Chad Steelberg, Chairman and CEO of Veritone. Their highly accurate transcription quickly recognizes specific words being spoken in a recording that can transform audio into actionable intelligence. Finding relationships and trends between words provides insight that was previously extremely difficult to extract, giving our customers an edge over their competitors, added Steelberg. Veritone is leading a new wave of AI that is changing multiple industries and its CMP allows businesses of all kinds to search across audio and video as others have done for search through text. This game-changing technology allows Veritone to expand its international footprint as a leading AI analytics, search and predictive solution for media firms, corporate enterprise, political campaigns and government. Veritones innovative AI platform renders every second and frame of audio and video content searchable for voice identification, objects, faces, license plates, logos, phrases, sentiment and translation, plus additional capabilities that continue to evolve. Headquartered in Newport Beach, California, Veritone, Inc. is a leading cloud-based Artificial Intelligence media technology company pioneering the worlds first Cognitive Media Platform (CMP), an ecosystem that enables media, enterprise, politics, corporate security and government to extract value from the intelligence embedded in the worlds public and private media audio and video content, which comprises the majority of all global data produced each year. Founded in 2014, Veritone has already won several of the industrys most prestigious awards including the 2016 Red Herring Top 100 North America Award and the 2016 SIIA Business Technology CODiE Award for Best Native Advertising Platform or Service. Veritone was founded by serial entrepreneurs and brothers, Chad and Ryan Steelberg. The duo have a successful track record in identifying new market opportunities, creating disruptive technology-based companies and leading them to successful exits for their stakeholders. To learn more about how Veritone seeds the cloud, visit http://www.veritone.com . VoiceBase provides easy-to-use APIs that automatically transcribe audio and video, extract relevant keywords and topics and enable the instant search and discovery of spoken information. Every month VoiceBase processes millions of recordings that allow users to access and analyze rich data from call centers, conference calls, webinars, educational lectures, podcasts and videos. VoiceBase is privately held, and has raised over $20 million dollars. They are based in San Francisco, California. Visit http://www.voicebase.com to request a demo and learn more. Share article on social media or email: ",http://www.prweb.com/releases/2016/10/prweb13743915.htm,"Veritone Adds to Speech Recognition Capability, Expands Ecosystem with Growing Suite of New Cognitive Engine Providers ...","Veritone Adds to Speech Recognition Capability, Expands ...",Veritone Adds Capability Expands Ecosystem Growing Suite New Cognitive Engine Providers Veritone VoiceBase Veritone Inc. Intelligence AI Media Platform CMP VoiceBase Inc. API Veritones CMP VoiceBases Veritones CMP Chad Steelberg Chairman CEO Veritone Steelberg Veritone AI CMP Veritone AI AI Newport Beach California Veritone Inc. Intelligence Media Platform CMP Veritone Red Herring Top North America Award SIIA Business Technology CODiE Award Best Native Advertising Platform Service Veritone Chad Ryan Steelberg Veritone VoiceBase APIs VoiceBase VoiceBase San Francisco California Visit Share,-1
296,"Posted By Martha Papadimitriou on Aug 21, 2016 | 0 comments Googles famous voice assistant, Google Now , has a very interesting aspect. It can detect the different tones of the human speech. For that you can thank the combination of machine learning and the all the date they have gathered. It can analyze murmurs and mumbles eventhe most garbled phrases. But, there are some elements that Google Now seems to have a slight weakness. And that is different accents. But, Google is going to improve that by recruiting users of Reddit . According to reports the infamous social network Reddit is calling voice volunteers to work for Google. But they are not alone on this task. They have asked the help of a third-party firm, called Appen . The company has started recruiting Reddit users with specific accents. Their goal is to improve Googles voice recognition engine. The terms of use and training are simple. According to The Verge , the selected participants were directed to a mobile onboarding webpage. The webpage featured a record icon on that page. When they tapped it phrases appeared in sequence. The people who talked to The Verge about the experience had a mixture of accents. From the U.K. and America to Indian and Chinese-accented English. Some of the phrases were focused on Google, such as OK Google, and Hey, Google. In addition the participants had to read brand names, toys, video games, movie titles, and YouTube channel names. At the end of the recording the data are collected and processed by Appens in-house team.Mark Brayan, from Appen, shared some details about the project to The Verge. The teamanalyze recordings from around the world in 130 languages. Then they carefully distilling sentences down into their grammatical fundamentals. But processing all these data is not an easy job. According to Brayan even small improvements require massive quantities of data and analysis. To go from understanding 95 percent of words to 99 percent, the recognizer has to digest infrequently used words, of which there are millions, One of the big challenges is what we call named entity recognition, Brayan said. Thats brand names, product names, individual names, and so on. So if youre launching in Canada, for example, you need not only the French language but also French-accented Canadian English. ",http://socialbarrel.com/google-now-and-reddit-joined-forces-for-better-speech-recognition/106165/,"Google's famous voice assistant, Google Now, has a very interesting aspect. It can detect the different tones of the human speech. For that you ...",Google Now and Reddit joined forces for better speech recognition,Martha Papadimitriou Aug Googles Google Google Google Reddit Reddit Google Appen Reddit Googles Verge U.K. America English Google Google Hey Google YouTube Appens Brayan Appen Verge Brayan Brayan Thats Canada French-accented Canadian English,4
297,"By Joseph Conn |December 12, 2015 Rachel Moscicki is on the leading edge of a movement for nurses to use speech recognition and natural-language-processing technology to record their clinical documentation, saving time and optimizing use of scarce staff. Software that converts the human voice to digital text and extracts meaning from those words previously was reserved for doctors. Now some health systems are deploying the technology for nurses and ancillary providers. Moscicki, a cardiology nurse practitioner at the Hudson Valley Heart Center in Poughkeepsie, N.Y., and some of her colleagues switched in May to using speech recognition for real-time documentation in the hospital's electronic health-record system . The heart center is part of the three-hospital Health Quest system, based in LaGrangeville, N.Y.,  which initially rolled out speech recognition for its physicians to replace their transcribed dictation as part of a documentation-improvement initiative. The system is now making that same technology available to nurses and other hospital staff, using software from Nuance Communications, a Burlington, Mass.-based developer of speech recognition and natural-language-processing technology. Moscicki and her colleagues use the new system for all their progress notes, admissions, patient histories, physical exam results and discharge summaries. Progress notes were previously recorded on paper. Moscicki said the software can translate her speech to text faster and more accurately than she can typeand she can type fast. But, she said, When you're using a dictation system, you don't have typos. And when she's done, the hospital's EHR system, from Cerner Corp., is immediately updated. Improving nurse efficiency will be a necessity given growing healthcare demands from baby boomers combined with a predicted nursing shortage. The U.S. Bureau of Labor Statistics projects a demand for 1.1 million new nurses over the next seven years. Half that number will be needed to replace nurses who will retire by 2020; the other half will be required to fill an expected 575,000 new positions. The squeeze is made more acute by a scarcity of qualified nursing-school faculty. The introduction of speech-recognition technology for nurses is significant because of the amount of time they typically spend on documentation. A 2008 study published in the Permanente Journal found nurses spent more time during their workdays on documentation than on direct patient care. The study assessed more than 700 medical-surgical nurses at 36 hospitals. Nursing experts say EHRs have not helped much with that time crunch, at least partly because they weren't designed with nurses in mind. A study published in the journal CIN: Computers Informatics Nursing in 2012 found that nurses at 55 hospitals spent 19% of their time on documentation, whether they used paper records or an EHR. It's probably worse now, said Carol Bickford, a senior policy fellow at the American Nurses Association. The reporting requirements are terrible. But, she added, Nurses make it work. We find the workarounds. A paradigm shift in nursing documentation is needed to improve care delivery, said Joyce Sensmeier, vice president of informatics for the Healthcare Information and Management Systems Society. Speech recognition could help with that. We need the structured data so we can use computers to aggregate, perform data analytics and look for treatment patterns to improve patient care, Sensmeier said. But we also need some of that contextual information that's in free text. Designing systems to do both would advance us much more rapidly, she said. Joe Petro, senior vice president of healthcare research and development at Nuance Communications, said using speech-recognition technology to reduce the time nurses spend on documentation frees up more time for patient care. Even if we could improve that with a modest goal of 10 minutes a day, that's an additional 40 hours over the course of a year we could move toward patient care, he said. MH Takeaways The introduction of speech-recognition technology for nurses is significant because they typically spend as much or more time on documentation as they do on direct patient care. There are opportunities with voice recognition, acknowledged Ann Shepard, chief nursing informatics officer at Catholic Health Initiatives . The Englewood, Colo.-based system currently uses speech-recognition systems as a documentation aid for physicians and therapists. Shepard foresees CHI nurses completing some of their documentation with voice recognition instead of typing. One possible target for that shift is hospital nurses' end-of-shift reports, which cover patient situations, backgrounds, assessments and recommendations (SBAR). A typical SBAR report includes both data, such as blood pressure and other vital signs, and a text narration, including the nurse's patient-care plan. But Bickford questions the viability of speech-recognition software for documentation, particularly in busy, noisy hospital environments. How can you have a conversation about protected information in a hallway she asked. Epic Systems Corp. , a leading EHR developer, is readying a mobile application for nurses and therapists that will convert speech to text and insert it into the patient's EHR, said Stephanie Johnson, a clinical information specialist at the Verona, Wis.-based company. Epic aims to offer the app as part of its 2016 upgrade next fall. I think there is a market for it, she said. Just the speed to capture things in real time on the mobile device will be extremely helpful. Epic is evaluating Florence, Nuance's voice-driven digital assistant with artificial intelligence. That project is currently in the prototype stage with no set launch date. I could say blood pressure is 120 over 80 and that would go right into our nursing flow sheet, Johnson said, explaining a potential use. Last month at Health Quest, a dozen speech therapists, many with nursing backgrounds, received training in the Nuance speech-recognition system and became its latest users. Health Quest has bought an enterprisewide license to implement the technology across its whole system, enabling any staff member who wants to use speech recognition to do so. That's definitely the way to go, said Adem Arslani, a nurse-informatics consultant who's working with Health Quest on the project. You don't want to create the (technology) haves and have-nots. Don Barbarino, a Health Quest speech pathologist who was part of last month's training group, expressed satisfaction with his experience. We started the first day and have been using it ever since, he said. It's cut my documentation at least in half. That time savings has enabled him to see one or two more patients a day. As a side benefit, he said he experiences less eye strain from staring at his computer screen while entering data. Joseph Conn reports on information technology, privacy and data security.  He has been a reporter and editor for 35 years for various news publications and taught journalism at Valparaiso (Ind.) University, where he earned his bachelor's degree in English. He also worked as a Peace Corps volunteer in Sierra Leone. Conn joined Modern Physician in 2000, serving as reporter, editor and online editor. He joined Modern Healthcare in 2005. ",http://www.modernhealthcare.com/article/20151212/MAGAZINE/312129980,Rachel Moscicki is on the leading edge of a movement for nurses to use speech recognition and natural-language-processing technology to ...,Nurses turn to speech-recognition software to speed documentation,Joseph Conn |December Rachel Moscicki Moscicki Hudson Valley Heart Center Poughkeepsie N.Y. May Health Quest LaGrangeville N.Y. Nuance Communications Burlington Moscicki Moscicki EHR Cerner Corp. U.S. Bureau Labor Statistics Permanente Journal EHR Carol Bickford American Nurses Association Nurses Joyce Sensmeier Healthcare Information Management Systems Society Sensmeier Joe Petro Nuance Communications MH Ann Shepard Catholic Health Initiatives Englewood Shepard CHI SBAR SBAR Bickford Epic Systems Corp. EHR EHR Stephanie Johnson Verona Epic Just Epic Florence Nuance Johnson Health Quest Nuance Health Quest Adem Arslani Health Quest Don Barbarino Health Quest Joseph Conn Valparaiso Ind University English Peace Corps Sierra Leone Conn Modern Physician Modern Healthcare,1
298,"The Amazon Echo is the epitome of an Internet of Things ( IoT ) device. It combines an embedded applications processor from Texas Instruments, MEMS microphones from Knowles, Wi-Fi and Bluetooth wireless connectivity, an AWS cloud backend, and support for diverse applications. Its also multi-function, which increases the platforms value for consumers (bundled services), as well as Amazon (multi-dimensional insights into customer behavior and trends). The glue that ties all of this together is, of course, software . The Echo s signature feature, automatic speech recognition (ASR), is enabled by software algorithms that not only provide the language modeling and natural language understanding capabilities that make the platform unique, but also help offset the rigors of reverberant speech. Reverberant speech is a phenomenon that occurs in indoor environments when an audible signal reflects or bounces off of various surfaces, creating noise in the form of echoes that diminish the direct path signal from speaker to microphone. As you can imagine, this wreaks havoc on speech recognition, but consider the real-world use case of the Amazon Echo wherein reverberant speech is often the only signal available from a speaker communicating with the device. Jeff Adams, CEO of Cobalt Speech & Language, Inc. and former Senior Manager of the speech and language groups at Amazon, worked on the Echo. He attributes the platforms success in situations where his wife yells, Alexa, what time is it and hears the answer even though shes three rooms away, down the hall, and around the corner to cloud-based deep neural networks (DNNs) capable of performing roughly 1 billion arithmetic operations per second in support of ASR algorithms, beamforming, and noise cancellation techniques. But, while Adams suggests that kind of computing power became possible after cycles of Moores law and could at some point be available on processors beyond the data center , those performance requirements dont leave much hope for accurate ASR today in embedded devices not backed by the power of the cloud . Even though acoustic and language processing models such as those used for the Echo can be compressed , the reality is that compression comes with tradeoffs. The more ASR models are compressed the less accurate they become, and typically the size of language libraries shrinks dramatically from the linguistic openness of platforms like the Echo to perhaps a few hundred or a few thousand words. Furthermore, even after compression youre probably still talking about hundreds of MB for such models, which is a huge burden on even high-end smartphones. However, innovations in sensor technology are emerging that could help remove some of the overhead associated with massive DNNs, namely the use of multiple, heterogeneous inputs. For instance, Cobalt is partnering with human-to-machine communications (HMC) company Vocal Zoom , a manufacturer of optical sensors that pair with acoustic microphones to eliminate background noise and improve directional acquisition for speaker isolation. The optical sensor technology works by converting vibrations from a speakers cheek, larynx, and other facial areas into an audio signal, though one devoid of background noise due to the low frequencies at which skin vibrates. This information is then fused with inputs from traditional acoustic microphones to generate noise-free audio signals that can be leveraged in the absence of cloud-based DNNs to reduce the effects of reverberant speech, and even enable applications such as access control and voice authentication. For example, such an implementation could prevent systems like the Echo from waking up when a TV commercial mentions Alexa (more on optical sensors can be found in  Delivering more natural, personalized, and secure voice control for todays connected world ). Additionally, Adams says that other sensors are starting to be considered in the ASR equation, particularly as his company works towards speech classification engines designed to infer background information about a speaker, such as age, gender, physical and emotional state, and even possibly to aid in early diagnosis of medical conditions like Parkinsons and Alzheimers. Cameras and inputs from medical devices would be obvious complements in these types of applications, which could lead to the next level of sensor data fusion for the Internet of Things. ",http://embedded-computing.com/28691-deconstructing-alexa-software-and-sensors-of-the-amazon-echo-and-beyond/,"The Echo's signature feature, automatic speech recognition (ASR), is enabled by software algorithms that not only provide the language ...",Deconstructing Alexa – Software and sensors of the Amazon Echo ...,Amazon Echo Internet Things IoT Texas Instruments MEMS Knowles Wi-Fi Bluetooth wireless AWS Amazon Echo ASR Amazon Echo Jeff Adams CEO Cobalt Speech Language Inc. Senior Manager Amazon Echo Alexa DNNs ASR Adams Moores ASR Echo Echo MB DNNs Cobalt HMC Vocal Zoom DNNs Echo Alexa Adams ASR Parkinsons Alzheimers Cameras Internet Things,4
299,"Human Interact revealed Starship Commander, a virtual reality choose-your-own-adventure science fiction game that places you at the helm of an interstellar starship. But unlike any other game youve played, Starship Commander accepts only voice commands. Virtual reality opens new doors to creative ideas. In the early days of this new medium, there are no norms and no rules. Guidelines for what makes a compelling virtual reality experience dont yet exist; for now, imagination and technology are the only limitations. To that end, Alexander Mejia, Owner and Creative Director at Human Interact, sought to build something truly groundbreaking for his first VR project, Starship Commander. Starship Commander is a first person VR narrative story set in deep space. You play as the commander of an XR71 space ship sent on a classified cargo transport mission to the Delta system. But more than the story, it's the input method that will raise your eyebrows. Starship Commander doesn't accept physical input commands. You must initiate all action by speaking to the computer. After all, you never see the captain of the Starship Enterprise manning the controls; commanders command. Speech recognition is something you don't see often in games, and when you do, the implementation usually isn't all that great. Human Interact said that in its quest to bring voice commands to Starship Commander, it tried several off-the-shelf voice recognition technologies with little success. The team needed to be able to insert a custom dictionary to account for the made-up words in the games storyline, such as the names of alien races. Human Interact was also looking for a solution that could interpret natural speech so that players wouldnt be limited to specific scripted phrases. Human Interact turned to Microsofts Cognitive Services and used the Custom Speech Service to insert the custom dialect from the games storyline into the AIs dictionary. During Microsoft Build 2016 , Microsoft introduced 22 Cognitive Services APIs, which allow developers to integrate technologies derived from Cortana into their applications. The company demonstrated how its technology could be used to interpret the speech of a young child or to automatically identify objects in a photo and create captions to describe them. It was only a matter of time before someone found a reason to use this technology in a game. Mejia noted that the Custom Speech Service understands how people talk and automatically generates additional recognized phrases after it receives a handful of options. He also said that Custom Speech Service cut the word recognition errors in half compared to other speech recognition services that he and his team tried. We were able to train the Custom Speech Service on keywords and phrases in our game, which greatly contributed to speech recognition accuracy, said Adam Nydahl, Principal Artist at Human Interact. The worst thing that can happen in the game is when a character responds with a line that has nothing to do with what the player just said. Thats the moment when the magic breaks down."" Human Interact said that Microsofts speech recognition lets you feel like you are part of the story. Instead of following a set script, you get to add your own personality to the dialog of the game. Virtual reality sells the promise of immersion, and how better to feel immersed in an experience than to feel like youre having a real dialog with characters in the game Starship Commander is coming to Oculus Rift on the Oculus platform and HTC Vive on the SteamVR platform. Human Interact has not yet announced a release date for the game. ","http://www.tomshardware.com/news/starship-command-leverages-cognitive-services,33606.html","Speech recognition is something you don't see often in games, and when you do, the implementation usually isn't all that great. Human Interact ...",'Starship Commander' Leverages Microsoft Cognitive Services For ...,Human Interact Starship Commander youve Starship Commander Alexander Mejia Owner Creative Director Human Interact VR Starship Commander Starship Commander VR Delta Commander Starship Enterprise Human Interact Starship Commander Human Interact Human Interact Microsofts Cognitive Services Custom Speech Service AIs Microsoft Build Microsoft Cognitive Services APIs Cortana Mejia Custom Speech Service Custom Speech Service Custom Speech Service Adam Nydahl Principal Artist Human Interact Thats Human Interact Microsofts Starship Commander Oculus Rift Oculus HTC Vive SteamVR Human Interact,2
300,"Google announced its plans to enter the competitive voice recognition market by opening up its speech recognition API to third-party developers, according to a report by TechCrunch . In an effort to draw in more developers, Googles speech recognition app will initially be offered for free at launch and will subsequently be available for a fee in the near future. Google has yet to unveil details of its pricing, but the company will likely provide low-cost pricing tiers that could eventually increase over time after it becomes an established player in the voice recognition market. The announcement follows ongoing rumors of Googles service that have been circulating for the past few weeks. Google officially announced details about the Google Cloud Speech API during its NEXT cloud user conference , along with several other machine learning developments and updates. The service will be offered in over 80 languages, will be compatible with any application in real-time streaming or batch mode, and will include a complete set of APIs for applications to see, hear and translate, the company said. Additionally, the Google Cloud Speech API is based on the same neural network technology behind Googles voice search in the Google app and voice typing feature in Googles Keyboard. Other capabilities include the ability to use speech recognition in loud environments and in real-time. By opening up its speech recognition API to developers, Google will compete against long-time voice recognition players like Nuance, which is currently leading the market. Nuance could potentially lose several customers, such as startups, as they turn to Googles technology for its improved voice recognition experience and lower price point. In addition to disrupting the voice recognition market, Googles new speech recognition API is also seen as an attack against Apple, which has yet to offer an API for developers to use virtual assistant Siris voice recognition capabilities in their own apps. Google previously offered limited access to its voice technology in its products, such as the introduction of its Voice Interaction API at Google I/O last year where Android developers were able to add voice interactions to their apps. However, this marks the first time Google has offered access to the speech recognition API directly. ",https://www.biometricupdate.com/201603/google-to-make-speech-recognition-api-available-to-developers,Google announced its plans to enter the competitive voice recognition market by opening up its speech recognition API to third-party ...,Google to make speech recognition API available to developers,Google API TechCrunch Googles Google Googles Google Google Cloud Speech API NEXT APIs Google Cloud Speech API Googles Google Googles Keyboard API Google Nuance Googles Googles API Apple API Siris Google Voice Interaction API Google I/O Android Google API,4
301,"On account of its flexible, software based nature, voice recognitiontechnology is incredibly versatile in terms of potential applications. Identifying and authenticating a user via the unique qualities of her voice is contactless, quick, and easy to deploy in any scenario where a microphone is available. Here are four unique examples of voice biometrics in action that illustrate the spoken authentication modalitys various strengths: When it comes to a crime scene the cliche is to dust for fingerprints or swab for DNA samples, but thats not the only detritus criminals can leave behind. There are some forensics situations in which only audio evidence is available to investigators, and thats where voice biometrics can be deployed to great effect. Early last year, Morpho (Safran) partnered with voice biometrics specialist AGNITIO to bring Voice ID technology into its multimodal suite of criminal identification products . Requiring only seconds to perform matching on live or recorded voices, and boasting an accuracy rating of over 99 percent, Voice ID brings a new angle to forensic investigations. Whats most impressive about Voice ID, and what makes it incredibly well suited to the world of forensics, is its language agnostic nature. AGNITIOs technology measures the biological traits that are expressed through the sound of a subjects voice rather than the shape and pattern of her speech. This means that regardless of whether police are looking for a French fraudster or an English extortionist, Voice ID can do its job. As mentioned previously during Voice Biometrics Month 2016 , voice biometrics is often paired with facial recognition to bolster a multi-factorsystem. In combination with face biometrics, voice recognition can act as an additional authentication factoras well asa built-in liveness detection testthe logic being that while speaking for authentication, the face recognition software can see your mouth moving. Because this specific combination of modalities is software based, it is often seen built in to banking apps. One such banking app is Wells Fargos Commercial Electronic Office (CEO) app, which recently won a Monarch Innovation Award  thanks in large part to its biometric security. In addition to standalone eye-vein biometrics, the CEO app leverages SpeechPros multimodal VoiceKey.OnePass solution. VoiceKey.OnePass combines the two contactless factors of voice and face to great effect, enablingany mobile device with a front facing camera and microphoneto ditchpasswords in favor of more secure and convenient authentication. One of the most popular and mainstream applications of biometrics in general is mobile payments, and voice recognition has also made its way into this highly competitive arena. Biometrics company VoiceVault, in partnership with Bay Area startup SayPay Technologies, has brought voice authentication to mobile commerce. The companys ViGo mobile voice biometrics platform was integrated with SayPays eponymous technology last year, aiming to bring convenient security to card-not-present payment transactions. Instead of using PIN or password security to authenticate transactions, SayPay has users speak a one-time-password. This combines the OTP token security with the ease of use offered by a vocal interface. Going a step further,the randomized nature of the passwords thwarts presentation attacks by design. While a static voice code might be vulnerable to a fraudster with a tape recording, this spoken OTP technique is much more secure. If you have an iPhone with the latest iOS, you also have voice recognition software in the form of its famous AI assistant, Siri. While the primary use of Apples AI persona is based around voice commandusers saying something and having the software respond in kinda recent upgrade has moved Siri in to the realm of user identity and authentication. Late last year, Apple upgraded Siri to also support voice recognition . The aim was to make Siri capable of discriminating between your own voice and the speech of others. On new iPhones only the devices authorized user can activate Siri, asking her to perform tasks, answer questions, or tell jokes. Furthermore, the inclusion of voice recognition on Siri has also opened up the possibility of voice-based unlock for the iPhone, allowing users who prefer to go hands free a biometric security feature of their own. Stay posted to FindBiometrics throughout May as we continue to examine the featured topic of voice biometrics. Participate in the conversation by following us on Twitter . May 18, 2016  by Peter B. Counter ",http://findbiometrics.com/4-applications-voice-recognition-305180/,"On account of its flexible, software based nature, voice recognition ... of a subject's voice rather than the shape and pattern of her speech.",Voice Biometrics Month 2016: 4 Unique Applications of Voice ...,Identifying DNA Morpho Safran AGNITIO Voice ID Voice ID Voice ID AGNITIOs Voice ID Voice Biometrics Month Wells Fargos Commercial Electronic Office CEO Monarch Innovation Award CEO SpeechPros VoiceKey.OnePass VoiceKey.OnePass VoiceVault Bay Area SayPay Technologies ViGo SayPays PIN SayPay OTP OTP AI Siri Apples AI Siri Late Apple Siri Siri Siri Siri Stay May Twitter May Peter B,7
302,"Adobe unveiled a new audio editing tool at MAX 2016 called Project VoCo , which lets users rewrite spoken content by subbing in or deleting words. The technology lacks the artificial cadence of a virtual assistant that spaces out words somewhat unnaturally, or the problems that early text-to-speech systems had where they could not pronounce certain phonetic combinations, like the oi in soy, and instead responded with strange noises . For adding in words, it does not matter if the speaker has actually said those words. If told to insert them into the sentence, it will, in the speakers (mimicked) voice. The software is designed to imitate speech after being fed 20 minutes worth of dialogue read in the speakers voice. Using this audio sample, it breaks down everything the speaker says into phonemes , and in splicing together cues from the input audio, it can predict what words it hasnt heard from the speaker would sound like, playing them back in the speakers voice. Phonemes  sounds like JH (in judge) and ER (bird)  have been used for years in more advanced systems to produce natural-sounding results, since these interact with one another to produce words as we hear them. Adobe has not divulged much more information about the project yet, such as when a commercial release will take place. Though it was only shown in English, the system could be adapted to other languages. Windows speech recognition software currently processes English, German, Spanish, French, Japanese, and Chinese, for instance, using the phonemes unique to each language. These processes are very intensive, as Googles Fernando Pereira recently told Backchannel : When you try to build a system for understanding natural language, and you dont have many examples of the kind of understanding you want  then you have to prescribe, you have to write  essentially teach it grammar  so that it can do the understanding. That teaching is very laborious. Earlier systems trained to pick out words could not make sense of slang, non-standard proper nouns, or unique sentence structures very well. In order to overcome these challenges, notes The Tartan , todays voice recognition software employs sophisticated statistical modeling algorithms to predict the most likely and most sensible outcome for the input. Speech recognition is becoming increasingly accurate . The future of this technology will also incorporate lip reading for more accurate results in a video, as opposed to purely audio, medium and will be usable for translating languages. Also read: Found in translation: how couples are using Google Translate to meet, marry and communicate That VoCo can handle continuous speech separates it from, say, customer service hotlines where the software learns only how to recognize a limited set of commands and something like 100 words, such as speak to an operator or letting someone read off their credit card number. The processing power and machine learning techniques available today let the software make more accurate guesses about how the words will sound. VoCo is also speaker dependent because it needs to receive sufficient unique input before it actually recognizes their speech patterns. But it makes the results sound very natural. Though the demonstration sentences had some stilted parts, to the naked ear some of the more common words sounded just like natural conversation. Adobe said it will introduce digital watermarks to reduce the risk of forgeries. Since so little is known, there are a lot of uses and troubling questions about VoCo. Could it be used to impersonate someone Almost definitely. For instance, it could be used to fake a celebrity or politician speaking at a private event, or to fool voice recognition IoT devices in the home. Less potentially harmful uses could include making fixes in recording voiceovers, dialog, and narration as well as podcasting, Abode said in a statement . Indeed, speech recognition and editing could become a whole new aspect of law enforcement, from using it in identifying suspects to the need for a new range of tools and practices to account for potentially falsified evidence. In theory, though, incriminating leaks could be spun for whole cloth and media feeding frenzies launched on the strength of a made-up conversation. This would be a serious issue, and requires some kind of detection system that flags when the speech has been manipulated by a program, just as today there are tools and methods to detect photoshopping . ",http://www.geektime.com/2016/11/07/abobes-project-voco-lets-you-audio-shop-speech/,"Windows speech recognition software currently processes English, German, Spanish, French, Japanese, and Chinese, for instance, using the ...",Abobe's Project VoCo lets you audio-shop speech,Adobe MAX Project VoCo Phonemes JH ER Adobe English Windows English German Spanish French Japanese Chinese Googles Fernando Pereira Backchannel Tartan Google Translate VoCo VoCo Adobe VoCo Almost IoT Less Abode,4
303,"Powerful speech technology from Chinas leading Internet company makes it much easier to use a smartphone. Stroll through Sanlitun, a bustling neighborhood in Beijing filled with tourists, karaoke bars, and luxury shops, and youll see plenty of people using the latest smartphones from Apple, Samsung, or Xiaomi. Look closely, however, and you might notice some of them ignoring the touch screens on these devices in favor of something much more efficient and intuitive: their voice. A growing number of Chinas 691 million smartphone users now regularly dispense with swipes, taps, and tiny keyboards when looking things up on the countrys most popular search engine, Baidu. China is an ideal place for voice interfaces to take off, because Chinese characters were hardly designed with tiny touch screens in mind. But people everywhere should benefit as Baidu advances speech technology and makes voice interfaces more practical and useful. That could make it easier for anyone to communicate with the machines around us. I see speech approaching a point where it could become so reliable that you can just use it and not even think about it, says Andrew Ng, Baidus chief scientist and an associate professor at Stanford University. The best technology is often invisible, and as speech recognition becomes more reliable, I hope it will disappear into the background. The systems offer a glimpse of a future in which theres less need to learn a new interface for every device. Voice interfaces have been a dream of technologists (not to mention science fiction writers) for many decades. But in recent years, thanks to some impressive advances in machine learning, voice control has become a lot more practical. No longer limited to just a small set of predetermined commands, it now works even in a noisy environment like the streets of Beijing or when youre speaking across a room. Voice-operated virtual assistants such as Apples Siri, Microsofts Cortana, and Google Now come bundled with most smartphones, and newer devices, like Amazons Alexa, offer a simple way to look up information, cue up songs, and build shopping lists with your voice. These systems are hardly perfect, sometimes mishearing and misinterpreting commands in comedic fashion, but they are improving steadily, and they offer a glimpse of a graceful future in which theres less need to learn a new interface for every new device. Baidu is making particularly impressive progress, especially with the accuracy of its voice recognition, and it has the scale to advance conversational interfaces even further. The companyfounded in 2000 as Chinas answer to Google, which is currently blocked theredominates the countrys domestic search market, with 70 percent of all queries. And it has evolved into a purveyor of many services, from music and movie streaming to banking and insurance. Breakthrough Combining voice recognition and natural language understanding to create effective speech interfaces for the worlds largest Internet market. Why It Matters It can be time-consuming and frustrating to interact with computers by typing. A more efficient mobile interface would come as a big help in China. Smartphones are far more common than desktops or laptops, and yet browsing the Web, sending messages, and doing other tasks can be painfully slow and frustrating. There are thousands of Chinese characters, and although a system called Pinyin allows them to be generated phonetically from Latin ones, many people (especially those over 50) do not know the system. Its also common in China to use messaging apps such as WeChat to do all sorts of tasks, such as paying restaurant tabs. And yet in many of Chinas poorer regions, where there is perhaps more opportunity for the Internet to have big social and economic effects, literacy levels are still low. It is a challenge and an opportunity, says Ng, who was named one of MIT Technology Reviews Innovators Under 35 in 2008 for his work in AI and robotics at Stanford. Rather than having to train people used to desktop computers to new behaviors appropriate for cell phones, many of them can learn the best ways to use a mobile device from the start. Ng believes that voice may soon be reliable enough to be used for interacting with all sorts of devices. Robots or home appliances, for example, could be easier to deal with if you could simply talk to them. The company has research teams at its headquarters in Beijing and at a facility in Silicon Valley that are dedicated to advancing the accuracy of speech recognition and working to make computers better at parsing the meaning of sentences. Jim Glass , a senior research scientist at MIT who has been working on voice technology for the past few decades, agrees that the timing may finally be right for voice control. Speech has reached a tipping point in our society, he says. In my experience, when people can talk to a device rather than via a remote control, they want to do that. Researchers at Baidus headquarters in Beijing are plugging away at a digital assistant that can hold a conversation. Last November, Baidu reached an important landmark with its voice technology, announcing that its Silicon Valley lab had developed a powerful new speech recognition engine called Deep Speech 2. It consists of a very large, or deep, neural network that learns to associate sounds with words and phrases as it is fed millions of examples of transcribed speech. Deep Speech 2 can recognize spoken words with stunning accuracy. In fact, the researchers found that it can sometimes transcribe snippets of Mandarin speech more accurately than a person. Few of those behind Deep Speech 2 speak Mandarin or Cantonese. Its a universal language engine. Baidus progress is all the more impressive because Mandarin is phonetically complex and uses tones that transform the meaning of a word. Deep Speech 2 is also striking because few of the researchers in the California lab where the technology was developed speak Mandarin, Cantonese, or any other variant of Chinese. The engine essentially works as a universal speech system, learning English just as well when fed enough examples. Most of the voice commands that Baidus search engine hears today are simple queriesconcerning tomorrows weather or pollution levels, for example. For these, the system is usually impressively accurate. Increasingly, however, users are asking more complicated questions. To take them on, last year the company launched its own voice assistant, called DuEr, as part of its main mobile app. DuEr can help users find movie show times or book a table at a restaurant. The big challenge for Baidu will be teaching its AI systems to understand and respond intelligently to more complicated spoken phrases. Eventually, Baidu would like for DuEr to take part in a meaningful back-and-forth conversation, incorporating changing information into the discussion. To get there, a research group at Baidus Beijing offices is devoted to improving the system that interprets users queries. This involves using the kind of neural-network technology that Baidu has applied in voice recognition, but it also requires other tricks. And Baidu has hired a team to analyze the queries fed to DuEr and correct mistakes, thus gradually training the system to perform better. In the future, I would love for us to be able to talk to all of our devices and have them understand us, Ng says. I hope to someday have grandchildren who are mystified at how, back in 2016, if you were to say Hi to your microwave oven, it would rudely sit there and ignore you. Weren't able to make it to EmTech Digital ",https://www.technologyreview.com/s/600766/10-breakthrough-technologies-2016-conversational-interfaces/,"Powerful speech technology from China's leading Internet company makes it ... The best technology is often invisible, and as speech recognition becomes more ...",Conversational Interfaces,Chinas Internet Stroll Sanlitun Beijing Apple Samsung Xiaomi Look Chinas Baidu China Baidu Andrew Ng Baidus Stanford University Voice Beijing Apples Siri Microsofts Cortana Google Amazons Alexa Baidu Chinas Google Combining China Web Pinyin Latin China Chinas Internet Ng MIT Technology Reviews Innovators AI Stanford Ng Robots Beijing Silicon Valley Jim Glass MIT Speech Baidus Beijing November Baidu Silicon Valley Deep Speech Deep Speech Mandarin Deep Speech Mandarin Cantonese Baidus Mandarin Deep Speech California Mandarin Cantonese Chinese Baidus DuEr DuEr Baidu AI Baidu DuEr Baidus Beijing Baidu Baidu DuEr Ng Hi EmTech Digital,3
304,"The smoother speech recognition technology gets, the less typing well do on our smartphones. I'm not typing this article. I'm dictating it to my iPhone as I walk down the busy city street on the way to my office in the West Village. Will Oremus is Slates senior technology writer. Email him at will.oremus@slate.com or follow him on Twitter . Admittedly the iPhones speech-recognition features went [sic] meant for composing full-length articles for publication. Sorry, that should have been werent. Some transcription errors are inevitable, but Im doing this to make a point. Our mobile devices have gotten surprisingly good at understanding usprobably a lot better than you remember, if you havent tried talking to your phone in a while. Speech recognition technology got a lot of hype around the time that Apple first released Siri, four years ago this week. But if youre like most iPhone users, you soon just missed the haunted voice assistant as little more than a parlor trick. (Sorry that was supposed to be dismissed not just missed. And vaunted not haunted.) Series frequent misunderstandingswhoops, I mean Siri is a frequent misunderstandingsdarn it, I mean the frequent misunderstandings by Siri  gave it more comedic value than practical value. Believe it or not, despite the voice typos above, thats no longer the case. Not only is Siri a better listener than it used to be, but Apples notes and mail apps have sprouted serviceable dictation features, too. And as much as Apples speech- recognition capabilities have improved, the ones Google has added to its mobile apps and android operating system may be even better. In both cases, typing by voice is now easier in many cases than doing it by touchscreen, especially if youre on the go. And on the coming wave of wearable devices, like Google Glass, voice commands are replacing typing altogether. Meanwhile, Amazons big pitch for its new set-top TV box , Amazon fire TV, is that its voice recognition features actually work  even for Gary Buse . Thats its. And Gary Busey. Get Slate in your inbox. Clearly the technology is not yet perfect. How mens are still problematic, for one thing. I mean homonyms are still problematic, although Google in particular has gotten quite good at discerning your meaning from the context. And if you want punctuation marks, you have to speak them out loud. Like, you have to say the word. If you want to end a sentence. Sorry, I mean you have to say the word period if you want to end a sentence. Im going to go back to typing on my laptop now, both because I need my notes and because Im sure both you and my editor are tired of the typos. [Im totally fine! Ed.] And to be honest, I was starting to feel a little like Joaquin Phoenix in Her , murmuring sweet nothings to my mobile device as I moseyed down Hudson Street. Still, I wouldnt have dreamed of trying to compose even a brief work-related email on a smartphone by voice just a couple of years ago, let alone a full-length Slate column. Now I do the former on a regular basis. And for some basic tasks, like placing a call to someone in my address book or typing up a grocery list, I almost never use the keypad anymore unless Im forced to. Which reminds me of one other obstacle: Talking to your mobile device typically requires an Internet connection. Speech recognition softwares reliance on the cloud is both an inconvenience and the source of its power. Youll notice that when you dictate something, theres a brief lag before it shows up on the screen. Thats because your device is zipping your voice signals to remote servers for processing and interpretation before it can transcribe them. One reason Googles technology has improved so rapidly, explains engineering director Scott Huffman, is that all that incoming voice data gives the companys machine-learning algorithms a lot to work with. Another is that the algorithms themselves have gotten more powerful. One of the big advances over the last year or two, he says, has been in using new kinds of machine-learning technology that are scaled to many, many machines. We call it deep neural networks, or deep learning. Were now able to apply very large-scale parallel computing to interpret the sounds that you make. The softwares first job is to figure out which sounds are your words, as opposed to ambient noise or the words of people around you. For a nonhuman, thats harder than you might think. Then it has to parse your speech by evaluating not only each sound you make, but also the linguistic context that surrounds itjust as people do subconsciously when they listen to one another. Sometimes you can actually see the software recalibrating on the fly. Recently I told my Google app, Remind me to email Ben at 4 oclock. At first I saw it type, Remind me to email Bennett. But when it heard the words 4 oclock, it realized I had more likely said Ben at then Bennett, and it duly set the proper reminder. This is exactly the type of computing problem at which Google excels. The companys core product, Web search, relies on its ability to intuit the intent behind a string of search terms, even if theyre misspelled or ambiguously phrased. A search for bank will turn up different results based on your location and search history. Similar smarts could soon be applied to the companys speech recognition technology, Huffman said. When youre in Boston, for instance, Google might be more likely to render red socks as Red Sox, especially if it knows youre a baseball fan. Apple wont talk as much about its own speech recognition technology, but its clearly working hard to keep up. It built Siri with the help of a partnership with Nuance , the company behind the popular Dragon speech recognition software for PCs. More recently, it appears to have acquired another speech recognition company , called Novauris Technologies, which has worked on technology to process speech locally on your device rather than sending it to the cloud. That could help it keep pace with other rivals like Intel, which are hoping to leapfrog Google and Apple in speed by cutting the Internet out of the equation . The smoother the technology gets, the less typing well do on our smartphones. An informal poll of my colleagues turned up several who already use voice functions for a range of applications, from setting timers and alarms to settling a bet at a bar. When youre out with friends, pulling out a phone and typing a query into Google feels antisocial, one colleague observed. But asking Google a question out loud and getting a spoken response just feels like part of the conversation. And it isnt just young, tech-savvy types who are doing it. Several people I talked to said its actually their parents who are using their phones voice features the mostbecause theyre the ones who most hate typing. I had a Kasparov vs. Deep Bluestyle race with my dad, staff writer Forrest Wickman told me. He uses [dictation] all the time and was convinced it was faster than typing by hand. Wickmans father lostbut I bet that within a year or two, hell win.  ",http://www.slate.com/articles/technology/technology/2014/04/the_end_of_typing_speech_recognition_technology_is_getting_better_and_better.html,"Admittedly the iPhone's speech-recognition features went [sic] meant for composing full-length articles for publication. Sorry, that should have ...",I Didn't Type This Article,West Village Will Oremus Slates Email @ Twitter Sorry Im Apple Siri Sorry Series Siri Siri Siri Apples Apples Google Google Glass Amazons Amazon Gary Buse Thats Gary Busey Get Slate Google Sorry Im Im Im Ed Joaquin Phoenix Her Hudson Street Slate Im Which Internet Youll Googles Scott Huffman Google Remind Ben Remind Bennett Ben Bennett Google Web Huffman Boston Google Red Sox Apple Siri Nuance Dragon Novauris Technologies Intel Google Apple Internet Google Google Kasparov Bluestyle Forrest Wickman,4
305,"SANTA CLARA, Calif., March 17, 2016 /PRNewswire/ -- Sensory Inc. , a Silicon Valley based company focused on machine learning techniques to improve the user experience and security of consumer electronics through state-of-the-art embedded voice and vision technologies, today announced the broad availability of its industry-leading TrulyHandsfree technology as deeply embedded ports on many of Intel's latest SoC platforms targeting the PC, tablet and smartphone industries. Sensory's TrulyHandsfree always-on, always-listening voice trigger component and low power sound detector (LPSD) ship from Intel as a deeply embedded port in the low-power DSPs of their Skylake, Anniedale, Broxton, Broadwell and Merrifield SoCs. ""Sensory put speech trigger technology on the map and powered the first devices to ever offer this kind of technology as a useable feature, sparking the mass adoption of voice trigger technologies in consumer electronics today,"" said Todd Mozer, CEO of Sensory. ""By working with Intel to offer deeply embedded ports of our TrulyHandsfree on their latest chips, we have made it simple for PC, tablet and smartphone manufacturers to enable voice wakeup to search or other functions from a low power always listening state."" Manufacturers who utilize any of the aforementioned Intel SoCs will have the ability to easily integrate the low-power, highly accurate speech trigger and trigger-to-search features of Sensory's TrulyHandsfree technology into their products. TrulyHandsfree enables products to wakeup and respond when a phrase like ""Alexa"", ""Hey Cortana"" or ""Okay Google"" is called, with no button pressing or manual manipulation needed for initiating voice control. The voice trigger component of TrulyHandsfree offers high accuracy in near or far field situations with great inherent noise robustness and can be configured by the OEM to offer preset fixed triggers, user-enrolled fixed triggers or user-defined trigger phrases. With trigger-to-search, users can say the trigger phrase and immediately follow it with a verbal request which is handed off to the applications processor. This audio handoff to the applications processor can seamlessly lead to a voice search in the cloud, or command and control on the application processor using Sensory's recognition solutions targeting O.S. platforms. Sensory offers advanced features that utilize the applications processors of these same chips through specialized software SDKs for most of today's most prevalent operating systems. Sensory's TrulyHandsfree software SDKs allow OEMs to integrate Sensory's industry-leading speech recognition engine at the OS level for state-of-the-art trigger-to-command capabilities and provides a complete voice user interface solution for advanced product, OS and app features. ""For OEM customers looking to provide a comprehensive always-listening hands-free voice interface on their products, we offer TrulyHandsfree command and control SDKs for Android, iOS, Linux and Microsoft operating systems. By combining the pre-ported DSP features of TrulyHandsfree including LPSD and voice trigger functionality with our more advanced phrase spotting and command and control offerings, we can ensure the best possible performance of our noise robust voice interface across the system,"" added Bill Teasley, VP of engineering at Sensory. Features of the TrulyHandsfree implementation on Intel's SoCs include: Proprietary low power sound detector technology; LPSD is a solution optimized for speech recognition technologies designed to wake the speech recognition engine for a voice trigger when potential speech is sensed. The solution conserves power by allowing the voice recognition technology to be switched off until needed for speech processing. Speech recognition support for more than 20 languages, serving more than 40 countries Sensory has ported TrulyHandsfree to more platforms than any other speech recognition company, with ultra-low-power deeply embedded ports available for leading DSP/MCU IP cores from ARM, Cadence, CEVA, NXP CoolFlux, Synopsys and Verisilicon, as well as for integrated circuits from Audience, Avnera, Cirrus Logic, Conexant, DSPG, Fortemedia, Intel, Invensense, Microsemi, NXP, Qualcomm, QuickLogic, Realtek, STMicroelectronics, TI and Yamaha. TrulyHandsfree supports US English, UK English, French, German, Italian, Japanese, Korean, Mandarin Chinese, Portuguese, Russian, and Spanish. For more information about this announcement, Sensory or its technologies, please contact sales@sensory.com ; Press inquiries: press@sensory.com  Sensory Inc. creates a safer and superior UX through vision and voice technologies. Sensory's technologies are widely deployed in consumer electronics applications including mobile phones, automotive, wearables, toys, IoT and various home electronics. Sensory's product line includes TrulyHandsfree voice control, TrulySecure biometric authentication, and TrulyNatural large vocabulary natural language embedded speech recognition. Sensory's technologies have shipped in over a billion units of leading consumer products. Visit Sensory at www.sensory.com TrulyHandsfree is a trademark of Sensory Inc. ",http://www.prnewswire.com/news-releases/sensorys-trulyhandsfree-speech-recognition-now-available-on-intel-soc-platforms-300237604.html,"TrulyHandsfree is the most accurate and lowest power speech recognition solution available on Intel low power DSPs in Skylake, Anniedale, ...",Sensory's TrulyHandsfree Speech Recognition Now Available on ...,SANTA CLARA Calif. March Sensory Inc. Silicon Valley TrulyHandsfree Intel Sensory TrulyHandsfree LPSD Intel DSPs Skylake Anniedale Broxton Broadwell Merrifield SoCs Sensory Todd Mozer CEO Sensory Intel TrulyHandsfree Manufacturers Intel SoCs Sensory TrulyHandsfree TrulyHandsfree Alexa Hey Cortana Google TrulyHandsfree OEM Sensory O.S SDKs Sensory TrulyHandsfree SDKs OEMs Sensory OS OS OEM SDKs Android Linux Microsoft DSP TrulyHandsfree LPSD Bill Teasley VP Sensory TrulyHandsfree Intel SoCs LPSD Sensory TrulyHandsfree DSP/MCU IP ARM Cadence CEVA NXP CoolFlux Synopsys Verisilicon Audience Avnera Cirrus Logic Conexant DSPG Fortemedia Intel Invensense Microsemi NXP Qualcomm QuickLogic Realtek STMicroelectronics TI Yamaha TrulyHandsfree US English UK English French German Italian Japanese Korean Mandarin Chinese Portuguese Russian Spanish Sensory Press sensory.com Sensory Inc. UX Sensory IoT Sensory TrulyHandsfree TrulySecure TrulyNatural Sensory Visit Sensory www.sensory.com TrulyHandsfree Sensory Inc,3
306,"How speech recognition can help you work anytime, anywhere How speech recognition can help you work anytime, anywhere Nuance Communications has launched a fully-featured, business-grade speech recognition app thats been designed from the ground up for iOS and Android devices When youre building a small business, theres never enough time to do everything you want to do. Your inbox fills, the correspondence keeps on building up and its a nightmare dealing with all the paperwork and forms. Yet we all have dead time when were travelling, or just getting away from the office, when we could be doing something to move things forwards. And why is it that the best ideas always come when youre least equipped to get them down Wouldnt it be great to turn them into something you could develop, even share A solution to these problems might be sitting in your briefcase, bag or pocket. Enter your smartphone or tablet and Dragon Anywhere . The latest product from the speech recognition experts at Nuance Communications is a fully-featured, business-grade speech recognition application thats been designed from the ground up for iOS and Android devices. Like the Dragon desktop products it offers fast, highly-accurate speech recognition that continually adapts to your voice, but while the Desktop programs use your laptop or PCs onboard power, Dragon Anywhere harnesses the power of the cloud. With Nuances servers handling all the hard work, the software can work hard and fast on relatively modest phones, while the software is continually improving without constant updates to the app. How could your business use it Well, just take a few examples. Dragon Anywhere makes drafting documents easy. Not only is the software accurate out of the box , but its always learning about how you speak and what you want to say, tailoring itself for your own, specific voice. You can add custom words to cover industry-specific terms or acronyms and even set-up auto text  frequently used text passages like a signature, a list of bullet-points or a paragraph of legal rubric  so that you can enter these with just a quick spoken command. Tablets and smartphones are light and convenient, but theyre not always the easiest devices to work with. Virtual keyboards can be a pain to use, selecting text for formatting or editing is a headache, while intrusive auto-correct features seem to only get in the way. Dragon Anywhere makes all of these problems disappear. Theres no need to type when you can dictate at normal talking speeds, while you can select words or sentences for editing or deletion with simple voice commands. You have full control of formatting options and can easily apply underlining, italics, colours or bold text. You can work faster and leave that Bluetooth keyboard in the bag. In many industries its simply not always practical to type. You could be out in the field, holding your phone or tablet, but you still need to fill in computerized-forms or make and update reports. Again, Dragon Anywhere has you covered. You can navigate through fields on a report template with intuitive next input field and previous input field commands. You can fill in fields and enter notes using speech alone, and even use the auto-text features to cut out unnecessary repetition. Its a whole lot easier than jabbing the screen with a finger. With Dragon Anywhere you can get a basic idea or barebones draft done whenever and wherever you need to, then work on it later when you get the chance. This all comes down to the apps cloud-based nature. Its designed to work seamlessly with services like Dropbox or Evernote, so that you can save a file to Dropbox or export it as a note to Evernote, where it can proliferate to Evernote online or apps on your other devices. Whats more, Dragon Anywhere is designed to work hand-in-hand with Nuances desktop speech recognition products, including Dragon Pro Individual and Dragon for Mac. Documents and notes you dictate in one will be instantly synchronized with the other, while key customization features like auto-texts and custom words and phrases sync automatically between the two, enabling hard-working professionals to dictate either on their mobile device or on their Mac, PC or laptop, with the same workflow and the same degrees of accuracy and personalisation. And because Dragon Anywhere is a subscription service, it can be used from several mobile devices through just the one subscription. This level of power and functionality doesnt come for free, but a subscription is cheaper than you might think, and there are free trials available to gauge how well Dragon Anywhere can work for you. Leave your laptop in the office and ditch the unwieldy keyboard cover. Let speech recognition do the hard work for you. Dragon Anywhere is available from 14.99 per month. Download the free trial for Android or iOS now. This article is brought to you in association with Nuance. ",http://www.pcadvisor.co.uk/feature/enterprise/how-speech-recognition-can-help-you-work-anytime-anywhere-3635366/,"The latest product from the speech recognition experts at Nuance Communications is a fully-featured, business-grade speech recognition ...","How speech recognition can help you work anytime, anywhere",Nuance Communications Android Dragon Anywhere Nuance Communications Android Dragon Desktop Dragon Anywhere Nuances Well Dragon Anywhere Dragon Anywhere Bluetooth Again Dragon Anywhere Dragon Anywhere Dropbox Evernote Dropbox Evernote Evernote Whats Dragon Anywhere Nuances Dragon Pro Individual Dragon Mac Mac Dragon Anywhere Dragon Anywhere Anywhere Android Nuance,1
307,"Microsoft has been developing several new features for its upcoming Windows 10 Anniversary Update release including a new app that combines specific Windows 10 UWP development with ink, speech and face recognition, according to a report by WinBeta . Many of these features were exhibited at Microsofts developer conference a few months ago. Several keynote speakers demonstrated at Build 2016 how the Windows team has improved the bulk of Windows 10 code and features, especially its varied input recognition. In one demo, members of the Windows team exhibited new and improved inking, Windows 10 Hello authentication and voice initiated Cortana commands. FamilyNotes is a notice board app designed to demonstrate modern features in a real world scenario, with support for ink, speech and some rather impressive behind-the-scene smarts using Microsoft Cognitive Services, according to the Windows Apps team. The first post details the apps features and development background, how it uses face detection and recognition, and the apps use of Ink and speech. Universal Windows Platform (UWP) sample app showing the usage of speech, Cortana, ink, and camera through a family note sharing scenario, can now be downloaded at GitHub . ",https://www.biometricupdate.com/201606/new-windows-uwp-app-combines-inking-face-and-speech-recognition,"New Windows UWP app combines inking, face and speech recognition ... ink, speech and face recognition, according to a report by WinBeta.","New Windows UWP app combines inking, face and speech ...",Microsoft Windows Anniversary Update Windows UWP WinBeta Many Microsofts Build Windows Windows Windows Hello Cortana Microsoft Cognitive Services Windows Apps Ink Universal Windows Platform UWP Cortana GitHub,2
308,"Google's Cloud Speech API will allow developers to convert audio to text within their own apps. The offering from Google will bring its neural network smarts to apps large and small, and opens up a wide range of interesting new possibilities. It also brings the fight to Nuance Communications' front door. Google is providing access to the limited preview of the Cloud Speech API through its developer website. Developers can take advantage of the API for free, for now, though presumably Google will start charging for access at some point. The API includes a number of key functions. The automatic speech recognition is powered by learning, networks computers. Google claims it has unparalleled accuracy, and the learning computers become more accurate over time and more people use the API. At launch, the Speech API recognizes 80 languages with some regional variants. Google didn't say how big its vocabulary is other than to call it ""extensive."" Nuance's mobile SDKs , by way of comparison, only cover about 40 languages. The API can capture audio from a microphone or in pre-recorded audio files, such as PCMU, FLAC, and AMR. Voice recordings are sent to Google's servers where they are transcribed into text, which is then streamed back to the app in real time. Google didn't say if or how the API handles voice recognition in an offline environment. The API can recognize spoken language even in noisy environments without hardware or software noise cancellation. Google says developers can set parameters to filter out inappropriate content if so desired. Developers can upload and store audio files. A future release of the API will allow developers to integrate those files with Google Cloud Storage.  The Cloud Speech API accesses the exact same toolset that Google uses for its own speech-recognition and voice-command tools in Google Search, Google Now, and the Google Keyboard. Anyone who's used Google voice search knows how quick and accurate its performance is. Developers can take advantage of this API to not only capture spoken words as text, but add support for voice-based commands. Given how Google's description of the API's workflow says that the API can accept either real-time speech (to which it will respond with a text stream as the speech is recognized) or a complete audio file, it's possible that the API will be a streaming API instead of RESTful one. If that's the case, it's also possible that it will rely on Google's Pub/Sub streaming API technology . In terms of abstracting the API, Google has yet to indicate if it will also provide SDKs for Android, iOS, or other mobile or server-side platforms. Nuance has individual SDKs for Android, iOS, and Windows. (Google's API differs from the Alexa Voice Services API from Amazon. That API allows hardware makers to add the Alexa AI to their devices and nothing more.) Access to the Google Cloud Speech API is limited, but Google didn't say how limited. Developers can sign up to test the preview at no cost.  Eric Zeman I am a journalist who covers the mobile telecommunications industry. I freelance for ProgrammableWeb and other online properties. ",https://www.programmableweb.com/news/google-to-offer-speech-to-text-api/2016/03/24,"The automatic speech recognition is powered by learning, networks computers. Google claims it has unparalleled accuracy, and the learning ...",Google to Offer Speech-to-Text API,Google Cloud Speech API Google Nuance Communications Google Cloud Speech API API Google API Google API Speech API Google SDKs API PCMU FLAC AMR Voice Google Google API API Google API Google Cloud Storage Cloud Speech API Google Google Search Google Google Keyboard Anyone Google API Google API API API API RESTful Google Pub/Sub API API Google SDKs Android SDKs Android Windows Google API Alexa Voice Services API Amazon API Alexa AI Google Cloud Speech API Google Eric Zeman ProgrammableWeb,4
309,"Does Speech Recognition Aid Clinical Documentation Improvement A review of literature on speech recognition shows mixed results for clinical documentation improvement. -Despite the availability of speech recognition software and natural language processing over the past two decades, research shows limited evidence proving these technologies to have a clearly positive impact on clinical documentation improvement. That conclusion comes from a systematic review of literature on the risks and benefits of speech recognition for clinical documentation by Tobias Hodgson and Enrico Coiera published recently in the Journal of the American Medical Informatics Association. ""SR is a widely used input modality for modern computer devices and has a long pedigree in the clinical setting,"" Hodgson and Coiera write. ""Surprisingly, our review revealed that the evidence base documenting the benefits and limitations of SRs use for clinical documentation is limited, incomplete, and relatively neutral to its benefits. Recent studies, which would benefit from more modern SR technologies, are absent."" The team of researchers pared the number of studies down from 538 to 23, which comprise the focus of their quantitative and qualitative analysis, spanning from the efficiency and accuracy of speech recognition to errors introduced by speech recognition and the costbenefit of speech recognition. The authors did note evidence supporting system-level benefits for improving clinical documentation speed in ""dramatic reductions"" to turnaround time (TAT) for report creation, but they exercise caution in divining the meaning of these findings: This is mainly due to the virtually instant delivery of reports possible with SR based systems. This improvement hides an editing and document creation time cost that falls directly on the clinician. The effective clinical adoption of technologies often depends on local costs being offset by local benefits, and the relatively low uptake of SR to date might in part be due to an imbalance in cost over benefit for the clinician preparing reports. Improved accuracy was another win for speech recognition in the systematic literature review. Yet the margins for error in medicine make the less-than-100-percent accuracy of this technology still problematic, according to Hodgson and Coiera. ""In fact many SR software developers now claim accuracy rates of up to 99%,"" they state. ""However, high accuracy rates do not necessarily mean that SR is clinically safe, and several studies have reported a range of errors, some of which are clinically significant and could lead to patient harm."" Numerous errors are listed in the review: ",https://ehrintelligence.com/news/speech-recognition-supports-clinical-documentation-improvement,"April 25, 2016 - Despite the availability of speech recognition software and natural language processing over the past two decades, research ...",Does Speech Recognition Aid Clinical Documentation Improvement?,Does Aid Clinical Documentation Improvement A Tobias Hodgson Enrico Coiera Journal American Medical Informatics Association SR Hodgson Coiera SRs SR TAT SR SR Hodgson Coiera SR SR,-1
310,"When I struggle to write, I change somethingI stretch or walk, stand instead of sit, or vice versa. Sometimes I talk instead of type. I configure speech-to-text software to capture my voice, then just talk. I often end up with a bunch of text to edit. I find talking to be an excellent alternative to typing to capture not yet fully formed ideas. Fortunately, I write with Google Docs, so there are several tools I can use to turn my voice into text. The best voice recognition tool for Google Docs, Google Voice Typing (Figure A), used to be found only on Android devices. Install the Google Docs app, open a document, and tap the microphone icon located to the left of the space bar on the on-screen keyboard. Then talk. Google Voice Typing turns your speech into text. On Android, Google Voice Typing turns speech into text accurately and quickly. Google also includes speech recognition in Chrome OS as an accessibility option (Figure B). To enable it, select the three-line menu, choose Settings, scroll to the bottom of the page, and select Show advanced settings. Look for the Accessibility option to enable the on-screen keyboard. When the keyboard displays, select the microphone displayed above the on-screen keyboard to activate speech recognition. On Chromebooks, enable the on-screen keyboard to access the built-in Chrome OS speech recognition system. Google Docs on the web: Add-on A third-party Add-on for Google Docs on the web also enables Speech Recognition (Figure C). Unlike the on-screen keyboard, the Add-on works only within a Google Doc; the Chrome OS on-screen keyboard works with all text fields. In Google Docs on the web, use the third-party Speech Recognition Add-on. To install the Speech Recognition Add-on, open a Google Doc, choose Add-ons, and then select Get add-ons. Next, search for Speech, then choose the + Free button to add it. Every time you want to start voice recognition, go to the Add-ons menu, choose Speech Recognition, and click Start. A sidebar will appear to the right of your document. Choose your language and dialect, select the blue Start button, and then start talking. Apple devices and Microsoft Windows systems also offer speech recognition options. Dictation on Apple mobile devices works much like Google Voice Typing on Android: tap the microphone to the left of the keyboard, then talk (Figure D). On Mac OS devices, turn on Dictation . Microsoft's speech recognition system requires a settings change, followed by a brief training session for the system to learn your voice in Windows 7 , 8/8.1 , and 10 . Apple also offers Dictation, which provides speech recognition on iOS that works with the Google Docs app. For accurate punctuation, say the symbol. These systems recognize common punctuation marks and terms. For example, take the following two sentences: ""In November 1660, 12 people met at Gresham College in London to talk about a common interest: science. The group met often to discuss experiments and share ideas, and it eventually evolved into what we now know as the Royal Society."" To properly capture and punctuate these sentences, you'd say: Note how you use the terms comma, colon, and period. Terms like ""new paragraph"" also work. I tested the control sentences above on my Toshiba Chromebook 2 with the built-in microphone and then with a Logitech G330 USB headset. I also tested the phrases with both the native Chrome OS speech recognition system and the third-party Google Docs Add-on. When I used the headset, accuracy improved (Figure E). With the headset, the Chrome OS speech recognition system captured the phrases with 100% accuracy. Google Voice Typing in a Google Doc on a Samsung Galaxy Note 4 for the same phrases also achieved 100% accuracy without a headset. Use a headset with a Chromebook to improve the accuracy of speech recognition. Why do you use speech-to-text tools While I use speech-to-text to capture ideas, a colleague of mine uses speech-to-text to reply to email and texts. He doesn't like typing on tiny touch screen keyboards, so he dictates his responses. He relies on the speech recognition software to get the message right. Most of the time, it does. Speech recognition also helps people who experience pain when typing to communicate. A person I know with carpal tunnel syndrome chooses to talk instead of type whenever possible. Have you tried Google's speech recognition systems on your Chromebook or Android device Whereand whydo you use speech-to-text tools Let us know in the discussion thread below. ",http://www.techrepublic.com/article/pro-tip-how-to-speech-to-text-in-google-docs/,"Google also includes speech recognition in Chrome OS as an accessibility option (Figure B). To enable it, select the three-line menu, choose ...",How to speech-to-text in Google Docs,Google Docs Google Docs Google Voice Typing Figure A Android Google Docs Google Voice Typing Android Google Voice Typing Google Chrome OS Figure B Settings Show Accessibility Chromebooks Chrome OS Google Docs Add-on Google Docs Figure C Add-on Google Doc Chrome OS Google Docs Add-on Add-on Google Doc Get Next Speech + Free Add-ons Start Start Apple Microsoft Windows Apple Google Voice Android Figure D Mac OS Dictation Microsoft Windows Apple Dictation Google Docs November Gresham College London Royal Society Toshiba Chromebook Logitech G330 USB Chrome OS Google Docs Add-on Figure E Chrome OS Google Voice Typing Google Doc Samsung Galaxy Note Chromebook Google Android Whereand,4
311,"Voice-activated devices such as the Amazon Echo are becoming ever popular, and you can make your own using a Raspberry Pi, an inexpensive USB microphone and some suitable software. You too can have your Raspberry Pi search YouTube, open web pages, launch applications and even respond to questions, simply by speaking. The Raspberry Pi has no built-in soundcard or audio jack, so you need a USB microphone or a webcam with built-in microphone for this project. We tested the software using a Microsoft HD-3000 webcam, but any compatible device will do. Visit there's a full list of Raspberry Pi-compatible webcams if you do not already have one, but be sure that whatever device you choose has an integrated microphone. If you only have a microphone with an audio jack, try searching Amazon or eBay for an inexpensive USB soundcard, which plugs into the USB port at one end and has an output for earphones and a microphone at the other. Theres a number of speech recognition programs for the Raspberry Pi. For this project, were using Steven Hicksons Pi AUI Suite, because its powerful as well as extremely easy to set up and configure. Once you follow the steps in the tutorial, you will be able to start the installer. The Pi AUI Suite gives you a choice of a number of programs to install. The first question you are asked is whether it should install the dependencies. These, quite simply, are the files the Raspberry Pi needs to download for voice commands to work, so select Y and press Return to agree to this. Next, you are asked if you want to install the PlayVideo program, which enables you to use voice commands to launch and play video files. If you choose Y, youre asked to specify the path to your media files  for example, /home/pi/Videos. Note that upper-case letters are important here. If the path is invalid, the program warns you. If you have a mic with an audio jack, you may be able to use a small USB soundcard to make it work with the Raspberry Pi Youre then asked if you want to install the Downloader program, which searches for and automatically downloads files from the internet for you. If you choose Y here, youre asked to provide settings for host, port, username and password. If you arent sure of these, press Return for now to choose the default options in each case. The following program is Google Text to Speech Service, which you may wish to install if you want the Raspberry Pi to read out the contents of text files. In order to use this service, the Raspberry Pi needs to be connected to the internet, because it connects to Googles servers to translate the text into speech, and then plays an audio file with the Raspberry Pis media player. If you decide to install this, you need a Google account. The installer asks you to enter your username. Do so, then press Return. Youre then prompted for your Google password. Enter this and press Return again. The installer also offers you the chance to install Google Voice Commands. This uses Googles own speech recognition service. Again, youre asked to provide your Google username and password to continue. Whether or not you choose the Google-specific software, the program also asks you whether you want to install the YouTube scripts. These tools enable you to speak a phrase such as YouTube fluffy kittens, which then causes a relevant video clip to be played. Simply type a new greeting and press Return. You can also set the quiet flag, so the Raspberry Pi doesnt respond verbally. Finally, the program gives you the option to install Voicecommand, which contains some of the more useful scripts, such as being able to launch your web browser by saying the word internet. The program asks you if you want to let Voicecommand set itself up automatically. If you experience an error at this stage, follow Step 3 of the walkthrough on the next page. Once installation of Pi AUI Suite is complete and you have run sudo voicecommand -c to set it to listen, you need to prime it with a keyword. By default, this is Pi, but feel free to alter this to something easier, such as the word Alexa"" if you want an Amazon Next, try out a few of the built-in voice commands. This is similar to Googles Im feeling lucky. Say YouTube and the name of the video in which youre interested  for example, YouTube fluffy kittens. Internet: Saying the word internet launches your web browser. By default, this is the Raspberry Pis built-in browser Midori, although you can change this. Download: Saying the word download plus a search term automatically searches the Pirate Bay website for the file in question  for example, you could say Download Ubuntu Yakkety Yak to get the latest version of the Ubuntu Linux operating system. Play: This command uses the builtin media player to play a music or video file  for example, Play mozartconcert.mp4 would play that particular file located in the media folder you specified in setup, such as /home/pi/Videos. Show me: Saying show me opens up a folder of your choice. By default, the command doesnt go to a valid folder, so you need to edit your configuration file to a valid location  for example, show me==/home/pi/Documents. Once the Voicecommand program is installed, you may wish to make a few basic changes to the setup before fine-tuning your configuration. You are asked a series of yes/no questions next. The first question asks whether you want to permanently set the continuous flag. In plain English, the Voicecommand program is asking whether, each time you run it, you want it to continuously listen for your voice commands. Select Y for now. Next, you are asked if you want the Voicecommand program to permanently set the verify flag. Selecting Y here means the program expects you to say your keyword (by default, the word Pi) before responding to commands. This can be useful if you want to set the Raspberry Pi to listen continuously and dont want it to act on everything you say. The following prompt asks if you want to permanently set the ignore flag. This means that if Voicecommand hears a command thats not specifically listed in your configuration file, it tries to look for a program in your installed applications and run it. For instance, if you say the word leafpad, which is a notepad application, Voicecommand searches for and runs this even if not specifically told to. We do not recommend you enable this feature. Because youre running Voicecommand as a SuperUser, theres too much risk that you could inadvertently tell the Raspberry Pi a command that could harm your files. If you want to set up extra applications to work with Voicecommand, you can edit the configuration file in each specific case. Voicecommand then asks you whether you wish to permanently set the quiet flag, so it doesnt give a verbal response when you speak. Choose Y or N as you see fit. Next, youre asked if you want to change the default duration for speech recognition. You should only change this if youre finding the Pi is having trouble hearing your commands. If you choose Y, youre asked to type in a number  this is the number of seconds that the Raspberry Pi listens for a voice command, and the default is 3. The program then gives you a chance to set up the text-to-speech options. Be sure to turn up your volume before doing this. The program attempts to say something and asks whether you have heard it. Use the up arrow to maximise the capture volume of your device (in this case, were using a Microsoft USB webcam) The default response of the system when responding to your keyword is Yes sir Choose Y on the next prompt to change this, then type in your desired response, such as Yes maam Press Return when youre done. The system plays back the response for you to confirm whether youre happy with the result. The procedure is the same for the default message for when the system receives an unknown command. The default response is Received improper command, but you can change this to something less robotic if you prefer by typing Y, then your chosen response  for example, Unknown command. You are now offered the chance to set up the speech recognition options. This automatically checks whether you have a compatible microphone installed. Voicecommand next asks you if you want the Pi to check your audio threshold for you. Make sure there is no background noise, press Y, then Return. It then asks you to speak a command to check that it has the right audio device selected. The program automatically determines the right audio threshold for you, so type Y to choose this. Finally, the Raspberry Pi asks you if you want to change the default keyword (Pi) to activate voice commands. Type Y, then enter your new keyword. Press Return when done. You are then asked to speak your keyword to acclimatise the Raspberry Pi to your speaking voice. If this seems correct, type Y to complete the setup. Follow Step 6 of the tutorial on the next page to run the Voicecommand software. Try to start out with a few simple commands. (See Basic Voice Commands boxout for details). Once youre comfortable with these, run the command sudo killall voicecommand to shut down the program and edit your configuration file if you wish. Once your Voicecommand software is up and running, you can edit the configuration file to add new commands or modify existing ones. Run the command sudo nano /root/.commands.conf to view the configuration file. As youll see, most of the lines begin with a # symbol, which means the Raspberry Pi ignores them. Delete the symbol to activate the line. If, for instance, you want to change the keyword that activates the voice recognition software from Pi to Alexa, you would change the line from #!keyword==pi to -!keyword=alexa. If you use the Firefox web browser instead of Midori, you may also want to change ~Internet==midori & to ~Internet==firefox-esr &. The software can run any command. For instance, to open the desktop by saying the word desktop, add the following line to the end of the file: desktop==home/pi/Desktop You can also launch programs as you would from the terminal  for example, notepad==leafpad As youre talking to the Raspberry Pi, you may want it to respond. Do this first by opening Terminal and installing the speech synthesis software Festival with the following command: You can also have the Raspberry Pi read out system information. For example, if you wanted the Raspberry Pi to tell you the date and time, you would add the following line to the config file: To improve your chances, be sure to stay near the USB microphone and speak slowly and clearly. If youre still having trouble being understood, open Terminal on your Raspberry Pi or connect via SSH and run the command alsamixer to open your sound settings. Press F4 to choose audio input, then press F6. Use the arrow keys to select your USB device, then press Return. This controls the volume of your USB microphone. Use the up arrow to push it to maximum (100). If your device isnt being detected at all, it may need more power than the Raspberry Pis USB ports can provide on their own. The best solution for this is to use a powered USB hub. Once the Download program is installed, if you experience an error connecting, bear in mind that access to the PirateBay website may be restricted where you are. In order to download files, you also need a BitTorrent client for the Raspberry Pi, such as the program Transmission. You can install this by opening Terminal or connecting to your Raspberry Pi over SSH and running the command sudo apt-get install transmission. Help with getting started and how to use the client is available from the Transmission website . Needless to say, you should only download files with the permission of the copyright holder. Google claims not to retain any of this data, but even if it is to be believed, any data transmitted over the internet can potentially be intercepted by a third party. Google does encrypt your connection to reduce the chance of this happening, however. If you find youre happy with the voice command feature, you might prefer the software to start automatically each time you boot the Raspberry Pi. If so, open Terminal on your Raspberry Pi or connect via SSH and run the following command: This opens the file that determines which processes start up when your Raspberry Pi boots. By default, this script does nothing. Use your arrow keys to scroll to the bottom of the file and, just above the line reading exit 0 , type the following: Press Ctrl+X, then Y, then Return to save your changes. Feel free to reboot the Raspberry Pi at this stage to make sure it works. If youre unsure whether Voicecommand is running, open Terminal and run the command ps -a to show a list of running processes. ",http://www.techradar.com/how-to/how-to-control-the-raspberry-pi-with-your-voice,"There's a number of speech recognition programs for the Raspberry Pi. For this project, we're using Steven Hickson's Pi AUI Suite, because it's ...",How to control the Raspberry Pi with your voice,Amazon Echo Raspberry Pi USB Raspberry Pi YouTube Raspberry Pi USB Microsoft HD-3000 Raspberry Amazon USB USB Raspberry Pi Steven Hicksons Pi AUI Suite Pi AUI Suite Raspberry Pi Y Return PlayVideo /home/pi/Videos USB Raspberry Pi Youre Downloader Return Google Text Speech Service Raspberry Pi Raspberry Pi Googles Raspberry Pis Google Do Return Google Return Google Voice Commands Googles Again Google Whether Google-specific YouTube YouTube Simply Return Raspberry Pi Voicecommand Voicecommand Step Once Pi AUI Suite Pi Alexa Amazon Next Googles Im Say YouTube YouTube Raspberry Pis Midori Pirate Bay Download Ubuntu Yakkety Yak Ubuntu Linux Play Voicecommand English Voicecommand Select Y Voicecommand Y Pi Raspberry Pi Voicecommand Voicecommand Voicecommand SuperUser Raspberry Pi Voicecommand Voicecommand Choose Y N Pi Raspberry Pi Microsoft USB Yes Choose Y Yes maam Press Return Y Unknown Voicecommand Pi Y Return Raspberry Pi Pi Type Y Press Return Raspberry Pi Y Follow Step Voicecommand Basic Voice Commands Voicecommand Raspberry Pi Pi Alexa Firefox Midori ~Internet==midori ~Internet==firefox-esr Raspberry Pi Terminal Festival Raspberry Pi Raspberry Pi USB Terminal Raspberry Pi SSH Press F4 F6 USB Return USB Raspberry Pis USB USB Download PirateBay BitTorrent Raspberry Pi Transmission Terminal Raspberry Pi SSH Transmission Google Google Raspberry Pi Terminal Raspberry Pi SSH Raspberry Pi Press Ctrl+X Y Return Feel Raspberry Pi Voicecommand Terminal,-1
312,"VocalZoom and Cobalt have announced a new partnership aiming to bring advanced voice control solutions to market. The companies plan to combine their technologies, which essentially means integrating together VocalZooms Human to Machine Communication (HMC) sensor and Cobalts speech recognition system. The HMC sensor combines microphone input with facial vibrations acquired via an optical sensor, producing what VocalZoom says is an isolated, near-perfect reference signal even in situations featuring high noise levels. When used with Cobalts speech recognition system, it can reportedly boost the latters accuracy by almost 60 percent. Commenting on the collaboration, Cobalt CEO Jeff Adams says they have achievedmore performance improvements working with VocalZooms HMC sensor in two months than would typically be seen in two years with conventional approaches. The companies say they are aiming their solution at the connected car , access control, and head-mounted display marketsareas in which voice-based interaction could play a major role going forward. And VocalZoom plans to show off its sensor at next weeks Mobile World Congress , via the 4YFN Connecting Startups conferences Intel Startup Pavilion. ",http://findbiometrics.com/vocalzoom-cobalt-hmc-speech-recognition-301165/,"When used with Cobalt's speech recognition system, it can reportedly boost the latter's accuracy by almost 60 percent. Commenting on the ...",VocalZoom and Cobalt Combine HMC and Speech Recognition,VocalZoom Cobalt VocalZooms Human Machine Communication HMC Cobalts HMC VocalZoom Cobalts Cobalt CEO Jeff Adams VocalZooms HMC VocalZoom Mobile World Congress Connecting Startups Intel Startup Pavilion,1
313,"To sell suckers, one uses deceit and offers glamour, wrote John Pierce of Bell Labs in 1969 as he discussed speech recognition. Seriously, that was his advice against mad inventors or untrustworthy engineers that weren't using a scientific approach. Glamour led to funding, which led to failure, which led to  A.I. winters  in which funding disappeared. Is innovation still being stifled because of those previous failures, or is there a fear of new ideas A.I.s glamour is exploited for marketing purposes today, but progress in A.I. is lacking because we aren't aiming at the right target . As Pierce said about the right approach based on the scientific method, the work should be an experiment, not just an experience. Many aspects of A.I. are being pursued by big corporate influencers: Nuance, Apple, IBM, Google, Facebook, Microsoft and others that claim glamorous brainlike breakthroughs. As I wrote previously, we won't see brainlike breakthroughs using computer algorithms because it's too hard for programmers . Im certain that we will rapidly create speaking machines once we start using science to do it. Science proves its worth daily in our lives: Without it, the struggle is relentless. Just search the Internet for epicycles to see how long the wrong model can confound us. (Because the Earth isnt at thecenterof the universe, the geocentric model with planetsorbiting in circular orbits was complex and futile.) I got into trouble this week. I wished my mother-in-law a happy birthday by text message: Happy birthday Gwendoline! The response was chilling: Thats not how you spell my name. With some fast typing, ""Gwendoleen"" got a revised birthday wish, and I got a typical example of why speech recognition fails today. Words are more than sounds. The problem of speech recognition is more evident when you hear a foreign language. Its a stream of unintelligible nonsense. You hear the sounds of the language with little idea where words start and end  the word boundary identification problem . Your own language seems different, being just communications with sequences of words. As the early A.I. pioneers found, it is a challenge to convert speech to text, and in my birthday example, like many language experiences, it took feedback to know a spelling correction was required. The glamour of speech recognition is that, like a human, a machine can act as a personal dictation service. This should save us time and improve our interactions with machines. Speech recognition is a glamorous term because it suggests that language is being understood. Thats what speech is about  sending ideas by voice. A better term would be sounds to text, to describe what today's statistical systems are doing. How did we get here In 1971, the DARPA Speech Understanding Research (SUR) programled to good progress with search technology. Subsequent advances with statistical tools resulted inthe adoption of a single approach today  matching sounds to words to phrases using statistics. A 2008 article in The New Yorker reviewed the ""probabilistic approach to speech recognition"" more than 35 years later. It quoted Dr. David Nahamoo , the chief technology officer for speech at I.B.M.s Thomas J. Watson Research Center: Brute-force computing, based on probability algorithms, won out over the rule-based approach. The battle was won, but the war for humanlike accuracy was lost. The article goes further: ""A speech recognizer, by learning the relative frequency with which particular words occur ... could be trained to make educated guesses. Such a system wouldnt be able to understand what words mean..."" Speech recognition doesn't recognize like a human. The statistical technology based on ""learning the statistics"" has fundamental limitations. Unlike humans, systems are unreliable: (a) because they are in noisy environments where multiple people speak at once,(b) because higher-level information, like context and meaning, is ignored (c) because other attributes, such as dealing with emphasis and the speed of speech are ignored and (d) because training data doesn't generalize: loading corpora creates statistics only on that data. Peter Norvig , the director of research at Google, covered the state-of-the-art of statistical learning at a 2011 symposium. In it, American linguist Noam Chomsky , ""derided researchers in machine learning who use purely statistical methods ... but who don't try to understand the meaning ..."" and then headded that ""It interprets success as approximating unanalyzed data."" Pierce wrote:  a general phonetic typewriter is simply impossible unless the typewriter has an intelligence and a knowledge of language comparable to those of a native speaker of English. In other words: dictation software needs language understanding. According to Nuances website, todays best-selling dictation software is Dragon NaturallySpeaking . As an occasional user, I have seen its limitations, where subtle and not-so-subtle errors are introduced into the dictated text. Its claimed 99% accuracy is poor because it generates sequential text without understanding. And accuracy drops (a) with foreign accents and (b) speech including features of conversation like backtracking. Dragon cannot detect an error in meaning, but humans are different: We immediately ask questions when something makes no sense or we ignore it completely. Worse, error correction by voice alone is not for the faint-hearted, and the method isn't humanlike. Suffice to say that when you can say, ""No, change 'Gwendoline' to end like the word 'keen',"" dictation will have come of age. The problem with machine dictation is that to extract speech, you need to understand what is being said. I can't take dictation for Swedish, for example. Machines for productivity should be intuitive, not force us to learn how to work with them. The idea that technology forces us to learn complex commands seems foolish in 2015, but thats what is needed with the best-selling software. If machines understood us, they would be far more useful. Imagine finding a form on a Web page seeking your personal details and saying: ""enter my details, please."" Today, that isn't an option. Dragon has the capability to fill out an online form that illustrates the problem.A tutorial shows you how it works. You use commands, not conversation, like this: Click textfield, choose 6, San Jose, for example. And instead of saying enter my birthday you select the field and then say zero six slash zero three slash nineteen sixty-eight if thats your birthday. It's not intuitive at all. Commands arent language. They are simply creating a verbal typewriter, and if you cant remember the commands, you cant use it. Future machines will use conversation and allow the example: ""enter my details,"" to enter your name, address, birthday and other relevant details. Getting back to the science and the real target is needed. The compromise to focus on engineering has produced limited results, but we need machine interaction to be much more natural. As Pierce said around 55 years ago, dictation must start with language understanding comparable to those of a native speaker. This article is published as part of the IDG Contributor Network. Want to Join ",http://www.computerworld.com/article/2935605/emerging-technology/speech-recognition-glamour-and-deceit.html,"To sell suckers, one uses deceit and offers glamour, wrote John Pierce of Bell Labs in 1969 as he discussed speech recognition. Seriously ...",Speech recognition: Glamour and deceit?,John Pierce Bell Labs Glamour A.I A.I.s A.I Pierce A.I Apple IBM Google Facebook Microsoft Im Science Just Internet Earth Gwendoline Gwendoleen A.I DARPA Speech Understanding Research SUR New Yorker Dr. David Nahamoo I.B.M.s Thomas J. Watson Research Center Peter Norvig Google American Noam Chomsky Pierce English Nuances Dragon NaturallySpeaking Dragon 'Gwendoline 'keen Swedish Imagine Web Dragon Click San Jose Commands Pierce IDG Contributor Network,4
314,"Researchers at Microsoft achieved what they say is a  breakthrough in speech recognition claiming they've developed a system that's  as effective or better than people with professional transcription skills. The  software's word error rate (WER) is down to 5.9 percent -- an improvement from  the WER of 6.9 the team reported in September. The milestone was enabled with  the new Microsoft Cognitive Toolkit ,  the software that enables those speech recognition advances (as well as image  recognition and search relevance). Microsoft announced both developments two  weeks ago, though the timing wasn't the best as IBM was holding its huge World  of Watson event in Las Vegas. Watson, of course, is Big Blue's AI system made famous several  years ago when it appeared on Jeopardy and, in advance of its latest rollout,  made the talk-show circuit including CNN and CBS's 60 Minutes, where IBM Chairman,  President and CEO Ginni Romettytalked up Watson's own achievements  including the ability to discover potential cancer cures deemed not possible by  humans, among other milestones. Microsoft believes it has the most powerful AI and cognitive  computing capabilities available. The Microsoft Cognitive Toolkit is the new  name for what it previously called the Computation Network Toolkit, or CNTK. In  addition to helping the researchers hit the 5.9 WER, the new Microsoft  Cognitive Toolkit 2.0 helped the researchers enable what the company is calling  ""reinforcement learning."" The new open source release, like its predecessors available  on GitHub, now supports Python including migration from C++. ""We've removed the  barrier for adoption substantially by introducing Python support,"" said Xuedong  Huang, a Microsoft distinguished engineer, in a recent interview. ""There are so  many people in the machine learning community who love using Python."" Most noteworthy, Huang said is that the new software has a  significant boost in performance, enabling it to scale across multiple Nvidia  GPUs, including those added with the field-programmable gate arrays in the  Azure cloud. Huang acknowledges the tool isn't as popular as other open source  frameworks such as Google's TensorFlow, Caf or Torch, but he argues it's more  powerful, extensible and able to scale across multiple machines and  environments. ""It' the fastest, most efficient distributed deep learning  framework out there available,"" Huang said. ""The performance is so much better.  It's at least two to three times faster than the second alternative."" The new  Microsoft Cognitive Toolkit includes algorithms that can't degrade computational  performance, he added. For its part, IBM made some pretty big news of its own. The  company released the new Watson Data Platform (WDP), a cloud-based analytics  development platform that allows programming teams including data scientists  and engineers to build, iterate and deploy machine-learning applications. WDP runs on IBM's Bluemix cloud platform, integrates with Apache  Spark , works with the IBM  Watson Analytics service and will underpin the new IBM Data Science Experience (DSX), which is a ""cloud-based, self-service social workspace that enables data  scientists to consolidate their use of and collaborate across multiple open  source tools such as Python, R and Spark,"" said IBM Big Data Evangelist James Kobielus in a blog  post outlining last month's announcements at the company's World of Watson  conference in Las Vegas. ""It provides productivity tools to accelerate data  scientists' creation of cognitive, predictive machine learning and other  advanced analytics for cloud-based deployment. It also includes a rich catalog  of learning resources for teams of data science professionals to deepen their  understanding of tools, techniques, languages, methodologies and other key  success enablers."" There are also free enterprise plans that include 10 DSX  user licenses and a Spark Enterprise 30 Executor Plan, he noted. IBM claims  more than 3,000 developers are working on the WDP and upwards of 500,000 users  are now trained on its capabilities. Has IBM already won the war against Microsoft, Google,  Amazon and Facebook when it comes to intelligent cognitive computing, AI and  machine learning Karl Freund, a senior analyst at Moor Insights and  Technology, said Microsoft  has a long way to go to compete with IBM Watson for mindshare without question. ""IBM is a brilliant marketing machine, and they are spending a lot of  money to establish the Watson brand. Microsoft has nothing comparable, Freund  said.""From a technology perspective, however, IBM has not convinced the  technologists that they have anything special. In fact, most people I  speak with would say that their marketing is ahead of their reality."" Freund said what IBM  is offering is a collaborative development platform.""Microsoft is  releasing their software as open source,"" he added. ""IBM is all about the  services, while Microsoft is seeking to gain broad support for their software  stack, regardless of where you run it."" Microsoft's  new toolkit and its multi-GPU support are  significant, while Watson is likely to appeal to existing Big Blue shops  including those with mainframes and organizations using the IBM SoftLayer  cloud. ",https://redmondmag.com/blogs/the-schwartz-report/2016/11/microsoft-ai-and-speech-breakthroughs.aspx,Researchers at Microsoft achieved what they say is a breakthrough in speech recognition claiming they've developed a system that's as ...,Microsoft's AI and Speech Breakthroughs Eclipsed by New IBM ...,Microsoft WER WER September Microsoft Cognitive Toolkit Microsoft IBM World Watson Las Vegas Watson Big Blue AI Jeopardy CNN CBS Minutes IBM Chairman President CEO Ginni Watson Microsoft AI Microsoft Cognitive Toolkit Computation Network Toolkit CNTK WER Microsoft Cognitive Toolkit GitHub Python C++ Python Xuedong Huang Microsoft Python Huang Nvidia GPUs Azure Huang Google TensorFlow Caf Torch Huang Microsoft Cognitive Toolkit IBM Watson Data Platform WDP WDP IBM Bluemix Apache Spark IBM Watson Analytics IBM Data Science Experience DSX Python R Spark IBM Big Data Evangelist James Kobielus World Watson Las Vegas DSX Spark Enterprise Executor IBM WDP Has IBM Microsoft Google Amazon Facebook AI Karl Freund Moor Insights Technology Microsoft IBM Watson IBM Watson Microsoft Freund IBM Freund IBM Microsoft IBM Microsoft Microsoft Watson Big Blue IBM SoftLayer,-1
315,"By Janene Pieters on September 30, 2016 - 12:08 An Australian tech company called Appen is in possession of the private telephone conversations of thousands of Dutch, according to the Volkskrant. Telephone experts think the most likely scenario is that the conversations were tapped by British intelligence agency GCHQ. Appen is a company working on software that converts speech to text, something the GCHQ is interested in, according to the newspaper. The Dutch conversations come from 2010 and 2011. The unauthorized use of private communications was accidentally discovered by a Dutch Appen employee, who told her story to the Volkskrant. She was hired to transcribe brief audio clips, all of them short conversations between Dutch. In one of them she recognized her ex-boyfriend leaving a voice message for his new girlfriend. She told him about it and he said he never consented to share his communications. The former employee listened to thousands of Dutch audio clips. And several dozen other people were also working on the project. Telecom Expert Rene Pluijmers of the National Forensic Investigation Office, thinks the most likely scenario as that the conversations were recorded by a security service. ""The British service GCHQ intercepted dozens of fiber optic cables that also come from the Netherlands and since 1981 have experience with speech recognition. They have an interest in automated identification of data and calls. They gave Appen the tapped calls in order to improve the software that can do this."" ",http://nltimes.nl/2016/09/30/report-thousands-tapped-dutch-conversations-hands-australian-tech-company,"Appen is a company working on software that converts speech to text, something ... and since 1981 have experience with speech recognition.",Report: Thousands of tapped Dutch conversations in the hands of ...,Janene Pieters September Appen Dutch Volkskrant GCHQ Appen GCHQ Appen Volkskrant Dutch Dutch Telecom Expert Rene Pluijmers National Forensic Investigation Office GCHQ fiber Netherlands Appen,0
316,"Get today's popular DigitalTrends articles in your inbox: Are you ready to start dictating your documents and text with your voice Instead of offering separateddictation or speech-to-text capabilities, Windows 10 conveniently groups its voice commands under Speech Recognition , which interprets the spoken word across the operating system for a variety of tasks. Lets go through how to properly set up these recognition features and improve Windows ear for your voice. Note thatspeech recognition is only currently available in English, French, Spanish, German, Japanese, Simplified Chinese, and Traditional Chinese. The first step is making sure that you have the right hardware for speech-to-text options. You may not think much about this step  after all, the Surface lines of tablets and laptops come with built-in mics, which are a common accessory in many of todays computers. That said, do you need anything else The problem here is one of quality. While built-in mics work well for more simple tasks  such asSkype conversations andquick voice commands  you have to consider distortion and mic quality if you really want to capitalize onspeech-to-text. In the past, Microsoft has warned that its speech recognition features are best suited for headset microphones that interpret sounds with greater clarity andare less susceptible toambient noise. If youre serious about using speech recognition for Windows 10, its a good idea to pick up a headset that is compatible with your computer. If you are going to buy hardware, do it sooner rather than later, as the speech features tend towork best if you dont switch devices after training. If you do decide to get a new mic, then you will want to head over to the Windows 10 search box and type in microphone. This will allow you to go directly to the Set up a microphone section of your Control panel. Choose to set up a new microphone through settings and make sure its working before progressing to the next step. With yourmic ready, its time to start configuring your various speech recognition capabilities. In Windows 10, this is a more seamless process than it has been in the past. These steps and tutorials will affect an array of Windows programs, but you may also want to make sure dictation is enabled in any writing apps that you prefer to use. Begin with the steps below. Step 1: Access Settings.You can do so by searching for it in the search box or via voice command, if you arent sure where it is in Windows 10. Step 2: In Settings, select the sidebar option for Ease of Access. This will bring up another menu with a number of accessibility options. Step 3: One of the first Access options youll see is for Narrator. The Narrator options allow you to control the ability to hear text and control/menu options that appear on the screen. This makes an excellent combination with speech-to-text if, for example, you are using it because of a disability. Spend some time adjusting the Narrator as needed before moving on. Step 4:Go to the Speech Recognition. If you cannot find it, then use the search box in the upper-right corner of the window to search for speech recognition, which should bring up an option to open the right menu. Step 5:Inside the Speech Recognition menu, you will find a collection of different tools and commands. If you havent already set up your microphone, this is a great time to do it using the Set Up Microphone feature. Afterward, useStart Speech Recognitionto activate the listening mode. After completing the brief wizard, Windows will inform you that you can now control your computer using your voice. Step 6:Now its time to take the Speech Tutorial. This is a mini-class designed to show you how Windows uses speech recognition, and what you need to get started. It includes lessons on basic commands to get around  Click and Select, for instance as well as commands focused on dictation, such as how to start new paragraphs or delete words. Even if youve done some Windows dictation in the past, its a good idea to start the Tutorial and follow the instructions to learn whats changed this time around. At this point, you can venture intoWindows docs and use speech-to-text with a variety of Microsoft files  youre all set. However, we recommend you stick around a bit longer to improve Windowsvoice recognition capabilities. Microsofts latest software has the ability to learn your voice with a little training, and that can really pay off with a few sessions. To begin, click on Train your computer to better understand you in the Speech Recognition menu. As indicated, this will be a training task where you read on-screen text to the computer so that it betterunderstands how you pronounce certain words and sounds. This is a piece of must-have customization for Windows speech-to-text, no matter how American Newscaster or Received Pronunciation your accent may sound. Run it a couple times to see if text accuracy improves as a result. Also, note the option toward the bottom that allows access to the Speech Reference Card. This gives you all the vocal shortcuts you need to get around in a small side screen/printout. Its a great tool for beginners who also want to control programs and software commands with their voices. ",http://www.digitaltrends.com/computing/windows-10-speech-to-text-guide/,"Note that speech recognition is only currently available in English, French, Spanish, German, Japanese, Simplified Chinese, and Traditional ...",Want to talk to your PC? Here's how to enable speech to text in ...,Get DigitalTrends Windows English French Spanish German Japanese Simplified Chinese Traditional Chinese Surface Microsoft Windows Windows Set Control Windows Windows Settings.You Windows Settings Ease Access Access Narrator Narrator Set Up Microphone Afterward Windows Speech Tutorial Click Select Windows Tutorial Microsoft Windowsvoice Microsofts Train Windows Newscaster Pronunciation Speech Reference Card,2
317,"LONDON--( BUSINESS WIRE )--Technavios latest report on the global        speech recognition software market provides an analysis of the        key trends expected to impact the market through 2020. Technavio defines an emerging trend as a factor that has the potential to        significantly impact the market and contribute to its growth or decline. The global speech recognition software market is expected to exceed USD        2.5 billion by 2020, growing at a CAGR of almost 17%. According to        Amrita Choudhury, lead analyst at Technavio for enterprise        application research, The increase in demand for authentication        in financial institutions, healthcare enterprises, and government        organizations is driving the growth of this market. In addition, the        increased investment by different countries on speech recognition        applications is expected to foster the market growth in the future. The top four emerging trends influencing the global speech recognition        software market according to Technavios ICT research analysts are: Normal passwords are easy to remember and crack. As a result,        organizations are increasingly adopting biometric passwords in which        symbols are replaced by the voice of the person. The voice of an        individual becomes the password and provides identification and        authentication. Voice passwords cannot be replicated and can help companies improve        customer services by enabling them to support the enrollment and        authentication of clients. Speech recognition applications are used for        customized user authentication when multiple end-users are present or        multiple users use the same product. For instance, voice passwords could        be used to secure interactions between mobile phones and electronic        devices in vehicles and homes. A number of vendors are working toward the integration of voice        verification with the speech recognition technology. Voice verification        helps identify the individual that is speaking. Instead of offering        voice verification as a stand-alone product, vendors are offering        integrated voice verification and speech recognition. A number of        vendors have either launched or are in the process of launching voice        recognition applications that involve the integration of these two        technologies, says Amrita. The gaming industry is a booming segment. Computer games and online        games are the two most important sub-segments of this industry. Speech recognition applications are used in cases where voice-driven commands        need to be used to control games. Speech recognition modules can work as a common platform for the design        and development of computer and online games. The voice-driven platform        is based on algorithms that are integrated with game engines. Market vendors are aware of the changing demands and the significant        potential for speech recognition software. In response, they are        developing innovative applications for particular technological        segments. Some of the innovative technologies in the mobile devices        segment include network-based, cloud-based, and hybrid speech        recognition applications. Hybrid applications combine the benefits of embedded applications, which        process voice-based commands, and network-based applications to provide        speech-enabled internet searching. For instance, customers can use an        automatic airline reservation system by using a speech-enabled internet        application on a mobile device. Vendors are also developing customized        speech recognition applications for warehouse workers. Some of the top vendors in the global speech recognition software        market, as researched by Technavio analysts are: ",http://www.businesswire.com/news/home/20160426005118/en/Technavio-Announces-Top-Emerging-Trends-Impacting-Global,LONDON--(BUSINESS WIRE)--Technavio's latest report on the global speech recognition software market provides an analysis of the key ...,Technavio Announces Top Four Emerging Trends Impacting the ...,LONDON BUSINESS WIRE Technavios Technavio USD CAGR Amrita Choudhury Technavio Technavios ICT Voice Voice Amrita Computer Hybrid Technavio,-1
318,"By Janene Pieters on September 30, 2016 - 12:08 An Australian tech company called Appen is in possession of the private telephone conversations of thousands of Dutch, according to the Volkskrant. Telephone experts think the most likely scenario is that the conversations were tapped by British intelligence agency GCHQ. Appen is a company working on software that converts speech to text, something the GCHQ is interested in, according to the newspaper. The Dutch conversations come from 2010 and 2011. The unauthorized use of private communications was accidentally discovered by a Dutch Appen employee, who told her story to the Volkskrant. She was hired to transcribe brief audio clips, all of them short conversations between Dutch. In one of them she recognized her ex-boyfriend leaving a voice message for his new girlfriend. She told him about it and he said he never consented to share his communications. The former employee listened to thousands of Dutch audio clips. And several dozen other people were also working on the project. Telecom Expert Rene Pluijmers of the National Forensic Investigation Office, thinks the most likely scenario as that the conversations were recorded by a security service. ""The British service GCHQ intercepted dozens of fiber optic cables that also come from the Netherlands and since 1981 have experience with speech recognition. They have an interest in automated identification of data and calls. They gave Appen the tapped calls in order to improve the software that can do this."" ",http://nltimes.nl/2016/09/30/report-thousands-tapped-dutch-conversations-hands-australian-tech-company,"Appen is a company working on software that converts speech to text, something ... and since 1981 have experience with speech recognition.",Report: Thousands of tapped Dutch conversations in the hands of ...,Janene Pieters September Appen Dutch Volkskrant GCHQ Appen GCHQ Appen Volkskrant Dutch Dutch Telecom Expert Rene Pluijmers National Forensic Investigation Office GCHQ fiber Netherlands Appen,0
319,"Get today's popular DigitalTrends articles in your inbox: Are you ready to start dictating your documents and text with your voice Instead of offering separateddictation or speech-to-text capabilities, Windows 10 conveniently groups its voice commands under Speech Recognition , which interprets the spoken word across the operating system for a variety of tasks. Lets go through how to properly set up these recognition features and improve Windows ear for your voice. Note thatspeech recognition is only currently available in English, French, Spanish, German, Japanese, Simplified Chinese, and Traditional Chinese. The first step is making sure that you have the right hardware for speech-to-text options. You may not think much about this step  after all, the Surface lines of tablets and laptops come with built-in mics, which are a common accessory in many of todays computers. That said, do you need anything else The problem here is one of quality. While built-in mics work well for more simple tasks  such asSkype conversations andquick voice commands  you have to consider distortion and mic quality if you really want to capitalize onspeech-to-text. In the past, Microsoft has warned that its speech recognition features are best suited for headset microphones that interpret sounds with greater clarity andare less susceptible toambient noise. If youre serious about using speech recognition for Windows 10, its a good idea to pick up a headset that is compatible with your computer. If you are going to buy hardware, do it sooner rather than later, as the speech features tend towork best if you dont switch devices after training. If you do decide to get a new mic, then you will want to head over to the Windows 10 search box and type in microphone. This will allow you to go directly to the Set up a microphone section of your Control panel. Choose to set up a new microphone through settings and make sure its working before progressing to the next step. With yourmic ready, its time to start configuring your various speech recognition capabilities. In Windows 10, this is a more seamless process than it has been in the past. These steps and tutorials will affect an array of Windows programs, but you may also want to make sure dictation is enabled in any writing apps that you prefer to use. Begin with the steps below. Step 1: Access Settings.You can do so by searching for it in the search box or via voice command, if you arent sure where it is in Windows 10. Step 2: In Settings, select the sidebar option for Ease of Access. This will bring up another menu with a number of accessibility options. Step 3: One of the first Access options youll see is for Narrator. The Narrator options allow you to control the ability to hear text and control/menu options that appear on the screen. This makes an excellent combination with speech-to-text if, for example, you are using it because of a disability. Spend some time adjusting the Narrator as needed before moving on. Step 4:Go to the Speech Recognition. If you cannot find it, then use the search box in the upper-right corner of the window to search for speech recognition, which should bring up an option to open the right menu. Step 5:Inside the Speech Recognition menu, you will find a collection of different tools and commands. If you havent already set up your microphone, this is a great time to do it using the Set Up Microphone feature. Afterward, useStart Speech Recognitionto activate the listening mode. After completing the brief wizard, Windows will inform you that you can now control your computer using your voice. Step 6:Now its time to take the Speech Tutorial. This is a mini-class designed to show you how Windows uses speech recognition, and what you need to get started. It includes lessons on basic commands to get around  Click and Select, for instance as well as commands focused on dictation, such as how to start new paragraphs or delete words. Even if youve done some Windows dictation in the past, its a good idea to start the Tutorial and follow the instructions to learn whats changed this time around. At this point, you can venture intoWindows docs and use speech-to-text with a variety of Microsoft files  youre all set. However, we recommend you stick around a bit longer to improve Windowsvoice recognition capabilities. Microsofts latest software has the ability to learn your voice with a little training, and that can really pay off with a few sessions. To begin, click on Train your computer to better understand you in the Speech Recognition menu. As indicated, this will be a training task where you read on-screen text to the computer so that it betterunderstands how you pronounce certain words and sounds. This is a piece of must-have customization for Windows speech-to-text, no matter how American Newscaster or Received Pronunciation your accent may sound. Run it a couple times to see if text accuracy improves as a result. Also, note the option toward the bottom that allows access to the Speech Reference Card. This gives you all the vocal shortcuts you need to get around in a small side screen/printout. Its a great tool for beginners who also want to control programs and software commands with their voices. ",http://www.digitaltrends.com/computing/windows-10-speech-to-text-guide/,"Note that speech recognition is only currently available in English, French, Spanish, German, Japanese, Simplified Chinese, and Traditional ...",Want to talk to your PC? Here's how to enable speech to text in ...,Get DigitalTrends Windows English French Spanish German Japanese Simplified Chinese Traditional Chinese Surface Microsoft Windows Windows Set Control Windows Windows Settings.You Windows Settings Ease Access Access Narrator Narrator Set Up Microphone Afterward Windows Speech Tutorial Click Select Windows Tutorial Microsoft Windowsvoice Microsofts Train Windows Newscaster Pronunciation Speech Reference Card,2
320,"LONDON--( BUSINESS WIRE )--Technavios latest report on the global        speech recognition software market provides an analysis of the        key trends expected to impact the market through 2020. Technavio defines an emerging trend as a factor that has the potential to        significantly impact the market and contribute to its growth or decline. The global speech recognition software market is expected to exceed USD        2.5 billion by 2020, growing at a CAGR of almost 17%. According to        Amrita Choudhury, lead analyst at Technavio for enterprise        application research, The increase in demand for authentication        in financial institutions, healthcare enterprises, and government        organizations is driving the growth of this market. In addition, the        increased investment by different countries on speech recognition        applications is expected to foster the market growth in the future. The top four emerging trends influencing the global speech recognition        software market according to Technavios ICT research analysts are: Normal passwords are easy to remember and crack. As a result,        organizations are increasingly adopting biometric passwords in which        symbols are replaced by the voice of the person. The voice of an        individual becomes the password and provides identification and        authentication. Voice passwords cannot be replicated and can help companies improve        customer services by enabling them to support the enrollment and        authentication of clients. Speech recognition applications are used for        customized user authentication when multiple end-users are present or        multiple users use the same product. For instance, voice passwords could        be used to secure interactions between mobile phones and electronic        devices in vehicles and homes. A number of vendors are working toward the integration of voice        verification with the speech recognition technology. Voice verification        helps identify the individual that is speaking. Instead of offering        voice verification as a stand-alone product, vendors are offering        integrated voice verification and speech recognition. A number of        vendors have either launched or are in the process of launching voice        recognition applications that involve the integration of these two        technologies, says Amrita. The gaming industry is a booming segment. Computer games and online        games are the two most important sub-segments of this industry. Speech recognition applications are used in cases where voice-driven commands        need to be used to control games. Speech recognition modules can work as a common platform for the design        and development of computer and online games. The voice-driven platform        is based on algorithms that are integrated with game engines. Market vendors are aware of the changing demands and the significant        potential for speech recognition software. In response, they are        developing innovative applications for particular technological        segments. Some of the innovative technologies in the mobile devices        segment include network-based, cloud-based, and hybrid speech        recognition applications. Hybrid applications combine the benefits of embedded applications, which        process voice-based commands, and network-based applications to provide        speech-enabled internet searching. For instance, customers can use an        automatic airline reservation system by using a speech-enabled internet        application on a mobile device. Vendors are also developing customized        speech recognition applications for warehouse workers. Some of the top vendors in the global speech recognition software        market, as researched by Technavio analysts are: ",http://www.businesswire.com/news/home/20160426005118/en/Technavio-Announces-Top-Emerging-Trends-Impacting-Global,LONDON--(BUSINESS WIRE)--Technavio's latest report on the global speech recognition software market provides an analysis of the key ...,Technavio Announces Top Four Emerging Trends Impacting the ...,LONDON BUSINESS WIRE Technavios Technavio USD CAGR Amrita Choudhury Technavio Technavios ICT Voice Voice Amrita Computer Hybrid Technavio,-1
321,"Pri podjetju Microsoft so pred kratkim potrdili, da bodo uporabniki operacijskega sistema Windows 10 lahko prenesli teko priakovano nadgradnjo Windows 10 Creators Update e 11. aprila letos. Nadgradnja bo prinesla zvrhan ko nadvse zanimivih novosti, ki bodo e tako uporaben operacijski sistem naredile e bolj zanimivega, varnega in opravilnega. Ker pa se na spletu e dalj asa irijo vsemogoe govorice o tem, kakne osebne podatke bo o nas zbiral operacijski sistem Windows 10 Creators Update, so pri Microsoftu sprejeli odloitev, da nam to razkrijejo, pri tem pa je zanimivo to, da bomo kot uporabniki lahko izbirali katere osebne podatke bomo poiljali oddaljenim strenikom podjetja Microsoft. Nadgradnja Windows 10 Creators Update nam bo prinesla povsem spremenjeni grafini vmesnik, preko katerega bomo lahko nadzirali nao zasebnost. Tu bomo namre lahko izkljuili poiljanje vseh tistih osebnih podatkov, za katere menimo, da jih podjetje Microsoft ne potrebuje. Tu bomo lahko izbirali med naslednjimi nastavitvami: Location (lokacija): Ta bo operacijskemu sistemu Windows in aplikacijam omogoila deljenje nae trenutne lokacije, pri emer bodo podatki posredovani Microsoftovim strenikom. Speech Recognition (prepoznava govora): Ta bo digitalni asistentki Cortani in trgovini Windows Store omogoala prepoznavo naega govora. Tailored experiences with diagnostic data (uporabnika izkunja glede na zgrane podatke): Ta bo Microsoftu omogoila prilagoditev delovanja operacijskega sistema naim potrebam. Relevant ads (ciljni oglasi): Ta bo aplikacijam omogoala prikazovanje reklamnih oglasov glede na nae zanimanje, kar naj bi jih naredilo bolj privlane za nas. Poleg omenjenih nastavitev nam bo Microsoft glede diagnostinih podatkov ponudil dve monosti, in sicer Basic (osnovna) in Full popolna. Basic: Zbiranje diagnostinih podatkov bo omejeno na tiste podatke, ki so kljuni za razumevanje naprave in njenih nastavitev. Ti podatki vkljuujejo osnovne informacije o napravi, informacije o zdruljivosti programske opreme in trgovine Windows Store. Full: Pri tem nain bo Microsoft zbiral podatke iz kar devetih kategorij, in sicer skupni podatki, nastavitev programske opreme in podatki o inventarju, podatki o izdelkih in storitve, podatki o spletnem iskanju, podatki o uporabljenih podatkih, povezavah, tipkanju in govorjenju ter podatki o licenciranju in nakupih. Z novimi monostmi bo podjetje Microsoft uporabnikom operacijskega sistem Windows 10 omogoilo veji nadzor nad zasebnostjo, kar je tudi v skladu z novo evropsko zakonodajo o varovanju osebnih podatkov. Pri tem se je potrebno sicer zavedati tega, da manj podatkov ko bomo delili s podjetjem Microsoft, slaba bo uporabnika izkunja. Zaradi tega dobro premislite o tem, katere podatke boste delili s podjetjem Microsoft in katere ne. ",http://www.racunalniske-novice.com/novice/dogodki-in-obvestila/ze-veste-katere-podatke-bo-novi-windows-10-zbiral-o-vas.html,Speech Recognition (prepoznava govora): Ta bo digitalni asistentki Cortani in trgovini Windows Store omogočala prepoznavo našega govora.,"Že veste, katere podatke bo novi Windows 10 zbiral o vas?",Pri Microsoft Update Nadgradnja Ker Update Microsoftu Microsoft Nadgradnja Update Tu Microsoft Tu Ta Windows Microsoftovim Ta Cortani Windows Store zgrane Ta Microsoftu Ta Poleg Microsoft Basic Full Zbiranje Ti Windows Store Pri Microsoft zbiral Z Microsoft Pri zavedati Microsoft Zaradi Microsoft katere,2
322,"Sensory Inc. announced it has launched TrulyHandsfree SDK, the new version of its embedded small-footprint voice user interface platform. Designed especially with software developers in mind, TrulyHandsfree significantly simplifies the development of speech recognition applications. As such, it significantly reduces the amount of time required to implement a speech-controlled user experience. TrulyHandsfree SDK features several new improvements, including support for fixed triggers, user enrolled triggers and commands (phrase spotting technology makes TrulyHandsfree highly robust to noise); speech trigger and commands can be added to an application with less than 20 lines of code; tasks details are managed with streamlined modular speech recognition functions; easier to digest documentation; all speech recognition models and task configurations are passed as one file to further simplify the API; and available JAVA API for Android ensures seamless integration with Android Studio (no JNI programming needed). Integrating always-on, always-listening speech triggers and speech command functionality into the user experience of software and apps used to require a deep understanding of how speech technologies work, said Jacques de Villiers, engineer of technology deployment at Sensory. With Sensorys latest TrulyHandsfree SDK, weve made it simple for developers to integrate these must-have UX technologies into their products. Weve worked closely with our partners to understand what they wanted and completely redesigned our SDK to incorporate dataflow and inversion-of-control techniques to greatly reduce the overhead and complexity of designing TrulyHandsfree technology into applications. Sensorys TrulyHandsfree trigger technology has become the most widely adopted keyword voice wakeup technology in the speech industry, with over a billion products using TrulyHandsfree shipped over the past several years from the worlds largest CE manufacturers. The TrulyHandsfree trigger is a highly noise robust, low power, and accurate, embedded phrase recognition technology that listens for a specific wake up phrase, while ignoring other conversations. It can be used in many different ways, supporting various wakeup voice triggers including fixed triggers, user-enrolled fixed triggers, user-defined triggers and speaker verification passphrases. The trigger also supports high accuracy noise immune command sets and works seamlessly with the applications processors of devices and cloud-based speech recognition technologies, which enables seamless trigger-to-search or trigger-to-cloud speech recognition functionality. The technology can also be used with Sensorys TrulySecure speaker verification security technology to allow for trigger-to-speaker-verification applications. In terms of private security measures, TrulyHandsfree operates embedded on device so no cloud connection is needed, eliminating the chance of any shared personal data being stolen from the cloud. Available for Android, iOS, Linux, QNX and Windows, TrulyHandsfree SDK currently supports a myriad of languages including US English, UK English, Arabic, Dutch, French, German, Italian, Japanese, Korean, Mandarin, Portuguese, Russian, Spanish, Swedish and Turkish. In addition, ultra-low-power deeply embedded ports of TrulyHandsfree are available for DSP/MCU IP cores from ARM, Cadence, CEVA, NXP CoolFlux, Synopsys and Verisilicon, as well as for integrated circuits from Audience, Avnera, Cirrus Logic, Conexant, DSPG, Fortemedia, Intel, InvenSense, NXP, Qualcomm, QuickLogic, Realtek, STMicroelectronics, TI and Yamaha. ",https://www.biometricupdate.com/201606/sensory-launches-new-voice-recognition-platform-sdk,"Designed especially with software developers in mind, TrulyHandsfree significantly simplifies the development of speech recognition ...",Sensory launches new voice recognition platform SDK,Sensory Inc. TrulyHandsfree SDK TrulyHandsfree TrulyHandsfree SDK TrulyHandsfree API JAVA API Android Android Studio JNI Jacques Villiers Sensory Sensorys TrulyHandsfree SDK UX Weve TrulyHandsfree Sensorys TrulyHandsfree TrulyHandsfree CE TrulyHandsfree Sensorys TrulySecure TrulyHandsfree Android Linux QNX Windows TrulyHandsfree SDK US English UK English Arabic Dutch French German Italian Japanese Korean Mandarin Portuguese Russian Spanish Swedish Turkish TrulyHandsfree DSP/MCU IP ARM Cadence CEVA NXP CoolFlux Synopsys Verisilicon Audience Avnera Cirrus Logic Conexant DSPG Fortemedia Intel InvenSense NXP Qualcomm QuickLogic Realtek STMicroelectronics TI Yamaha,3
323,"VIENNA--( BUSINESS WIRE )-- Speech        Processing Solutions , the world number 1 for professional dictation        solutions , is proud to announce very positive feedback on its latest        voice recorder, the Philips SpeechAir, and the SpeechLive cloud        solution, which now also offers speech recognition. Both SpeechAir and SpeechLive are ideally suited to professionals who are often on the move and do not        always work from their usual office workplace. Ingo Bischof, MBA,        Managing Director of IVAM Real Estate, is delighted with the advantages        of these two solutions: ""With Philips dictation solutions it is        possible for us to log and record issues in the properties that we        maintain significantly more quickly. The recordings are then sent to our        back office, saving us a considerable amount of time, as transcription        can begin before we return to the office."" The SpeechAir is equipped with three professional-quality        microphones providing ultimate sound quality in any recording        situation . Background noise is filtered and the voices are recorded        in perfect quality. This ensures excellent playback and speech        recognition results. At the start of the year, the SpeechAir was awarded        Nuance Communications' rating for best recording        accuracy . The slide switch provides an efficient and intuitive user        interface operated with just one hand. Recordings can be started,        paused, stopped, played back, rewound and fast-forwarded using the slide        switch . The Wi-Fi and Bluetooth connections allow wireless transfer of finished recordings and access to        the customer's data in the respective document management system, email        or calendar. Dr Doris Ulreich-Laussermayer, a doctor in obstetrics and        gynaecology, appreciates this function. ""I can send additions to the        PC in the medical practice via Wi-Fi on the move or from home and my        assistant can edit them. Another thing that I especially like about        SpeechAir is its portability, as I can move freely around the practice.        The SpeechAir's antimicrobial surface is another positive aspect for        hygiene in my medical practice,"" says the doctor. SpeechLive speech        recognition  speak, send, receive the finished document The Philips SpeechLive cloud dictation solution now includes speech        recognition available in 21 languages. Spoken words are converted into a        written text in next to no time. SpeechLive can be used at any time and        is easily accessible via any web browser with no software installation        required. ""Our company is split between two different locations. Last        week I was working in the office in Melbourne; this week I am outside of        the city in Mount Waverley. It's great to be able to work from anywhere,        without having to think about taking something with me. I can sign in        from any computer,"" raves Nicole Honan, Office Manager at the        law firm Hicks Oakley Chessell Williams in Australia. For more information about our solutions visit: Speech        Processing Solutions is the global leader in professional dictation        solutions. The company was founded in 1954 in Austria as a Philips        subsidiary, and has been a driving force for innovative speech-to-text        solutions for 60 years. The company developed ground-breaking        products such as the mobile Philips        SpeechAir , the Philips        Pocket Memo voice recorder , the Philips        SpeechMike Premium USB dictation microphone and the Philips        Dictation Recorder app for smartphones, thus meeting its demands for        excellence and superior quality. Thanks to the newest innovation, Philips        SpeechLive , dictations and recordings will become faster and easier        than ever before with cloud-based workflow services. Speech Processing        Solution's perfectly tailored offers and products help professionals        save time and resources and maximize efficiency. Connect with Speech Processing Solutions on: ",http://www.businesswire.com/news/home/20160927005172/en/Speech-to-Text-Solutions-Philips-Speed-Users%E2%80%99-Daily-Workflow,New Speech-to-Text Solutions from Philips Speed up Users' Daily Workflow ... This ensures excellent playback and speech recognition results.,New Speech-to-Text Solutions from Philips Speed up Users' Daily ...,VIENNA BUSINESS WIRE Processing Solutions Philips SpeechAir SpeechLive SpeechAir SpeechLive Ingo Bischof MBA Managing Director IVAM Real Estate Philips SpeechAir Background SpeechAir Nuance Communications Bluetooth Dr Doris Ulreich-Laussermayer Wi-Fi SpeechAir SpeechAir Philips SpeechLive Spoken Melbourne Mount Waverley Nicole Honan Office Manager Hicks Oakley Chessell Williams Australia Processing Solutions Austria Philips Philips SpeechAir Philips Pocket Memo Philips SpeechMike Premium USB Philips Dictation Recorder Philips SpeechLive Speech Processing Solution Speech Processing,5
324,"The following blog post, unless otherwise noted, was written by a member of Gamasutras community. The thoughts and opinions expressed are those of the writer and not Gamasutra or its parent company. Plan Be, a game I created that will be out in January for free, is a voice controlled game. http://gamejolt.com/games/plan-be/86295 Its one of the few out there, and if Im not mistaken, the only one that focused on storytelling via speech input. Firstly Ill talk a bit about Speech Recognition, then Ill talk about voice and games and then well dive in the storytelling + voice = experience part of my game. If you are looking for technical speech recognition information you will not find any in this article. Feel free to contact me with any questions. Voice or speech recognition is the ability of a machine or program to receive and interpret dictation, or to understand and carry out spoken commands. The way it does that is very complex, so in order to avoid the trouble of creating that from scratch (which I will be honest, is not in my set of skills) I have used the Windows Speech Recognition software. That is a way of me telling you that you wont be able to play my game if you dont have that installed. But, it is already pre-installed on almost all PCs that have Windows so you'll be fine! In my game I use this software to recognize the players voice and phrases they say. Of course, the player cannot utter any phrase they want and expect the game to do something. There are various phrases and words that have been written in a file that the game will understand. If you are thinking that all the above create restrictions, you are right. You are expected to talk in English - Windows Speech Recognition will use the language installed on your PC so if its not in English it wont work. You are expected to talk clearly  you can whisper or shout but you must do so clearly. You are expected to say the exact phrase the game asks you  if you leave words behind it wont work. You are expected to calibrate it to your own voice for better recognition. But those are ok. Really. English is not my native language but it recognizes me just fine. It is well responsive, something that at first I was not expecting. It can understand a variety of words and phrases that exist in a huge database. Trust me, I have tried to narrate a paragraph from Romeo and Juliet and it understood everything. It really is a great experience but it does have flaws that might break or make the game for some. Bad English accents, not speaking clearly, low quality microphones, noise around the room are not problems to be dismissed. I have tried different microphones and my old one wasnt catching the phrases as good as my newer one. Its not a nice experience. It can get frustrating repeating the same words or trying to speak with an accent only for the game to not understand you. I accept those limitations because this is speech recognition after all. Its not perfect and we cannot expect it to be. Thats why I can understand if for some it might break the game. The fact that you have to repeat phrases exactly like the game demands, was one of my concerns, from a design standpoint. Not because you have to repeat them exactly like they are, but because you have no choice in what you say. This might be counter-intuitive for some players, might be uncomfortable or pressuring. It would be amazing if we uttered whatever we wanted and the game understood it and then did something, but I do not believe that this is possible right now. Speech recognition is very specific and sensitive and not only that, but when you have a defined experience it isnt possible to leave the chance to the player. Imagine a character asking you something and you have to respond back. What do you say How do you say it Does what you say matter to the gameplay and does it change the story Those things sound vague and difficult to do. In my game I have added something that tries this method called the hidden dialogue. Ill mention it later in a bit more detail. So with all the above in mind, I had a clear picture of how I could use this software to create a game in which you can talk. I knew the restrictions, I knew what I could use, the responsiveness, the flexibility. Now it was time to build the game. Easy right When we say using your voice in games what comes to your mind Do you think about various commands, like run, shoot, go right, jump Do you think of existing games and try to think what it would be like to use your voice to make the character shoot or walkDo you think of other new innovative ideas where you could use your voice everywhere in your game Thats all cool! I thought of that too but I only used some to none of that and Ill explain why. As we have seen above, voice recognition is pretty responsive but it still has flaws. Imagine playing a game where you talk to your PC but it responses slowly, might not understand the speech and might even process the speech as something completely different from the intentions of the player. Lets play out an example and see how it goes. Ill take an existing game and give it some voice commands.Game: Call of Duty.Voice commands: run, shoot, jump, pause game. So we start the game and we play and suddenly a wild enemy appears! Oh but where do I shoot Left Right Up, down Screen width divided by 2, minus 30 pixels to the right Where do I shootOk fine, lets say the targets appear only on the left, right, up and down side of the screen. The game takes a second to register that but at the same time the enemy saw me and starts shooting back. I yell Run! But where do I run Ok weve been through that before, so lets just add left, right, up anddown to this one too. Run left! I yell, and my character runs left behind cover and the enemy advances. Phew, I probably lost some health getting shot already but I still got some time before he gets me. Shoot right! I repeat and nothing happens. It didnt register. I say it again but by the time I do, the enemy comes and continues to shoot me. Pause game I say and it takes me 1 second to say it, 1 second until it registers and now Im dead. Had so much fun playing! Not only an FPS is a fast-paced game, it needs absolute precision. FPS wouldnt be the genre for games with voice controls because simply the input bypressing a button is better than the input of speech when trying to shoot a gun. At least right now, with our technology. Ok, so shoot, run and 360 no scope may not be the best things to voice to get an immediate response. What about using the interface with voice, like I tried to do so with Pause game Sounds very cool! You can do it in your game, but I didnt. Why Because the button input in this case is still better than the speech input. If I need to visit the bathroom, I need to pause the game RIGHT NOW. Innovation sounds very nice, but Id rather have user satisfaction as a priority first. You can of course, use both voice and buttons, which seems like a better decision but Idecided to implement voice only where I deemed it necessary and where it felt like the actually context supports it (something I'll talk about later). If any action in your game can be done better with any input but speech input, youre using speech recognition wrong. That doesnt mean you cant do it or you shouldnt. This is just my opinion and what it meant for me to create a game that utilizes voice recognition. So then, what kind of game can we create that takes all this into account and how can we create it Maybe we should stop thinking too hard and start thinking simply. What do we do with voice in real life Ill give you 3 chances to find at least one good answer. My answer to that was we speak, like in conversations. Now, of course there are complications with that too but all of the complications with this are the same with any voice controlled game. What if the player is shy What if the player doesnt want to speak What if they dont know the language or they do but they have different accents Those are all valid but they are still the same problems youd encounter with any voice controlled game. Can I avoid them I guess I can avoid them as much as I can avoid creating a Kinect game which requires motion and gestures but players are bored to move. So keeping all of those things in my mind, I decided to follow the idea of having conversations and if by speaking to a character we can create a new type of storytelling in games. If by playing a game in which you have to talk to a character will create a bond with them, or immerse you more into the world, if its fun to do, if its engaging and interesting or none of that. As we all know though, storytelling in games isnt only the cutscene dialogue Ok so lets define video game storytelling before we move on- at least the way I see it. Video game storytelling isnt just the writing and dialogue. Because games are not a passive entertainment but an active one, in which you participate, they are interactive, the gameplay and the mechanics of the game are just as important to storytelling. Storytelling in games is the dialogue, the background chat of the crowd, the way the character acts or speaks, its the world and level design,the art style,the mechanics,the objectives,the lighting, the UI, the music, the animations, the clothing design, the pacing- ok its a LOT of things! So creating a story-driven game is not as simple as writing the script and be done with it. The whole development team has to be in on it and think as storytellers. In my case, I had to think of all these factors and aspects to create the experience. Ill mostly talk about the voice though. Thats why Id rather use the word storytelling and not writing because writing is too specific and points to the script of the story.I actually started creating the main mechanics of the game BEFORE having any clue what I would write for the story. I only had my pitch idea which was this: Location: CDC Underground Facility. Sitting behind a computer desk, locked into the security room, you try to guide a scientist to safety as an accident caused most of the personnel to have unstable and violent conditions. Now, even if I started creating the game before the story, I made sure I had this pitch to guide me simply because this is the core concept of the game and as we said storytelling isnt only the script. A very important aspect I had to take into account before creating anything was what kind of game can I create I had to think of a concept that would actually justify the voice use. We mentioned above Call of Duty and how voice might be a little problematic with that game. If you think about it, its not only problematic because of its genre, but because there is absolute no reason to use it in the game. What is the context that drives it What is the reasoning behind it and how does the player understand that reasoning without explaining Why do you absolutely need to use voice If I didnt buy into it and it didnt feel normal I wouldnt want to play it. So the above description of a game that is a not-so-unique idea popped in my head. In movies you usually see scenes where a guy in front of a computer guides someone else with a walkie talkie or with an ear piece. This absolutely clicked with me and interested me because it made sense! I would use my voice to guide someone and talk to them. From a concept, gameplay and story standpoint it fits perfectly! If you use speech recognition for the sake of using it you are doing it wrong. Give it a reason to exist. Nobody wants to talk to a computer, they feel silly. Tackle that by finding a good reason for them to talk. In my game you literally talk to a character, its no extra addition or an afterthought; its the very main mechanic. You will never doubt why you have to use voice to do that, simply because you cannot do it in any other way! The pitch helped me see clearly a lot of things about the game mechanics and how the voice takes part in the game world.Having the pitch in my mind was enough to make me create all the basic programming, mechanics and even the art style of the game. The art style became a big UI in which you could see a log screen of the dialogue, a documents folder with picked up documents, the controls of the game, the microphone and phrases youd utter and the character stats which are Stamina and Oxygen. This is what it looks like: It didnt look like that at first, it has been through a lot of re-iteration until I decided to make itmorefamiliar with a Windows 8 style UI to represent a window you could actually have on your PC. There are some small details here and there too that make it more alive like the real date and time you play the game, FPS counter, location of the character, the name of the program used to keep track of the facility, some tips appearing every once in a while for the player, etc.Those too add to the storytelling. If youre supposed to be looking at a radar on your computer it must appear like you are looking at a radar on your computer. So not only the UI must look like it but also the game itself, which is that window on the left that takes the most space on the screen.It shows a map/blueprint of the facility, the enemies and the main character, the doors, loudspeakers and pick up items. The facilitys design went through iteration too (as everything really) but Im talking about it because at first it was just simple blue. But it didnt look like an actual place. Adding environmental items to it and some blueprint style lines, made it look like a real place, gave a sense of space and made it feel residential; a place people could work. Yeah that matters to the storytelling too. The static that appears on the radar once in a while matters too, the fact that the character blinks only when he is stationary matters too, the fact that enemies are red and triangular to show danger (even though a bit clich) matters too. Everything matters and thinking about all these things to make it more polished is what will make it a better experience. All these details came a lot afterwards as this is mainly polishing and re-defining things. The main concept and mechanics of the game went like this: Pretty clich stuff and basic but I was ok with them because the focus would be the voice. I mention the word clich a lot. Its true. I wanted innovation but at the same time I wanted to keep familiarity. It would help the players mind ease into the concept while adding something new to it. Its not wrong to do something completely new and by doing what I did does not mean I took the safe or easy route. It means that I decided it would be better for the idea I had in mind, the story and the mechanics. Remember, this project is only the beginning of what we can do with voice recognition. Lets build on it. So, all those mechanics add to the story and the story helps the mechanics and here's how that happens:The fact that I am in the control room and the facility is on lockdown, means only I have access to some of the facilitys features like the doors and loudspeakers. Using those, I can help the scientist move forward. That also means something else; that the scientist is relying on me getting him out. That in return means that I am someone willing to help. It also means that specific characters attributes for both the main characters are already in place and I didnt even have to think about them. The mechanics demand it in the story. Clearly we can see from this trail of thought how story derives from the mechanics. It went from the simple act of opening doors to the characters personality traits. Then those personality traits can be used to define the core character which you use to flesh out, give backstory to support that and then let them loose in the game world you created to make decisions. My journey of creating the game story, started not only from those gameplay mechanics but also from the very main mechanic of it - the voice. And thats all it took. I used it as a restriction and not a feature because I like working under pressure or limitations. So Im going to be talking to a character in the game but what else Talking to a character is a story voice mechanic and I needed a gameplay voice mechanic too. That became the dictation or commands you have to give to the character. So you dont only talk to them, you utter commands like run, keep going, stop, wait, stop running, move back etc. In case you noticed, the above commands arent like the ones we have talked about above in our Call of Duty example. They arent shoot right, move left. They are more natural. In the very beginning (not of the universe) the gameplay consisted of looking at your level, and finding markers on it where the player could move to. Then you uttered for example Marker C4 and the character went there. Imagine playing like that, moving from marker to marker like a little soldier. Thats not actually bad if you are creating a strategy game where you have to move into positions. And my gameplay consisted of that until I have written the first draft of my story. Then I had to reconsider the gameplay. Lets revisit the pitch idea: Location: CDC Underground Facility. Sitting behind a computer desk, locked into the security room, you try to guide a scientist to safety as an accident caused most of the personnel to have unstable and violent conditions. The keyword is guide. You are supposed to be helping the scientist not commanding him. Saying Move to Marker C4 is simply not natural. A scientist in that facility that works with you, knows where to go if the story demands they go, lets say, to a specific laboratory. He isnt a fool that needs to be told to move every 5 meters. The characters are real people and the scientist is not your pawn. You are working like a team trying to survive this. So the gameplay had to change and they way you talked had to change. The facility is dark, its under lockdown and he knows where to go but he is afraid; he needs your help as you can see the facility on the computer monitor and any other movement. He starts moving towards the open door but an enemy is close and you tell him to stop. The enemy leaves and you tell him to continue because now it is safe to move. After a while it seems safe and he speaks to you, wondering what is going on. You reply back. Now this is the experience I wanted to create. One step towards this experience was having voice commands sounding not like commands. The marker system didnt completely perish out of existence though. I used it for another aspect of the game. Say you need him to deviate from the path he is taking towards that laboratorybecause theres a pick up item in a room. You tell him to wait and then mark the spot on the level and tell him to go there. He picks it up and you tell him to continue and he does.So there is a commanding feeling but now its minimal and happens for a reason. In one of the paragraphs above, I have mentioned the hidden dialogue. This basically means that you can ask anything you want and as long as you say it in a right way and that something is written as a valid phrase to use, then the character will respond back. There is nothing that forces you to say it, its completely optional. Those are things that you might be thinking about or facts from the story and the game world. You can go ahead and talk about them. This is very experimental and somewhat difficult. Like I mentioned above speech recognition is sensitive, so you need to say something specific for it to register. But the main problem isnt this. The main problem is the players thought. What in the world will the player thinkto say, how will they say it, and how do I know that That can be answered with playtests. Watching people play the game and asking them to write down what they would ask. That would be great but unfortunately due to time limitations I had no such luxury. I did take player feedback for that but it wasnt as extensive as I would have liked. Moving on I had to make the gameplay more natural and the character feel humane. Talking to him and not really commanding him are good steps towards that butI decided to take it to a next step. The character is a human being. He knows where to go but he is counting on you to tell him to stop or move. Fair enough. What happens in the game if an enemy grabs him and he starts losing health - health is the oxygen in the game for various spoiler-y reasons,go play the game when it comes out okay Me as a player can tell him to run or keep moving and he will do so. What if I dont tell him anything, will he just stand there taking damage Thats what usually happens with game characters but in this game the scientist IS NOT the character you ""embody"". You playyou, yourself, and the scientistis someone else. There are two main characters in the game: the scientist and you sitting behind the PC monitor. So in order to make him feel real, I made it so if you do not tell him to run or move after a few seconds he will do so himself. Would anyone just stand there dying No. In the game as a player you also let the character know when he will use an oxygen pack. If the oxygen levels in his suit fall to a very small percentage he will use an oxygen pack all by himself. Like I said,he is a human being and wont let himself die if you dont tell him. Does all this sound natural It does to me. Does it raise some questions like does that take control from the player Yes it does raise questions. I had to answer that too when I was developing the game. Of course, I mustnt take control from the player but you have to understand that this actually gives more control to the player. Why Because when an enemy grabs you, you start losing oxygen rapidly. If you let the characters oxygen reach a very small percentage and then an enemy grabs you, you are most likely a lost cause. In order to avoid that, you have to keep the oxygen at normal levels, so as a player you keep an eye on that and make sure it doesnt reach low levels. This makes the player vigilant and careful and they have to decide when to use oxygen. On the other hand if they dont, the game wont completely punish them, instead the scientist will take the initiative and use an oxygen pack. This is how I balanced it to make sure the control is in the player's hands but at the same time you feelthat the character in real personand notjust another AI. Same with the character getting away from an enemys grasp. He wont escape immediately; he will do so after some seconds and only if he finds another place to go to. If he doesnt find a good spot to move he wont. The seconds the player has to wait are also more than the normal amount it would take to utter a command meaning if the character escapes, you have deliberately waited so he would do that (or you panicked and you didnt know what to say  trust me it happened). You can obviously see how I approached the rest of the game too. You might wonder does it really take this much thought to create such a simple game I agree the game is very simple actually. Some gameplay aspects are seen in other games, some decisions made are very basic when designing a game and some of the things above are really common sense (maybe). So why am I writing all this Because this game is very experimental. I have not played a game that attempted to combine story and voice recognition before. I didnt know where to start from in order to create it. Where are the blueprints and the design documents that state what I have to do This isnt another third person game and think how many of those we have but how difficult is to create a new third person IP. This is why Im writing this. Im not here to say that the game is perfect because I have thought of all the above beforehand. Im not here to say this is a great story-driven game made with voice. No. Maybe youll hate the story, maybe youll hate the gameplay or maybe youll love everything! Im here to say this is the first game that attempts that and whether people say its awful or incredible, this wont matter but what will matter is going to be the fact that we can take this feedback and make the next story-driven, voice controlled game even better. The game will be out in January for free. Check it out here: http://gamejolt.com/games/plan-be/86295 Tips for creating a voice controlled game: Speech recognition is not perfect yet. But its really good and you should use it. If any action in your game can be done better with any input but speech input, youre using speech recognition wrong. If youre going to be talking to a character they must be less AI and more humane. The mechanics of your game arent separate from the voice use. Make sure all makes sense and be consistent in the how you use voice. People feel silly talking to the PC. Give them a good reason; justify the use of voice in your game. If youll use voice make sure its not an afterthought for your game. Decide if youll use it for speech or dictation or both. Menu controlling needs thinking. Think about how voice is going to make it a better experience; dont just add it for the sake of adding it. Make sure the grammar you use for valid speech is something that people feel natural saying. Playtest it! Manage active words and phrases with rules. You do not need the dialogue phrases when you are not engaging in a dialogue. Think of how you will teach your players the grammar set they can use if you have too big of a grammar set. Make speech failure into a feature; make the character say sorry didnt hear you or add static to show that it wasnt registered. Pick your grammar carefully. Do not add sea and see for obvious reasons. Dont expect people will get accustomed to it easy. Ease them into it. Give players some breathing room between the times they have to use their voice. Deluxe Entertainment Services Group  Santa Monica,                                                                                                               California,                                                                         United States Intrepid Studios Inc  San Diego,                                                                                                               California,                                                                         United States ",http://www.gamasutra.com/blogs/ValentinaChrysostomou/20151207/260585/Speech_Recognition_and_Storytelling_in_Plan_Be.php,"Firstly I'll talk a bit about Speech Recognition, then I'll talk about voice and games and then we'll dive in the storytelling + voice = experience ...",Speech Recognition and Storytelling in Plan Be,Gamasutras Gamasutra Be January Im Ill Ill Feel Voice Windows Windows English English Really English Trust Romeo Juliet Bad Thats Ill Easy Thats Ill Imagine Ill Duty.Voice Right Up Screen Ok Run Phew Shoot Pause Im Had FPS FPS Ok Pause Sounds RIGHT NOW Innovation Id Ill Kinect Ok Video UI LOT Ill Thats Id BEFORE CDC Underground Facility Call Duty Stamina Oxygen Windows UI FPS UI Im Pretty Remember Im Duty Marker C4 Imagine CDC Underground Facility Move Marker C4 Say Fair Me Thats IS NOT No AI IP Im Im Im Im January Tips AI Menu Make Manage Think Make Dont Deluxe Entertainment Services Group Santa Monica California United States Intrepid Studios Inc San Diego California United States,4
325,"Your message has been sent. The Siri effect is spreading. Starting this fall , Nuance, a company that makes advanced speech recognition and transcription software, will bundle its Dragon Assistant, a Siri-like, vocal sidekick, with select Intel-powered Ultrabooks and All-in-One desktops. Dragon Assistant could change how people use speech recognition in everyday computing. Even if youre a good typist, speech recognition can enhance your productivity by letting you handle everything from basic commands to emails without touching the keyboard. My husband was an early and unexpected convert. In 2010, he had surgery on his left shoulder and was unable to use his left hand for a couple of months. His company bought him a copy of Dragon Dictate. He liked it so much that he still uses it today, long after his shoulder healed. He says it makes him faster and more productive (and he already types, on average, 130 words per minute), and its also easier for him to work in non-traditional environments, such as in the car or outside by the pool. Everyone worries about the training timeand there is some of thatbut it varies from product to product. Choosing the right program will ensure that the benefits are worth the investment.These tips will help you get the most out of Nuances speech recognition programs, whether youre using the traditional desktop program, the mobile app, or the new, Ultrabook-bundled Dragon Assistant. PC users have five Dragon products available. Dragon Notes ($20) is a sticky-note program with speech recognition capabilities. Dragon Home ($75) is for basic userspeople who are looking to write emails, school papers, and grocery lists, and use basic voice commands to control their computer. The $150 Dragon Premium adds support for working in spreadsheets and presentations, while the $600 Dragon Professional adds advanced custom commands and transcription tools. The $800 Dragon Legal includes everything in Dragon Professional, along with preloaded legal terms and the ability to automatically format legal citations. Intels reference design for the sort of Haswell-based Ultrabooks now shipping with Dragon Assistant built in, as shown at CES 2013. Nuances newest offering, Dragon Assistant, works more like Nuances mobile apps , like Dragon Mobile Assistant for Android and Dragon Dictation for iOS its a verbal gofer, rather than a dictation app.According to Sarah Gaeta, vice president and general manager of the Dragon Desktop department at Nuance, anything beyond very quick email replies is better done with Nuances more advanced products. According to Gaeta, theres no substitute for spending some quality time training your Dragon program of choice. Unlike Siri, Dragon software is designed to learn your voice as you use it, so it can better respond to your commands and dictation. The more you use Dragon, the more accurately it recognizes your accent, how you pronounce different vowels, the cadence of your speech, and any vocal quirks. Dragon stores that information in your personal profile, and it can store profiles on multiple people so they can all use Dragon on one computer. Set up your own profile in Dragon and use it for a few hours so the software can learn your unique speech patterns and vocal mannerisms. Take a few hours to use it as much as possible: dictating short emails, practicing voice commands, or even just speaking to the program like you would to another human being. According to Gaeta, Dragon Assistant requires about 30 minutes,while initially training Dragon Dictate or Dragon NaturallySpeaking might take two to three hours. Its best to do this initial training in a quiet room, so Dragon can learn your voice and, more importantly, learn what isnt your voice. Once you train your Dragon, youll be able to dictate to your PC in a noisy coffee shop, and it will pick up onlythe words you say, ignoring the loud, oversharing couple behind. If you want Dragon to understand you, speak slowly and enunciate clearly. This sounds like common sense, but its not, Gaeta says. Its crazy; we put someone in a room with a microphone and they feel like theyre performing. They get nervous and speak way too quickly. Dragon software will walk you through a few simple tutorials that will train you to speak slower and more clearly, which will improve your productivity when using Dragon products (as well as your real-life conversation skills.) Learning to speak slowly and enunciate is more challenging than it sounds. Theres a reason that people take public speaking lessons, and while I wouldnt recommend doing that just for the sake of improving your productivity, a few great, free speech tutorials on YouTube will help you spiff up your speech patterns.Speaking clearly also makes a difference with Dragon Assistant. Working with speech recognition software is ultimately faster than typing, but it wont feel that way at first. When youre typing, youre probably notthinking through entire sentences before you type them. Theres no reason for you to have a properly formulated sentence in your head when you start writingyou have plenty of time to perfect it while you type. If Dragon fails to accurately determine which word you meant to write, you can use the built-in voice commands to correct the error and Dragon will learn from the mistake. You cant just say one word at a time as you speak a sentence, thoughwhether to a real person or to Dragon software.Dragon uses context to determine which words youre saying, so it needs full sentences to be able to distinguish whether you mean youre or your, or theyre, their, or there. Luckily, you have some help: In the desktop clients (Dragon NaturallySpeaking and Dragon Dictate for Mac), you can use the keyboard alongside the voice dictation. You do have to be aware of when youre using the keyboard, though, because basic commands (such as scratch that to erase the words you just said) wont work for typed text. The traditional Dragon programs are most useful for email, or email-like tasks, because spoken English is so different from written Englishmany things that can be said cannot be written, and vice versa. For example, its acceptable to say, I got this from the restaurant we ate at last night, but youd have to write I got this from the restaurant at which we ate last night. Likewise, writing My brother, an athletic director, lives in Tokyo, looks better than if you said conversationally, My brothers an athletic director who lives in Tokyo. Dragon Professional includes a streamlined transcription module thats built for writing long-form articles and essays. Because spoken and written English are so different, native speakers will find that Dragon is better suited to writing that mimics spoken language. So youll be able to fly through tasks such as note-taking and emails, but youll find it much more difficult to dictate formal or creative writing. Both Dragon Professional and Dragon Legal include a transcription agent module that makes recording and revising such writing much easier. Dragon Assistant is useful for more basic tasks, such as Web searches, social network status updates, and writing short, quick emails. Like Siri, Dragon Assistant accepts natural-language prompts. For example, you can say Whats the weather like today instead of Weather in San Jose on September 10. Nuances other Dragon programs dont yet accept natural-language promptsyoull have to learn their built-in commandsbut Gaeta says its coming soon. Even the speediest typist can benefit from Dragon software by learning to use it in situations when accessing a full QWERTY keyboard is impractical. I type about 125 words per minute when Im at my desk, but when my hands are otherwise occupiedeating, getting ready for workor when Im in a bumpy car ride or similar situation that makes typing difficult, its nice to know that I can still get some work done. Just remember that Dragon is a tool, not a crutch; Nuance may be trying to build software that loves you , but its not going to do your homework for you (yet). Proofread and double-check all Dragon-dictated documents before sending them out. To comment on this article and other PCWorld content, visit our Facebook page or our Twitter feed. ",http://www.pcworld.com/article/2048713/speech-recognition-the-new-pc-power-tool.html,"The Siri effect is spreading. Starting this fall, Nuance, a company that makes advanced speech recognition and transcription software, will ...","Speech recognition, the new PC power tool",Siri Nuance Dragon Assistant Ultrabooks All-in-One Dragon Assistant Dragon Dictate Dragon Assistant Dragon Dragon Notes Dragon Home Dragon Premium Dragon Professional Dragon Legal Dragon Professional Intels Ultrabooks Dragon Assistant CES Dragon Assistant Nuances Dragon Mobile Assistant Android Dragon Dictation Sarah Gaeta Dragon Desktop Nuance Nuances Gaeta Dragon Siri Dragon Dragon Dragon Dragon Dragon Gaeta Dragon Assistant Dragon Dictate Dragon NaturallySpeaking Dragon Dragon Dragon Gaeta Dragon Dragon YouTube Dragon Assistant Dragon Dragon Dragon Dragon NaturallySpeaking Dragon Dictate Mac Dragon English Englishmany Likewise Tokyo My Tokyo Dragon Professional English Dragon Dragon Professional Dragon Legal Dragon Assistant Web Siri Dragon Assistant Whats Weather San Jose September Dragon Gaeta Dragon QWERTY Im Im Just Dragon Nuance PCWorld Facebook Twitter,1
326,"By utilizing speech-recognition software, public-safety officers find that they can spend more protecting citizens and less time on paperwork. Public-safety professionals play an important role in their communities, but there is a crucial part of their job that usually goes unnoticedreporting. Even though it is often excluded from the flashy television dramas, reporting conducted by police officers, firemen and other first responders is one of the most important parts of ensuring criminals are punished and emergency situations are documented appropriately. Yet, just like heavy administrative work impacts the productivity of business executives, lawyers, social workers, and financial advisors , first responders can easily become overwhelmed by paperwork, leading to long hours filing reports and limiting their time to complete other important duties. Ultimately, this can result in a rushed process, impacting their ability to file accurate reports in a timely manner. Like many departments, the Chatham-Kent Police Service (CKPS) in Ontario, Canada,faced these types of challenges with its reporting process, but was quick to embrace technologies like speech recognition to increase its efficiency and the overall quality of reporting. For CKPS, speech recognition accelerates data collection and document creation for both constables and data entry specialists while improving the detail and accuracy of reports. In the past, CKPS officers would dictate reports and send audio files to Civilian Data Entry (CDE) personnel, so that the recorded report could be typed out, word-for-word. This resulted in a significant backlog of case reports, which impacted quality and required extra time from both officers and CDEs to ensure that the information was accurate. In an effort to make improvements, CKPS initiated an overhaul of this reporting process, incorporating upgraded dictation and speech-recognition software. This enabled officers to dictate their reports using their mobile device. Audio files could then be transcribed through the CKPSserver, delivering a text transcript back to the officer. From there, the officer could edit the report text and make corrections before sending it to the CDEs. Before long, more than 70% of CKPS officers were using this new reporting system, and the early results demonstrated immediate improvements in the report-submission process. Taking a closer look at CKPSs adoption of speech-recognition technology, we can see some key benefits that emerged through its revamped reporting process, including improved quality of submitted reports and better time management, which allows officers to allocate more time to protecting and serving their community. Accurate reports can go a long way toward quickly and effectively resolving cases. These reports may ultimately end up in the hands of a district attorney or judge, who expects all of the important details to be included. By using their voice to dictate their reports, officers and other first responders can capture their case notes as soon as an incident is concluded, when those important details that can ultimately impact a courtroom decision are still fresh in their minds. This level of detail and accuracy ultimately has a trickle-up effect, as lawyers and judges can take that information and make informed decisions. ",http://urgentcomm.com/blog/speech-recognition-improves-important-reporting-processes-public-safety-services,"By utilizing speech-recognition software, public-safety officers find that they can spend more protecting citizens and less time on paperwork.",Speech recognition improves important reporting processes for ...,Chatham-Kent Police Service CKPS Ontario Canada CKPS CKPS Civilian Data Entry CDE CDEs CKPS Audio CKPSserver CDEs CKPS CKPSs,-1
327,"Robots do not treat all humans equally. This summer, Last Week Tonight host John Oliver and our very own Jorge Ramos both kvetched about how computers often don'tunderstand them. It's not because they are particularly complicated men. It's because Oliver is British and Ramos is Mexican; both speak with accents. I have to often, with automated machines, do an American accent,"" Olivertold Ramos. ""Its electronic imperialism. YouTube is full of videos of frustrated people, some with accents, trying to talk to voice recognition systems. This dude , who has an accent, wants to play music on his car, but the car hears""play all""as ""redial."" Another accented speaker gets so frustrated with his car's voice recognition system that he begs it to shut down, but instead, it starts up the navigation feature. During a live demo , Microsoft's voice-to-text app, typed up ""dear aunt"" when the speaker very clearly, but with an accent, said ""dear mom."" When the presenter tried to fix it, the machine continued making errors. Marsal Gavalda, who recently joined messaging app YikYak as chief of machine learning, has been on a mission to make people more aware of ""electronic imperialism,"" givingtalks at conferences on the topic, like at SpeechTek in San Francisco this week. ""Speech technologies have proven so useful and successful atpowering intelligent applications. At the same time, we need to be cognizant they don't work so well for everyone,"" Gavaldatold me. ""We need to prevent a 'speech divide,' a class of people for whom speech technologieswork well and another for whom they don't. You're putting those peopleat a disadvantage."" Those of us who can effectively communicate withdevices by voice are doing so more and more.Fifty five percent of teens and 41% of adults use voice search more than once a day, according to a recent Google survey . When you're driving, it's safer to use your voice rather than your fingertips, but not if the voice recognition system doesn't work for you. And that's more likely to happen if you don't speak English. That's because the more data any artificial intelligencesystem has, the better it is. American-style English has a huge advantage here. Most of the available voice data that feeds into virtual helperslike Siri, Google Now and Microsoft's Cortana is in standard U.S. English. Only Chinese even begins to rival it. It shows in how well AI voice systems recognize what we're saying. Google told me that with their voice recognition system, which works on mobile and on the web, the word error rate for U.S. English is just 8%, a number thatfactors in mistakes made onodd proper names and words, like astreet address or unique restaurant names. For Spanish and British English, it's higher, around 10%. For ""tier 2 languages""those that have gotten less attention from tech companieserror rates hover around or above 20%. That meansthe machine you're trying to talk to misses at least 1 in every 5 of the words you're saying, which rendersitbasically unusable.There are billions of peoplewho are out of luck because they don't speak the default voice recognition languagesEnglish, French, Spanish, or Chineseor speak those languages with heavy accents . And voice recognition systems embedded in cars and in call centers are particularly awfulbecause they're using an older, less sophisticated type of artificial intelligence, not the type that's powering Google search.That creates a terrible feedback loop: if machines aren't good at understanding you, you're less likely to talk to them, meaning that the systems used to train them aren't going to collect the data they needto get better. The voice recognition market is estimated to be $2.5 billion this year and it'sexpected to grow as voice capabilities move fromour handheld devices to appliances, robots, and cars. But corporatedecisions on whichtypes of languages and accents get better voice recognition isn't likely to be determined by a need toright an unfair ""digital speech divide."" Businesses will decide based on profit margins, market size,and how much money people that speak like that have to spend. Baidu, frequently referred to asthe Google of China, is focusing on Chinese because that's where their core user base is. Google told me thatit's set an end-of-the-year goal to make itsvoice recognition systems for Italian, German, Spanish, Japanese, Korean and Russian as good as English. ""After [tier 1]we'll pay more attention totier 2, especially as the next billion users will come from emerging markets,"" like India, said Johan Schalkwyk, Google's chief of speech recognition. ""These users will be coming online soon and voice may be the only way they communicate with the device."" Tech giants like Google, Microsoft and Facebook know that their next billion users will come frommarkets where ""tier 2"" languages, like Hindi, aredominant. Some of thework of adapting voice recognition systems for those new audiences isalready happening. When Google started building an Indonesian speech recognition system, the word error rate was 40%; now, says Schalkwyk, it's around 18%. Part of what's driving these initial improvements is that Google is launching products in emerging markets, like Indonesia. Google's newly revamped Translate app, for instance, ""knows"" Indonesian. Microsoft has similar goals. ""We have a pretty aggressive and not fully disclosed language expansion plan for Cortana, over next 18 months to two yearsso we're not U.S. centric,"" said Spencer King, the principal program director for Cortana. Companies are tackling accentstoo. Both Microsoft and Google are working on ways to automatically detect whether a user is speaking a given language with an accent. For Cortana, users can opt in to a feature that recognizes non-native accents. That cues Microsoft's AI to ""listen"" to the user differently, which can result in a significant increase in accuracy, King says. ""We use that information to train that model separately from the native speaker model,"" Kingtold me. The company has been doing that for about a year and half. With Google Now, you can also selectrecognition forU.S. English or Australian English, but in the future, the search giant wants to make that unnecessary and to automatically detect whether anAmerican or an Aussie is talking to it. All this requires lots of data.For asystem to work well, Google says it needs at least 5,000 hours of user voicedata. That may not soundlike a lot, but getting it is harder than it seems.For Afrikans, for example, Google says it has a hard time getting enough data to dramatically improve the system. And many recordings, in any language,aretoo noisy or the quality is too poor, so it's unusable. And yes, to make systems better, that means companies hold on to what you say to your virtual assistantto improve the AI. If you do a query by voice with Google Now, your data is logged anonymously for two years. If you opt in to the Audio History feature , you canmanagethe audio data and delete it; it'sdeleted forever, Google's Schalkwyk said. Microsoft also says that it's careful to anonymize the user data it feeds into its voice recognition systems. So, how long until every user can converse with a virtual assistant without the conversation ending in frustrated cursing The experts I spoke to say that's hard to predict. The really big improvements will come first in products developed by the likes of Google, Apple and Microsoft.Call centerswill take a little bit more time, said Tim Tuttle, the CEO of AI company ExpectLabs. But Tuttledid offer a glimmer of hope. Companieswon't have to start from scratch with every language.Many times, they can take the models they use for one language, and use them to improve another,which should quicken the pace of development. Once you've learned Spanish, picking up Italian or Portuguese is easier. Luckily, machines can, at some level, do this too. Daniela Hernandez is a senior writer at Fusion. She likes science, robots, pugs, and coffee. ",http://fusion.net/how-voice-recognition-systems-discriminate-against-peop-1793850122,"We need to prevent a 'speech divide,' a class of people for whom speech ... When Google started building an Indonesian speech recognition ...",How voice recognition systems discriminate against people with ...,Week Tonight John Oliver Jorge Ramos Oliver Ramos Olivertold Ramos Microsoft Marsal Gavalda YikYak SpeechTek San Francisco Speech Gavaldatold Google English English Siri Google Microsoft Cortana U.S. English AI Google U.S. English English Chineseor Google Baidu Google China Chinese Google English [ India Johan Schalkwyk Google Google Microsoft Facebook Hindi Google Schalkwyk Google Indonesia Google Translate Microsoft Cortana U.S. Spencer King Cortana Microsoft Google Cortana Microsoft AI King Kingtold Google English Aussie Google Afrikans Google AI Google Audio History Google Schalkwyk Microsoft Google Apple Microsoft.Call Tim Tuttle CEO AI ExpectLabs Tuttledid Daniela Hernandez Fusion,4
328,"Dr Stuart Cunningham from the Department of Human Communication Sciences and Centre for Assistive Technology and Connected Healthcare at the University of Sheffield discusses the progress on an innovative project which is using speech and language technology for articulation rehabilitation. Speech and language technology, and particularly automatic speech recognition, is being increasingly used in therapies and interventions for many different conditions. This reflects the growing everyday use of speech interfaces with smartphones and tablets. Traditionally, articulation therapy is delivered by speech and language therapists, and is used to treat clients who have difficulties producing speech sounds correctly. It involves the client repeatedly speaking the sounds or types of sounds that they have difficulty with. Through repetition, and the expert feedback of their therapist, the client slowly improves the accuracy of their speech productions. This can lead to meaningful improvements in the intelligibility of their speech, and hence their ability to communicate independently with others. Unfortunately the repetitious nature of the practice is crucial to successful outcomes for the client, but this obviously makes it time-consuming and expensive to deliver. Many speech difficulties can require some articulation therapy, but two groups of clients most frequently identified by speech and language therapists are people who have had a stroke and developed a condition known as dysarthria. The other group is children with hearing impairment. In both these groups the repetitious practice and feedback can help clients either relearn how to say difficult speech sounds, or learn to more consistently produce a specific speech sound. To address the difficulties in delivering articulation therapy, the STAR (Speech Technology for Articulation Rehabilitation) project, funded by the National Institute of Health Research, is developing an app based on novel speech recognition technology. Researchers at Barnsley Hospital and the University of Sheffield have developed automatic speech recognition technology that can objectively score speech productions. Research has shown that feedback based on these scores can be used to help people modify their speech productions. Programmers at Therapy Box Ltd, an award-winning developer of communication apps, are currently developing the app. The app will prompt the client to produce a word either visually, or by playing an example for them to imitate. Then, every time they speak the word, visual feedback is given on how close what they said is to the target specified by their therapist. In the app, these speaking exercises will be presented as motivational games. Clients could then try and achieve a particular target in their practice sessions  or they may have targets set by their therapist in terms of number of repetitions or a particular score to achieve. Dr Stuart Cunningham from the Department of Human Communication Sciences and Centre for Assistive Technology and Connected Healthcare at the University of Sheffield discusses the progress on an innovative project which is using speech and language technology for articulation rehabilitation. Speech and language technology, and particularly automatic speech recognition, is being increasingly used in therapies and interventions for many different conditions. This reflects the growing everyday use of speech interfaces with smartphones and tablets. Traditionally, articulation therapy is delivered by speech and language therapists, and is used to treat clients who have difficulties producing speech sounds correctly. It involves the client repeatedly speaking the sounds or types of sounds that they have difficulty with. Through repetition, and the expert feedback of their therapist, the client slowly improves the accuracy of their speech productions. This can lead to meaningful improvements in the intelligibility of their speech, and hence their ability to communicate independently with others. Unfortunately the repetitious nature of the practice is crucial to successful outcomes for the client, but this obviously makes it time-consuming and expensive to deliver. Many speech difficulties can require some articulation therapy, but two groups of clients most frequently identified by speech and language therapists are people who have had a stroke and developed a condition known as dysarthria. The other group is children with hearing impairment. In both these groups the repetitious practice and feedback can help clients either relearn how to say difficult speech sounds, or learn to more consistently produce a specific speech sound. To address the difficulties in delivering articulation therapy, the STAR (Speech Technology for Articulation Rehabilitation) project, funded by the National Institute of Health Research, is developing an app based on novel speech recognition technology. Researchers at Barnsley Hospital and the University of Sheffield have developed automatic speech recognition technology that can objectively score speech productions. Research has shown that feedback based on these scores can be used to help people modify their speech productions. Programmers at Therapy Box Ltd, an award-winning developer of communication apps, are currently developing the app. The app will prompt the client to produce a word either visually, or by playing an example for them to imitate. Then, every time they speak the word, visual feedback is given on how close what they said is to the target specified by their therapist. In the app, these speaking exercises will be presented as motivational games. Clients could then try and achieve a particular target in their practice sessions  or they may have targets set by their therapist in terms of number of repetitions or a particular score to achieve. Speech and language therapists can use the information collected by the app when setting new therapy targets for the clients. The information could also be used to monitor the progress of clients between their therapy appointments. Generally, using a computer-supported therapy tends to increase the amount of practice that people do. One of main innovations of the STAR app is the ability of the app to learn about an individual client. For each activity that a therapist creates with a client they are able to train the app to identify the target sound and the mistakes or incorrect sounds that client might make. In this way the app will be able to score the speech of the client based directly on the targets identified by the therapist. We are currently still in the development phase of the project. Before the project ends, we will be evaluating the final prototype with clients and therapists in the NHS. If the evaluation is successful we hope the app will be available to clinicians and the public within the next 12 months. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health. ",http://www.nationalhealthexecutive.com/Comment/star-speech-technology-for-articulation-rehabilitation-,"Speech and language technology, and particularly automatic speech recognition, is being increasingly used in therapies and interventions for ...",STAR: speech technology for articulation rehabilitation,Dr Stuart Cunningham Department Human Communication Sciences Centre Assistive Technology Connected Healthcare University Sheffield Speech STAR Speech Technology Articulation Rehabilitation National Institute Health Research Barnsley Hospital University Sheffield Research Therapy Box Ltd Dr Stuart Cunningham Department Human Communication Sciences Centre Assistive Technology Connected Healthcare University Sheffield Speech STAR Speech Technology Articulation Rehabilitation National Institute Health Research Barnsley Hospital University Sheffield Research Therapy Box Ltd Speech STAR NHS NHS NIHR Department Health,3
329,"Your message has been sent. There was an error emailing this page. Dragon has long ruled supreme over the landscape of speech recognitionbut no more.Simpler or less expensive (if not quite as powerful) options are carving out little fiefdoms. The more choices, the better, too, given that using voice commands can stave off or reduce repetitive strain injuries. The spoken word also suits some projects better than typing. We found five Windows-based options worth a test drive. They were tested under pretty rugged conditions, too: my native Scottish brogue, and my German accent honed by 13 years spent in Wuerzburg. While none was quite able to slay the powerful Dragon, they could still be useful for many tasks. Although Microsoft doesnt trumpet the fact, newer versions of Windows arrive with the companys own Windows Speech Recognition softwarepreinstalled. I thought I was going to detest it, but I came away impressedespecially considering its free, and you already have it on your Vista, Windows 7, or Windows 8 PC. Windows Speech Recognition is unobtrusive, free, and already installed. To switch on Windows Speech Recognition, go to your Start menu and in the search box at the bottom, type speech recognition. Click the option that pops up, and a window will open where you can enable the feature, as well as read a short text to give Windows an idea of what your voice is like. You can also access the feature through the Control Panel. Once everything is set up, a small status box will pop up. Youll use that to switch Speech Recognition on and off. To begin, just say clearly into the microphone start listening. The app then beeps and springs into action, waiting for your next command. You can tell Windows Speech Recognition to pretty much do anything on your PC. It can open browsers (including new tabs), apps, as well as Microsoft Office documents. Once you have a document open, you can dictate text and it will instantly appear there. You can also tell the computer to shut down or restart, as well as call up the start menu or a command line box. When you are finished, just say stop listening and it will go to sleep, waiting for you to wake it up again. Its easy to use, and the price is right, but Windows Speech Recognitions performance could stand some improvement. I found the accuracy level dipped when I dictated long texts into a MS Office doc. Nor did it respond well to my German accent, so other accents may stymie it as well. Chrome users might have noticed the little microphone at the far right of the search bar in Google. Thats Google Voice Search . Google Voice Search makes Chrome all the more worthwhile. Googles voice recognition efforts currently include the search engine and a translation service , but the latteris iOS-only. Google Voice Search is available onlyon the Google Chrome browser, which works on multiple platforms. Using Chrome, go to the main Google webpage . Assuming you have a microphone set up on your PC (perhaps in a headset or webcam), click the microphone icon. The first time, it asks you to grant Google permission to access your microphone. When youve done that, it invites you to speak your search terms. It recognizes speech almost instantly, and what the computer thinks youve said will pop up on the screen. Your search results also appear quickly, without any need to click a mouse. The moment you stop speaking, all the words turn black. A moment later, the search results appear. The Google voice recognition algorithm is fairly good at recognizing what you said. I gave it 10 search terms and said them in my regular (Scottish) accent as well as English with a German accent. It stumbled on only a few occasions, and that was when I said them in my native accent. Dragons been in the speech-recognition game long enough to have itsalgorithm down to a fine art . However, that art comes at a steep price: The Home version of Dragon NaturallySpeaking costs $75, and the Premium currently costs $150. If you have a recent Intel-powered Ultrabook or all-in-one desktop, you might have Dragon Assistant preinstalled. (Youll also find a preinstalled version on every iPhone and iPad, and Nuance confirmed earlier this year that the company makes Siri for Apple.) Otherwise, your lowest-cost preview is the $20 Dragon Notes , which lets you test out the voice-recognition capabilities in a stripped-down program. Dragon NaturallySpeaking steps you through its training. Upon opening Dragon NaturallySpeaking for the first time, you have to do the usual text dictationsto set up a user profile. Then the application loads a bar at the top of your screen. In that bar is an orange microphone logo, which indicates that Dragon is currently not listening. To start the application, just say wake up, and the orange logo will turn green. This is where you see what Dragon can do. One fantastic feature is its integration with Web apps and your email, including Gmail. Anywhere you find a text box, Dragon will enter any text you tell it to. It can write your emails for you, perform Google searches, post to Facebook and Twitter, and launch Web browsers. You can even tell it where to place the cursor. Dragon is smart enough to learn from you. Dictation accuracy is where Dragon really shines, though. You can open the usual word processing apps such as Microsoft Word and Notepad, and begin dictating your text. If it misunderstands or misspells your words, you can tell Dragon to go back and fix it. It gives you a menu of possible other words and lets you choose the one you want. If the word you want is not there, you can enter it so Dragon knows it for the future. You can also specify which parts of the text you want highlighted, italicized, underlined, made into a new paragraph, and so forth. Once youre finished, order Dragon to close the file, print it, save it, or open another document. Although it is clearly superior to the other tools, some minor things bugged me about this software. For example, the user profile bar does not fit nicely at the top of the browser, so it looks messy and out of place. It also cant launch websites, open new browser tabs (like Windows Speech Recognition does), or close apps. Dragons integration with other apps makes it useful all across your PC and the Web. But my biggest gripe is that it installs two extra pieces of software that some people might consider a nuisance. According to a Nuance spokesman, the Rich Internet Application Support browser add-on allows you to control more of your Internet browsing using speech. This sounds good, but in my experience, it crashed frequently and slowed my browser down. You can uninstall it just like any other pluginso if your browser starts crawling like an arthritic tortoise, you know what to do. A piece of included software called Common Software Manager has filled the online forums with unhappy users throwing around words like malware. All the CSM does is check for Dragon updates on a regular basis, but that action can be alarming if you dont know which program is doing it and why. A Dragon spokesman has assured me that the CSM can be uninstalled without any side effects to Dragon itselfbut of course, you wont get your software updates if you do that. While Dragon piles on the features, online app TalkTyper is the complete opposite. It gives you the chance to dictate your text, then it gives you a few basic options for what to do with that text. For those who prefer minimalism, that may be enough. Web-based dictation tool TalkTyper requires Google Chrome. TalkTyper uses the Google voice algorithm, so youre getting the benefit of an effective tool. Of course, it also requires Google Chrome. Simply click the green microphone icon on the right and dictate your text. When youre satisfied with the text on the screen, click the down-arrow button to move it into the lower box, where the options reside. Those options are copying the text to the clipboard, emailing your text, printing the text, tweeting it, and translating it into another language. If you want the text translated, simply click the button and choose the translating language. TalkTyper automatically opens a new browser tab with your text inserted into Google Translate. I expected TalkTyper to recognize words as ably as Google did, but it had a problem understanding my deep Scottish brogue. Even when I spoke clearly, it tripped up on some of the words, and I wasnt exactly dictating rocket science. TalkTyper should be used onlyfor simpler stuff, shorter spoken contentmaybe an email or a tweet here and there. If you want to dictate your magnum opus, Dragons the better choice. Pronounced tasty, Tazti stands out from the others with two distinctive features. The first is that you can use the app to control PC commands and games with your voice. And if Tazti doesnt have the command you need, you can create it (and another 299 while youre at it). Rather than taking dictation, Tazti takes orders. It helps you control games, open apps, and even use the command line. Whether its opening an installed app, launching a website, opening a directory, or using the command line, you can set up Tazti just as you please. If you dont need a command, you can edit it or delete it entirely. You can even use the commands click and double click to remove the need for using the mouse. However, Taztis one big drawback is it wont let you dictate text to a document. Its not that kind of voice recognition. Developer Voice Tech Group acknowledges that other products profiled here are much better at dictation, so the company decided to focus its efforts elsewhere. Instead, Tazti is more geared towards gamers who want to use their voice to send their characters into battle, or for disabled users who prefer to launch programs, start a media player, and browse the Web without the need to use a keyboard. The fact that you can control important parts of Windows with Tazti makes it worthy of inclusion here, even if it doesnt offer dictation. You can test-drive Tazti for free for 15 days. After that, it costs $40. After testing all five apps, its obvious that Dragon NaturallySpeaking far and away beats the competition. Windows provides a very strong rival with the added bonus of being free, but its speech recognition wasnt as strong, and it fails to locate folders that are right under its nose. If you are going to use speech recognition on a frequent basis, it would be worth investing $75 for home ($150 for business) and upgrading to Dragon. Try out speech recognition now with the built-in Windows app or with Google in the Chrome browser. Youll be amazed at how much you enjoy dispensing with the keyboard. To comment on this article and other PCWorld content, visit our Facebook page or our Twitter feed. Tazti speech recognition software doesn't take dictation, but it does take orders. Set up your own custom commands for opening other apps, websites, and command lines Control a wide variety of PC games with your voice, as well as normal Windows OS apps ",http://www.pcworld.com/article/2055599/control-your-pc-with-these-5-speech-recognition-programs.html,Dragon has long ruled supreme over the landscape of speech recognition—but no more. Simpler or less expensive (if not quite as powerful) ...,Control your PC with these 5 speech recognition programs,Dragon Scottish Wuerzburg Dragon Microsoft Windows Windows Vista Windows Windows Windows Start Windows Control Panel Windows Microsoft Office Windows MS Office Chrome Google Thats Google Voice Search Google Voice Search Chrome Googles Google Voice Search Google Chrome Chrome Google Google Google English Home Dragon NaturallySpeaking Premium Ultrabook Dragon Assistant Youll Nuance Siri Apple Dragon Dragon NaturallySpeaking Dragon NaturallySpeaking Dragon Dragon Web Gmail Anywhere Dragon Google Facebook Twitter Web Dragon Dictation Dragon Microsoft Word Notepad Dragon Dragon Dragon Windows Web Nuance Rich Internet Application Support Internet Common Software Manager CSM Dragon Dragon CSM Dragon Dragon TalkTyper TalkTyper Google Chrome TalkTyper Google Google Chrome Simply TalkTyper Google Translate TalkTyper Google Scottish TalkTyper Dragons Pronounced Tazti PC Tazti Tazti Tazti Taztis Developer Voice Tech Group Tazti Web Windows Tazti Tazti Dragon NaturallySpeaking Windows Dragon Windows Google Chrome PCWorld Facebook Twitter Tazti Control Windows OS,-1
330,"Post a comment /         Oct 28, 2016 at 11:49 AM You know how things can sneak up on you, particularly trends. There are dribs and drabs and all of a sudden a swirling tornado of evidence that something is happening I have had that experience these last few weeks with a realization that the phenomenon of voice will likely be the next meaningful user interface in healthcare. We may all be excited by man-made artifacts like artificial intelligence (AI), virtual reality (VR), 3D, and a multiplicity of other two-letter acronyms that spring from the minds of engineers, but the natural human thing that is our voice may well be the most interesting 200,000-year overnight sensation to bring meaning to medicine. I remember back as far as college, when dinosaurs still roamed the earth and iPhones were just a twinkle in Steve Jobs eye, that I took part in some student experiments about voice recognition technology that were largely absurd in their lack of utility. Fast forward and the SIRI revolution wasnt that much better. Yes, occasionally by shouting at your phone you could find your way to Sand Hill Road, but more likely it would instead offer you the mating habits of the sandy toad. Voice has been an interesting novelty but not ready for prime time in a world  medicine where minimum viable product is just not good enough. You cant have a patient say they need an ambulance and instead get a turkey sandwich. And frankly, because of historical limitations of technology, it hasnt entered that much into the conversation (pun intended) to use our natural voice as a user interface with anything other than, well, people. But all of a sudden in the past several weeks I have been overrun by companies that have decided voice is the next best thing to typing and probably better for many situations  at least four such companies have shown up in my orbit during that time whereas I have seen perhaps one such focused startup in the 15 years prior. In that same period, I was asked to moderate a panel at the MedCity ENGAGE conference about how the voice-driven Amazon Echo could potentially revolutionize healthcare. I hadnt given it a lot of thought before but was forced to think about the pros and cons of speech as the ultimate user interface as I prepared for the panel. I was strolling through the airport bookstore on my way to the conference and came upon Tom Wolfes new offering, The Kingdom of Speech, which documents a historical debate among linguistics experts about whether speech determines societys complexities or vice versa. And all of a sudden I had an epiphany. This voice thing. It might be for real this time. The companies I have met these past few weeks are using voice to solve long-standing problems in healthcare in interesting ways. One has created a technology to solve the problem of poor speech recognition with an in-ear device. The device learns each individual persons unique voice and creates an interface where that voice can reliably interact with a variety of applications, such as a physician speaking directly to the electronic medical record (EMR) (as opposed to scribes) and hearing it back. Im not sure if this particular company has cracked the code on perfect speech recognition, but it was a novel approach to a long-standing problem at the root of making this whole field meaningful. Physicians struggle with their documentation requirements at a level that the health system marketplace simply hasnt recognized as a key part of physician disaffection. If this could be resolved by just speaking at the EMR during office visits, clinicians might have nearly twice the time to actually see patients each week. This alone could be the answer to the long-discussed physician shortage. Another company claims to have developed a speech recognition algorithm that will recognize the garbled voice of those with medically caused speech impairments, such as stroke, Parkinsons disease, some forms of autism, and turn those peoples communications into entirely understandable speech. If it works as advertised, its an amazing breakthrough that could change the lives of many. Social isolation caused by an inability to communicate is a tragic problem for people who rely on human interaction; and social isolation is a key driver in poor health outcomes, leading to increased depression and even early death. Yet another company is building medical applications (aka skills) on top of the Amazon Echo platform which are targeted, at least at the moment, largely to seniors living in their homes (FYI, the hardware is the Echo, the software is Alexa, the apps built on Alexa are the skills for those not in the know). The idea is that the Echo and its Alexa software can aid in supporting cognition (just ask Alexa your caregivers name if you forget), medication reminders (is it time for my medication How much do I take), post-discharge instructions (Alexa, tell me again what rehab exercises I am supposed to do today) and transportation (Alexa, please get me an Uber to my doctors appointment). Other uses for the Echo have already been piloted by groups such as Boston Childrens Hospital. The hospital built a skill that allows parents to ask basic pediatric questions of their Echo, such as when to worry about a fever or how to dose OTC medications. Astra Zeneca is piloting a post-discharge coaching program for heart attack survivors. Dr. John Loughnane, Chief of Innovation at Winter Street Ventures, was on my Amazon Echo panel and talked about a particularly poignant use of the technology for a patient who had serious medical issues that caused pain, anxiety, and depression. Recognizing the healing power of music, Loughnane helped the patient create musical playlists that soothed those three particular conditions for the patient and showed him how to summon those playlists through Alexa in times of need. The patient said it changed his life. This may not sound like medicine to you, but in a world where most doctors would hand over an opioid prescription, I much prefer the musical alternative, as you can tell by my headlines. There is ample clinical and scientific evidence of the value of music to the mind and the ability of patients to use mindfulness to address these serious conditions that might otherwise send them to the ER. Its notable that there are already about 3 million+ Echo devices sold in the U.S. and Amazon projects that there will be 10 million sold in 2017. No one knows consumer distribution like Amazon. Can you imagine some Silicon Valley health tech startup outselling Amazon Yeah, me neither. Imagine the Trojan Horse opportunity here for healthcare, where a widespread platform (Echo sales are rising while iPhone sales are dropping) for health is sitting there using a nearly universal user interface that costs patients exactly nothing to acquire  their own voice. Nate Treloar, president and COO of Orbita, also on my MedCityENGAGE panel, called voice the universal remote control. No doubt it has the potential to be a massive improvement the so-called button-based universal remote control that I currently use with my television and which has been thrown across the room in frustration hundreds of times. If I could just tell it what to do and not have to remember the dizzying array of button commands that would be better. And Im sure thats what many seniors think when they look at their smartphone apps and wonder how to really use them  most are so poorly designed and the problem is compounded by issues of hand dexterity, arthritis, and poor eyesight. Granted, voice as an interface still has a way to go to become ubiquitous. As I mentioned on my panel, the most frequent command I give to SIRI is not printable in a family blog. The number of mistakes it hears is barely acceptable for getting a phone number, much less for drug advice. But there have been marked advances and they are improving. Just this week, once again, Microsoft announced that it had achieved human parity for its own speech recognition system. And for some applications, such as supporting someone with mild cognitive impairment to remember what day it is, what their schedule is, what their caregivers name is, perfection may not be necessary. For other uses, such as medication compliance, it clearly is. Privacy and security are also major issues needing attention. On the privacy front, there is no hiding what you say out loud  anyone can overhear you. On the security front, it is pretty clear that the realm of IOT (a three letter acronym (!) meaning Internet of Things) is not quite ready for prime time, as the recent Denial of Service attach on DYNs service showed us, taking down Amazon, Facebook and Twitter. On the other hand, coming as it did during the travails of the current election, perhaps the inability to reach Twitter is a blessing. The Amazon Echo, while incredibly useful for summoning essential items like food and transport and designer shoes, cant distinguish between different peoples voices yet. That is a problem where there is a risk of abuse, particularly over use of the purchasing feature by seniors easily taken advantage of by those without their best interests at heart. Its also a problem for those whose memory isnt what it used to be (like me!) who might accidentally order the same thing multiple times. Alexa developers will need to find a way to more easily decouple purchasing from the skills they create and to allow for the differentiation of speakers by the device. The idea of voice as a universal remote control or natural user interface is so compelling. It is free, available, and almost every patient has one and knows how to use it. If the challenges can be overcome, the opportunities are remarkable to contemplate. At our panel, Dr. Loughnane said, Voice is how we engage each other; we need to move it from novelty to standard of care. If that could be made real, it could be a remarkable step forward for patients and providers alike. This post appears through the MedCity News MedCitizens program . Anyone can publish their perspective on business and innovation in healthcare on MedCity News through MedCitizens. Click here to find out how . Lisa Suennen is Senior Managing Director of GE Ventures.  In this role, she is responsible for leading GE Ventures healthcare venture fund. She is also Managing Partner at Venture Valkyrie Consulting. She is Founder and CEO of CSweetener, a not-for-profit organization that uses technology and know-how to connect women new to and nearing the C-Suite with female and male mentors who have been there and a have a desire to give back. She spent 15 years as a Partner at Psilos Group.  Lisa is currently and Board Member of digital health company Beyond Lucid Technologies; a Board Member of the Dignity Health Foundation, Heart To Heart and and global digital health organization HealthXL.  She is also on the Advisory Boards of Qualcomm Life, the California Health Care Foundation Innovation Fund, and BDC Capital. Lisa writes a widely read blog on healthcare and healthcare investing at www.venturevalkyrie.com and hosts a popular podcast called Tech Tonics with Dr. David Shaywitz.Lisa is on faculty at the University of California, Berkeley, where she teaches graduate courses on healthcare venture capital and the changing healthcare economy in the Haas School of Business. ",http://medcitynews.com/2016/10/voice-recognition-technology-in-healthcare/,One has created a technology to solve the problem of poor speech recognition with an in-ear device. The device learns each individual ...,Voice will likely be the next meaningful user interface in healthcare,/ Oct AM AI VR Steve Jobs Fast SIRI Sand Hill Road Voice MedCity ENGAGE Amazon Echo Tom Wolfes Kingdom Speech EMR Im Physicians EMR Parkinsons Amazon Echo FYI Echo Alexa Alexa Echo Alexa Alexa Alexa Alexa Uber Echo Boston Childrens Hospital Echo OTC Astra Zeneca Dr. John Loughnane Chief Innovation Winter Street Ventures Amazon Echo Loughnane Alexa ER Echo U.S. Amazon Amazon Silicon Valley Amazon Yeah Trojan Horse Echo Nate Treloar COO Orbita MedCityENGAGE Im SIRI Microsoft IOT Internet Things Denial Service DYNs Amazon Facebook Twitter Twitter Amazon Echo Alexa Dr. Loughnane Voice MedCity News MedCitizens MedCity News MedCitizens Click Lisa Suennen Managing Director GE Ventures GE Ventures Partner Venture Valkyrie Consulting Founder CEO CSweetener Partner Psilos Group Lisa Board Member Beyond Lucid Technologies Board Member Dignity Health Foundation Heart Heart HealthXL Advisory Boards Qualcomm Life California Health Care Foundation Innovation Fund BDC Capital Lisa Tech Tonics Dr. David Shaywitz.Lisa University California Berkeley Haas School Business,7
331,"No longer just the Master Chiefs AI buddy, Cortana is rapidly becoming a complex and multi-talented virtual assistant. Her applications for general PC and mobile use are well known, but Microsoft is keen to educate developers on how she can be used in games. James Batchelor finds out more ",http://www.develop-online.net/tools-and-tech/how-windows-10-and-cortana-are-bringing-speech-recognition-to-games/0215391,"Speech recognition has rarely been attempted in games. Outside of quirky examples such as Nintendo's virtual pet title Hey You, Pikachu!",How Windows 10 and Cortana are bringing speech recognition to ...,Master Chiefs AI Cortana Microsoft James Batchelor,-1
332,"Its not the first tech giant to release deep learning software for free; last November, Google open-sourced its TensorFlow machine learning system that powered its Photos search functionality . But according to Microsofts chief speech scientist, the companys CNTK toolkit is just insanely more efficient than anything we have ever seen. Part of that could be attributed to CTNKs ability to harness the power of graphics processing units, or GPUs. The company claims that its toolkit is the only publicly available one that can scale beyond a single machine and take advantage of several GPUs on multiple machines for superior performance. By pairing CNTK with Microsofts networked GPU system, called Azure GPU Lab, the company was able to train deep neural networks for speech recognition in its Cortana virtual assistant 10 times faster than before . The researchers made CNTK available to academic researchers last April under a more restricted open-source license. But now that its freely available, they believe it could be useful to anyone from deep learning startups to more established companies that are processing a lot of data in real time. ",https://thenextweb.com/microsoft/2016/01/26/microsofts-deep-learning-toolkit-for-speech-recognition-is-now-on-github/,"In order to speed up their AI and speech recognition projects, researchers at Microsoft have developed a toolkit that uses deep neural networks ...",Microsoft's deep learning toolkit for speech recognition is now on ...,November Google TensorFlow Photos Microsofts CNTK Part CTNKs GPUs GPUs CNTK Microsofts GPU Azure GPU Lab Cortana CNTK April,-1
333,"Transcription and written translation share a number of similarities (convert language input in one form or another into text) and have their differences (transcription typically involves only one language). From a supply-chain point of view, the two activities are similar as they allow for work to be instantly sent across the globe. Arguably, transcription is the less complex activity of the two. While there is such a thing as a factually perfect transcription (i.e., everything said was perfectly transcribed), a translation, by default, is an interpretation of the source content and, therefore, open to debate. As progress in machine learning and artificial intelligence accelerates, one would expect its impact on the human workforce to be experienced first in transcription. Exactly three weeks after Slator broke the story on Googles  nearly indistinguishable  claim when they launched the new, neural-network-powered Google Translate, Microsoft claimed a historic achievement on their official blog : speech recognition technology that recognizes the words in a conversation as well as a person does. The subject of the blog article was a paper published the day prior, on October 17, 2016, entitled Achieving Human Parity in Conversational Speech Recognition and authored by the Microsoft Speech & Dialogue research group. In it, Microsofts researchers say they had achieved a word error rate of 5.9%, an improvement from the 6.3% the team reported a month before. Said Microsoft Engineer and Chief Speech Scientist Xuedong Huang, Weve reached human parity. This is an historic [sic] achievement. Depending who [sic] you ask, speech recognition is either solved or impossible. The truth is somewhere in betweenGerald Friedland, UC Berkeley The word error rate or WER is a common metric for evaluating speech recognition, as the BLEU score is for machine translation. The blog article went on to point out that a 5.9% error rate is about equal to that of people who were asked to transcribe the same conversation. The goal being, of course, to approach zero and achieve 100% accuracy. Just 10 days before Microsoft published its paper, PC World Senior Editor Mark Hachman had called Microsofts speech recognition the  weakness no one mentions  and said his Windows Speech Recognition test drive yielded a 6.4% word error rate, which was pretty bad on paper. Qualifying that it was just the baseline and that, properly trained, Microsoft employees claim their speech recognition can achieve 99% accuracy, Hachman nonetheless concluded that training speech within Windows is a lengthy process. It actually took 10 minutes, which Hachman said felt like a lifetime. He went on to say that training speech is faster (perhaps a minute or so) for Nuances speech recognition software Dragon, which announced its own breakthrough back in August. According to Vlad Sejnoha, Chief Technology Officer at Nuance, Dragons deep neural nets can now continuously learn from the users speechand drive accuracy rates in some instances up to 24% higher. Dragon and similar voice recognition software is to transcriptionistswhat translation productivity tools (or CAT tools) are to translators: They do not replace the transcriptionist, but make them more productive. To extend the analogy, the technology that operates on the original audio file to produce textual output would be the equivalent of machine translation. What is the impact of productivity tools like Dragon on human transcriptionists and the overall transcription market [Transcription is part of the broader document preparation services market, that is supposedly worth USD 5bn in the US alone. Fact is, Slator could not pin down a credible figure.] And is fully automated speech recognition technology about to replace human transcriptionists in the real world Slator spoke to four professional transcriptionists for their take on the matter. One source we spoke to said there are still way too many variables for technology to be able to completely replace the entire human transcription process. According to Belle Lapa, who founded Scriberspoint in 2012 after having worked for such companies as Lingo24 and S&P, The way a speaker speaks, overlapping speakers, background noise and, perhaps, most importantly, the very variable nature of language itself have yet to be factored in. In terms of voice input to increase productivity (i.e., listen to audio, repeat what you hear into mic instead of typing), transcriptionists today still need to train their speech recognition tools for them to be useful, in much the same way translators do translation productivity tools. Think adaptive machine translation and the need to personalize the system. According to another professional transcriptionist to whom we spoke, using Dragon alone, untrained, yielded poor results. It was simply unusable, said the source, adding, A human transcriptionist can achieve 8090% accuracy with Dragon and, with the help of a real-time editor, can get it up to 9396%. The source said that, to make a transcriptionist work faster, a tool has to reach 6575% accuracy (using our own in-house accuracy checking tool). Anything below would be next to useless. It just doesnt make sense to keep deleting things if its that bad. It just slows up the typist, our source pointed out. The same source added that accuracy also depends on accents, naming as typical transcriptionist waterloos audio from speakers with Indian, Portuguese, and French accents.Even when using Nuances Dragon, we still need humans to train the tool for context, nuance, homophones, proper nouns, accents, etc., our source said. Our source agreed to speak to us on condition of strict anonymity as professional transcriptionists are bound by stringent non-disclosure agreements. The source did disclose, however, that transcriptionists generally do not see their job as a real career. Explained our source: Its probably what Id call a high-paying, labor-intensive job. We are usually there to keep our bills paid  while we work to go somewhere else. Most of the best transcriptionists in our department have moved on to other things not transcription-related. Those who stayed go on to management. Theres just no real career growth there. What else can you do, really Type faster Speed-read Asked if they feel, at all, threatened at being replaced by technology, our source said, Its not like wed picket if the company suddenly announces a tool has replaced us. Wed hate losing a high-paying job, for sure. But, like I said, we dont view transcription as a real career. Besides, said our source, technology still has a long way to go before achieving human-quality transcription  an on-the-ground assessment, despite The Economist saying that, Thanks to deep learning, machines now nearly equal humans in transcription accuracy. Our source, a transcriptionist at a Fortune 500 company, said, Anything purely done by tech is unusable. We did try, but the results were so bad. The best tools we had still needed human supervision and they are still being developed with the idea of having humans eventually edit the end result. For her part, Scriberspoints Belle Lapa said certain parts of a transcriptionists job have indeed been made easier by technology. Foot pedals have replaced keyboard shortcuts and voice-recognition tools and software are able to capture speech and turn them into text, with varying degrees of accuracy, according to Belle. Belle is optimistic that speech-to-text technology can only get better with time. She said they now use a very good dictionary, glossary, and database tools, which help them in subjects like legal and medical where terms are highly specialized. The Scriberspoint founder, who is based in the Philippines, benefitted greatly from what she described as the last big boom in transcription when the US required subtitles for the hearing impaired. Since then the number of clients and their transcription needs have remained stable or moving upward, never downward. She regards India as their greatest competitor, if only because of the cheaper rates they charge, but said she is not worried, for now, about running out of clients. Not so optimistic is Carla [last name withheld on request], who is also based in the Philippines and is now a content editor at a financial intelligence firm. Looking at the transcription industry from a distance, having left it for a while now, she would not call it a hot market. She said, Transcribing through typing is increasingly being replaced by voice transcription or captioning, and sees technology replacing human transcription first taking place in captioning, subtitling, news, and medical. Carla added, however, that if it involves voice transcription with near real-time editing, then there will be reasonable opportunities in financial, legal, and medical. As for competitor India, she said the country may offer cheaper labor and tech, but the Philippines would be in the higher tier if you were to factor in English listening skills, acquired typing skills, and adaptability. Much less optimistic is Noriel Ramientas II, who recalled that when he started doing home-based transcription, part-time, in February 2012, everything was good  the pay was good, projects kept coming in, it was all just peachy. Since March 2015 though that hasnt been the case. Fewer projects came in, he said, and the pay had reached a ceiling of USD 35 per audio hour, compared to five years ago when one could charge as high as USD 50 for the same audio length. I dont mean to burst anyones bubble but I dont think the transcription market  at least as far as home-based transcriptionists are concerned  is growing. Thats why I havent been doing it for more than a year now, said Noriel. He described other online jobs (blogging, virtual assistance, bookkeeping) as more readily available and higher paying, and the demand for transcriptionists, diminishing, as more people, most notably those from India, flock to online job marketplaces like Upwork and Guru. As for tech replacing humans, Noriels view was pretty simple. He said it is just a tool. Tools never function on their own. Someone sentient must put a tool to use before it is deemed useful. The experts seem to agree, especially as far as speech recognition technology and other transcription tools go. Although Dragon founder James Baker once said large vocabulary speech recognition was a solvable problem within his lifetime, more recently, Gerald Friedland said, We used to joke that, depending who [sic] you ask, speech recognition is either solved or impossible. The truth is somewhere in between. Friedland, Audio and Multimedia Research Director at the UC Berkeley-affiliated International Computer Science Institute, was quoted in an April 2016 Wired article, called Why Our Crazy AI Still Sucks at Transcribing Speech . Similar technological forces drive transcription and translation. And the lessons that transcription holds for translation are not new: Go niche, specialize, and embrace technology to increase productivity. ",https://slator.com/features/transcription-canary-translation-gold-mine/,"Depending who [sic] you ask, speech recognition is either solved or impossible. The truth is somewhere in between—Gerald Friedland, UC ...",Is Transcription the Canary in the Translation Gold Mine?,Slator Googles Google Translate Microsoft October Achieving Human Parity Conversational Microsoft Speech Dialogue Microsofts Said Microsoft Engineer Chief Speech Scientist Xuedong Huang Weve ] Friedland UC Berkeley WER BLEU Microsoft World Senior Editor Mark Hachman Microsofts Microsoft Hachman Windows Hachman Nuances Dragon August Vlad Sejnoha Chief Technology Officer Nuance Dragons Dragon CAT Dragon [ Transcription USD US Fact Slator Slator Belle Lapa Scriberspoint Lingo24 S P Dragon A Dragon Nuances Dragon Id Wed Besides Economist Fortune Anything Scriberspoints Belle Lapa Foot Belle Belle Scriberspoint Philippines US India Carla [ Philippines Carla India Philippines English Noriel Ramientas II February March USD USD Thats Noriel India Upwork Guru Noriels Tools Dragon James Baker Gerald Friedland Friedland Audio Multimedia Research Director UC Berkeley-affiliated International Computer Science Institute April Wired AI Speech Go,-1
334,"Transcription and written translation share a number of similarities (convert language input in one form or another into text) and have their differences (transcription typically involves only one language). From a supply-chain point of view, the two activities are similar as they allow for work to be instantly sent across the globe. Arguably, transcription is the less complex activity of the two. While there is such a thing as a factually perfect transcription (i.e., everything said was perfectly transcribed), a translation, by default, is an interpretation of the source content and, therefore, open to debate. As progress in machine learning and artificial intelligence accelerates, one would expect its impact on the human workforce to be experienced first in transcription. Exactly three weeks after Slator broke the story on Googles  nearly indistinguishable  claim when they launched the new, neural-network-powered Google Translate, Microsoft claimed a historic achievement on their official blog : speech recognition technology that recognizes the words in a conversation as well as a person does. The subject of the blog article was a paper published the day prior, on October 17, 2016, entitled Achieving Human Parity in Conversational Speech Recognition and authored by the Microsoft Speech & Dialogue research group. In it, Microsofts researchers say they had achieved a word error rate of 5.9%, an improvement from the 6.3% the team reported a month before. Said Microsoft Engineer and Chief Speech Scientist Xuedong Huang, Weve reached human parity. This is an historic [sic] achievement. Depending who [sic] you ask, speech recognition is either solved or impossible. The truth is somewhere in betweenGerald Friedland, UC Berkeley The word error rate or WER is a common metric for evaluating speech recognition, as the BLEU score is for machine translation. The blog article went on to point out that a 5.9% error rate is about equal to that of people who were asked to transcribe the same conversation. The goal being, of course, to approach zero and achieve 100% accuracy. Just 10 days before Microsoft published its paper, PC World Senior Editor Mark Hachman had called Microsofts speech recognition the  weakness no one mentions  and said his Windows Speech Recognition test drive yielded a 6.4% word error rate, which was pretty bad on paper. Qualifying that it was just the baseline and that, properly trained, Microsoft employees claim their speech recognition can achieve 99% accuracy, Hachman nonetheless concluded that training speech within Windows is a lengthy process. It actually took 10 minutes, which Hachman said felt like a lifetime. He went on to say that training speech is faster (perhaps a minute or so) for Nuances speech recognition software Dragon, which announced its own breakthrough back in August. According to Vlad Sejnoha, Chief Technology Officer at Nuance, Dragons deep neural nets can now continuously learn from the users speechand drive accuracy rates in some instances up to 24% higher. Dragon and similar voice recognition software is to transcriptionistswhat translation productivity tools (or CAT tools) are to translators: They do not replace the transcriptionist, but make them more productive. To extend the analogy, the technology that operates on the original audio file to produce textual output would be the equivalent of machine translation. What is the impact of productivity tools like Dragon on human transcriptionists and the overall transcription market [Transcription is part of the broader document preparation services market, that is supposedly worth USD 5bn in the US alone. Fact is, Slator could not pin down a credible figure.] And is fully automated speech recognition technology about to replace human transcriptionists in the real world Slator spoke to four professional transcriptionists for their take on the matter. One source we spoke to said there are still way too many variables for technology to be able to completely replace the entire human transcription process. According to Belle Lapa, who founded Scriberspoint in 2012 after having worked for such companies as Lingo24 and S&P, The way a speaker speaks, overlapping speakers, background noise and, perhaps, most importantly, the very variable nature of language itself have yet to be factored in. In terms of voice input to increase productivity (i.e., listen to audio, repeat what you hear into mic instead of typing), transcriptionists today still need to train their speech recognition tools for them to be useful, in much the same way translators do translation productivity tools. Think adaptive machine translation and the need to personalize the system. According to another professional transcriptionist to whom we spoke, using Dragon alone, untrained, yielded poor results. It was simply unusable, said the source, adding, A human transcriptionist can achieve 8090% accuracy with Dragon and, with the help of a real-time editor, can get it up to 9396%. The source said that, to make a transcriptionist work faster, a tool has to reach 6575% accuracy (using our own in-house accuracy checking tool). Anything below would be next to useless. It just doesnt make sense to keep deleting things if its that bad. It just slows up the typist, our source pointed out. The same source added that accuracy also depends on accents, naming as typical transcriptionist waterloos audio from speakers with Indian, Portuguese, and French accents.Even when using Nuances Dragon, we still need humans to train the tool for context, nuance, homophones, proper nouns, accents, etc., our source said. Our source agreed to speak to us on condition of strict anonymity as professional transcriptionists are bound by stringent non-disclosure agreements. The source did disclose, however, that transcriptionists generally do not see their job as a real career. Explained our source: Its probably what Id call a high-paying, labor-intensive job. We are usually there to keep our bills paid  while we work to go somewhere else. Most of the best transcriptionists in our department have moved on to other things not transcription-related. Those who stayed go on to management. Theres just no real career growth there. What else can you do, really Type faster Speed-read Asked if they feel, at all, threatened at being replaced by technology, our source said, Its not like wed picket if the company suddenly announces a tool has replaced us. Wed hate losing a high-paying job, for sure. But, like I said, we dont view transcription as a real career. Besides, said our source, technology still has a long way to go before achieving human-quality transcription  an on-the-ground assessment, despite The Economist saying that, Thanks to deep learning, machines now nearly equal humans in transcription accuracy. Our source, a transcriptionist at a Fortune 500 company, said, Anything purely done by tech is unusable. We did try, but the results were so bad. The best tools we had still needed human supervision and they are still being developed with the idea of having humans eventually edit the end result. For her part, Scriberspoints Belle Lapa said certain parts of a transcriptionists job have indeed been made easier by technology. Foot pedals have replaced keyboard shortcuts and voice-recognition tools and software are able to capture speech and turn them into text, with varying degrees of accuracy, according to Belle. Belle is optimistic that speech-to-text technology can only get better with time. She said they now use a very good dictionary, glossary, and database tools, which help them in subjects like legal and medical where terms are highly specialized. The Scriberspoint founder, who is based in the Philippines, benefitted greatly from what she described as the last big boom in transcription when the US required subtitles for the hearing impaired. Since then the number of clients and their transcription needs have remained stable or moving upward, never downward. She regards India as their greatest competitor, if only because of the cheaper rates they charge, but said she is not worried, for now, about running out of clients. Not so optimistic is Carla [last name withheld on request], who is also based in the Philippines and is now a content editor at a financial intelligence firm. Looking at the transcription industry from a distance, having left it for a while now, she would not call it a hot market. She said, Transcribing through typing is increasingly being replaced by voice transcription or captioning, and sees technology replacing human transcription first taking place in captioning, subtitling, news, and medical. Carla added, however, that if it involves voice transcription with near real-time editing, then there will be reasonable opportunities in financial, legal, and medical. As for competitor India, she said the country may offer cheaper labor and tech, but the Philippines would be in the higher tier if you were to factor in English listening skills, acquired typing skills, and adaptability. Much less optimistic is Noriel Ramientas II, who recalled that when he started doing home-based transcription, part-time, in February 2012, everything was good  the pay was good, projects kept coming in, it was all just peachy. Since March 2015 though that hasnt been the case. Fewer projects came in, he said, and the pay had reached a ceiling of USD 35 per audio hour, compared to five years ago when one could charge as high as USD 50 for the same audio length. I dont mean to burst anyones bubble but I dont think the transcription market  at least as far as home-based transcriptionists are concerned  is growing. Thats why I havent been doing it for more than a year now, said Noriel. He described other online jobs (blogging, virtual assistance, bookkeeping) as more readily available and higher paying, and the demand for transcriptionists, diminishing, as more people, most notably those from India, flock to online job marketplaces like Upwork and Guru. As for tech replacing humans, Noriels view was pretty simple. He said it is just a tool. Tools never function on their own. Someone sentient must put a tool to use before it is deemed useful. The experts seem to agree, especially as far as speech recognition technology and other transcription tools go. Although Dragon founder James Baker once said large vocabulary speech recognition was a solvable problem within his lifetime, more recently, Gerald Friedland said, We used to joke that, depending who [sic] you ask, speech recognition is either solved or impossible. The truth is somewhere in between. Friedland, Audio and Multimedia Research Director at the UC Berkeley-affiliated International Computer Science Institute, was quoted in an April 2016 Wired article, called Why Our Crazy AI Still Sucks at Transcribing Speech . Similar technological forces drive transcription and translation. And the lessons that transcription holds for translation are not new: Go niche, specialize, and embrace technology to increase productivity. ",https://slator.com/features/transcription-canary-translation-gold-mine/,"Depending who [sic] you ask, speech recognition is either solved or impossible. The truth is somewhere in between—Gerald Friedland, UC ...",Is Transcription the Canary in the Translation Gold Mine?,Slator Googles Google Translate Microsoft October Achieving Human Parity Conversational Microsoft Speech Dialogue Microsofts Said Microsoft Engineer Chief Speech Scientist Xuedong Huang Weve ] Friedland UC Berkeley WER BLEU Microsoft World Senior Editor Mark Hachman Microsofts Microsoft Hachman Windows Hachman Nuances Dragon August Vlad Sejnoha Chief Technology Officer Nuance Dragons Dragon CAT Dragon [ Transcription USD US Fact Slator Slator Belle Lapa Scriberspoint Lingo24 S P Dragon A Dragon Nuances Dragon Id Wed Besides Economist Fortune Anything Scriberspoints Belle Lapa Foot Belle Belle Scriberspoint Philippines US India Carla [ Philippines Carla India Philippines English Noriel Ramientas II February March USD USD Thats Noriel India Upwork Guru Noriels Tools Dragon James Baker Gerald Friedland Friedland Audio Multimedia Research Director UC Berkeley-affiliated International Computer Science Institute April Wired AI Speech Go,-1
335,"To use this website, cookies must be enabled in your browser. To enable cookies, follow the instructions for your browser below. There is a specific issue with the Facebook in-app browser intermittently making requests to websites without cookies that had previously been set. This appears to be a defect in the browser which should be addressed soon. The simplest approach to avoid this problem is to continue to use the Facebook app but not use the in-app browser. This can be done through the following steps: Enabling Cookies in Internet Explorer 10, 11 Click the Tools button, and then click Internet Options. Click the Privacy tab, and then, under Settings, move the slider to the bottom to allow all cookies, and then click OK. ",http://www.theaustralian.com.au/business/technology/review-dragon-professional-leaves-rivals-at-a-loss-for-words-in-dictation/news-story/c74dd00fe27be79b34f6f34de096280d,Windows 10 has a dedicated speech recognition system but it was full of errors even after some training. There's also Cortana but as a cloud ...,Review: Dragon Professional leaves rivals at a loss for words in ...,Facebook Facebook Internet Explorer Click Tools Internet Options Privacy OK,9
336,"Having computers understand what we are saying with voice recognition has long been a dream of science fiction writers. (Just watch any episode of Star Trek: The Next Generation to see how everyone interacts with the Enterprise.) Yet the road to seamless, human interaction has been anything but smooth. As a matter of fact, its rather like watching a small child grow up and learn to better interact with its world. Computerized voice recognition was something actually pioneered by Bell Laboratories in 1952 when it designed Audrey to understand digits, but only from a single voice. The idea was to eventually allow people to dial the phone by voice alone. IBM also came to the table in 1962 with its demonstration of Shoebox at the Worlds Fair. Shoebox understood 16 spoken words as well as the numbers zero through nine. The U.S. Department of Defense took a pretty serious look at the technology in the 1970s with Carnegie Mellons Harpy speech-understanding system . All told, it could recognize approximately 1,011 words, which is the vocabulary of the average three-year-old. More importantly, Harpy introduced a more efficient search approach that  would lead to overall advances in how speech recognition could become  more successful by looking at a whole sentence rather than individual words.  It's something that our own brains do when we don't always hear words  completely, essentially filling in the missing blanks based upon what  makes sense in the context of a complete sentence. In the 1980s, things got better but computer systems were limited partly because of their processing power. The systems had to be trained to a particular user's word-base and could only recognize words if spoken slowly, one at a time. Bell did make some advances in being able to distinguish multiple voices but again, things were somewhat limited by the processing power available at the time. The 1990s changed things as the first consumer product, DragonDictate , was released in 1990 for $9,000. Seven years later, Dragon Naturally Speaking could understand things a little faster at around 100 words per minute, but it still required training and the cost was still prohibitive at $695. It was clear that things were going in the right direction by this time. Speech recognition didnt really get a lot better early into the 2000s until Google came along and decided that searching the web by voice was a high priority and dedicated research into the topic. There was also a Stanford Research Institute spin-off led by Dag Kittlaus, Adam Cheyer and Tom Gruber that really began to understand speech in a more natural way. In 2010 this spin-off was sold to Apple and has become the basis for Siri , the voice recognition built-in to Apple products. Today, Cheyer and Kittlaus are back with a new product called Viv , which will take things a step further than Siri. Dubbed as The Global Brain, the product hopes to realize a dream of being able to say order a pizza and have it delivered without ever having to touch your phone. A Washington Post article describes the process of ordering a pizza, which may sound simple but actually has a number of hurdles to overcome to make the process seamless. Its like a virtual assistant, but even more complex. Viv is already working with multiple partners to make these type of transactions possible without having to install special apps or other setups that can slow you down (especially the first time you try to use them). The future of talking to your phone is really just beginning. Computerized voice recognition has come along way since its rather humble beginnings over 50 years ago. I am quite confident it wont take another 50 years for it to essentially be perfect, its just a question of how we will use it. While its often fun to say look whos talking now, it might be more appropriate (and more interesting) to say look whos listening now. Syd Bolton is the curator of the Personal Computer Museum and the manager of Information Technology at ACIC/Methapharm.  You can reach him via e-mail at sbolton@bfree.on.ca or on Twitter @sydbolton . ",http://www.torontosun.com/2016/05/19/the-evolution-of-voice-recognition-technology,"More importantly, Harpy introduced a more efficient search approach that would lead to overall advances in how speech recognition could ...",The evolution of voice-recognition technology,Just Star Trek Enterprise Bell Laboratories Audrey IBM Shoebox Worlds Fair Shoebox U.S. Department Defense Carnegie Mellons Harpy Harpy Bell DragonDictate Dragon Naturally Speaking Google Stanford Research Institute Dag Kittlaus Adam Cheyer Tom Gruber Apple Siri Apple Cheyer Kittlaus Viv Siri Dubbed Global Brain A Washington Post Viv Syd Bolton Personal Computer Museum Information Technology ACIC/Methapharm @ Twitter @,-1
337,"Global Markets and Technologies for Voice Recognition, 2021 - Market is Expected to Increase from $104.4 Billion in 2016 to $184.9 Billion in 2021 at a CAGR of 12.1% Dublin, Jan.  04, 2017  (GLOBE NEWSWIRE) -- Research and Markets has announced the addition of the ""Global Markets and Technologies for Voice Recognition"" report to their offering. The scope of this report covers the overall voice recognition  technologies market with market sizing and trends analysis for the most  recently completed actuals for 2015 as well as forecasts, trends and  compound annual growth rates (CAGRs) for 2016 through 2021. The market is segmented into end user, technology and regional  segments. End user segments include consumer entertainment, telematics  and home applications, as well as enterprise applications within the  military, legal, contact center and warehouse sectors. The healthcare  user segment is also examined including interactive voice response,  assistive technology and transcription applications. Technologies covered include overall hardware, software and devices  as well as automatic speech recognition, text-to-speech, speaker  verification, speech analytics, call center, interactive voice response,  voice-enabled mobile search, games and set top boxes, digital signal  processors, gateways, microelectromechanical systems and Bluetooth  technology. The market is also segmented by region, specifically, North  America, Latin America, Europe and Asia-Pacific. Device applications, a key driver of current demand for voice  recognition, are also explored with subsegments ranging from voice  integrated navigation systems to wearable devices. An overview of the software and hardware for voice recognition technologies and their markets. Analyses of global market trends, with data from 2015, 2016 and  projections of compound annual growth rates (CAGRs) through 2021. Discussion of top supplier market shares, new technologies, and the unique challenges faced by each category in the future. Examination of the importance of traditional and emerging  voice-enabled devices, as these media will promote and extend voice  recognition's reach. Reviews of enabling technologies, corporate and national research  and development funding, the organizational and economic makeup of the  voice recognition industry, and the legislative, political, and  environmental issues facing the industry. Listings of recent patent grants, as well as related mergers and acquisitions, licensing arrangements, and partnerships. Profiles of major players in the industry. ",https://globenewswire.com/news-release/2017/01/04/903086/0/en/Global-Markets-and-Technologies-for-Voice-Recognition-2021-Market-is-Expected-to-Increase-from-104-4-Billion-in-2016-to-184-9-Billion-in-2021-at-a-CAGR-of-12-1.html,"Technologies covered include overall hardware, software and devices as well as automatic speech recognition, text-to-speech, speaker ...","Global Markets and Technologies for Voice Recognition, 2021 ...",Markets Technologies Voice Recognition Billion Billion CAGR Dublin Jan. GLOBE NEWSWIRE Research Markets Markets Technologies Voice Recognition CAGRs End Bluetooth North America Latin America Europe Asia-Pacific Device CAGRs Reviews,3
338,"Global RIS and PACS Market to Reach Worth USD 4,022.0 Mn by 2023: Intensive Electronic Medical Record (EMR) Adoption is one of the Major Drivers of this Market According to the latest report published by Credence Research, Inc. RIS and PACS Market  (RIS Market  Delivery Mode: Cloud based, On-Premise, Web-based, Product Type: Integrated RIS, Standalone  RIS, End-User: Hospitals, Imaging Centers and Other; PACS Market  Product Type: Departmental and Enterprise, Modalities: X-Ray, CT Scan, MRI, PET & SPECT and Ultrasound): Market Growth, Future Prospects and Competitive Analysis, 2016-2023,themarket was valued at USD 4,022.0 Mn in 2015, and is expected to reach USD 6,430.9 Mn by 2023, expanding at a CAGR of 5.8% from 2016 to 2023. Browse the fullreport RIS and PACS: Market Growth, Future Prospects and Competitive Analysis, 2016-2022 at http://www.credenceresearch.com/report/ris-and-pacs-market A radiology information system is a computer system that is aimed to support effective workflow and business analysis in a radiology department. This system helps in managing all the business functions of the department, from patient management, analysis, and inventory control. These systems provide ease of use and are easily accessible. Moreover, these systems offer benefits, such as high security, reliability, and privacy as they can be accessed by only authorized users. The radiology information system (RIS) is considered the core system for the electronic management of imaging departments. Picture Archiving and Communication System (PACS) facilitates short- and long-term storage, retrieval, management, distribution and presentation of medical images. PACS allows healthcare systems to image management on departmental and enterprise levels. Rapid evolution and adoption of health information technology is supporting the demand for PACS. Growth of this market will be driven by government reforms and initiatives towards healthcare centralization. Penetration of PACS into specialties such as endoscopy and oncology will lead to additional PACS applications and thus market growth. Furthermore, features such as integrated speech recognition and advanced clinical decision support systems will also contribute to expansion of PACS market. The introduction of PACS established a co-dependent relationship between the RIS and PACS. Optimal efficiency for the imaging department occurs when PACS and RIS integration enables seamless information sharing between RIS and PACS. The geographical segmentation of the global RIS and PACS market comprises North America, Europe, Asia-Pacific, Latin America and Middle East and Africa. North America accounted for the largest market in the global RIS and PACS market. Due to improved quality of patient care and clinical outcomes and the growing implementation of health information exchanges (HIEs) and EHR systems. In addition to the demand for RIS and PACS solutions from healthcare providers, the mounting need of healthcare insurance providers to capably manage a detailed record of claims and reimbursements also assists the growth of the North American RIS and PACS market throughout the forecast period. Asia-Pacific was observed as the most potential market for RIS and PACS throughout the forecast period owing to the nation and state-wide healthcare IT policies in Australia, China and Japan. Moreover, factors such as improved healthcare information technology systems, growth in the incidence rate of chronic diseases, and rise in demand for better healthcare services are some of the key drivers responsible for the growth of the RIS and PACS market in the region. The key players operating in the global RIS and PACS market include Fujifilm Teramedica, Inc, Merge Healthcare, McKesson Corporation, Siemens Healthineers, Carestream Health, Inc. The other notable players include Pismeo, Agfa Healthcare, Cerner Corporation, Philips Healthcare and Allscripts Healthcare Solutions, Inc. and others. Credence Researchis a worldwide market research and counseling firm that serves driving organizations, governments, non-legislative associations, and not-for-benefits. We offer our customers some assistance with making enduring enhancements to their execution and understand their most imperative objectives. Over almost a century, weve manufactured a firm extraordinarily prepared to this task. ",https://www.pdfdevices.com/global-ris-and-pacs-market-to-reach-worth-usd-4022-0-mn-by-2023-credence-research/,"Furthermore, features such as integrated speech recognition and advanced clinical decision support systems will also contribute to expansion ...",Global RIS and PACS Market to Reach Worth USD 4022.0 Mn by ...,RIS PACS Market Reach Worth USD Mn Electronic Medical Record EMR Adoption Major Drivers Market Credence Research Inc. RIS PACS Market RIS Market Delivery Mode Cloud On-Premise Product Type RIS Standalone RIS End-User PACS Market Product Type Departmental Enterprise CT Scan MRI PET SPECT Ultrasound Growth Future Prospects Competitive Analysis USD Mn USD Mn CAGR RIS PACS Growth Future Prospects Competitive Analysis A RIS Archiving Communication System PACS PACS PACS Growth PACS PACS Furthermore PACS PACS RIS PACS Optimal PACS RIS RIS PACS RIS PACS North America Europe Asia-Pacific Latin America Middle East Africa North America RIS PACS Due HIEs EHR RIS PACS RIS PACS Asia-Pacific RIS PACS IT Australia China Japan RIS PACS RIS PACS Fujifilm Teramedica Inc Merge Healthcare McKesson Corporation Siemens Healthineers Carestream Health Inc Pismeo Agfa Healthcare Cerner Corporation Philips Healthcare Allscripts Healthcare Solutions Inc. Researchis,3
339,"Nov 17, 2015, 23:09 ET                     from Inspur Group Co., Ltd. AUSTIN, Texas, Nov.17, 2015 /PRNewswire/ -- The leading server vendor Inspur Group and the FPGA chipmaker Altera today launched a speech recognition acceleration solution based on Altera's Arria 10 FPGAs and DNN algorithm from iFLYTEK, an intelligent speech technology provider in China, at SC15 conference in Austin, Texas. The launch results in Inspur becoming a HPC systems vendor with HPC heterogeneous computing application capabilities in GPU, MIC and FPGA. The deep learning speech recognition acceleration solution leverages an Altera Arria 10 FPGA, iFLYTEK's deep neural network (DNN) recognition algorithms and Inspur's FPGA-based DNN parallel design, migration and optimization with OpenCL. The solution has a hardware platform in CPU+Arria 10 FPGA heterogeneous architecture and software in a high-level programming model in OpenCL to enable migration from CPU to FPGAs. ""Software algorithms for deep learning models need be fine-tuned and optimized continuously. Server accelerators with fixed functionalities will have increasingly low efficiency over time and waste space and electricity,"" said Yu Zhenhua, director of technology, iFLYTEK Co., Ltd. ""In contrast, FPGAs are flexible, customizable and power-efficient. This is also an important reason that iFLYTEK decided to migrate DNN algorithms to a FPGA platform."" Field-Programmable Gate Arrays (FPGA), which have the characteristics of both an application-specific integrated circuit (ASIC) and a general chip, have the ability to do data parallel and task parallel computing simultaneously, which allows them to be more efficient in dealing with specific applications. FPGAs are currently used in logic control, signal processing and image processing and recently in online recognition systems. ""Inspur's Arria 10 FPGA-based deep learning speech recognition solution further demonstrates the performance-per Watt advantages that FPGA accelerators provide,"" said David Gamba, general manager of the computer & storage business unit at Altera. ""This success in solution development will become an important reference for FPGAs in the deep learning field."" Meanwhile, Inspur is also expanding its software cooperation on the speech recognition system, designing OpenCL programming frameworks combined with iFLYTEK's applications, to increase the efficiency of application programming. With these efforts, Inspur can enable the migration of more applications to FPGA-based platforms and foster an FPGA ecosystem, which includes FPGA software, hardware and an applied algorithms library. When speaking about further cooperation, Hu Leijun, vice president of Inspur, said that Inspur is committed to providing clients with computing solutions that best suit their needs. Given FPGA-based solutions great advantages in terms of performance per watt, Inspur will expand its software cooperation with IFLYTEK and Altera on FPGA-based deep learning online speech recognition applications. Moreover, Inspur will develop an FPGA-based system solutions, covering full cabinet computing, Internet and storage solutions, with the aim of making these solutions available for applications and clients in other fields. In the future, a CPU+FPGA solution will probably be the new heterogeneous computing model for HPC,more and moreHPC applications, data center applications and Internet deep learning applications will use CPU+FPGA solution. High performance: When processing 100 bounds data, the DNN running time based on two Intel's Xeon E5-2650 V2 CPU (16 cores) is 242.027s, while the DNN running time based on Altera's Arria 10 FPGA is 84.312s, with a faster performance acceleration of 2.871. Low power consumption: The power consumption of Altera's Arria 10 FPGA and two Intel Xeon E5-2650 V2CPU are respectively 30W and 190W, with the power consumption of FPGA-based system is 15.7 percent that of a CPU system. In an actual test of the DNN algorithms, an FPGA-based system can realize high performance per watt, up to 30GFlops/W, greatly saving application power costs. Easy to program: It only took four man-months for software engineers to do FPGA-based DNN parallel program development with OpenCL programming models. If traditional underlying languages, such as Verilog and VHDL, were used, it would take 12 man-months at least to do similar development, with collaboration between software engineers and hardware engineers required. High adaptability: FPGA can execute data-parallel computing with a DNRange model or task-parallel computing with a Pipeline model to address more applications and bring overall performance improvement to more applications and software. Inspur Group leverages its world-leading research, development and innovation systems, including the National Key Lab for High-Performance Server and Storage Technology, National Engineering Center for Information Storage Technology, Inspur-Intel China Parallel Computing Joint Lab , Inspur-NIVDIA Cloud Supercomputing Application Innovation Center, Inspur's own impressive R&D, system building, operation & maintenance and service capabilities in supercomputers from teraflops to petaflops as well as a complete set of HPC hardware and software product lines provides superior industry-leading supercomputing systems and application services across many sectors in China. Application of Inspur's products and technologies includes university and scientific research institutions, oil & gas exploration, weather forecasting, aerospace and aviation, manufacturing design, animation rendering and environmental monitoring. iFLYTEK Co., Ltd. iFLYTEK, the largest intelligent speech technology provider in China, has long engaged in developing intelligent speech technologies and has world-leading achievements in many technologies, including speech synthesis, speech recognition, speech evaluation and natural language processing. The accuracy of iFLYTEK's industry-leading speech recognition technology is 98 percent. Altera programmable solutions enable designers of electronic systems to rapidly and cost effectively innovate, differentiate and win in their markets. Altera offers FPGA, and complementary technologies, such aspower solutions to provide high-value solutions to customers worldwide. ",http://www.prnewswire.com/news-releases/inspur-and-altera-launch-speech-recognition-fpga-solution-with-opencl-300180790.html,"17, 2015 /PRNewswire/ -- The leading server vendor Inspur Group and the FPGA chipmaker Altera today launched a speech recognition ...",Inspur and Altera Launch Speech Recognition FPGA Solution with ...,Inspur Group Co. Ltd. AUSTIN Texas Nov.17 Inspur Group FPGA Altera Altera Arria FPGAs DNN China SC15 Austin Texas Inspur HPC HPC GPU MIC FPGA Altera Arria FPGA DNN Inspur DNN OpenCL CPU+Arria FPGA OpenCL CPU FPGAs Software Server Yu Zhenhua Co. Ltd. FPGAs DNN FPGA Gate Arrays FPGA ASIC FPGAs Inspur Arria Watt FPGA David Gamba Altera FPGAs Inspur OpenCL Inspur FPGA FPGA Hu Leijun Inspur Inspur Inspur IFLYTEK Altera Inspur Internet CPU+FPGA HPC Internet CPU+FPGA DNN Intel Xeon E5-2650 V2 CPU DNN Altera Arria FPGA Altera Arria FPGA Intel Xeon E5-2650 V2CPU CPU DNN DNN OpenCL Verilog VHDL DNRange Pipeline Inspur Group National Key Lab High-Performance Server Storage Technology National Engineering Center Information Storage Technology Inspur-Intel China Parallel Computing Joint Lab Inspur-NIVDIA Cloud Supercomputing Application Innovation Center Inspur R D HPC China Inspur Co. Ltd. China Altera Altera FPGA,4
340,"Lexacom Echo is a new cloud-based speech recognition solution, which has been designed to remove many of the barriers traditionally associated with the use of this technology. It is available to anyone using Lexacoms digital dictation platform Lexacom 3. Lexacom is the only provider of advanced digital dictation and workflow solutions that can offer the most flexible range of fully integrated services, making us the number one choice for healthcare providers. Lexacom 3 can be configured specifically for any organisational structures requirements. For more information, contact sales@lexacom.co.uk or 01295 236910. Speech recognition technology can help practices create documents accurately, securely and efficiently, saving GPs and practice managers time. Until now, one of the biggest barriers to using speech recognition has been the cost of buying the service. However, with Lexacom Echo occasional users can opt for a pay-as-you-go option where they only pay for what they use, with no minimum term or pre-payment, while more regular users can opt for a monthly subscription giving them unlimited use of the service. The product requires no upfront investment nor the need for any upgraded or expensive PC hardware. Lexacom Echo provides two modes for dictation. Accessed via Lexacom 3, the system offers deferred speech recognition as a standard service and, at the touch of a button, direct live speech recognition compatible with any third party software. Profession-specific dictionaries are regularly updated, to provide full support for medical terminology. Lexacom 3 is the only digital dictation solution approved for use with all of the UKs primary care clinical systems and integrates with third party outsourced transcription services (provided by trusted partners) and Docmails Print to Post services. Lexacom Echo is compatible on any PC running Lexacom 3. All updates to the system are included in the charges and bespoke one-to-one training sessions are available for users to ensure they can get the most from the software. ",http://www.gponline.com/new-speech-recognition-technology-help-gp-practices-streamline-work/article/1388745,"Lexacom Echo is a new cloud-based speech recognition solution, which has been designed to remove many of the barriers traditionally ...",New speech recognition technology could help GP practices ...,Lexacom Echo Lexacoms Lexacom Lexacom GPs Lexacom Echo Lexacom Echo Lexacom Lexacom UKs Docmails Print Post Lexacom Echo Lexacom,9
341,"Voice-controlled interfaces are showing up in mobile phones, TVs, and automobiles. One company believes it can give just about everything a voice. Until recently, the idea of holding a conversation with a computer seemed pure science fiction. If you asked a computer to open the pod bay doorswell, that was only in movies. But things are changing, and quickly. A growing number of people now talk to their mobile smart phones, asking them to send e-mail and text messages, search for directions, or find information on the Web. Were at a transition point where voice and natural-language understanding are suddenly at the forefront, says Vlad Sejnoha, chief technology officer of Nuance Communications, a company based in Burlington, Massachusetts, that dominates the market for speech recognition with its Dragon software and other products. I think speech recognition is really going to upend the current [computer] interface. Progress has come about thanks in part to steady progress in the technologies needed to help machines understand human speech, including machine learning and statistical data-mining techniques. Sophisticated voice technology is already commonplace in call centers, where it lets users navigate through menus and helps identify irate customers who should be handed off to a real customer service rep. Now the rapid rise of powerful mobile devices is making voice interfaces even more useful and pervasive. Jim Glass, a senior research scientist at MIT who has been working on speech interfaces since the 1980s, says todays smart phones pack as much processing power as the laboratory machines he worked with in the 90s. Smart phones also have high-bandwidth data connections to the cloud, where servers can do the heavy lifting involved with both voice recognition and understanding spoken queries. The combination of more data and more computing power means you can do things today that you just couldnt do before, says Glass. You can use more sophisticated statistical models. The most prominent example of a mobile voice interface is, of course, Siri, the voice-activated personal assistant that comes built into the latest iPhone. But voice functionality is built into Android, the Windows Phone platform, and most other mobile systems, as well as many apps. While these interfaces still have considerable limitations (see Social Intelligence), we are inching closer to machine interfaces we can actually talk to. Nuance is at the heart of the boom in voice technology. The company was founded in 1992 as Visioneer and has acquired dozens of other voice technology businesses. It now has more than 6,000 staff members at 35 locations around the world, and its revenues in the second quarter of 2012 were $390.3 million, a 22.4 percent increase over the same period in 2011. In recent years, Nuance has deftly applied its expertise in voice recognition to the emerging market for speech interfaces. The company supplies voice recognition technology to many other companies and is widely believed to provide the speech component of Siri. Speech is ideally suited to mobile computing, says Nuances CTO, partly because users have their hands and eyes otherwise occupiedbut also because a single spoken command can accomplish tasks that would normally require a multitude of swipes and presses. Suddenly you have this new building block, this new dimension that you can bring to the problem, says Sejnoha. And I think were going to be designing the basic modern device UI with that in mind. Inspired by the success of voice recognition software on mobile phones, Nuance hope to put its speech interfaces in many more places, most notably the television and the automobile. Both are popular and ripe for innovation. To find a show on TV, or to schedule a DVR recording, viewers currently have to navigate awkward menus using a remote that was never designed for keying in text queries. Products that were supposed to make finding a show easier, such as Google TV, have proved too complex for people who just want to relax for an evenings entertainment. At Nuances research labs, Sejnoha demonstrated software called Dragon TV running on a television in a mocked-up living room. When a colleague said, Dragon TV, find movies starring Meryl Streep, the interface instantly scanned through channel listings to select several appropriate movies. A version of this technology is already in some televisions sold by Samsung. Apple is widely rumored to be developing its own television, and its speculated that Siri will be its controller. The idea has been fueled by Walter Isaacsons biography of Steve Jobs, in which the late CEO is said to have claimed that hed finally solved the TV interface. Meanwhile, the Sync entertainment system in Ford automobiles already uses Nuances technology to let drivers pull up directions, weather information, and songs. About four million Ford cars on the road have Sync with voice recognition. Last week, Nuance introduced software called Dragon Drive that will let other car manufacturers add voice-control features to vehicles. Both these new contexts are challenging. One reason voice interfaces have become popular on smart phones is that users speak directly into the devices microphone. To ensure that the system works well in televisions and cars, where there is more background noise, the company is experimenting with array microphones and noise-canceling technology. Nuance makes a number of software development kits available to anyone who wants to include voice recognition technology in an application. Montrue Technologies, a company based in Ashland, Oregon, used Nuances mobile medical SDK to develop an iPad app that lets physicians dictate notes. Its astonishingly accurate, says Brian Phelps, CEO and cofounder of Montrue and himself an ER doctor. Speech has turned a corner; its gotten to a point where were getting incredible accuracy right out of the box. In turn, the kits shore up Nuances position, helping the company improve its voice recognition and language processing algorithms by sending ever more voice data through its servers. As MITs Glass says, there has been a long-time saying in the speech-recognition community: Theres no data like more data. Nuance says it stores the data in an anonymous format to protect privacy. Sejnoha believes that within a few years, mobile voice interfaces will be much more pervasive and powerful. I should just be able to talk to it without touching it, he says. It will constantly be listening for trigger words, and will just do itpop up a calendar, or ready a text message, or a browser thats navigated to where you want to go. Perhaps people will even speak to computers they wear, like the photo-snapping eyeglasses in development at Google. Sources at Nuance say they are actively planning how speech technology would have to be architected to run on wearable computers. ",https://www.technologyreview.com/s/427793/where-speech-recognition-is-going/,I think speech recognition is really going to upend the current [computer] interface. Progress has come about thanks in part to steady progress ...,Where Speech Recognition Is Going,TVs Web Were Vlad Sejnoha Nuance Communications Burlington Massachusetts Dragon [ Jim Glass MIT Smart Glass Siri Android Windows Phone Social Intelligence Visioneer Nuance Siri Speech Nuances CTO Sejnoha UI Nuance DVR Google Nuances Sejnoha Dragon Dragon Meryl Streep Samsung Apple Siri Walter Isaacsons Steve Jobs CEO Sync Ford Nuances Ford Nuance Dragon Drive Both Nuance Montrue Technologies Ashland Oregon SDK Brian Phelps CEO Montrue ER Nuances MITs Glass Nuance Sejnoha Google Nuance,1
342,"Sonja Brown and Vicky Humberstone of VoicePower in Harrogate are expanding their firm as demand for their voice recognition and digital dictation software grows. A Harrogate-based technology company which is helping NHS departments to reduce waiting times is preparing for further growth in the next 12 months. VoicePower supplies speech recognition and digital dictation software which enables businesses and organisations to work more efficiently. With growing pressure on NHS services, GP practices and other healthcare groups across the North have used the companys recommended software to make savings on both time and costs. Director Sonja Brown said: We have worked with NHS trusts whose departments were running significantly behind targets and were struggling to keep up with their workload. By introducing speech recognition software, enabling consultants to have their comments transcribed instantly rather than waiting for recordings to be transferred to and processed by a secretary, several departments have reduced their turnaround times for results and letters to just a few hours. In cases such as pathology departments, where patients are waiting for their results before being given the treatment they need, this is particularly rewarding. Founded by Colin Wormald in 1993, VoicePower was taken over by Ms Brown and business partner Vicky Humberstone when he retired in 2010. Since then, they have spent time developing its services and expanding its client base, and new opportunities have helped to fuel demand. The company now has a team of eight working across the North and is now expecting to take on two new employees in the next 12 months. Ms Brown, who has been accepted onto the Goldman Sachs 10,000 Small Businesses programme, said: This is an ideal time for VoicePower to develop and grow. People are becoming much  more familiar with speech recognition and related technology, because it is available in their existing environment: on phones, in cars, via games consoles and so on. Suddenly, they are realising the potential for its application in business  and the savings of time, money and effort that can be achieved as a result. ",http://www.harrogateadvertiser.co.uk/news/business/health-sector-demand-fuelling-growth-at-voice-recognition-firm-1-8416112,VoicePower supplies speech recognition and digital dictation software which enables businesses and organisations to work more efficiently.,Health sector demand fuelling growth at voice recognition firm,Sonja Brown Vicky Humberstone VoicePower Harrogate NHS VoicePower NHS GP North Director Sonja Brown NHS Colin Wormald VoicePower Ms Brown Vicky North Ms Brown Goldman Sachs Small Businesses VoicePower,-1
343,"Siri, are you afraid of Facebook's new speech recognition tech 6 Jan 2015 at 05:01, Neil McAllister Facebook is buying speech recognition startup Wit.ai, in an apparent bid to counter rival chatterbox tech from the likes of Apple, Google, and Microsoft. ""Facebook's mission is to connect everyone and build amazing experiences for the over 1.3 billion people on the platform  technology that understands natural language is a big part of that, and we think we can help,"" Wit.ai said in a blog post on Monday. Wit.ai's offering, in a nutshell, is speech recognition as a service. The company was cofounded in 2013 by Alex Lebrun, a former vice president at voice recognition heavyweights Nuance, and Willy Blandin, a specialist in machine learning and natural language processing. Developers use the firm's online console to define examples of the type of voice commands they expect users of their applications to say, along with the associated ""intents""  signals that are tripped when the Wit.ai speech recognition engine recognizes the appropriate phrases. Developers then code their apps to capture audio of the user's voice and upload it to Wit.ai's servers, which process it and return structured data based on what the recognition engine thinks it heard. When the application gets a message indicating a certain intent was requested, it can take the appropriate action. The engine can even learn as users send in more input from a given application, offering new possible phrasings for given intents. If the engine guesses wrong, developers can go in and retrain it or add new intents as necessary. It's not hard to guess why this tech would be attractive to Facebook, which so far has trailed behind its rivals in the speech recognition department. Apple's Siri personal assistant has been a major selling point of recent iThings, and Microsoft has been trying to make similar waves with its own Cortana. Google, meanwhile, has been busy baking speech tech into everything it can think of. Speech recognition is especially important for the mobile market, where talking to your phone is often easier than typing in long messages using fiddly onscreen keyboards. And given that most of Facebook's daily and monthly active users are now accessing the service via their mobile devices  and 59 per cent of the social network's ad revenue now comes from mobile  speech recognition is an area that it desperately needs to address. The terms of the deal were not disclosed, but it could be a big payday for Wit.ai's early investors. The Palo Alto, Californiabased firm raised its initial $3m of funding as recently October 2014, in a seed round led by Silicon Valley venture capitalists Andreessen Horowitz. How Wit.ai's current executive team will merge into Facebook was also not explained. One important change was mentioned, however. Currently, the Wit.ai service is free for ""open data projects,"" where developers allow others to reuse their intents and expressions. Commercial users have to pay, however, with pricing as high as $1,499 per month. On Monday, the Wit.ai team said that under Facebook, ""The platform will remain open and become entirely free for everyone.""  ",https://www.theregister.co.uk/2015/01/06/facebook_buys_wit/,"Facebook is buying speech recognition startup Wit.ai, in an apparent bid to counter rival chatterbox tech from the likes of Apple, Google, and ...","Siri, are you afraid of Facebook's new speech recognition tech?",Siri Facebook Jan Neil McAllister Facebook Wit.ai Apple Google Microsoft Facebook Wit.ai Monday Wit.ai Alex Lebrun Nuance Willy Blandin Developers Wit.ai Wit.ai Facebook Apple Siri Microsoft Cortana Google Facebook Wit.ai Palo Alto October Silicon Valley Andreessen Horowitz How Wit.ai Facebook Wit.ai Monday Wit.ai Facebook,-1
344,"ODG's combination AR/VR glasses are drawing attention at CES as part of a swell of interest in augmented and virtual-reality products. The beeping, flashing, pulsating glory of the worlds largest consumer electronics trade show has returned to Las Vegas. The first batch of new products and services went on display at CES on Tuesday, and startups and industry giants will debut more gadgets and technologies throughout the week. Just a few of the curious wares spotted by IEEE Spectrum editors last night include a battery-powered scarf that filters air pollution, a hairbrush that uses sound waves to analyze dryness and frizz, a smart cane that detects falls , and a connected cat feeder that avoids overfeeding by recognizing felines by implanted microchips. Also, a US $120 camera that lets you stare at the inside of your refrigerator, should you ever choose to do that (assuming the milk isnt blocking the view). Major technology companies have also begun to make their announcements about new products they will launch in 2017. Qualcomm released its newest chip, the Snapdragon 835 , which, rumor has it , could turn up in Samsung Galaxy 8 smartphones later this year. Huawei said its newest Honor smartphone, called the 6X , which boasts a battery life of 2.1 days and costs only $250, is now available in the United States.And Faraday Future unveiled its long-awaited self-parking FF 91 electric car , which integrates more than 30 sensors including cameras and a retractable lidar system to navigate into a parking space all on its own. Looking at deeper trends, several experts said the most meaningful long-term developments will come from the companies scraping away at voice recognition. Once we master it, they believe, voice-recognition capabilities will fundamentally change the way we interact with and build electronics. This was a strong element of Tuesdays analysis of the global consumer marketby Shawn DuBravac , chief economist, and Steve Koenig , senior director for market research,of the Consumer Technology Association (CTA), which runs CES. In DuBravacs opinion, voice-recognition technology has improved enough in the past few years that it is now poised to usher in an era of so-called faceless computing. In particular, the word error rate for voice-recognition systems dropped from 43 percent in 1995 to just 6.3 percent this year, and is now on par with humans. We have seen more progress in this technology in the last 30 months than we saw in the first 30 years, DuBravac said. Another analyst attending CES that I spoke to was Ronan de Renesse , a consumer technology analyst for the business intelligence firm Ovum, who said he was watching a startup called Voicebox , which has worked on voice recognition for partners including Samsung, AT&T, and Toyota. In addition to redefining the traditional computer interfaces, voice recognition could improve a host of products that are already on the market. CTA estimates total sales of voice-activated digital assistants such as Google Home or Amazon Echo to be about 5 million units to date, and expects that to double to 10 million in 2017. With all of these products, clarity and functionality are key. DuBravac figures there are currently about 1,500 apps (called skills in Amazon-speak) that can interact with Alexa, Amazons voice-activated personality and says he would not be surprised to see 700 new ones announced just this year at CES. Aside from voice recognition, de Renesse also thinks that virtual reality and augmented reality will be at the forefront of CES again this year. These technologies had a flagship 2016 with the release of the HTC Vive and Oculus Rift headsets, but some have since complained that the technology isnt catching on as quickly as theyd hoped. One reason could be that theres still a profound lack of high-quality VR and AR content to enjoy for those who do shell out $600 or more for a headset. DuBravac says these criticisms are partly a symptom of too-high initial expectations for VR, and not necessarily a reflection of the technology itself. Hes still optimistic, however, because he sees companies investing in VR content. As for his assessment of the progress made in 2016: If you had realistic expectations about what would happen and the deployment of hardware would look like, then I think you saw a market starting to take hold, he says. Anshel Sag , an analyst at Moor Insights & Strategy, is also frustrated by the proclamations that VR is struggling. Even though he doesnt expect any major VR announcements at CES 2017, he says thats because the product-release cycles of VR companies simply didnt sync up with CES this year. But he cautions anyone from reading too much into this. Nonetheless, several headset manufacturers and content developers are planning to put their best foot forward at the show. Samsung will continue to push mobile VR, which operates on less expensive headsets, such as the $60 Samsung Gear , that allow you to insert your smartphone to stream VR. Sag has also been impressed by a company called ODG , which is working on a pair of heavy-duty eyeglasses that convert from viewing in AR to VR. Funnily enough, CES might also be at least partly to blame for VR criticisms. Every year at the show, analysts and journalists try to predict the new fads and hottest products that will redefine consumer technology as we know it. Too often, they are surprised when those trends fail to materializeor reach the adoption rates they had expected. The truth is that breakout tech stars are a relatively rare sight, even at CES. In fact, the vast majority of global consumer tech spending80 percentgoes toward just seven types of products. The CTAs Steve Koenig calls those technologies the magnificent seven. That includes smartphones, laptops, tablets, desktops, digital cameras, TVs, and smart watches (a recent addition as the Apple iWatch outpaced the iPhone in first-year sales). On their own, smartphones account for a staggering 47 percent of global consumer spending on technology and remain the center of the consumer tech universe, as Koenig puts it, with their own ecosystem of apps and services. But to be fair, they were also first released more than a decade ago. Rather than looking at everything through the lens of mass adoption, DuBravac saysthe market for most tech products is actually very fragmented. As an example, he points to the wearables market and the $125 VERTbelt for athletes that measures their jumps during practice and games. In his assessment, lots of startups will offer products for a specific use and find plenty of customers without ever reaching mass adoption. And thats fine, too. ",http://spectrum.ieee.org/tech-talk/consumer-electronics/gadgets/ces-2017-the-year-of-voice-recognition,"Once we master it, they believe, voice-recognition capabilities will fundamentally change the way we interact with and build electronics.",CES 2017: The Year of Voice Recognition,ODG AR/VR CES Las Vegas CES Tuesday IEEE Spectrum US Major Qualcomm Snapdragon Samsung Galaxy Huawei Honor United States.And Faraday Future FF Tuesdays Shawn DuBravac Steve Koenig Consumer Technology Association CTA CES DuBravacs DuBravac CES Ronan Renesse Ovum Voicebox Samsung AT T Toyota CTA Google Home Amazon Echo DuBravac Amazon-speak Alexa Amazons CES Renesse CES HTC Vive Oculus Rift VR AR DuBravac VR Hes VR Anshel Sag Moor Insights Strategy VR CES VR CES Samsung VR Samsung Gear VR Sag ODG AR VR Funnily CES VR Too CES CTAs Steve Koenig TVs Apple Koenig DuBravac VERTbelt,3
345,"GOOGLE HAS BEEN expounding on the improvements to speech recognition in the Google App. The Google App, which is the official title of what most of us refer to as Google Now , doesn't tend to get as much attention as its equivalents Siri on iOS and Cortana on Windows. Partly this is because Google has sidestepped the whole 'personality' side of development and quietly got on with making it more useful. Satyajeet Salgar, product manager for the Google App, talked in a blog post today about how recent developments in machine learning functionality have allowed it to treat queries of a more complex nature with more context than ever before. ""Now were 'growing up' just a little more. The Google App is starting to truly understand the meaning of what youre asking. We can now break down a query to understand the semantics of each piece, so we can get at the intent behind the entire question,"" he said. ""That lets us traverse the knowledge graph much more reliably to find the right facts and compose a useful answer. And we can build on this base to answer harder questions."" We tried this out using Michael Jackson as an example, and got mixed results. She did well here, citing a reference to Billie Jean, and explaining that it was one of the biggest selling singles of all time. Ten out of 10 here. 29 August 1958. She waffles on a bit here based on some details got from YouTube. Covering her arse, she says 'According to Wikipedia ...' before answering. INQ: Who was president when he died At this point, she goes a bit doolally, confirming defiantly: 'Roosevelt.' At which point we descend into farce. So, yes, Google Now is getting better, but the window in which to ask contextual questions is very short. If you leave it too long before asking a question using a pronoun, even if it was the last one you asked and it's still on the screen, it loses its train of thought. At one point she started telling us about the life of Jesus. To add to the fun yesterday, our always-on Google Now phone ignored ""OK Google"" five times in a row, but was triggered by walking past a speaker playing Take My Breath Away by Berlin. Speech recognition is getting better, but there's a lot of work to do. Progress might now start to be a little quicker with Google recently open-sourcinglearning platform Tensorflow .  ",http://www.theinquirer.net/inquirer/news/2435120/google-improves-its-speech-recognition-with-mixed-results,"The Google App, which is the official title of what most of us refer to as Google Now, doesn't tend to get as much attention as its equivalents Siri ...",Google improves its speech recognition with mixed results,GOOGLE HAS BEEN Google App Google App Google Siri Cortana Windows Google Satyajeet Salgar Google App Google App Michael Jackson Billie Jean August YouTube Wikipedia Google Jesus Google Google Take My Breath Away Berlin Google Tensorflow,4
346,"Use voice commands to control applications, compose text, and make Windows talk like William Shatner. By Dan Patterson | January 25, 2017, 9:57 AM PST Tech giants are locked in a hot race to put voice-controlled virtual assistants on your phone and in your pocket, car, and home. Amazon's Alexa dominated 2017's Consumer Electronics Show, Google Assistant is woven into the fabric of all Google services, Siri accessible with one-tap on everything from the iPhone to AirPods , and Microsoft's Cortana could represent the future of Windows ' mobile strategy. The boom in personalized AI products implies a macro trend towards voice-controlled computer interface products that, if embraced by business and consumers, could portend a profound shift in how humans interact with machines. Due to the prevalence of mobile devices, it's easy to forget that Apple , Google , and Microsoft all offer sophisticated voice-controlled virtual assistants for PCs as well. Microsoft's speech recognition AI is not perfect but is a cornerstone of Windows 10, and with the right software and speech training can be used to control a bevy of desktop applications. Gaming helped push the limits of PC speech interface innovation. Late last year an inspired developer discovered how to use Amazon's Echo to control the space simulator Elite:Dangerous . Amazon recently launched a game engine called Lumberyard and attracted Elite:Dangerous competitor Star Citizen to the cloud platform . Soon Amazon could offer game and PC controls as a core ingredient of the Alexa platform. Elite: Dangerous and Star Citizen voice controls both rely on a Windows application called VoiceAttack . Though it was developed to control video games VoiceAttack can use Microsoft's speech recognition software to launch and control many Windows 10 applications. First, prepare your machine to install the program. VoiceAttack supports and updates the software, but as with any installation make sure you back up your machine and make sure your Windows 10 install is bug-free. Next, point your browser to https://voiceattack.com/#download-1 to download the latest version of the software. Note that VoiceAttack offers a free trial and costs $9.99 for the premium version. Free users can try out every feature available to premium users but are limited to a total of 20 voice commands. Finally, install the software and follow the configuration prompts. Make sure to read and follow the instructions carefully and avoid quickly skipping through the wizard. Attach a Windows 10-compatible microphone to your computer. This can be your webcam mic, a high-quality external mic, or a desktop mic. Generally, the better your microphone the better Windows will recognize your voice. This story was produced using the Logitech C920 webcam microphone. Before you continue with VoiceAttack configuration, you will train Microsoft's speech recognition software. In the Windows 10 Cortana search menu type, speech and click the microphone icon. This will launch the Windows Speech Recognition application. Follow the speech recognition fine-tuning instructions and make sure to save or print the Windows Speech Recognition commands reference sheet . As with the VoiceAttack installation, do not skip the tutorial. Next, configure Windows Voice Training and follow the prompts to speak to your computer. Windows will walk you through a tutorial session that involves reading several paragraphs out loud. Each session takes approximately 10 minutes to complete. This is the most time-consuming but important part of the process. For best results, make sure you tune Windows voice recognition two or three times. Now you're ready to configure and use VoiceAttack. Launch the application. You will likely be met with a prompt to close Microsoft's Speech Recognition. Click Yes and allow VoiceAttack to be your primary voice recognition application. VoiceAttack will rely on the Windows voice training but operate as the default front end application. Launch VoiceAttack and issue a spoken command, like open Slack. VoiceAttack will show you a log of recognized and unrecognized commands. VoiceAttack also allows you to select and customize various input methods and applications. Add custom commands by clicking the + icon in the application menu. Here you can add custom commands, or import premade commands. The VoiceAttack forums are loaded with Windows commands . Practice. It takes time to train Microsoft and VoiceAttack to understand your unique vocal patterns. The more you use the applications, the better they respond. Bonus: Replace the default voice with William Shatner. HCS Voice Packs creates audio packages that can replace the default Windows and VoiceAttack voices. Our favorite, of course, is the William Shatner voice pack . The legendary Star Trek captain  produced hours of dialogue for both Elite: Dangerous and Windows and at your command will share information about the Big Bang, black holes, and other cool space facts. Speech interface devices are in their infancy and sure to improve with time. The more you train Microsoft's voice software, the better your results will be. Though limited now, the number of applications compatible with voice commands is rapidly growing. Using applications like VoiceAttack is not just fun and convenient, the process also helps us understand the evolution and potential future of computing. We encourage your thoughts and experiences with virtual assistants and speech-controlled applications in the comments below. Keep up to date on all of the latest in tech. Click here to subscribe to the TechRepublic Best of the Week newsletter. ",http://www.techrepublic.com/article/how-to-control-your-windows-10-pc-using-voiceattack/,"Microsoft's speech recognition AI is not perfect but is a cornerstone of Windows 10, and with the right software and speech training can be used ...",How to control your Windows 10 PC using VoiceAttack,Use William Shatner Dan Patterson | January AM PST Tech Amazon Alexa Consumer Electronics Show Google Assistant Google Siri AirPods Microsoft Cortana Windows AI Due Apple Google Microsoft Microsoft AI Windows Late Amazon Echo Elite Dangerous Amazon Lumberyard Elite Star Citizen Soon Amazon Alexa Elite Star Citizen Windows VoiceAttack VoiceAttack Microsoft Windows First VoiceAttack VoiceAttack Make Windows Windows Logitech C920 VoiceAttack Microsoft Windows Cortana Windows Windows VoiceAttack Windows Voice Training Windows VoiceAttack Microsoft Click Yes VoiceAttack VoiceAttack Windows VoiceAttack Slack VoiceAttack VoiceAttack Add VoiceAttack Windows Practice Microsoft VoiceAttack William Shatner HCS Voice Packs Windows VoiceAttack William Shatner Star Trek Elite Windows Big Bang Speech Microsoft VoiceAttack Click TechRepublic Best Week,-1
347,"A New Chip Makes Voice Control More Efficient, Less Creepy Maximizing battery life remains the great challenge of every smartphone manufacturer. People use their phones for everything these days, and of course they want a battery that lasts forever, and chargesin minutes. Engineers have a few ways of tackling this problem beyondpacking a bigger, and potentially more dangerous, lithium-ion battery inside. The most effective trick is making the chips, drivers, and other components as energy efficient as possible. The obvious targets include big screens, 4G modems, and Bluetooth. But researchers are taking a close look at the always-on voice assistants that let you bark a command without touching a button. Those things can suck down a lot of power, but researcher at MITs Microsystems Technology Laboratories found a way of making them thriftier , too. Alexa, Siri, and Google Assistant use the cloud to process voice commands. In a clever twist, the MIT chip handles much more of thatprocessing itself, easing the burden on other componentsand saving power. Even if power consumption isnt an issue, hardware accelerators can be useful for making devices simpler and lower cost, says Michael Price, who designed the new chip. If you can offload a difficult computation from the main processor, that processor doesnt have to be as fast. That means manufacturers can use a less expensive processor. Cheaper is good, but increased efficiency is betterand Price set out to radically reduce the amount of power required to drive voice-assistant features. Smartphones generally need 1watt of power to drive a single speech-recognition query, Price says. The system his team developed requires about 1/100 as much in the worst case.1 Some basic voice-processing functions sipped just 0.2 milliwatts, making it 5,000 times more efficient than the 1 watt benchmark. The primary energy savings comes inmaking the chip more adept at recognizing speech. Instead of streamingaudio over a web connection to a server, the processor converts speech to text locally. Processing those queries as a text file consumesfar less power. The system also is more power-savvy when detecting speech; a low-power circuit within the chip detects when ambient noise is interrupted by a voice, then triggers the primarysystem when a voice command is registered. The research process includeda counterintuitive finding. Prices team tested three circuits, and found that the most power-hungry of them resulted in the greatest energy savings. Why Because it registered fewer false-positives than the others, which often activated the speech-processing chip after mistaking ambient noise for a voice command. Its minuscule power requirement means you could seethe chip providingvoice-control capabilities in the next wave of smaller IoT devices with tiny batteries. That said, Price couldnt comment on whether youll see thetechnology in consumer products anytime soon. MIT developed the chip specifically for battery-powered gadgets, but similar components could impacthow plug-in devices such as the Amazon Echo and Google Home work. If an in-home device is doing speech recognition locally and that turns out to be a processing bottleneck, then our technology could be useful, Price says. The sound clips recorded by your Echo and Home stay on the companies servers even after theyre processed. Amazon and Google have excellent security and privacy protection records, but more on-device processing means less personal data stored in the cloud. Price says converting speech to text on the device before zinging it to a server would eliminate some info from the captured data, such as the speakers age, accent, gender, and ambient noise in the background. Of course, privacy is up to the system designer, Price says. Theres nothing stopping them from saving the audio on a device or transmitting it, even if the speech recognition is done locally. True. But any technology that makes voice commands more efficient and less creepy is a good thing. 1UPDATE March 10, 2017 10:00 a.m. ET: This story has been updated to correct statistics for MITs new chip technology. The worst-case scenario in their testing drew 1/100th of a watt, not 1/1,000th of a watt. ",https://www.wired.com/2017/03/mit-microsystems-technology-lab-voice-chip/,"Smartphones generally need 1 watt of power to drive a single speech-recognition query, Price says. The system his team developed requires ...","A New Chip Makes Voice Control More Efficient, Less Creepy",New Chip Makes Voice Control Efficient Less Creepy Maximizing Bluetooth MITs Microsystems Technology Laboratories Alexa Siri Google Assistant MIT Michael Price Cheaper Price Price IoT Price MIT Amazon Echo Google Home Price Echo Home Amazon Google Price Price True March MITs,7
348,"In the trials analysis of 1,455 reports, physician documentation was completed 26% faster using web-based ASR assistance. Medical speech recognition is a meaningful and effective tool for the clinical documentation process. DUSSELDORF, GERMANY - (HealthTech Wire / News)  The speed and accuracy of medical documentation when dictated by physicians compared to when typed by them, has been the subject of an independent scientific trial at the Dsseldorf University Clinic. Believed to be the first of its kind worldwide, the trial also set out to establish the effects of using medical speech recognition solutions  from Nuance Communications - in a university hospital with respect to documentation completeness, length and, importantly, physician satisfaction over other means of entering information into Electronic Patient Records (EPRs). Key findings showed that the overall increase in documentation speed through web-based automatic speech recognition (ASR) assistance was 26%. They also showed that participants documented an average of 356 characters per report when not assisted by ASR and 649 characters per report when assisted by ASR  an 82% increase. In the trials analysis of 1,455 reports, physician documentation was completed 26% faster using web-based ASR assistance. In terms of physicians well-being, using a scale of 1-3 (where 1 = good), the trials participants average mood rating was 1.3 using ASR, compared to 1.6 without ASR assistance. In their conclusion, the authors of the report stated that medical documentation with the assistance of speech recognition technology leads to an increase in documentation speed, document length and completeness and an uplift in participants' mood when compared to self-typing. They also concluded that medical speech recognition is a meaningful and effective tool for the clinical documentation process. The research findings come at a key time for healthcare providers as they make the transition to EPRs and consider the most effective data input options in order to realise the full benefits of deploying digital records. Given that healthcare professionals spend a significant portion of their time on the computer and are rarely trained typists, alternatives to typing - such as modern and accurate speech recognition technology have the potential to improve the satisfaction of health professionals through increased documentation efficiency, accuracy and quality. Commenting on the findings, Alan Fowles, Senior Vice President, Nuance Healthcare International Sales & Operations, stated: Medical professionals consistently voice their desire to have their administration burden reduced in order to have more time to spend with their patients, to put them back at the centre of care. This most recent trial demonstrates that using speech recognition can help free healthcare providers to deliver the personal attention that both patients demand and that healthcare professionals want to provide. He added: This research complements another piece of research we commissioned in the UK and Germany. The Clinical Documentation Challenge found that clinicians spend up to 50% of their time on clinical documentation, which has wide economic implications on hospitals in the UK. In a free webinar on 17th March, 15.00-15.45hr Nuance will present a more in-depth look into the studys results. They will also demonstrate the opportunities of using medical speech recognition today. The webinar will be held in German. Please register here . ",http://www.hitcentral.eu/healthtech-wire/independent-trial-finds-speech-recognition-solutions-increase-physician,This most recent trial demonstrates that using speech recognition can help free healthcare providers to deliver the personal attention that both ...,Independent trial finds speech recognition solutions increase ...,ASR Medical DUSSELDORF GERMANY HealthTech Wire / News Dsseldorf University Clinic Nuance Communications Electronic Patient Records EPRs Key ASR ASR ASR ASR ASR ASR EPRs Alan Fowles Senior Vice President Nuance Healthcare International Operations UK Germany Documentation Challenge UK March Nuance German Please,1
349,"Kids needed to say just two words - ""jump"" and ""go"" - to control a video game called Mole Madness, but Disney researchers had to design a speech technology system capable of sorting through the overlapping speech, social side talk and creative pronunciations of young children to make it work. The keyword-spotting system developed by Disney Research works better for this game application than commercial speech recognition systems, which are derived largely from adult speech. ""The system's responsiveness and accuracy helped children enjoy the rapid-paced, multi-player game,"" said Jill Fain Lehman, senior research scientist at Disney Research. ""Speech recognition applications have become increasingly commonplace as the technology has matured, but understanding what kids say when they play remains difficult,"" said Jessica Hodgins, vice president at Disney Research. ""This latest work by our researchers could make it possible to design any number of speech-based game or entertainment applications for children, including interactions with robots."" Lehman and her colleagues, Nikolas Wolfe and Andre Pereira, will present their findings at the Workshop on Child Computer Interaction Sept. 6-7 in San Francisco and at the International Conference on Intelligent Virtual Agents Sept. 20-23 in Los Angeles. ""Kids don't necessarily pronounce words quite like adults and when they are playing together, as they like to do, they often engage in side banter, or exclamations of excitement, or simply talk over each other,"" Lehman said. ""That makes it tough for a speech-based system, even one that just has to detect the words 'go' and 'jump' as in Mole Madness."" In the cooperative two-player game, the players have to move an animated mole through its environment, gathering rewards as they avoid obstacles. To move the mole horizontally, one player says ""go,"" while the other player moves the mole vertically by saying ""jump."" During game play, the players often say their commands simultaneously. In other cases, they make statements to each other, such as ""Don't say 'go' yet,"" that can be misinterpreted by the system. Sometimes, they're just making observations, such as ""He's funny."" They also sometimes speak very quickly, or speak slowly, or change pronunciations - ""g-g-g-g-go"" - in an effort to exert greater control over the game. To train their keyword-spotting system, the researchers had 62 children ages 5-10 play the game, both in pairs and paired with a robot called Sammy, while a human ""wizard"" listened in another room and tried to map each ""go"" and ""jump"" into a button press on a game controller. The system uses separate models of go, jump, mixed, social speech and background noise, built from 150-millisecond segments of the training data. Though ""go"" and ""jump"" normally take about 300 milliseconds to say, the system used the 150-millisecond window to increase responsiveness and thus make the game more compelling. Overall, the system was 85 percent accurate in recognizing the keywords. Almost 40 percent of the words overlapped in the child-child pairings and 26 percent overlapped in the child-robot pairings. Children spoke the keywords faster than normal 32 percent of the time when playing with each other and with the robot, but slower-than-normal 27 percent of the time with the robot and 17 percent with another child. A commercial continuous speech recognition system was about 35 percent less accurate overall than the keyword spotter, having particular trouble recognizing ""go,"" recognizing overlapping speech and recognizing fast speech. When 24 additional children ages 4-9 subsequently played the automated game, the system had some trouble understanding ""jump,"" perhaps because the group included four-year-olds, who weren't represented in the original training data. Most of the very young children adapted to the system's difficulties by simply pronouncing the word more carefully or repeating it more often so that the game could proceed. Though the keyword spotter wasn't perfect, it could respond faster than the human wizard, which made the game more compelling. When three mothers of young children reviewed the video and rated the engagement in the game of each player, they judged that the children were less than halfway between ""could take it or leave it"" and ""very much into the game"" when the human wizard was relaying their commands, but judged to be solidly enjoying the game when the automated system was in control. ""This technology can be reproduced with other vocabulary, allowing designers and developers to build novel children's applications that use limited speech as an input method,"" Lehman said. ",https://phys.org/news/2016-09-speech-technology-enables-kids-video.html,The keyword-spotting system developed by Disney Research works better for this game application than commercial speech recognition ...,Speech technology enables kids to control video game,Mole Madness Disney Disney Research Jill Fain Lehman Disney Research. Jessica Hodgins Disney Research Lehman Nikolas Wolfe Andre Pereira Workshop Child Computer Interaction Sept. San Francisco International Conference Intelligent Virtual Agents Sept. Los Angeles Kids Lehman 'jump Mole Madness 'go Sammy Children Lehman,3
350,"The speed and accuracy of Soundhounds in-car voice recognition floored me. A new voice recognition technology that could be in your next car should have Apple and Google worried. It's called Houndify , and it lets drivers pull up information like driving directions and weather simply by asking. This in and of itself isn't exactly groundbreaking; you can of course pull up the weather by asking, say, Siri or Google Now. But it's the speed and accuracy of parent company SoundHound's technology that completely floored me when I met CEO Keyvan Mohajer for a demo. ""We can do speech recognition and natural language understanding simultaneously,"" Mohajer told me on the CES show floor on Friday morning. ""We call it 'speech-to-meaning.' What Apple, Google, and [voice recognition pioneer] Nuance do is speech-to-text and then text-to-meaning."" This technology has been in development for 10 years, Mohajer said, adding that it's unlikely that too many other startups will attempt to break into the space because it takes several years to develop the technology. (The company was founded by a couple of Stanford PhDs.) Not only does breaking a user's voice request into two operations make the whole process slower, Mohajer said, it also makes it less accurate because ""if you're doing speech-to-text without caring about the meaning it's more likely to make a mistake,"" he said. ""But if you care about the meaning from the beginning you can reduce errors by quite a bit."" Sounds great, I said, now how about a demo Mohajer was more than happy to oblige, as seen above. In the demo, Mohajer asked a mock car dashboard a series of increasingly complex commands like ""What time is it,"" ""What time is it in Tokyo when it's 2pm in San Francisco, California,"" and ""Show me hotels in San Francisco for tomorrow staying for two nights that cost between $200 and $400 per night and are pet friendly."" All of these commands were answered by Houndify nearly instantly, without the pauses you might find while Siri is ""thinking"" about an answer. Nvidia, the company best known for its line of high-end graphics processors, will integrate the Houndify technology into its Drive CX dashboard platform, which several car manufacturers, including Audi, BMW, and Mercedes, have already committed to supporting. And while the Houndify technology works best when there's an active internet connection, it will still be able to perform certain in-car functions, like opening doors and windows, while offline. While SoundHound was at CES to specifically discuss its in-car technology, the underlying technology is also being integrated into Android and iOS mobile apps called Hound that are currently being beta-tested. These apps should be released soon, I'm told, giving more people the opportunity to try out the impressive voice recognition tech. ",https://motherboard.vice.com/en_us/article/your-next-cars-dashboard-may-have-voice-recognition-thats-better-than-siri,"We can do speech recognition and natural language understanding simultaneously, Mohajer told me on the CES show floor on Friday ...",Your Next Car's Dashboard May Have Voice Recognition That's ...,Soundhounds Apple Google Houndify Siri Google SoundHound CEO Keyvan Mohajer Mohajer CES Friday Apple Google ] Nuance Mohajer Stanford PhDs Mohajer Mohajer Mohajer Tokyo San Francisco California Show San Francisco Houndify Siri Nvidia Houndify Drive CX Audi BMW Mercedes Houndify SoundHound CES Android Hound,4
