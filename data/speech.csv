,Contents,Date,Summary,Title,URL
0,"The research team at IBM recently announcedthey've reached a new industry record in speech recognition with a word error rate of 5.5% using the SWITCHBOARD linguistic corpus. This brings it closer to what's considered to be the human error rate of 5.1%. Humans typically miss one to two words out of every 20 words they hear. In a five-minute conversation, that could be as many as 80 words.  The research project includes applying deep learning technologies and incorporating acoustic models. The speech recognition model used Long Short Term Memory (LSTM) and WaveNet language models with a score fusion of three acoustic models. The acoustic models included aLSTM with multiple feature inputs, another LSTM trained with speaker-adversarial multi-task learning and a third model with a residual net (ResNet) with 25 convolutional layers and time-dilated convolutions.The last model learns from positive examples but also takes advantage of negative examples, so it performs better where similar speech patterns are repeated.  Yoshua Bengiofrom Montreal Institute for Learning Algorithms (MILA) Lab at University of Montreal commented about the speech recognition.  In spite of impressive advances in recent years, reaching human-level performance in AI tasks such as speech recognition or object recognition remains a scientific challenge. Indeed, standard benchmarks do not always reveal the variations and complexities of real data. For example, different data sets can be more or less sensitive to different aspects of the task, and the results depend crucially on how human performance is evaluated, for example using skilled professional transcribers in the case of speech recognition.  He also said IBM research helps with advancing speech recognition by applying neural networks and deep learning into acoustic and language models.  In other speech processing news, IBM added Diarization to their Watson Speech to Text service which helps with use cases like distinguishing individual speakers in a conversation. All these achievements help with introducing technologies that will match the complexity of how the human ear, voice and brain interact.",2017-03-31,The research team at IBM recently announced they've reached a new industry record in speech recognition with a word error rate of 5.5% using ...,Using Deep Learning Technologies IBM Reaches a New Milestone ...,https://www.infoq.com/news/2017/03/ibm-speech-recognition
1,"This story was delivered to BI Intelligence Apps and Platforms Briefingsubscribers. To learn more and subscribe, please click here.  IBM has taken the lead in the race to create speech recognition that's in parity with the error rate of human speech recognition.  As of last week, IBMs speech-recognition team achieved a 5.5% error rate, a vast improvement from its previous record of 6.9%.  Digital voice assistants like Apple's Siri and Microsoft's Cortana must meet or outdo human speech recognition  which is estimated to have anerror rate of 5.1%, according to IBM  in order to see wider consumer adoption. Voice assistants are expected to be the next major computing interface for smartphones, wearables, connected cars, and home hubs.  While digital voice assistants are far from perfect,competition among tech companies is bolstering overall voice-recognition capabilities, as tech companiesvie to outdo one another. IBM is locked in a race with Microsoft, which last year developed a voice-recognition system with an error rate of 5.9%, accordingto Microsoft's Chief Speech Scientist Xuedong Huang; this beat IBM by an entire percentage point.  Despite progress, however, existing methods to study voice recognition lack an industry standard. This makes it difficult to truly gauge advances in the technology. IBM tested a combination of Long Short-Term Memory (LSTM), a type of artificial neural network, and Google-owned DeepMinds WaveNet language models, against SWITCHBOARD, which is a series of recorded human discussions. And while SWITCHBOARD has been regarded as a benchmark for speech recognition for more than two decades, there are other measurements that can be used that are regarded as more difficult, like CallHome,"" which are more difficult for machines to transcribe, IBM notes. Using CallHome, the company achieved a 10.3% error rate.  Moreover, voice assistants need to overcome several hurdles before mass adoption occurs:  The Internet of Things (IoT) is growing rapidly as companies around the world connect thousands of devices every day. But behind those devices, theres a sector worth hundreds of billions of dollars supporting the IoT.  Platforms are the glue that holds the IoT together, allowing users to take full advantage of the disruptive potential of connected devices. These platforms allow the IoT to achieve its transformational potential, letting businesses manage devices, analyze data, and automate the workflow.  Peter Newman, research analyst for BI Intelligence, Business Insider's premium research service, has compiled a detailed report on the evolving IoT platform ecosystem. This reportsizes the market and identifies the primary growth drivers that will power the IoT platform space in the next five years. It also profiles many of the top IoT platforms by discussing key trends in the platform industry such asplatform consolidation.  Here are some of the key takeaways:  Interested in getting the full report Here are twoways to access it:",2017-03-14,"IBM has taken the lead in the race to create speech recognition that's in parity with the error rate of human speech recognition. As of last week, ...",IBM edges closer to human speech recognition,http://www.businessinsider.com/ibm-edges-closer-to-human-speech-recognition-2017-3
2,"  IBM recently announced that it reached a new industry record in conversational speech recognition, which could have big implications for the future of artificial intelligence (AI).  The IBM team's system achieved a 5.5% word error ratedown from 6.9% last year. The benchmark was measured on a difficult speech recognition task, with the machine deciphering recorded conversations between humans discussing day-to-day topics such as buying a car. This recording is known as SWITCHBOARD, and has been used for more than two decades to test speech recognition systems, according to a blog post by George Saon, a principal research scientist at IBM.  IBM used deep learning technologies to reach the 5.5% record. Researchers combined Long Short Term Memory (LSTM) and WaveNet language models with three acoustic models, according to the blog post.  ""Within the acoustic models used, the first two were six-layer bidirectional LSTMs. One of these has multiple feature inputs, while the other is trained with speaker-adversarial multi-task learning,"" Saon wrote. ""The unique thing about the last model is that it not only learns from positive examples but also takes advantage of negative examples - so it gets smarter as it goes and performs better where similar speech patterns are repeated.""  The previous record was set by Microsoft's Artificial Intelligence and Research group in October 2016, when researchers developed a system that they claimed recognized speech as accurately as a professional human transcriptionist, with a word error rate of 5.9%. However, Saon argued in his post that human parity is actually a 5.1% word error ratelower than any company has yet to achieve.  ""We're not popping the champagne yet,"" Saon wrote. ""While our breakthrough of 5.5% is a big one, this discovery of human parity at 5.1 percent proved to us we have a way to go before we can claim technology is on par with humans.""  SEE: How we learned to talk to computers, and how they learned to answer back  Reaching human-level performance in AI tasks such as speech or object recognition remains a scientific challenge, according to Yoshua Bengio, leader of the University of Montreal's Montreal Institute for Learning Algorithms (MILA) Lab, as quoted in the blog post. Standard benchmarks do not always reveal the variations and complexities of real data, he added. ""For example, different data sets can be more or less sensitive to different aspects of the task, and the results depend crucially on how human performance is evaluated, for example using skilled professional transcribers in the case of speech recognition,"" Bengio said.  Saon also noted that finding a standard measurement for human parity is a complex task as well. While many use SWITCHBOARD, another corpus called CallHome offers a different set of linguistic data created from colloquial conversations between family members, on topics that are not pre-arranged. These conversations are more difficult for machines to transcribe than those from SWITCHBOARD. IBM achieved a 10.3% error rate on this measure, but determined that human parity would be 6.8%.  ""The ability to recognize speech as well as humans do is a continuing challenge, since human speech, especially during spontaneous conversation, is extremely complex,"" said Julia Hirschberg, a professor and Chair at the Department of Computer Science at Columbia University, in the blog post. ""It's also difficult to define human performance, since humans also vary in their ability to understand the speech of others. When we compare automatic recognition to human performance it's extremely important to take both these things into account: the performance of the recognizer and the way human performance on the same speech is estimated.""  IBM's breakthrough could have major implications for the future of AI and the Internet of Things (IoT) in the enterprise, according to Mark Hung, research vice president and lead analyst of Internet of Things at Gartner.  ""With the proliferation of conversational AI platforms such as Alexa and Google Assistant, continued reductions in error rate will be imperative to drive greater adoption of speech as the UI for consumer and enterprise applications,"" Hung said.  IBM has recently made major investments in its Watson division, with a new $200 million global headquarters for Watson Internet of Things opening recently in Munich, Germany as part of a $3 billion investment in IoT that IBM pledged in 2014. IBM also recently added diarization to its Watson Speech to Text service, making it possible for the processor to distinguish individual speakers in a conversation.  1. Last week, IBM announced that it achieved a new industry record in speech recognition, with a 5.5% word error rate.  2. Last year, Microsoft claimed to have reached human parity with its speech recognition system's error rate of 5.9%, but IBM researchers argue that human parity would actually be 5.1%.  3. IBM's breakthrough could have implications for improving the use of artificial intelligence and the Internet of Things in the enterprise.",2017-03-13,"Why IBM's speech recognition breakthrough matters for AI and IoT ... that it reached a new industry record in conversational speech recognition, ...",Why IBM's speech recognition breakthrough matters for AI and IoT,http://www.techrepublic.com/article/why-ibms-speech-recognition-breakthrough-matters-for-ai-and-iot/
3,,2017-03-17,Germany's Federal Migration Office (BAMF) will use speech recognition software to establish where migrants entering the country have come ...,Germany to use speech recognition to establish migrant origins,http://www.politico.eu/article/germany-to-use-speech-recognition-to-establish-migrant-origins-refugees-asylum/
4,"Here at The Next Platform, we tend to focus on deep learning as it relates to hardware and systems versus algorithmic innovation, but at times, it is useful to look at the co-evolution of both code and machines over time to see what might be around the next corner.  One segment of the deep learning applications area that has generated a great deal of work is in speech recognition and translationsomething weve described in detail via efforts from Baidu, Google, Tencent, among others. While the application itself is interesting, what is most notable is how codes and systems have shifted to meet the needs of new ways of thinking about some of the hardest machine learning problems. And when we stretch back to the underpinnings of machine translation and speech recognition, IBM has some of the longest historyeven if that history doesnt have a true deep learning element in relatively recently.  In his 36 years at IBM focusing on speech and language algorithms, Michael Picheny, senior manager for IBMs Watson Multimodal division (an area that focuses on language and image recognition, among other areas), much has changed for both code and the systems required to push speech recognition. While IBM, like many others doing deep learning at scale, has also landed in the large-scale deployment of GPUs for neural networks, the path to that point was long and complex. It is just in the last few years that the critical combination of advanced neural network models and the hardware to run them in real-time and at scale have been made available. And this one-two punch has shifted IBMs approach to speech algorithmic development and deployment.  Picheney says that back when he joined IBM, they were the only company doing speech analysis and recognition with statistical and computational approaches. Others were focused on physical modeling of the underlying processes in speech. They were the only ones solving the problem with compute and mathematical techniques, it was the neatest thing Id ever seen. His early speech recognition work at IBM was done on large mainframes entirely offline, an effort that was later deployed on three separate IBM minicomputers working in parallel to achieve real-time performancea first of its kind capability. Then, in the early 1980s, came the IBM PCs, which could have custom accelerators lashed on until the 1990s, when the work could be done entirely on a CPU. Speech recognition teams in Pichenys group have come full circle to accelerator cards with a reliance on GPUs, which he says are a spot-on architecture for speech, even if there are some limitations hardware-wise on the horizon for the next level of speech recognition using deep learning models.  Code-wise, much has changed for IBM when it comes to speech recognition. Picheny says that earlier systems were a combination of four componentsa feature extractor, acoustic model, language model, and speech recognition engine or decoder. As neural networks evolved, both internally and in the larger ecosystem (Caffe, TensorFlow, etc.), these components have been fused and merged, creating a master modelone that requires significant computational resources and scalability from both hardware and software. Back then, he explains that it was difficult to create an architecture that would run all of these different elements efficiently since they all had their own optimization and other features.  What we are seeing over time is that deep learning methods seem to be taking over more of the speech recognition functions. Bit by bit, deep learning structures and mechanisms are replacing older mechanisms that made it harder to do the heavy duty scaling. In the next couple of years, perhaps longer, we will see deep learning architectures used for all components of speech recognitionand image as well.  He notes that over time too, it might be possible for these many functions to be packaged into a single special purpose chip to make it even easier to scale.  People working in deep learning have become very agile in what theyve learned to do. The field is moving so fast; there are advances in one framework, another in a different oneall of the deep learning packages have their pros and cons, especially for speechWeve worked with all the major packages, some are better than others, but I dont want to say which. But we also have built a good bit of our own code, Picheny says.  For speech, IBM has its own custom-developed neural network models that feed into Watson (which is still a nebulous thing hardware and software-wise, were still working on that story), For these models, the drivers are computation speed and memory. As it turns out, these are the biggest limitations as well, particularly on the memory front.  GPUs are very fast but limited in memory. Thats where the bottleneck is for training on huge quantities of speech. Theres advantage in keeping everything on local memory on the GPU versus fetching it off the chip. And there are algorithms where people are trying to combine results across multiple GPUs, which means making performance tradeoffs for that level of parallelization. What we really want are faster GPUs with more memory.  Aside from custom ASICs just for speech, we asked Picheny about other architectures that seem to be getting traction in deep learning outside of pure-play deep learning chips from companies like Nervana Systems (now part of Intel). One of the likely candidates for speech acceleration could be neuromorphic devicesand IBM has developed its own (TrueNorth). There is a lot of work on neuromorphic architectures but the limitation of these chips, even though they do fascinating things, is that you have to program them far differently than a GPU. And with a GPU there are big communities of people writing special purpose libraries. The limitation is that the people developing their algorithms dont want a new way of programming what theyre already doing.  FPGAs have a similar problem, Picheny says, but are more of an intermediate solution, but programming is still not easy. He says the availability of a rich array of libraries in the GPU CUDA ecosystem is the reason deep learning will continue to be done on GPUs in this middle period before we might see specialized chips just for this task.  On the note above about Watson being nebulous: Pichney agrees that it is hard to pin down just how many different frameworks and models are part of Watsons overall AI umbrella, but this is not by design. Everything is evolving so quickly, especially in the last two years. He says that what Watson was in the beginning, just as with his own speech recognition algorithms and clusters, isnt the same now. While we might not get answers from IBM about just how many components Watson encapsulates and what hardware is required to make it all work for key applications, one can image that Pichenys story about the merging and fusing of various components into master networks with specialized functionality might ring equally true for Watson.",2017-04-07,"And when we stretch back to the underpinnings of machine translation and speech recognition, IBM has some of the longest history—even if ...",From Mainframes to Deep Learning Clusters: IBM's Speech Journey,https://www.nextplatform.com/2017/04/07/mainframes-deep-learning-clusters-ibms-speech-journey/
5,,2017-03-22,Sara Murphy and Hailey Gentz will represent Osage High School at the IHSSA All-State Individual Speech Festival when Murphy performs ...,Students receive All-State Speech recognition,http://globegazette.com/mcpress/news/local/students-receive-all-state-speech-recognition/article_d4012565-a271-58b0-ad4d-8b3ca3af7187.html
6,"Xiaomi at the beginning of 2017 showcased its new Mi TV 4 series at the CES 2017 trade show. The company is now upgrading the TV 4 series, and introducing its all-new Mi TV 4A series in China.  The all-new Mi TV 4A will be marketed as a smart TV with AI-based speech recognition. The new MI TV 4A series include four screen-sizes - 43, 49, 55, and 65-inches, and they will be available in AI with speech recognition and non-AI variants. The basic Mi TV 4A 43-inch (1080p) model will be priced at CNY 2,099 (roughly Rs. 20,000) while the 49-inch (1080p) model will be priced at CNY 2,599 (roughly Rs. 24,750). The Mi TV 4A 55-inch and 65-inch 4K models have been priced at CNY 3,199 (roughly Rs. 30,500) and CNY 5,699 (roughly Rs. 54,000).  The 49, 55, and 65-inch models will be also available in with AI Speech Recognition as well as come with new Mi Bluetooth Remote Control. The Mi TV 4A 49-inch (1080P) 32GB AI model has been priced at CNY 2,899 (roughly Rs. 27,500); Mi TV 4A 55-inch (4K) 32GB AI variant will be available at CNY 3,599 (roughly Rs. 34,000), and Mi TV 4A 65-inch (4K) 32 GB AI variant will be available at CNY 6,199 (roughly Rs. 59,000).  The third-generation Mi Bluetooth Remote Control has been priced at CNY 99 (roughly Rs. 1,000), and will come with Bluetooth, Speech Recognition, Mi Touch, and Infrared features.  At launch, Xiaomi talked how Mi Touch has replaced the traditional navigation button on the remote control, and lets you use gestures to control your TV. The Chinese company claimed that the feature will come in handy while searching or browsing on a long webpage.  Much like the Mi TV 4 series, the new Mi TV 4A series also features AI (artificial intelligence) based recommendations UI called PatchWall. Notably, the PatchWall is a UI layer on top of the Android OS and it is based on deep learning AI technology.  The Mi TV 4A series is powered by a quad-core Amlogic T962 64-bit processor coupled with 2GB of RAM and comes with 32GB ROM. Connectivity options include Wi-Fi 802.11ac (2.4/5 GHz dual band Wi-Fi), Bluetooth 4.2, Dolby, and DTS Audio. The Mi TV 4A also sports a blue light-reducing mode to reduce eye strain problem. The processor on the Mi TV 4A series also supports HDR 10 and HLG.  In terms of screen resolution, the Mi TV 4A 1080p models offer screen resolution of 1080x1920 pixels, and will sport Samsung/ LG/ AUO/ CSOT panel while the 4K models will boast of a screen resolution of 3840x2160 pixels with Samsung/ LG/ CSOT panel.  Xiaomi's Wang Chuan, Dirrector for Internet TV related products such as Mi TV and Mi Box, at the launch claimed that the company's deep learning AI system will understand user preferences and intelligently classify content. Chuan added that Xiaomi started working on AI speech recognition three years back, and it lets users control and interact with the TV. Xiaomi revealed that it uses Sogou ASR speech recognition technology which is claimed to have an accuracy of up to 97 percent.  Unfortunately, the Chinese company hasn't revealed plans to launch the new Mi TV 4A series outside China.",2017-03-23,The all-new Mi TV 4A will be marketed as a smart TV with AI-based speech recognition. The new MI TV 4A series include four screen-sizes - 43 ...,Xiaomi's Mi TV 4A Series Launched With AI-Based Speech ...,http://gadgets.ndtv.com/tv/news/xiaomis-mi-tv-4a-series-launched-with-ai-based-speech-recognition-1672577
7,"Verint Systems Inc. (VRNT) today announced that a large municipal transportation authority in the U.S. will implement the voice self-service and speech recognition solutions from its Customer Engagement Optimization portfolio.  When this transportation authority sought new solutions to support its move from an on-premises to cloud environment, as well as to advance key compliance needsincluding functionality to support the Payment Card Industry Data Security Standard (PCI DSS)it turned to Verint.* The authority also wanted to ensure the technology it selected would enable it to enhance security and further the success of its prepaid card program that enables citizens to reload their transit cards through voice self-service.  When deployed, the software solutions will be integrated with the authoritys contact center and back-end data systems, and will help deliver timely, high-quality voice self-service to improve citizen experiences, achieve higher completion rate goals, and decrease transfer rates and cost per contact. The authority and citizens it serves also will benefit from the solutions personalization capabilities, which help reduce typical consumer frustration with voice self-service systems.  This U.S. transportation authority selected our solutions based on our cloud environmentwhich it deemed more secure than its legacy offeringand our long history and success working with state government programs, says Michael Southworth, general manager, voice self-service and government solutions, Verint. Today, we support more than 60 programs in 44 states, reinforcing our experience and dedication to the market and local government agencies.  Verint (VRNT) is a global leader in Actionable Intelligence solutions with a focus on customer engagement optimization, security intelligence, and fraud, risk and compliance. Today, more than 10,000 organizations in approximately 180 countriesincluding over 80 percent of the Fortune 100count on intelligence from Verint solutions to make more informed, effective and timely decisions. Learn more about how were creating A Smarter World with Actionable Intelligence at www.verint.com.  * The transportation authority purchased the Verint solutions in November 2016 and plans to roll the technology out in the first half of this year.  This press release contains forward-looking statements, including statements regarding expectations, predictions, views, opportunities, plans, strategies, beliefs, and statements of similar effect relating to Verint Systems Inc. These forward-looking statements are not guarantees of future performance and they are based on management's expectations that involve a number of risks, uncertainties and assumptions, any of which could cause actual results to differ materially from those expressed in or implied by the forward-looking statements. For a detailed discussion of these risk factors, see our Annual Report on Form 10-K for the fiscal year ended January 31, 2016, our Quarterly Report on Form 10-Q for the quarter ended October 31, 2016, and other filings we make with the SEC. The forward-looking statements contained in this press release are made as of the date of this press release and, except as required by law, Verint assumes no obligation to update or revise them or to provide reasons why actual results may differ.  VERINT, ACTIONABLE INTELLIGENCE, MAKE BIG DATA ACTIONABLE, CUSTOMER-INSPIRED EXCELLENCE, INTELLIGENCE IN ACTION, IMPACT 360, WITNESS, VERINT VERIFIED, KANA, LAGAN, VOVICI, GMT, VICTRIO, AUDIOLOG, CONTACT SOLUTIONS, OPINIONLAB, ADTECH, CUSTOMER ENGAGEMENT SOLUTIONS, CYBER INTELLIGENCE SOLUTIONS, VOICE OF THE CUSTOMER ANALYTICS, NEXTIVA, EDGEVR, RELIANT, VANTAGE, STAR-GATE, ENGAGE, CYBERVISION, FOCALINFO, SUNTECH, and VIGIA are trademarks or registered trademarks of Verint Systems Inc. or its subsidiaries. Other trademarks mentioned are the property of their respective owners.",2017-03-23,... a large municipal transportation authority in the U.S. will implement the voice self-service and speech recognition solutions from its Customer ...,Large Municipal Transportation Authority to Implement Voice Self ...,http://finance.yahoo.com/news/large-municipal-transportation-authority-implement-123000881.html
8,"Market Research Report on the Automatic Speech Recognition Market analyzed two aspects. One part is about its production and the other part is about its consumption. In terms of its production,analyze the production, revenue, gross margin of its main manufacturers and the unit price that they offer in different regions from 2012 to 2017. In terms of its consumption,analyze the consumption volume, consumption value, sale price, import and export in different regions from 2012 to 2017.Also make a prediction of its production and consumption in coming 2017-2022.  At the same time,classify different Automatic Speech Recognition based on their definitions. Upstream raw materials, equipment and downstream consumers analysis is also carried out. Wha is more, the Automatic Speech Recognition industry development trends and marketing channels are analyzed.  1. To provide detailed analysis of the market structure along with forecast of the various segments and sub-segments of the global Automatic Speech Recognition market.  2. To provide insights about factors affecting the market growth. To analyze the Automatic Speech Recognition market based on various factors- price analysis, supply chain analysis, porte five force analysis etc.  3. To provide historical and forecast revenue of the market segments and sub-segments with respect to four main geographies and their countries- North America, Europe, Asia, and Rest of the World.  4. To provide country level analysis of the market with respect to the current market size and future prospective.  5. To provide country level analysis of the market for segment by application, product type and sub-segments.  6. To provide strategic profiling of key players in the market, comprehensively analyzing their core competencies, and drawing a competitive landscape for the market.  7. To track and analyze competitive developments such as joint ventures, strategic alliances, mergers and acquisitions, new product developments, and research and developments in the global Automatic Speech Recognition market.  Other manufacturers you interested in can be added to the report by us.  Finally, the feasibility of new investment projects is assessed, and overall research conclusions are offered.",2017-04-05,Automatic Speech Recognition Market Market Research Report on the Automatic Speech Recognition Market analyzed two aspects. One part ...,Global Automatic Speech Recognition Market Forecast Research ...,http://www.technologynewsextra.com/global-automatic-speech-recognition-market-forecast-research-report-2017-2022/10416.html
9,"Xiaomi has a lot in store for us. After putting the Redmi 4A with 4G VoLTE support on sale, Redmi 4X, Mi 5C, that selfie stick, and the Pinecone processor, Xiaomi has just rolled out a new Mi TV product. The Mi TV 4A is a new smart TV that comes with AI speech recognition.The Mi TV 4A starts with an affordable price of only 2099 which is roughly $304 in the US.  This new TV takes advantage of an original Samsung/LG/AUO/CSOT panel and comes equipped with a high-performance Quad-core 64-bit processor, 2GB RAM, 8GB ROM/32GB ROM, WiFi, and Bluetooth 4.2 connectivity. The product already supports HDR 10 and HLG, Dolby and DTS Audio, and features a deep learning artificial intelligence (AI) system. The AI variants will even arrive with a special Mi Bluetooth Remote Control but is sold separately for only 99($14).  The Xiaomi Mi TV 4A will be ready in four sizes: 43, 49, 55, and 65. Both the 43- and 49-inch models have 1080p displays but only the latter has a 32GB AI variant. The Mi TV 4A 43 costs 2099($304) while the 49 (1080P) costs 2599($377) or2899($420) for the AI variant. The bigger and 4K models, the 55-inch and 65-inch Mi TV 4As are priced 3199($467)/3599($522) and 5699($837)/6199($899), respectively.  The Mi TV 4A is targeted for the whole family. Young kids and the elderly can enjoy it because it is easy to navigate and is fun to use. The AI system is important because it learns and understands your preferences and follows personalized recommendations.",2017-03-21,The Mi TV 4A is a new smart TV that comes with AI speech recognition. The Mi TV 4A starts with an affordable price of only ￥2099 which is ...,Xiaomi unveils new Mi TV 4A with AI speech recognition,https://androidcommunity.com/xiaomi-unveils-new-mi-tv-4a-with-ai-speech-recognition-20170321/
10,"With the exception of helping create Expedia,Microsoft has struggled to figure out travel. But it is hoping that artificial intelligence (AI) will be its route finally to leapfrog ahead of Google and Oracle, playing a larger role as middleman.  The example illustrates the companys current approach to the travel sector. The technology giant appears to be more interested in building middleware services that sit between customers and travel companies.  The company has a mixed past record of engaging with consumers on its own. For example, in 2008, it bought an airfare prediction website called Farecast for $115 million and integrated it with its flight search results on its Bing search engine. But that project, along with the MSN Travel mobile app, didnt gain traction with consumers.  The companys focus now is on the new AI and cognitive computing layers that it predicts are going to disrupt travel. By 2018 half of all consumers will interact with cognitive computing, thanks in part to the spread of AI-powered platforms like Amazons Alexa, Apples Siri, and  of course  Microsofts Cortana.  Microsoft says 145 million people use Cortana at least once a month. But that definition of use is fairly light, and the voice assistant is not yet ingrained in the habits of consumers enough to be a go-to when it comes to, say, travel search.  Stuart Greif, senior executive, Travel/Hospitality, QSR, and Transportation Industry Solutions at Microsoft, has been out on the industry circuit, sharing the companys predictions about where travel technology is headed. Most recently Greif spoke last week at a conference in San Francisco run by venture capital firm Thayer Ventures, where Skift saw his presentation first-hand.  Greif says, Were not looking to build the next great travel platform. Our message to the travel industry is that we want to partner on delivering solutions.  It has targeted several sectors of the industry, but its chances seem better in some than in others.  Mobile apps may be replaced by voice-activated internet, Greif says. We think the app world is going to go away. People can debate that, but why do you need to go in and out of a dozen different travel apps by touch if you run nearly everything on your device by voice command  Greif argues bots are really the new apps. Lets say you often book hotels for business travel You dont want to have to re-enter your information at every new hotel site or travel agency you use.  Our view is, Im going to have a bot on a system like Skype or Cortana or Alexa or Siri, and Marriott is going to have its own bot, and Marriott is going basically ask me, would you like to check in And my bots going say, Hey is it okay to provide these details My bot is going to say, Yes Cortana, please give the information.  Greif says the rise of voice-activated search may upend the direct booking wars between suppliers and third-parties.  When you search for travel queries on a voice-powered platform, who owns that search asks Greif. Is it Google Is it Siri, Alexa Is it Cortana If artificial intelligence can look across the entire spectrum of everything thats available and choose what to bring it back as relevant, whose is it bringing back Is it bringing back brand.com Is it bringing back the OTAs  Greif conceded he didnt have answers to the tantalizing questions he raised. And while the companys Cortana is often seen as superior technologically to voice assistants from Google, Apple, and Amazon, there is a marketing and commercialization effort that needs to go along with the technological work to enable Microsoft to become a vital part of travel distribution.  As of today, Cortana doesnt known many travel-specific commands.  Whats more, if it chooses to compete in the battle to funnel travelers to booking options, it needs to catch up to rivals by rapidly developing more third-party speakers that support Cortana. Its prevalence via the Windows platform has not, so far, seemed like enough. Otherwise, Google, Apple, and Amazon may outpace it in having physical platforms distributed worldwide, making the questions Greif poses about who owns search moot.  A year ago Microsoft invited travel companies to build bots on its Skype messaging service, to enable the brands to automate many parts of their customer interactions. But brands like Kayak, Expedia, Hipmunk, and Cheap Flights, have ignored it, focusing instead on Facebook Messengers platform.  Its not hard to wonder if a similar fate awaits Cortana unless Microsoft adjusts its ground game in travel.  Microsoft translator can translate speech across nine languages in real time in sixty languages, via text input. But it is still some years off from real-time translation happens via speech.  Speech recognition could, in theory, someday replace airline gate agents and hotel front desk clerks, by capturing and processing requests, reducing error rates. As of today, the speech recognition is possible, but not the ability to send commands into travel company systems, Greif says.  Visual recognition could, in theory, replace airline agents who need to match faces with photo identification for, say, allowing a bag to be checked in for a flight. Already Microsoft says its computers are better at matching faces than humans are, on average.  How long does Greif think it takes for AI to be able to actually have the business logic down thats required for people to do things like search and book an itinerary that might be complicated  I think the search part is the easy part, he says. Its all the back-end integrations and business decision-making thats gonna take awhile.  As an example, an unnamed large hotel chain recently gave Microsoft links to five different frequently asked questions as part of a project to create a demonstration of how an automated, chat-based customer service interface might work.  It only took Microsofts developers and machines 10 minutes to read the content and come up with an interface that could provide those answers to a wide away of question phrasings.  But Greif says bookings are more complicated. He believes the industry will start putting these solutions into production within about five years time.  Despite Microsofts ambitions with Cortana, it has less experience in the quirks of travel marketing and distribution of its competitor Google.  It seems more likely to gain market share by providing truly business-to-business enterprise services to hotels for operations. Last year Microsoft began demonstrating its concept of the connected hotel that used next-generation guest experience management system iRiS and next-generation hotel operating system that acts as middleware for interpreting data in operations and finance.  Greif says the key issue for hotels is aligning all the systems in a guest room and the back office so that they are all participating in real-time. Right now the systems arent connected to know when water pressure is not working or the TV is not working and so the company cant get ahead of customer complaints with predictive maintenance.  But the companys active demonstrations with travel suppliers suggest that it poses a significant competitive threat to todays largest hospitality tech providers, such as Oracle, and airline providers, such as Amadeus in partnership with Accenture.  Whether Microsoft is a victor or not in the AI race, Greif seems persuasive on his larger point that the arrival of high-powered computing in the cloud, more sophisticated computer algorithms, and the popularization of mobile, internet-connected devices is finally ushering in the AI era.  Platforms that can support AI are truly being popularized now, which means that hotels, airports, and airlines are increasingly powered by artificial intelligence. So there is some merit to the buzz.  The platforms are getting better at recognizing spoken queries and at helping to categorize various types of knowledge so as to deliver relevant and intelligent responses within particular areas of focus for a specific task or sets of tasks.  For example, in speech recognition, Microsoft had a historic achievement last year. It achieved parity in speech recognition accuracy with humans. That doesnt mean its computers hear perfectly, but it hears just as well as humans do.  That said, the hype machine is on overdrive, Greif concedes. He notes that a few years ago every startup pitch described itself as a data analytics company, and today every startup claims to be in AI.  In the meantime, Microsoft is claiming it has a new company culture that will enable it to adapt to the new, fast-changing circumstances. Greif says, Its a whole new Microsoft. Its a much more open company now. I get to keep my iPhone, and thats always nice.",2017-04-12,"Microsoft translator can translate speech across nine languages in real ... As of today, the speech recognition is possible, but not the ability to ...",Microsoft Bets on Artificial Intelligence to Help It Succeed Again in ...,https://skift.com/2017/04/12/microsoft-hopes-artificial-intelligence-will-help-it-succeed-again-in-travel/
11,"Shanghai, China, March 15, 2017 - STMicroelectronics (NYSE: STM), a global semiconductor leader serving customers across the spectrum of electronics applications and a top MEMS supplier, and iFLYTEK (SHE: 002230), one of the leading voice-recognition cloud service providers in China, have introduced the market's first IoT development platform that enables voice-recognition cloud services in Chinese. The joint solution is on display at electronica China 2017, Shanghai New International Expo Center, Hall E4 Booth 4102, March 14-16, 2017.  The new platform combines ST's SensorTile multi-sensor module, STM32 ODE (Open Development Environment), and Open.software package with iFLYTEK's voice-recognition technology. It gives designers a complete toolset for the development of voice-enabled Smart-Home, Smart-Driving, IoT, and robotics applications.  The SensorTile module captures voice inputs through the digital MEMS microphone (MP34DT04) and transmits them using the Bluetooth Low Energy network processor (BlueNRG-MS) to iFLYTEK's cloud through a smartphone with the voice-recognition result back within seconds.  ""ST's SensorTile is a perfect match for developers integrating voice-control capabilities in applications across Smart-Home, Smart-Industry, and Smart-Driving segments. iFLYTEK has been empowering developers with the best performing and easy-to-use speech-recognition service,"" said Jidong YU, Senior Vice President of iFLYTEK Co., Ltd. ""We have been working with ST to enable the SensorTile platform with a high-performance Chinese-language recognition. Leveraging iFLYTEK's more than 270,000 developers on xfyun.cn and ST's smart IoT development tools, we look forward to creating more designs together in future.""  ""The implementation of iFLYTEK's automatic speech-recognition services on SensorTile accelerates and simplifies voice-enabled IoT design,"" said Collins Wu, Marketing Director, Analog and MEMS Group, Greater China & South Asia, STMicroelectronics. ""Leveraging a powerful open-software ecosystem, including the STM32(TM) Open Development Environment, shortens time to market and makes IoT design simple and cool.""  ST's Analog and MEMS Group has also played an active role in nurturing the Innovator Community and Smart Hardware Development Platform in China, establishing a Chinese-speaking engineer community, st_AMSchina, a service subscription on Wechat, as well as the MEMS QQ Group.  STMicroelectronics' 13.5mm x 13.5mm SensorTile is currently the smallest turnkey sensor board of its type, containing ST's MEMS accelerometer, gyroscope, magnetometer, pressure sensor, and MEMS microphone. With the on-board low-power STM32L4 microcontroller, it can be used as a sensing and connectivity hub for developing products such as wearables, gaming accessories, and smart-home or Internet-of-Things (IoT) devices.  ST is a global semiconductor leader delivering intelligent and energy-efficient products and solutions that power the electronics at the heart of everyday life. ST's products are found everywhere today, and together with our customers, we are enabling smarter driving and smarter factories, cities and homes, along with the next generation of mobile and Internet of Things devices.  By getting more from technology to get more from life, ST stands for life.augmented.  In 2016, the Company's net revenues were $6.97 billion, serving more than 100,000 customers worldwide. Further information can be found at www.st.com.  IFLYTEK CO.,LTD.(iFLYTEK) is a national key software enterprise dedicated to the research of intelligent speech and language technologies, development of software and chip products, provision of speech information services, and integration of E-government systems. The intelligent speech technology of iFLYTEK, the core technology of the company, represents the top level in the world.  Established in 1999, iFLYTEK was listed in the Shenzhen Stock Exchange in 2008 (stock code: 002230). With vigorous support from major shareholders including USTC Holdings Co., Ltd., Shanghai Guangxin, and Legend Capital, iFLYTEK boasts the longest fundamental research history, the largest professional research team, the greatest capital investment, the best evaluation results, and the largest market share among all business entities in the speech technology field in China.",2017-03-15,"iFLYTEK has been empowering developers with the best performing and easy-to-use speech-recognition service, said Jidong YU, Senior Vice ...",STMicroelectronics Collaborates with iFLYTEK for Chinese Voice ...,https://globenewswire.com/news-release/2017/03/15/937743/0/en/STMicroelectronics-Collaborates-with-iFLYTEK-for-Chinese-Voice-Recognition-Cloud-Services.html
12,,2017-04-12,"Sentences tend to come out a little short and, like any speech recognition system it does make the odd error - only it's funnier here, because ...",Drop everything and download: Giphy Says,https://www.stuff.tv/sg/features/drop-everything-and-download-giphy-says
13,"Crescendo Systems received two prestigious healthcare awards at the recent Nuance Annual Partner Event in Barcelona. As well as picking up the award for Healthcare Marketing Excellence, Crescendo were also delighted to be accorded Elite Healthcare Partner status in recognition of their work with Nuances Dragon Medical speech recognition product.  We are thrilled with these Nuance Healthcare Awards, said John Bendall, Operations Director of Crescendo Systems, Having stepped up our healthcare activity over the past year in response to the rapidly evolving NHS digital landscape, we feel that these awards are an endorsement of the high quality added value services that Crescendo bring to the Dragon Medical proposition.  Crescendo not only specialise in creating custom interfaces for Dragon Medical with leading clinical systems such as TPP SystmOne and EMIS Web but have also integrated Dragon Medical into their own digital dictation platform, allowing doctors the freedom to create clinical documents entirely by voice or send speech recognised text to their back office staff for editing.  Our Dragon Medical portfolio has been designed to give doctors the ability to choose how speech recognition works best for them and then to reinforce these choices by providing expert implementation, training and advice, continued John, We want Crescendo customers to benefit in every possible way from knowing that they are working with a Nuance Elite Healthcare Partner and we will continue to work closely with Nuance to provide solutions that help healthcare organisations improve efficiency and channel those gains back into patient care.  About Crescendo Systems:   For over 25 years, Crescendo Systems Corporation (http://www.crescendosystems.co.uk) has been delivering powerful clinical documentation, voice processing, speech recognition, transcription, workflow and referral management solutions to countless healthcare facilities around the world.  In 2003, Crescendo Systems Ltd. was the first subsidiary to be opened in Europe and it is now the preferred supplier for tens of thousands of UK healthcare professionals. Crescendo Systems are a Nuance Elite Healthcare Partner, offering specialist added value and professional services to Nuances Dragon Medical speech technology.  Developed by Crescendo with care teams and for care teams, Centro (http://www.trustcentro.co.uk) is a revolutionary Clinical Documentation System aimed at Trusts embracing the NHS Digital Challenge. Designed to maximise efficiency gains and savings, Centro offers superior clinical documentation processing by combining a seamless, digitally-rich and mobile workflow with a collaborative care approach that delivers timely, more informed patient care.",2017-04-04,Our Dragon Medical portfolio has been designed to give doctors the ability to choose how speech recognition works best for them and then to ...,Crescendo Recognised for Excellence in Healthcare by Speech ...,http://www.prweb.com/releases/2017/04/prweb14207930.htm
14,,2017-03-22,"In addition, data from the startup's backend, which is linked to its speech recognition software development kit, showed that Xiaomi used its ...","Chinese AI startup partners with Xiaomi on a verbal agreement, and ...",https://www.techinasia.com/mobvoi-xiaomi-smart-tv-contract-claims
15,"A deep neural network architecture which can directly translate speech from one language into text in another language is being developed by Google researchers.  The study, titled Sequence-to-Sequence Models Can Directly Transcribe Foreign Speech, describes using a modified sequence-to-sequence model, which has had previous success in speech recognition, to create a powerful encoder-decoder network for machine translation.  The paper explains that the new model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the source language transcription during training.  In testing, the research team reported state-of-the-art performance on conversational Spanish to English speech translation tasks. The experiments used the Fisher Callhome Spanish-English dataset and found that the proposed model could outperform cascades of speech recognition and machine translation technologies.  Using the BLEU (bilingual evaluation understudy) scoring framework, which evaluates the quality of machine-translated text, the proposed system recorded 1.8 points over other translation models.  According to the study, when Spanish transcripts were used as training data for additional supervision across independent automatic speech recognition (ASR) and speech translation (ST) decoders, additional improvements of at least 1.4 BLEU points were obtained.  In future work, the Google researchers plan to construct a multilingual speech translation system in which a single decoder is shared across multiple languages.  With big advances in deep learning, human versus AI competitions are springing up across the world. In February, human translators battled against AI machine translators in Seoul, South Korea. While Google Translate and Naver Papago, both based on Neural Machine Translation (NMT), won the high-profile battle in terms of speed, they fell considerably behind their human counterparts on quality. It is however expected that with further NMT developments, such as this latest Google research, machine translation will improve at a fast rate and could reach human-level accuracy very soon.",2017-04-04,"... Foreign Speech, describes using a modified sequence-to-sequence model, which has had previous success in speech recognition, to create ...",Google model instantly translates speech to foreign text using AI,https://thestack.com/big-data/2017/04/04/google-model-instantly-translates-speech-to-foreign-text-using-ai/
16,"SALT LAKE CITY & HANOI, Vietnam--(BUSINESS WIRE)--FluentWorlds, provider of the worlds most sophisticated and flexible 3D/VR platform for language immersion, and Topica EdTech Group, a leading online education provider in Southeast Asia, today announced their partnership to introduce their jointly produced product  Topica Ivy Native 3D  allowing thousands of students to learn to speak the English language in a 3D context.  Topica Ivy Native 3D teaches English language skills in 3D environments. It engages learners through the most effective means possible  immersive, real-life experiences delivered via video game technology in 55 different virtual environments. The product also offers proprietary Voice Recognition that gives students immediate feedback on the phoneme level.  We are honored to work with Southeast Asias top online educational institution  Topica, said David Bradford, FluentWorlds CEO. They are a forward-looking institution that is bringing their students the best and brightest new technologies.  Tuan Pham, Topicas CEO, said, We are very excited about this partnership with FluentWorlds. Their combination of powerful pedagogy with gamification, unique 3D graphics and real-time speech recognition will create a very engaging learning environment for our students.  Dr. Linda Bradford, founder and chief product officer of FluentWorlds, added, We believe our focus on apps, video gaming, and 3D technology, along with this partnership with Topica, point to a groundswell of momentum around 3D language learning.  The partnership was introduced through Tim Doner, the worlds youngest hyperpolyglot who speaks over 20 languages. He is also the global brand ambassador for FluentWorlds.  FluentWorlds is a privately held company that specializes in the delivery of flexible and sophisticated language programs via immersive 3D and virtual reality experiences that users can access via their mobile devices or desktop and engage with as if they were video games. The company's proprietary speech recognition and accent reduction feature is revolutionizing language learning. See more on FluentWorlds and Perfect Pronunciation at www.FluentWorlds.com and through this short video: https://vimeo.com/203390395.  Topica Edtech Group is a leading online education provider in Southeast Asia. Topica Native provides online English speech tutoring courses in Thailand, Indonesia and Vietnam, and was the first in the world to launch an Augmented Reality app for speech tutoring. Topica Edumall is the largest platform of short-skill courses in Thailand and Vietnam. An earlier Topica initiative was personally launched by the Microsoft Chairman Bill Gates. Topica employs 1,400 full-time staff and 2,000 instructors throughout Southeast Asia. For more information, visit www.topica.asia.",2017-04-11,"Their combination of powerful pedagogy with gamification, unique 3D graphics and real-time speech recognition will create a very engaging ...",FluentWorlds Partners with Topica to Bring English Language ...,http://www.businesswire.com/news/home/20170411005238/en/FluentWorlds-Partners-Topica-Bring-English-Language-Learning
17,"In 2016, an unusually high number of bright, shiny objects were waved in the faces of media executives: Virtual reality! Live video! Artificial intelligence! Personalization! Voice! Bots!  Like all good media executives, many of them pounced, and by the end of 2016, publishers big and small were declaring theyd opened VR studios, built bot teams and unleashed crack squads of live video mavens to help them win the internet.  But now, reality has set in. By some estimates, augmented and virtual reality are a decade away from becoming mainstream technologies. Bots and the artificial intelligence that powers them are much more valuable to retailers than they are to media companies. Live video barely works for anything besides sports, and by the time you read this, Facebook and YouTube will probably have changed their minds about what kind of video theyd like media companies to make (again).  There is, however, one exception to this rule. Even though the number of voice-powered devices like the Echo, Dot and Google Home sold by the end of 2017 is expected to be far below the number of people that, say, go fishing every year (about 33 million), voice will have more effect on media  both positive and negative  than any of the other whizbang technologies that have grabbed so many headlines the previous two years.  Voice works   While people expect artificial intelligence and virtual reality will be ready for prime time eventually, the speech recognition technology that makes voice possible is already here. In 2010, machines could understand about a million words at about 70 percent accuracy. By 2015, that number had risen past 10 million, and at about 90 percent accuracy, according to Google research that Kleiner Perkins Caufield Byers analyst Mary Meeker shared in 2016.  Getting to 100 percent, where Google or Alexa (or something else we havent met yet) understands words we mispronounce, or utter by mistake, is probably another few years away. But today, its possible to ask an assistant about almost anything, and people are asking: according to Hound, a voice-enabled platform built by Shazam competitor Soundhound, its active users pose Hound multiple questions every day, on a wide variety of topics.  Andrew Ng, chief scientist at Baidu, says that half of all internet queries by 2020 will be done either through voice or speech.  The hardware is already here   Another big piece of this puzzle thats already been solved is hardware. It could be years before Oculus Rift (which costs $600 and requires a powerful PC with serious graphics processors) become widely affordable and before smartphones that can handle live video streaming are ubiquitous.  Meanwhile, an Echo Dot will set you back just $50, and pretty much every smartphone on the market can handle cloud-powered voice queries. And if you dont have your phone on you, thats not necessarily a problem, either.  Its not just your phone, your Amazon Echo, says Beerud Sheth, founder of voice and chat development platform Gupshup. In addition to the Echo and Google Home, Ford, VW and BMW are all working on cars that have Amazons voice platform, Alexa, inside them. Google Assistant, which is expected to become standard on high-end Android smartphones, is also expected to make its way into Android Auto, an operating system available in a growing number of connected automobiles.  And that, Sheth says, is just the beginning. Soon, voice platforms will be accessible from smart devices that we would never consider technologically advanced. Its your toaster oven, maybe even your table and chair, Sheth says.  The advertising infrastructure is there   Whenever a new medium emerges, it takes a while to figure out what its advertising will look like (or, in this case, sound like). But voice has a head start. Thanks to nearly a century of terrestrial radio advertising, publishers including The Washington Post are already monetizing their flash briefings on Alexa.  And once Google and Amazon add the ability to programmatically target listeners, the money is going to flow. Audio advertising is already projected to account for nearly 12 percent of marketers budgets and programmers ad placements by the end of 2017, more than double the share it claimed in 2015, according to a survey conducted by Ad Age and The Trade Desk.  Add in the ability to claim an offer, or make a purchase directly through a media companys skill, which is expected to be possible by the end of 2017, and its on: Suddenly, voice becomes a way to drive transactions, subscriptions, and other meaningful revenue sources  What radios always been missing is a direct back channel, said Pat Higbie, the founder of XAPP Media, a digital ad developer and a top Alexa developer. What we have here is the intimacy of radio as well as the instantaneous feedback from users.  The money for future innovation is already there   Amazon has set aside up to $100 million to invest in companies that it thinks could boost voice. An accelerator program, created in partnership with the mentorship-focused accelerator firm Techstars, will launch in July. And while Google does not have a similar voice-oriented fund, partners at its venture capital arm, GV, have said that they think voice is going to be the future.  They are very invested in identifying use cases theyre not thinking about internally, says Cody Simms, who heads accelerator programs for Techstars, said of Amazon.  The platforms are already there   If theres one thing guaranteed to inject rocket fuel into a new idea or technology, its goliaths like Google, Amazon or Microsoft battling to own its ascent. Consider what competition between Facebook and Google did to increase the profile of streaming video.  That fight will be good for speeding the innovation thats sure to occur on Google Assistant, Alexa et al. But what will be most interesting is seeing what happens when that innovation starts to really distinguish these nascent platforms from one another.  For now, there is very little that separates Google from Amazon. But once Google allows users to control things like Gmail, or Gcal, or YouTube from Assistant, or starts using peoples search histories to personalize each listeners voice experience, its going to be very different from Amazon, which in turn will have access to an enormous trove of user purchase history, intent and other data.  People arent going to want to have to interact differently, depending on what microphone theyre talking to, said David Beisel, a partner and co-founder at NextView Ventures. Theres a lot of complication there.  But thats not a problem for right now. For now, the stage is set for voice to take off. And while Amazon, Google and Microsofts long-term strategic visions for voice may differ, theyre all going to be focused on the same thing, for now.  If they want to foster a rich ecosystem, Beisel said, theyre going to have to reward folks for it.",2017-04-10,"While people expect artificial intelligence and virtual reality will be ready for prime time eventually, the speech recognition technology that ...",Can you hear me now?: Voice picks up steam as bot hype fades,https://digiday.com/media/can-hear-now-voice-picks-steam-bot-hype-fades/
18,,2017-04-06,"AutoCorrect and speech recognition software should be like the other half of an old married couple, always finishing our sentences perfectly.",Ben Bromley: Smartphone can't Finnish my sent tenses,http://lacrossetribune.com/news/local/ben-bromley-smartphone-can-t-finnish-my-sent-tenses/article_943eb98f-8215-552d-8a25-1fda9299c8bd.html
19,"Microsoft's rollout of Windows 10 Creators Update has begun, complete with a privacy dialogue box shown by default to all users.  When will you get the update Microsoft says the first phase will target newer devices  we will then expand the Creators Update release to additional devices based on the feedback.  The process will take several months, but if you are impatient, you can upgrade immediately using the Update Assistant.  Windows 10 Mobile is also being updated, with rollout for phones beginning on April 25.  The notification that an update is ready comes in the form of a dialogue asking you to review your privacy settings. If you defer reviewing your settings, you also defer the update. The privacy settings review screen is simplified, with an on/off switch for five categories: Location, Speech recognition, Diagnostics, Tailored experiences with diagnostic data, and Relevant ads.  Enjoy this screen while you can see it though, because you cannot easily get back to it. Changing privacy settings after you click Accept is a matter of going back to the privacy section in Settings, with 18 categories at last count. Another oddity is that the simplified settings screen does not reappear (in my testing) if another user signs in, even though they are per-user settings.  A further complication is that the Cortana app has its own settings. When installing this update, you may see a dialogue prompting you to Use Cortana, or Not now.  This states that Microsoft collects and uses information including your location and location history, contacts, voice input, speech and handwriting patterns, typing history, searching history, calendar details, content and communication history for messages and apps, and other information on your device. In Microsoft Edge, Cortana uses your browsing history. If you choose to accept, but later change your mind, the setting is rather buried. You have to open Cortana, click the Notebook icon, then About Me, then Sign out.  Privacy matters aside, once you install Creators Update, future updates will download more quickly thanks to Microsofts new Unified Update Platform which enables differential downloads.  Despite the companys intense upgrade campaign, Windows 10 remains behind Windows 7 in terms of global installed base. Statcounter shows Windows 10 at 34 per cent versus Windows 7 at 47 per cent. View by region though, and the picture changes. In the UK, Windows 10 is at 50 per cent versus 7 at 35 per cent; and in the USA 44 per cent versus 41 per cent. Figures vary according to who you ask, but adoption is happening.  Should you upgrade Creators Update is a misnomer; the real value of this release is in numerous small improvements to Windows. Many of these are listed in our review; but there is more. Business users will like the MBR2GPT utility, for example, which helps you convert a drive to GPT partitioning without destroying data, enabling migration to UEFI (Unified Extensible Firmware Interface) in place of BIOS, and security features including Device Guard and Secure Boot. Note that this tool alone is not sufficient; you will also need to change system firmware settings.  Developers should note new capabilities in the Windows 10 application platform, listed here. Notable features include app streaming, which lets users launch an application before it is fully installed; a new system-wide Tasks API; Project Rome which now lets you have an application on Android which hands over seamlessly to a Windows application when you start working on a PC; and new extensions for the Desktop Bridge, which lets wrap Windows desktop applications so they can be installed and managed like UWP (Universal Windows Platform) applications. Among other things, these extensions make it easier to transition users from an existing installation of your application to a Desktop Bridge version.  Windows 10 is getting better, but while its share of Windows installations grows, Microsoft will be aware of another key statistic, that Android has crept ahead as the most used platform for accessing the internet. Windows still dominates the desktop, but the ascendancy of mobile continues. ",2017-04-11,"The privacy settings review screen is simplified, with an on/off switch for five categories: Location, Speech recognition, Diagnostics, Tailored ...",Windows 10 Creators Update general rollout begins with a privacy ...,https://www.theregister.co.uk/2017/04/11/windows_10_creators_update_general_rollout_begins_complete_with_privacy_dialog/
20,"Research provides partial guidance. In her April Physics Today feature, ""From Sound to Meaning,"" University of Connecticut cognitive scientist Emily Myers recounts her group's discovery that people retain what they've learned in a language class better if they go to bed before they get the chance hear a lot of their own language during the rest of the day. Evening classes are better.  That and other findings draw on big strides in a cross-disciplinary effort that is currently advancing understanding of how people derive meaning from sounds.  Myers starts off by explaining phonemes, the ""abstract units of perception and production that, when swapped, produce a change in the word."" Phonemes vary across cultures. For example, for an English speaker, the ""r"" and ""l"" are heard as distinct phonemes. In contrast, the Japanese do not have distinct ""r"" and ""l"" phonemes. To a Japanese listener, ""play"" and ""pray"" sound the same.  Myers is not only talking about linguistics here: Physics is important too for turning vocalizations into understanding. For example, differences in voice onset time (VOT), the speed with which a sound is vocalized, bear on how the sound is interpreted. She describes how the human mind becomes adept at working with these and other variables so that speakers and their listeners can enter a community of understanding. It is a distinctly biological ability (at least for now), an observation that Myers supports by illustrating how Siri or Alexa speech-recognition interfaces go awry when they try to interpret rapid speech.  Yet, the human system for producing and understanding speech is not terribly resilient. Myers describes how we develop a communicative ability through ""perceptual narrowing,"" which may lead to skill and perceptiveness for the inevitable situations where communication occurs amidst interference. What follows perceptual narrowing is ""perceptual entrenchment,"" also an inevitability. One not-so-fortunate byproduct is adults' difficulties with new languages or unfamiliar accents.  So, if you are puzzled why Siri sometimes misunderstands you or why children learn languages better than adults do, turn to Myers's article. It's freely available on Physics Today's website, and can be accessed directly here:  Explore further: Why deaf people can have accents, too  More information: Emily B. Myers. From sound to meaning, Physics Today (2017). DOI: 10.1063/PT.3.3523   ",2017-04-11,"It is a distinctly biological ability (at least for now), an observation that Myers supports by illustrating how Siri or Alexa speech-recognition ...",Scientists make strides explaining how we discern language,https://medicalxpress.com/news/2017-04-scientists-discern-language.html
21,"This fintech CEO says AI-powered voice recognition and text transcription let traders communicate with one another without typing or picking up a phone.  IfNader Shwayhat, CEO of fintech firm of GreenKey Technologies, has his way every conversationon the trading floor will be instantly recorded, transcribed and stored as soon as it happens. As banks and hedge funds pay ever-closer attention to bad behaviour after a litany of scandals, his firm uses an AI-driven speech recognition tool to keep tabs on the trading floor.  Shwayhat is a veteran of venture-backed fintech businesses focused on trading and big-data analytics and holds patents on chat-based trading communication technologies. He said that advanced voice interfaces and AI-driven speech recognition represent the next wave of innovation on trading floors.  When you look at the trader voice market, which is based on economic models and technologies that are decades old, we havent seen disruptive innovation yet, but were looking to change the entire definition of a voice turret, Shwayhat said.  We essentially created an app that can be installed on a smartphone, tablet, laptop or desktop computer, which offers a lot more capability aside from being able to focus on the data that is in the phone calls, he said.  Were going to be scaling and growing the business over the next 24 months, with a lot of growth in the New York City area, Singapore and Hong Kong, Shwayhat said. Well be scaling our support services and sales staff here in New York and internationally as well  in Europe, mainly London, and Asia, well primarily be hiring in the front office, and the development team managed out of Chicago will be expanding as well.  GreenKey is looking to hire engineers and computer developers with experience working in JavaScript, Python and HTML5.  WebRTC is what our core stack is based on, Shwayhat said. Were hiring developers in Chicago, and data science is a massive aspect of what we do, so well look to add PhDs in data science.  Between full-time employees and advisory consultant staff our headcount is currently around 20, and I can envision that doubling or tripling within the next 12-to-18 months, with approximately two-thirds of the hiring to be in New York, London and Hong Kong.  Back in 2004, Shwayhat co-founded Pivot, an instant-messaging startup geared toward financial services that was spun off from his previous employer Eze Castle Integration and eventually bought by CME Group, a derivatives exchange.  Post-acquisition, he became the new parents global head of sales and marketing and co-founded CME Direct, an internal, acquisition-driven startup offering technology tools for traders.  Most recently, Shwayhat was the global head of sales and business development for Novus Partners, a portfolio intelligence company specializing in big data and analytics for institutional investors and asset managers.  Like many successful fintech entrepreneurs, Shwayhat is an engineer. He earned a Bachelors degree in mechanical engineering and a Masters degree in financial and operations engineering from the University of Michigan.  What career advice can he give to tech-savvy students interested in a career in financial services or fintech  Financial services is one of the largest employers of tech graduates in the country, specifically the market we play in, financial technology, Shwayhat said. Fintech is a massive area, and beyond the traditional banking, consulting and trading skills sets, finance and math are primary.  Computer science probably one of the most valuable majors in financial services, because every trader is having to also be a quant developer in their own right, he said. Bankers have to have tech skills to evaluate M&A opportunities, beyond the finance and math backgrounds that people traditionally think about.   Follow @danbutchrwrites",2017-03-30,"As banks and hedge funds pay ever-closer attention to bad behaviour after a litany of scandals, his firm uses an AI-driven speech recognition ...",This firm is using AI to keep tabs on investment banks' traders,http://news.efinancialcareers.com/us-en/278764/ai-speech-recognition-trading-greenkey-fintech
22,,2017-04-12,Google realized six years ago that the AI speech-recognition technology that it was deploying to users required the storage of vast amounts of ...,Why Facebook Is Doubling Down on Its AI,http://host.madison.com/business/investment/markets-and-stocks/why-facebook-is-doubling-down-on-its-ai/article_aa26ea86-620a-5f15-a529-250a4af36772.html
23,"Verint speech recognition solutions to be implemented in MTA  Verint Systems Inc.s (Nasdaq: VRNT) voice self-service and speech recognition solutions from its Customer Engagement Optimization portfolio will be implemented a large municipal transportation authority in the US, the company said.  When this transportation authority sought new solutions to support its move from an on-premises to cloud environment, as well as to advance key compliance needsincluding functionality to support the Payment Card Industry Data Security Standard (PCI DSS)it turned to Verint.  The authority also wanted to ensure the technology it selected would enable it to enhance security and further the success of its prepaid card program that enables citizens to reload their transit cards through voice self-service.  When deployed, the software solutions will be integrated with the authoritys contact center and back-end data systems, and will help deliver timely, high-quality voice self-service to improve citizen experiences, achieve higher completion rate goals, and decrease transfer rates and cost per contact. The authority and citizens it serves also will benefit from the solutions personalization capabilities, which help reduce typical consumer frustration with voice self-service systems.  Verint is a global leader in actionable intelligence solutions with a focus on customer engagement optimization, security intelligence, and fraud, risk and compliance.",2017-03-28,Verint Systems Inc.´s (Nasdaq: VRNT) voice self-service and speech recognition solutions from its Customer Engagement Optimization portfolio ...,Verint speech recognition solutions to be implemented in MTA,http://www.financial-news.co.uk/40436/2017/03/verint-speech-recognition-solutions-to-be-implemented-i/
24,"Thomas Payne, MD, FACP, attending physician in the General Internal Medicine Center at the University of Washington Medical Center-Roosevelt in Seattle, dictates notes into a recorder between patient appointments, recording notes on one patient before seeing the next.  He then uses speech-recognition software to move those recorded notes into his electronic health record (EHR) system.  Payne said his system ensures he gets to his notes quickly after each visit, and saves him time. I do leave the clinic sooner than my colleagues do, by 30 minutes or so, he said.  Although Payne acknowledged that his process might not be the right choice for every primary care provider, he does see an overall need to develop better practices around documentation.  Documentation is one of the most time-consuming parts of a doctors day, particularly in primary care. Its an area ripe for improvement, said Payne, who is also medical director for IT services at the University of Washington School of Medicine and board chair for the American Medical Informatics Association (AMIA).  The rapid rise of EHRs has brought with it both changes and challenges in how physicians record and share their patient notes. Leaders in the medical community have found that when it comes to aiding documentation, the systems need to do a better job.  The Report of the AMIA EHR 2020 Task Force on the Status and Future Direction of EHRs cited the need for EHRs to simplify and speed documentation, through other members of the care team entering the information, automatic data capture by devices or other information systems, and even having patients enter data themselves.  Payne said most EHRs arent designed to support documentation in a way that works well for physicians and their staff.  For example, he said many doctors record their notes in narrative form, writing down information that while not necessarily related to what brought a patient in for that particular visit is important to document the patients overall well-being.  However, Payne said most EHRs want doctors to check boxes or use drop-down fields to add details about a patient; EHRs generally cant take the information recorded in narrative form and use it to populate the preset fields. That means doctors recording the same information multiple times, moving from one field to another on their computer screens.  Thats not particularly satisfying nor is it the best use of that physicians time, Payne said.",2017-04-12,He then uses speech-recognition software to move those recorded notes into his electronic health record (EHR) system. Payne said his system ...,EHR documentation challenges remain,http://medicaleconomics.modernmedicine.com/node/434546
25,"With an estimated 100 million speakers, Swahili is the second-most-widely-used language on the African continent, after Arabic. Yet services such as automatic speech recognition (ASR) arent commercially available in this language, denying many users with disabilities and those who aren't literate the information they desperately need in their daily lives. This could change very soon though, as academic research and technology startups are converging to provide localized technologies to Swahili speakers.  One of these very promising innovations is about to be rolled out in Kenya. Uliza (meaning ask in Swahili) is a voice interface that allows users to access information from the Internet usinga basic mobile phone.  All users need to do is call in andask a question in Swahili. Within 15 to 90 minutes, an answer agent (an actual person working behind the scenes) responds with a voice answer. At the moment, a crowd of around 50 agents treat the queries by transcribing the voice recordings, searching for answers online in multiple languages, translating the information and sending it back to the caller in Swahili.  During the pilot carried out in the Kenyan capital Nairobi and in Western Kenya, some 600 beta users sent questions about their local representatives, asked for help with Swahili homework, and requested medical information that would be too delicate to bring up in person.  During Uliza's pilot project, these were the words included in the most frequent questions asked by users (translated from Swahili into English).  Uliza will solve another problem for its future users: the lack of access to information hosted on the Internet. There are many overlapping reasons for this situation: unaffordable mobile data bundles, distance to the nearest cybercafe, illiteracy in languages of wider communication, compounded by a dearth of available content in local languages.  Uliza's crowdsourcing model is admittedly labor-intensive but it has a major advantage: by having human beings handle the transcription and translation, it temporarily bypasses the lack of large voice datasets that typically constrains ASR efforts in African languages, while simultaneously collecting data from real speakers in a variety of accents and dialects. Uliza founder Grant Bridgman plans to use this database of short recordings and transcriptions to build machine learning capability and fully automate the system in the future. In this talk at Tufts University, Bridgman introduced the concept behind the project:  A good amount of research has already gone into building automatic speech recognition software for Swahili and other widely spoken African languages, but its taking a while for the technology to find its way into peoples hands. In an interview with Global Voices, Bridgman explained:  Companies looking to set up helplines for a rural customer base at a reduced cost are potential customers for the initial phase of Ulizas growth. Eventually, a full service allowing mobile phone users without access to the Internet to find answers to their questions and upload their own voice content will be implemented. The cost to the user would be minimal  close to the price of an SMS.  Ulizas model could be viable for other languages with a large enough number of speakers. But for the vast majority of the 2,000 languages spoken on the African continent, this isn't the case. But solutions might be coming froma research project led by Preethi Jyothi at the Beckman Institute, where ateam of researchers used a probabilistic method tocrowdsource transcriptions from non-native speakers. Once fine-tuned, probabilistic transcription couldopen up the possibility of ASR for less-represented languages, hopefully at a reasonable cost.",2017-03-23,"Yet services such as automatic speech recognition (ASR) aren't commercially available in this language, denying many users with disabilities ...",These Techies Want Your Phone to Speak to You in Swahili,https://globalvoices.org/2017/03/23/swahili-africas-second-top-language-gets-some-much-needed-recognition/
26,,2017-04-12,We can program speech recognition that's as capable as a human. Software is learning to detect emotions. And we're teaching robots how we ...,How Robots And Humans Can Peacefully Coexist,http://www.abc2news.com/newsy/how-robots-and-humans-can-peacefully-coexist
27,"AUSTIN, Texas, April 6, 2017 /PRNewswire/ -- Certified eSupport, Corp., one of North America's premiere technical services organizations for industry leaders in cloud and enterprise speech solutions, has announced the launch of their new virtual training portal, CES University. This customer-focused knowledge center will serve as a professional resource and training hub for dictation, transcription, and speech recognition products. CES University will greatly enhance service to the healthcare, legal, and corporate organizations that utilize the solutions offered by Certified eSupport.  To further support its continued growth, Certified eSupport is also expanding its team. To accommodate the addition of both sales and technical staff, the organization has relocated to 2305 Donley Drive, Suite 104, in Austin, TX, as of April 1, this year.  The expansion has been a long-time dream for Joshua Stewart, Director of Technical Services for CES, and his team. ""The goal of this online educational endeavor is to provide the unique resources and training needed to implement and use speech solutions successfully,"" Stewart said.  Built upon in-house documentation, interactive training, product workshops, and online certification courses, CES University will draw more attention to the benefits of speech solutions, increase customer expertise, and position the organization for further growth. In addition to CES University, the Certified eSupport Web site features the CESU Blog, providing insight on the latest industry news, as well as a Knowledge Base, where users can find more information about the speech recognition, dictation and transcription software and hardware solutions offered, including Nuance's Dragon speech to text software, as well as Philips and Olympus digital voice recorders.  Stewart explained that this growth in a virtual environment translated into the company's overall growth, with notable advantages for their users. ""As our business expanded, we found we needed more space for research and development, product integration, and in-house training,"" Stewart said, adding that the new facility allows for a more strategic organizational layout, focused specifically on role development and collaboration. ""CES University will help ensure that anyone using speech solutions is well-equipped and well-informed. To further benefit our customers, we will be submitting our certification programs to several professional organizations, so that they can be counted as continuing education credits,"" he said. ""We're providing an environment where businesses can utilize our resources to boost their adoption rates and maximize their return on investment.""  Certified eSupport, Corp. is an organization made up of engineers, certified instructors, trainers, Microsoft Certified Professionals and Network Administrators with a focus in voice processing, speech recognition and enterprise-wide software solutions, specializing in the healthcare, legal and corporate verticals. With an expertise in digital dictation, transcription, speech recognition and enterprise solutions, CES is an industry expert in Dragon Voice Recognition Software, Dragon Medical Practice Edition, Philips or Olympus Dictation or Transcription Solutions.  As a complete solutions provider, services include Support, Training, Customization, Consultation and Certification Preparation on several industry leading manufacturers. For more information, visit https://www.certifiedesupport.com/.  This content was issued through the press release distribution service at Newswire.com. For more info visit: http://www.newswire.com  To view the original version on PR Newswire, visit:http://www.prnewswire.com/news-releases/certified-esupport-launches-ces-university-prompting-expansion-relocation-300435819.html",2017-04-06,"With an expertise in digital dictation, transcription, speech recognition and enterprise solutions, CES is an industry expert in Dragon Voice ...",Certified eSupport Launches CES University Prompting Expansion ...,http://finance.yahoo.com/news/certified-esupport-launches-ces-university-151100320.html
28,"Amazon Web Services just unveiled a new service for running callcenters, dubbedAmazon Connect, leveragingthe same technology used by Amazon.coms own customer service systemto route and manage calls using automatic speech recognition and artificial intelligence.  The announcement is the latest move by the cloud giant beyond its core infrastructure technologies and into higher-level cloud services. Amazon saysthe service incorporates its Lex technology, an artificial intelligence service for speech recognition and natural language processing, which also powers the companys Alexa virtual assistant.  The company says Amazon Connect works with existingAWS services such as DynamoDB, Amazon Redshift, or Amazon Aurora, as well as third-party CRM and analytics services. Salesforce says its integrating its Service Cloud Einstein with Amazon Connect. It uses a graphical interface to let companies set up a workflow for calls without coding.  Amazon Connectuses a traditional cloud pay-as-you go pricing model, charging companies depending on their usage, rather than an up-front licensing fee. AWSchief evangelist Jeff Barr does a deep dive on the technology in this post, and AWS GM Matt Wood provides an overview in this video, noting that the service will ultimately expand beyond the U.S. and Europe to other markets around the world.  The call-center technologymarket includes many existing players such as Seattle-based Spoken, as well as Zendesk,Cisco Systems, Avaya,Genesysand many others.  Ten years ago, we made the decision to build our own customer contact center technology from scratch because legacy solutions did not provide the scale, cost structure, and features we needed to deliver excellent customer service for our customers around the world, said Tom Weiland, Amazons vice president of worldwide customer service, in a news release. This choice has been a differentiator for us, as it is used today by our agents around the world in the millions of interactions they have with our customers.Were excited to offer this technology to customers as an AWS service  with all of the simplicity, flexibility, reliability, and cost-effectiveness of the cloud.  Amazons announcementconfirms a report by The Information news site last month.  Developing story, more to come.",2017-03-28,"Amazon says the service incorporates its Lex technology, an artificial intelligence service for speech recognition and natural language ...",Amazon Web Services jumps into call-center market with new ...,http://www.geekwire.com/2017/amazon-web-services-jumps-call-center-market-new-amazon-connect-service/
29,"Download the PDF of this article.  This electronic Swiss army knife may not be as functional as Doctor Whos sonic screwdriver, but the Azulle Lynk (see figure) can be quite handy for the odd electronic device connected to an HDTVsay, a gaming system like Sonys PS4 or Microsofts XBOX One. Voice command systems make those gaming platforms easy to use, but sometimes having a QWERTY keyboard can be handy.  The Azulle Lynk is a five-in-one wireless remote control with a QWERTY keyboard on the back.  The Azulle Lynk is a five-in-one wireless remote control. It comes with a 2.4-GHz USB transceiver that works with most PC and gaming systems, including Windows and Linux platforms. The wireless support operates up to 25 ft. This interface links an air mouse and QWERTY keyboard to those systems. There is also a conventional IR remote, and the system is programmable so it can handle most IR-enabled devices. The built-in microphone can be used with speech recognition software or voice applications like voice-over-IP (VoIP) phone support, including applications like Skype.  The Azulle Lynk is a good complement to the many mini-PC sticks out there. I tried it with a Raspberry Pi and Azulles Access Mini Stick (see Fanless Mini PC Stick Handles USB 3.0 Peripherals on electronicdesign.com). It proved to be quite handy, although it does have its quirks.  The Lynk will not replace a set-top box remote as its primarily designed to control a PC or gaming system. It has a Windows start menu button, along with left and right mouse click buttons. There are multimedia buttons for volume and play/pause. The area around the center/left mouse button provide cursor movement, making menu navigation easy. The six-axis gyro support makes the air mouse responsive, although some may prefer to adjust its movement speed.  The one quirk is that the IR support can be toggled on and off. Programming the IR support is relatively easy, but one needs to contend with how it will be used. For example, the volume of a display may be controlled via the IR interface, or it may be easier to control it from the PC side of things that would use the wireless support. The Lynk can handle the chore, but if you can get away without it you will find the system much easier to contend with. Luckily, I could set the volume once on the HDTV and control it from the PC side.  The Lynk fits nicely in my hand, and flipping it over to use the keyboard was easy. While comfortable to use, its obviously not a full-size keyboard. Text entry is fine, but numeric and function keys are a bit more problematic. Figuring out which shift keys need to be used takes some practice. For example, numbers use the Sym key. Likewise, thumbs tend to be the normal button pressing digits. It works and is actually comfortable, but not necessarily fast. Still, you can always grab a full-size keyboard if you plan on writing the next great novel.  The buttons on either side can be illuminated. This can be handy in a home theater environment where a dark or dim setting is often preferable.  I have found the Lynk to be very useful with Azulle Access Mini Stick operating as a media player. I dont need another keyboard or mouse attached; the combination is sufficient to start applications and control them, as keyboard data entry usually isnt required. The Lynk keyboard is handy when doing searches on YouTube or similar sites. I leave content creation to my laptop with a regular keyboard.  Another use for the Lynk is for presentations. Running a slideshow is easy, with the air mouse making a great pointer. The keyboard is also sufficient for entering limited amounts of text, like entering search text or user names when showing off an application. I find the Lynk much easier to use while standing than trying to use a conventional keyboard.  True, the Lynk may not for everyone. Personally Ive found it to be quite useful for media PC applications.",2017-04-12,"The built-in microphone can be used with speech recognition software or voice applications like voice-over-IP (VoIP) phone support, including ...",Handheld Keyboard Remote is Handy for Media PCs,http://electronicdesign.com/blog/handheld-keyboard-remote-handy-media-pcs
30,"Global Speech Recognition Industry 2016 Market Research Report Purchase This Report by calling ResearchnReports.com at +1-888-631-6977.Global Speech Recognition Market had made research and come up with a report that focuses on the major players in the Global Speech Recognition Market throughout the world. The report includes information like company profiles, specification and product picture, production, capacity, contact information, cost and revenue. Likewise, equipment and upstream raw materials as well as downstream demand analysis is also tackled.The report investigates and analyzes the Global Speech Recognition Market and shows a comprehensive evaluation of the evaluation and its specifications. Another aspect that was taken is the cost analysis of the main products dominant in the global ice cream industry considering the profit margin for the manufacturers.Click here to Download Sample PDF illustration: www.researchnreports.com/request_sample.phpid=49467 Through this report, the core driving factors of the Global Speech Recognition Market were identified and the business partners and end-users were also elaborated. The business sector structure, business patterns and challenges affecting the market globally were also included in the extensive analysis for this research report. Various interviews and talks were held with the prominent leaders in the industry in order to obtain reliable and updated information pertaining to the market.Ask for Discount: www.researchnreports.com/ask_for_discount.phpid=49467 The report firstly introduced the Speech Recognition basics: definitions, classifications, applications and industry chain overview; industry policies and plans; product specifications; manufacturing processes; cost structures and so on. Then it analyzed the world's main region market conditions, including the product price, profit, capacity, production, capacity utilization, supply, demand and industry growth rate etc. In the end, the report introduced new project SWOT analysis, investment feasibility analysis, and investment return analysis.For more inquiry before purchase: www.researchnreports.com/enquiry_before_buying.phpid=49467 Table of Contents1 Speech Recognition Overview2 Global Speech Recognition Competition by Manufacturers, Type and Application3 USA Speech Recognition (Volume, Value and Sales Price)4 China Speech Recognition (Volume, Value and Sales Price)5 Europe Speech Recognition (Volume, Value and Sales Price)6 Japan Speech Recognition (Volume, Value and Sales Price)7 India Speech Recognition (Volume, Value and Sales Price)8 Southeast Asia Speech Recognition (Volume, Value and Sales Price)9 Global Speech Recognition Manufacturers Analysis10 Speech Recognition Manufacturing Cost Analysis11 Industrial Chain, Sourcing Strategy and Downstream Buyers12 Marketing Strategy Analysis, Distributors/Traders13 Market Effect Factors Analysis14 Global Speech Recognition Market Forecast (2016-2021)Complete report available at: www.researchnreports.com/instrument-sensors/Global-Speech... About Research N Reports:Research N Reports is a new age market research firm where we focus on providing information that can be effectively applied. Today being a consumer driven market, companies require information to deal with the complex and dynamic world of choices. Where relying on a sound board firm for your decisions becomes crucial. Research N Reports specializes in industry analysis, market forecasts and as a result getting quality reports covering all verticals, whether be it gaining perspective on current market conditions or being ahead in the cut throat global competition. Since we excel at business research to help businesses grow, we also offer consulting as an extended arm to our services which only helps us gain more insight into current trends and problems. Consequently we keep evolving as an all-rounder provider of viable information under one roof.Contact:Research N Reports10916, Gold Point Dr, Houston, TX, Pin - 77064,Sunny Denis(Sales Manager),+1-8886316977,",2017-03-22,Global Speech Recognition Industry 2016 Market Research Report Purchase This Report by calling ResearchnReports.com at ...,What Is The Future Of “Global Speech Recognition Market” In Global ...,http://www.openpr.com/news/477053/What-Is-The-Future-Of-Global-Speech-Recognition-Market-In-Global-Industry-Find-Out-Its-Uses-Benefits-etc.html
31,"Remember that time, way back in the early 2000s, when Ozzy Osbourne got angry with his new BMW because its voice-control system couldnt understand him  Today, we are all Ozzy  constantly rephrasing and e-nun-ci-ating to try and get Siri, Cortana, Alexa or whoever to understand us. While speech-controlled systems can be okay in a quiet environment, when theyre fighting against the background noise of tyres, engines and other people talking, it all goes to pot. Experts in the field regularly quote a  possibly apocryphal  tale of someone sending a speech-to-text message which incorrectly informed a family member that their mother had arrived, dead, at her destination.  Help has arrived in the shape of Liopa (Irish for lips), a Belfast-based start-up, borne out of research at Queens University Belfast. The company is developing software and systems which can read your lips, in a way unaffected by background noise, which can dramatically improve the accuracy of speech-controlled systems.  Were setup to commercialise some research that had been done in Queens, around eight to 10 years of research, on what we call viseme, says Liopa chief executive Liam McQuillan, who is a telecoms veteran. That is the way your lips move when you speak. Theyve found that someones lip movements are very speaker-specific, much in the way that your fingerprints are, so the idea was to develop this technique as a way of online user verification. So anywhere you could train a camera on someones face and get them to say something, you could track their lip movement, once theyre pre-enrolled, and you can compare that movement against a challenge phrase or a series of numbers to be read out.  The initial work was in turning the lip-reading system into a security protocol, but that plan ran up against two hurdles: the fact that the biometric security market is already crowded, and that the consequences of getting a false positive result could be disastrous. As McQuillan points out: You make a false match and someone can get into your bank account. So for the last nine months we kind of pivoted a little, and started to develop the speech recognition from where it is today, which is digit recognition, through to where its at limited vocabulary or menu control and eventually on to natural speech.  So where we see this really playing is as a supporting technology to audio speech recognition. All the big guys are investing heavily in that area  tens of millions into personal assistants, such as Cortana, Siri, Amazon with Alexa, all of which are based on audio speech recognition.  We see the car thing as a good initial use case, where we can train a camera on the drivers face. Today its an RGB camera, but were developing an infrared one to take the outside illumination out of the equation. So where theres a lot of background noise, road noise, engine revs, winding down the window, as long as you can see the drivers face and read their lip movements you can combine the two techniques, and our software will improve audio speech recognition platforms.  The idea is to combine both the audio and the visual inputs to create a more accurate system, as Fabian Campbell-West, Liopas head of research and development explained: A lot of the audio command-and-control stuff is fairly clear. Where it can be difficult at the moment is when you try and dictate a number, for instance. So what were focusing on right now are the commands that control the in-car environment, and were confident we can get very good accuracy with those.  What the system cant do well, yet, is deal with people using free speech or speaking colloquially. Right now, the Liopa system has to first be trained into your lip movements, and then will require you to stick to a specific set of vocabulary and grammar.  Where everyone wants to get to is free speech, which were not at yet, says Richard McConnell, Liopas chief operating officer and technical officer. You create whats called a model, so you train the system to your lip movements. Then theres the fine vocabulary and grammar model. Free speech is definitely on our road map though. The key thing for us is that in a noisy environment were getting 70-80 per cent accuracy at a point where the audio is almost unusable. The research here has identified that theres about 30 specific visemes, and the more combinations of those that you capture the better.  There are two aspects to the system that will be exceptionally tempting to the car makers who may become Liopas customers. First, the technology needs only a decent camera which can see the drivers face, and a bit of software, all of which McConnell says can be readily accommodated by existing onboard systems: At the minute it just needs an ordinary camera, and the computing power required we think is quite small. Theres enough compute power in a car already to deal with what we need. And when you add things like gesture control, it all helps.  The granularity of image that were looking at only needs to be 16x16 pixels, but we can add accuracy with infrared cameras and depth sensors, a bit like the Kinect system in a gaming console. But generally theres no special hardware required. We are moving towards using AI and neural network-type systems; but, again, many cars have systems which can already cope with that.  Secondly, theres a chance for car makers, and their software and interface designers, to catch up with the likes of Apple and Googles Android. With the introduction of Apple CarPlay and Android Auto, the two smartphone software giants are increasingly taking over the space within the car, and many vehicle manufacturers are known to be concerned over the transfer of user data and experience. Developing a superior voice-control system could be a useful hook in getting drivers to switch out of CarPlay and back to the proprietary vehicle software.  Weve met with a number of car companies, and Toyota was especially keen on it, says McQuillan. So a couple of months ago we closed a pre-seed round, which takes us up to the summer and allows us to develop an in-car system, and then well go out on a proper funding round, and well be talking to the car manufacturers.  Given the enormous concerns over driver distraction and accidents caused by phone use behind the wheel, anything that can make a voice-controlled system easier and more accurate to use, and which might keep drivers away from their keypads, will surely snag the interest of car makers. And possibly a few long-haired rock gods too.",2017-04-05,"So for the last nine months we kind of pivoted a little, and started to develop the speech recognition from where it is today, which is digit ...",Read my lips: Irish start-up gets teeth into verification tools,http://www.irishtimes.com/business/innovation/read-my-lips-irish-start-up-gets-teeth-into-verification-tools-1.3032289
32,"Getting an AI to understand speech is already a tough nut to crack. A group of Australian researchers wants to take on something much harder: teaching once-deaf babies to talk.  Think about what happens when you talk to Siri or Cortana or Google on a phone: the speech recognition system has to distinguish your OK Google (for example) from background noise; it has to react to OK Google rather than OK something else; and it has to parse your speech to act on the command.  And you already know how to talk.  The Swinburne University team working on an app called GetTalking can't make even that single assumption, because they're trying to solve a different problem. When a baby receives a cochlear implant to take over the work of their malfunctioning inner ear, he or she needs to learn something brand new: how to associate the sounds they can now hear with the sounds their own mouths make.  Getting those kids started in the world of conversation is a matter of habilitation  no rehabilitation here, because there isn't a capability to recover.  GetTalking is the brainchild of Swinburne senior lecturer Belinda Barnet, and the genesis of the idea was her own experience as mother to a child with a cochlear implant.  As she explained to The Register: With my own daughter  she had an implant at 11 months old  I could afford to take a year off to teach her to talk. This involves lots of repetitive exercises.  That time and attention, she explained, is the big predictor of success.  In the roughly 10 years since it became standard practice to provide implants to babies at or before 12 months of age (fully funded by Australia's national health insurance scheme Medicare since 2011), 80 per cent of recipients achieve speech within the normal range.  What defines the 20 per cent that don't get to that point Inability, either because of family income or distance from the city, to spend a year sitting on the carpet with flash-cards.  That makes it hard for parents in rural or regional locations, regional, or low-income mothers, Barnet said.  The idea for which Barnet and associate professor Rachael McDonald sought funding looks simple: an app to run on something like an iPad that gives the baby a bright visual reward for speaking.  However, it does test the boundaries of AI and speech recognition, because of a very difficult starting point: how can an app respond to speech when the baby has never learned to speak  Apple never revealed the price it paid to acquire the team that developed Siri, but rumours of US$150 million don't sound unreasonable  and Siri takes its input from someone who knows how to speak.  For all the effort that's gone into speech recognition and AI, we also know it remains so difficult it's been automated for only a couple of per cent of languages.  Leon Sterling, a Swinburne computer science researcher, had his interest piqued as a member of the university panel assessing the project, and is helping bring a long experience of AI research to the project.  He explained the hidden complexities behind what needs to present itself as a simple app.  You've got to get the signal, you have to extract the signals, separate them from the background noise, the parents speaking, et cetera.  Most of those problems have precedent, but GetTalking needs yet more machine learning  like trying to measure the child's engagement with the app. You've got to look at the ability to observe, to tag video strings together with audio strings.  The team understands that an app can't replace a speech therapist or parent, but only support them  and that adds new complexities like building in the knowledge of how children interact with physiotherapists. You need to understand the developmental stages of children when they're interacting with the app.",2017-03-26,Think about what happens when you talk to Siri or Cortana or Google on a phone: the speech recognition system has to distinguish your OK ...,Researcher hopes to teach infants with cochlear implants to speak ...,https://www.theregister.co.uk/2017/03/26/speech_recognition_for_kids_with_cochlear_implants/
33,"It seems that Microsoft is trying to make Skype the worlds best interpreter, as the company has just announced that Skype Translator, the real-time speech translation service, is now supporting the Japanese language.  This means that Skype Translator is now supporting ten languages: English, Arabic, Chinese (Mandarin), French, German, Italian, Portuguese (Brazilian), Russian, Spanish, and, of course, Japanese. So, for example, if you are an Italian who doesnt know Russian, dont worry because Skype Translation can translate to your native language.  According to reports, the Skype Translator is using two neural-network based Artificial Intelligence (AI) technologies: Machine Translation and Automatic Speech Recognition. Microsoft claims that this feature is using TrueText natural language processing technology and a text speech synthesizer, allowing users to hear and not only read the translation.  However, we have to mention that Microsoft is not the only company whos trying to fix, one way or another, the language barriers between English and Japanese speakers. For example, back in January 2017, Google has added a real-time Japanese translation capability to the World Lens app, a very popular application for travelers.  At the same time, Google Translate app allows you to take a picture of a Japanese text and view it in English, but the new real-time speech recognition is surely a feature that will be used by companies from Japan to keep in touch with their associates and customers from all over the world.  Its not sure yet what new languages will be added to Skype Translator, but were pretty sure that Microsoft is trying very hard to lure more companies to use its Skype services in the future.  Google is also trying to keep up the pace with Microsoft, as the system developed by the big search engine company is now able to translate the whole sentences at a time, with more accurate results than before.",2017-04-12,... Translator is using two neural-network based Artificial Intelligence (AI) technologies: Machine Translation and Automatic Speech Recognition ...,Skype Is Now Able To Translate Voice And Video Calls To Japanese,http://www.donklephant.com/skype-is-now-able-to-translate-voice-and-video-calls-to-japanese/
34,"With the worlds largest number of internet users (731 million, as of December 2016), China remains one of the hottest e-commerce, social media and online services markets. However, the country has a long way to go in order to reach the internet penetration levels of developed countries, as only around half of the Chinese population have access to the internet. As Chinese internet users grow, naturally the business of the countrys internet giants grows more and more. Whats more, the presence of internet giants such as Google, Facebook and Twitter in the country remains limited, which leaves the majority of the digital market to local players. The key local players on the Chinese technology frontline are the trio known as the BAT companies  Baidu, Alibaba and Tencent.  Baidu is often referred to as the Chinese Google. The companys 2016 revenue slightly underperformed against its 2015 results, thus Baidus investors will be focused on revenue and net profit growth in near future. Some analysts predict Baidu, who remain focused on the PC market, is going to lose market share to its competitors, whose products are primarily designed for mobile users.  Around 80% of Baidus total revenue comes from the segment of search services, and applications such as Baidu Encyclopedia, maps, image search, video search, news search, and so on. Last year Baidu received a total of $5.5bn in advertising revenue, which accounts for 20% of the total Chinese digital ad market.  The other two business segments  transaction services (Baidu Wallet) and video streaming (iQiyi)  account for the rest of their revenues. iQiyi is the largest online video platform in China (often benchmarked against YouTube) and continues to invest both in licensed content and in-house production, since exclusive content is a prerequisite for gaining higher-margin subscriptions.  However, two things are intriguing about Baidu from a strategic perspective:  Two years ago, Baidus technology pipeline started delivering results as the companys speech recognition technology Deep Speech 2 became a pioneer in recognising different Chinese dialects. Its autonomous car tests were a success. Baidus mapping technology could prove to be a significant advantage as the company plans to begin small-scale production of autonomous cars next year, but mass production remains delayed till after 2020.  One year ago, Baidu launched a HealthTech solution with huge growth potential: the medical assistant chatbot Melody. Also in 2016, the company presented its facial recognition technology, which was said to have up to 99.77% accuracy.  This year the internet giant revealed its text-to-speech system, which synthesises artificial human speech from text. Baidu is also developing a holographic technology for VR and planning to launch a concert streaming and movie production service based on VR.  By investing heavily in technology and R&D, Baidu is positioning itself to profit from exponential growth in AI, augmented reality, VR and self-driving cars.  While a detailed strategic analysis of the Chinese e-commerce giant could take up an entire doctoral dissertation, to put it briefly, the Group has two ambitious long-term objectives:  In order to achieve its goals, Alibaba has made investments in a wide range of sectors  smart logistics, payment services, cloud computing, online marketing services, travel booking, music and video streaming. Because of its ambitious goals, its solid existing customer base, the synergies between the companies in the group and strong overall cash flows, Alibaba remains the largest retail commerce company in world.  According to eMarketer, Alibaba received a total of $11bn in advertising revenue, which accounts for around 40% of the total digital ad market in China. Alibaba also remains the dominant niche player on the online B2B marketplace, while key competitors Amazon and eBay focus more on B2C.  From a mid-term perspective, Alibaba is looking at two strategic goals. The first is to acquire a customer base in the growing middle class beyond Chinas metropolitan areas, in order to increase its active buyer base. Apart from traditional customer acquisition strategies, online to offline (O2O) could be an excellent model for expansion in those locations as Alibabas online discovery tools and mobile payments platform can be easily combined in order to benefit both buyers and sellers.  Alibabas second mid-term goal is gaining market share in the entertainment sector  the group is diversifying its earnings with investments in digital media, entertainment and last month, Alibaba announced its plan to enter the game distribution industry. Entertainment revenues are derived from digital content such as film, music and sports and online distribution of movie tickets, online video, streaming and over-the-top content TV, which are gaining popularity at solid speeds on the Asian markets. The group also has huge potential for the integration of anchor assets in the entertainment field with its payment, cloud and e-commerce solutions.  In January, Alibaba representatives said the company is considering a European logistics centre in Bulgaria. Bulgarias strategic location (close both to leading European and growing Turkish markets), well-educated workforce and improving road and rail infrastructure could lead to an expanding market share of the group in both Europe and Turkey, and thus assist the company to achieve part of its international expansion goal. The logisticscentre in Bulgaria could be the first stepping stone in the groups globalisation strategy.  Tencent offers a wide portfolio of digital services: social media, online gaming, cloud and payment services, music streaming, film production, microblogging, and more. Since 2010, Tencent is on the growth track and the company reported a 43% rise in 2016 net profit. Last years growth is mainly due to the companys increased business in mobile games, social media platforms, cloud and payment services and the giants key focus remained the transition from PC to mobile.  Tencent operates the largest social communication platform in China, and the company revealed that its social media platforms were growing faster than Facebook. WeChat is the key brand in Tencents portfolio, as it is the most used app in China for 2016. Especially, WeChats red envelope application was a social hit around the holidays  users sent 14.2 billion virtual red envelopes over Chinese New Year. WeChat was ranked as the most valuable Chinese brand, valued at $106bn. Another Tencent messaging app, QQ, was ranked second in the list of most popular apps in China.  Tencent received a total of $3.2 billion dollars of advertising revenue last year, accounting for around 11% of the total digital ad market in China. Payment services are also leading to solid growth as TenPay and QQ Wallet are widely used for utility payments and public transport cards. Other services such as QQ Music continue to gain market share  the service now has around 10 million users who have signed up for its paid music services.  Though the key growth drivers for the company were cloud services, payments, smartphone games and messaging platforms, due to intense competition from rivals, the gross profit margin from payments and cloud services is expected to remain relatively low in the future. So Tencent can still ride the organic growth trend, but it needs to increase the popularity of key services such as mobile and PC games, and work on further monetising its messaging apps.  From an opportunity perspective, Tencent has potential to enhance valuable synergies between services in its portfolio  for example between Didi (the company which acquired Ubers Chinese operations) and WeChat or QQ Wallet and QQ Music or between its film and music businesses. The tech company can further develop partnerships with global exclusive content providers and global powerhouses, as it has done with Starbucks, as a means to position itself in the high-end of the online content value chain. Tencent recently acquired a 5% stake in Tesla, which could be the first step in a beneficial cross-continental partnership.  One trend is certain: for the rest of the year, Chinese tech companies are going to develop new services, spend more on R&D, acquire stakes in local and overseas companies and test new breakthrough technologies.",2017-04-11,"Two years ago, Baidu's technology pipeline started delivering results as the company's speech recognition technology Deep Speech 2 became ...","Baidu, Alibaba and Tencent: Strategies of the Chinese Internet Giants",http://themarketmogul.com/baidu-alibaba-tencent-strategies-internet/
35,"Big Market Research analysts forecast the Global Automotive Voice and Speech Recognition System Market to grow at a CAGR of 4.31% during the period 2016-2020.  The VR systems, which we know today, featuring advanced speech recognition, emerged in the automotive domain in 2004-2005. One of the first vehicles to release it was Honda Acura RL, wherein the system was a part of the standard equipment.  Honda also offered the VR system in its Acura MDX and Odyssey models as optional equipment.  The Honda VR system was based on the IBM-embedded ViaVoice software, which was used to jointly develop the system by the two companies.  The system was designed to access the majority of the in-car navigation system with voice inputs by the user. There were more than 700 commands and millions of city and street names, which were built into the system.  However, this was not the first VR system; earlier, Mercedes-Benz had come up with a rudimentary system (used only to operate the on-board telephone) back in 1996. It did not take off during those early years, mostly due to the limited nature of consumer exposure (Mercedes only offered it in the S and CL Classes), as well as because the technology was still in its infancy.  The report covers the present scenario and the growth prospects of the global automotive VR system market for 2016-2020. To calculate the market size, Technavio considers the revenue generated from the total consumption of automotive VR system market globally.  The report does not include revenue generated from the aftermarket service of the product.  Big Market Research report, global automotive VR system market 2016-2020, has been prepared based on an in-depth market analysis with inputs from industry experts. The report covers the market landscape and its growth prospects over the coming years.  The report also includes a discussion of the key vendors operating in this market.",2017-04-05,Global Automotive Voice and Speech Recognition System Industry Research Report from 2016 to 2021: This study is helpful for market players ...,Learn details of the global automotive voice and speech recognition ...,https://www.whatech.com/market-research/transport/283287-learn-details-of-the-global-automotive-voice-and-speech-recognition-system-market
36,"Ever wondered what it would be like for artificial intelligence to trip-out while watching Bob Ross paint a pretty picture  This video was created by Alexander Reben, an engineer turned artist who uses technology to explore how machines are changing the human world and vice versa. It features an episode of the stoner-favorite television show The Joy of Painting with Bob Ross through Googles neural network DeepDream.  DeepDream is a convolutional neural network, a style of computing inspired by the brain, that identifies and recognizes images and patterns. Most of the time, it's used to create nightmarish visions like these, but its alsoa surprisingly insightful visualization that shows how computers think in regards to tasks like image classification and speech recognition.  As Rebenexplains in the description: ""This artwork represents what it would be like for an AI to watch Bob Ross on LSD (once someone invents digital drugs). It shows some of the unreasonable effectiveness and strange inner workings of deep learning systems.The unique characteristics of the human voice are learned and generated as well as hallucinations of a system trying to find images which are not there.  Google made the code forDeepDream open-source, meaning there are plenty of videos, images, and apps that utilize it. Somebody has also published the code and a guide on GitHub, complete with a nightmarish rehash of Fear & Loathing in Las Vegas.",2017-04-10,... a surprisingly insightful visualization that shows how computers think in regards to tasks like image classification and speech recognition.,Watch Artificial Intelligence Lose Its Mind While Watching Bob Ross,http://www.iflscience.com/technology/watch-artificial-intelligence-lose-its-mind-while-watching-bob-ross/
37,,2017-04-11,"... comprising TBM1: Getting to know my home, TBM4: Visit My Home, FBM2: Navigation Functionality and FBM3: Speech Recognition.",European Robotics League gearing up for 2017-2018 season,http://robohub.org/european-robotics-league-gearing-up-for-2017-2018-season/
38,Your phone's motion sensors can give away PINs and passwords,2017-04-07,The technology's end-to-end speech translation capability works by using two neural-network based AIs. Its Automatic Speech Recognition AI ...,Microsoft Translator turns your words into spoken Japanese,https://www.engadget.com/2017/04/07/microsoft-translator-spoken-japanese/
39,"ADVANCED VOICE RECOGNITION SYSTEMS, INC. (OTCMKTS:AVOI) Files An 8-K Entry into a Material Definitive AgreementItem 1.01. Entry Into a Material Definitive Agreement  On March 31, 2017, Advanced Voice Recognition Systems, Inc. ( ) entered into a Letter of Engagement for Legal Services Limited Scope Agreement ( ) with Schmeiser, Olsen & Watts LLP ( ) to which the Firm will serve as local counsel in the United States District Court, District of Arizona.  The Firm has been hired to represent AVRS as local counsel in connection with forthcoming litigation in the U.S. District Court, District of Arizona. Under the Agreement, AVRS will pay the Firm an evergreen fee advance of $3,000 for deposit into the Firms trust account. The Firm will draw from the $3,000 placed in the Firms trust account to compensate for services as performed. AVRS may terminate the Agreement at any time.  In 2015, AVRS entered into an Advisory Services Agreement (the  ) with Dominion Harbor Group, LLC ( ), a copy of which was filed as to AVRSs Current Report on Form 8-K filed with the U.S. Securities and Exchange Commission on August 21, 2015. to the Advisory Services Agreement, Dominion will pay the Enforcement Expenses described in Section 10 of the Agreement on behalf of AVRS.  The foregoing is qualified in its entirety by reference to the Limited Scope Agreement, which is filed as to this Current Report on Form 8-K and is incorporated herein by reference.  About ADVANCED VOICE RECOGNITION SYSTEMS, INC. (OTCMKTS:AVOI)   Advanced Voice Recognition Systems, Inc. is a software development company. The Company specializes in creating interface and application solutions for speech recognition technologies. The Companys principal proposed product is speech recognition software and related firmware, which allows for dictation into a range of applications, including disk operating system (DOS) applications running in Windows, UNIX and mainframe applications accessed through terminal emulation programs, various custom applications, and all Windows 3.x, 95, 98, 2000, XP, Vista and Windows 7 programs. This product is designed to allow for deferred dictation, where the text is saved with the associated audio, and the users can resume when stopped and can play back dictated content. Similarly, the recognized text and associated audio can be saved to be used when text is corrected. ADVANCED VOICE RECOGNITION SYSTEMS, INC. (OTCMKTS:AVOI) Recent Trading Information   ADVANCED VOICE RECOGNITION SYSTEMS, INC. (OTCMKTS:AVOI) closed its last trading session down -0.00085 at 0.00795 with 23,000 shares trading hands.  An ad to help with our costs",2017-04-07,The Company specializes in creating interface and application solutions for speech recognition technologies. The Company's principal ...,"ADVANCED VOICE RECOGNITION SYSTEMS, INC. (OTCMKTS ...",https://marketexclusive.com/advanced-voice-recognition-systems-inc-otcmktsavoi-files-an-8-k-entry-into-a-material-definitive-agreement-6/91032/
40,"Scientists at Oxford say they've invented an artificial intelligence system that can lip-read better than humans.  The system, which has been trained on thousands of hours of BBC News programmes, has been developed in collaboration with Google's DeepMind AI division.  ""Watch, Attend and Spell"", as the system has been called, can now watch silent speech and get about 50% of the words correct. That may not sound too impressive - but when the researchers supplied the same clips to professional lip-readers, they got only 12% of words right.  Joon Son Chung, a doctoral student at Oxford University's Department of Engineering, explained to me just how challenging a task this is. ""Words like mat, bat and pat all have similar mouth shapes."" It's context that helps his system - or indeed a professional lip reader - to understand what word is being spoken.  ""What the system does,"" explains Joon, ""is to learn things that come together, in this case the mouth shapes and the characters and what the likely upcoming characters are.""  The BBC supplied the Oxford researchers with clips from Breakfast, Newsnight, Question Time and other BBC news programmes, with subtitles aligned with the lip movements of the speakers. Then a neural network combining state-of-the-art image and speech recognition set to work to learn how to lip-read.  After examining 118,000 sentences in the clips, the system now has 17,500 words stored in its vocabulary. Because it has been trained on the language of news, it is now quite good at understanding that ""Prime"" will often be followed by ""Minister"" and ""European"" by ""Union"", but much less adept at recognising words not spoken by newsreaders.  A lot more work needs to be done before the system is put to practical use, but the charity Action on Hearing Loss is enthusiastic about this latest advance.  ""AI lip-reading technology would be able to enhance the accuracy and speed of speech to text,"" says Jesal Vishnuram, the charity's technology research manager. ""This would help people with subtitles on TV, and with hearing in noisy surroundings.""  Right now the system has limitations - it can only operate on full sentences of recorded video. ""We want to get it to work in real time,"" says Joon Son Chung. ""As it keeps watching TV, it will learn."" And he says getting the system to work live is a lesser challenge than improving its accuracy.  He sees all sorts of potential uses for this technology, from helping people to dictate instructions to their smartphones in noisy environments, to dubbing old silent films.  In many cases, the AI lip-reading system could be used to improve the performance of other forms of speech recognition.  Where the Oxford researchers and the hearing loss charity agree, is on the fact that this is not a case where AI is going to replace humans.  Professional lip-readers need not fear for their jobs - but they can look forward to a time when technology helps them become a lot more accurate.",2017-03-16,"In many cases, the AI lip-reading system could be used to improve the performance of other forms of speech recognition. Where the Oxford ...",Towards a lip-reading computer,http://www.bbc.com/news/technology-39298199
41,"Enterprise Connect 2017, which CommsTrader was lucky enough to attend this year, highlighted a significant transition in the world of communications at an enterprise level, and potentially outlined a turning point for the industry. Sure, weve seen the changes coming, but now theyre finally here.  The transition were talking about is the final farewell to IP-PBX, and the beginning of the end for companies that depend on those services. Think of this change on the same level as the change that cell-phone technology had on landline phones. Its big. People still communicate from home phones, but the device businesses and legacy networks are fading into the past.  The keynote speakers for Enterprise Connect this year were just one of the biggest symbols of the latest transition. This is the first time ever that not one of the keynotes was presented by a company that makes even a minimum of 10% of its revenue from IP-PBX. Rather, the keynote speakers were:  Another sign of the transition for companies today, is represented by the finalist companies in the list for the Best of Enterprise Connect award. The top six were:  Check out the eventual winnerof the Best of Enterprise Connect 2017.  Its fair to say that this year, the emphasis has been on improving user experience, and providing a communication service thats integrated with different applications. Basically, everything about Enterprise Connect this year was something of a celebration of the transition that the communications industry is making.  Its time for us all to move into the future.",2017-04-11,"Enterprise Connect 2017, which CommsTrader was lucky enough to attend this year, highlighted a significant transition in the world of ...",Enterprise Connect Reflects Major Industry Transition for 2017,https://www.commstrader.com/news/industry/enterprise-connect-reflects-major-industry-transition-2017/
42,,2017-04-05,Speech Recognition – Let Cortana and Windows Store apps recognize your voice… and send data to Microsoft to improve speech recognition.,Windows 10 Creators Update puts all your privacy settings in one ...,https://liliputing.com/2017/04/windows-10-creators-update-puts-privacy-settings-one-place.html
43,"The new research report on Speech & Voice RecognitionMarket offered by DecisionDatabases.com provides Global Industry Analysis, Size, Share, Growth, Trends and Forecast Till 2022.  The global speech & voice recognition market research report provides detailed information about the industry based on the revenue (USD MN) for the forecast period. The research study is a descriptive analysis of this market emphasizing the market drivers and restraints that govern the overall market growth. The trends and future prospects for the market are also included in the report which gives an intellectual understanding of the speech & voice recognition industry. Furthermore, the report quantifies the market share held by the major players of the industry and provides an in-depth view of the competitive landscape. This market is classified into different segments with detailed analysis of each with respect to geography for the study period.  A glimpse of the major drivers and restraints affecting the speech & voice recognition market is mentioned below.  1. Growing demand for speech based biometric system for identification purpose   2. Growing fraudulent activities worldwide   3. Rising demand for voice authentication in mobile banking applications   4. Rising Usage of IoT devices and integration of speech recognition technology  1. Lack of accuracy   2. Restricted usage for cloud-based services due to oligopoly industry  The report offers a value chain analysis that gives a comprehensive outlook of the speech & voice recognition market. The attractiveness analysis of this market has also been included so as to evaluate the segments that are anticipated to be profitable during the forecast period.  The speech & voice recognition market has been segmented based on technology such as speech recognition, and voice recognition. The study incorporates periodic market estimates and forecasts. Each technology has been analyzed based on the market size at regional and country levels.  The speech & voice recognition market has been segmented based on vertical such as automotive, enterprise, finance, consumer, government, retail, healthcare, and others. The report provides forecast and estimates for each vertical in terms of market size during the study period. Each vertical has been further analyzed based on regional and country levels.  The speech & voice recognition market has been segmented based on application such as artificial intelligence, and non-artificial intelligence. The report provides forecast and estimates for each application in terms of market size during the study period. Each application has been further analyzed based on regional and country levels.  For More Information About This Report: http://www.decisiondatabases.com/ip/6421-speech-voice-recognition-market-report  Geographically, the speech & voice recognition market has been segmented into regions such as North America, Europe, Asia Pacific, Latin America, and Rest of the World. The study details country-level aspects based on each segment and gives estimates in terms of market size.  The report also studies the competitive landscape of the global market with company profiles of players such as Nuance Communications Inc., Microsoft Corporation, Agnitio SL, Raytheon BBN Technologies, Validsoft UK Limited, Sensory, Inc., Biotrust ID B.V., Voicevault Inc., Voicebox Technologies Corp., Lumenvox, LLC, M2SYS LLC, Advanced Voice Recognition Systems, Inc., and Mmodal Inc. A detailed description of each has been included, with information in terms of headquarters, inception, stock listing, upcoming capacities, key mergers & acquisitions, financial overview, and recent developments. This analysis gives a thorough idea about the competitive positioning of market players. The report also gives information of speech & voice recognition markets mergers/acquisitions, partnerships, collaborations, new product launches, new product developments and other industry developments.  Low migration inks are specially designed inks for food packaging. They are formulated with specific materials that ensure migration within the accepted limits. It contains elements that do not migrate or migrate within limits. Indicative analytical testing or relevant adverse calculations support the use of low migration inks.   View More:http://www.decisiondatabases.com/ip/261-low-migration-inks-market-report",2017-04-04,"The new research report on Speech & Voice Recognition Market offered by DecisionDatabases.com provides Global Industry Analysis, Size, ...",Global Speech & Voice Recognition Market | Forecast Research ...,http://www.military-technologies.net/2017/04/04/global-speech-voice-recognition-market-forecast-research-report-2015-2022/
44,Technology: made by men and tested on men,2017-04-09,"Speech recognition software, for instance, recognises men's voices more accurately than female voices. The consequence is a higher rate of ...",Technology: made by men and tested on men,https://www.ft.com/content/29a005c4-1a2d-11e7-bcac-6d03d067f81f
45,"Sign up for one of our email newsletters.  The humans never had a chance.  As expected, the latest poker-playing bot powered by an artificial intelligence designed by a duo from Carnegie Mellon University beat a team of some of the best poker players in China.  Lengpudashi , the AI developed by Professor Tuomas Sandholm and Noam Brown, a graduate student at CMU, finished five days of Heads-Up, No-Limit Texas Hold'em with nearly $800,000 in chips and walked away with $290,000.  No one from Team Dragons finished in the black.  ""We all knew the outcome. There was little chance for the humans,"" said Kai-Fu Lee, head of Sinovation Ventures , a Chinese venture capital firm that organized the winner-take-all exhibition match in Hainan, China.  Lee, himself a CMU alum who worked on speech recognition in Pittsburgh in the 1990s, challenged Lengpudashi to about 30 hands in a celebrity match during the exhibition.  ""It was fun watching me get butchered,"" Lee said.  Sandholm and Brown said Lengpudashi showed the same superhuman performance as Libratus, an similar artificial intelligence designed by the duo that beat some of the best professional poker players in the world during the Brains Vs. AI challenge in January at Rivers Casino in Pittsburgh. Team Dragons, the computer's competition in China, tried to gain the upper hand by filling out its six-person rosters with three top poker players and three computer scientists.  ""They thought they would understand the algorithms and come up with countermeasures,"" Lee said.  The computer whizzes tried to get inside the bot's ""head,"" but they couldn't.  ""Even though they came in well-prepared, they were not able to effectively exploit any weakness in the AI's strategy,"" Brown wrote.  Lengpudashi, or ""cold poker master"" became known as Libratus' Chinese brother throughout the match. Brown said the two were similar with only minor tweaks.  For example, Lengpudashi did not make improvements to its play overnight, studying its own strategy and fixing any weaknesses it found as Libratus did throughout the 20-day Brains Vs. AI challenge to stay one step ahead of the humans. Lengpudashi played the same strategy through the competition, Brown wrote.  RELATED: How a pokerbot beat four of the best players in the world  Lee, whose firm targets AI companies in the United States and China, said there were several reasons for inviting Lengpudashi, Sandholm and Brown to China even though there was nearly no chance they would lose. Lee wanted to increase the awareness of artificial intelligence in China. Artificial intelligence underscores most tech conversations in America and big names from Bill Gates to Elon Musk to Stephen Hawking serve as both evangelists and cautionary advisers, Lee said. That conversation about AI isn't happening in China, Lee said. He wanted to build on AI's recent win in Go, a popular game in China, with an exhibition of its possibilities in poker.  The poker match was streamed on 30 different websites and attracted more than 50 million viewers, Lee said.  In terms of academic research into AI, China is three to five years behind the United States, Lee said. China, however, could adopt the advantages of AI quickly. The country's financial system is outdated, providing an opportunity for it to leapfrog into using artificial intelligence to make decisions. Few people in China use credit cards but many people use their phones to make purchases, Lee said. That's why his firm invested in a start-up firm that uses artificial intelligence to approve small loans, $200 to $300, in a matter of seconds.  Centralized control of health data might make it easier to aggregate that data for use in medical applications of artificial intelligence. And the central control of infrastructure might be the framework to pave the way for improvements for autonomous vehicles.  Lee also used the poker match to promote his firm. Sinovation Ventures will be starting an artificial intelligence institute, and Lee hopes to attract top minds from around the world to work at it. Lee will be in Pittsburgh this summer and fall to talk with CMU students and faculty about joining him in China.  Finally, Lee wanted to show off his alma mater.  ""The fact that CMU is bar none the No. 1 AI university in the world is not as well known, especially in Asia, and I want them to know how great we are,"" Lee said.  Aaron Aupperlee is a Tribune-Review staff writer. Reach Aupperlee at aaupperlee@tribweb.com or 412-336-8448.",2017-04-10,"Lee, himself a CMU alum who worked on speech recognition in Pittsburgh in the 1990s, challenged Lengpudashi to about 30 hands in a ...",Poker match shows off Pittsburgh's artificial intelligence dominance ...,http://triblive.com/local/allegheny/12179279-74/poker-match-shows-off-pittsburghs-artificial-intelligence-dominance-in-china
46,"When you've got both hands on the wheel and both eyes focused on the road, the last thing you want to do is start arguing with your voice-powered assistant about what exactly you were asking it to look up. Unfortunately, that happens a little bit too frequently on my road trips when my failure to use precise phrasing stumps Siri and sends me spiraling into road rage.    So imagine how pleasant it was when the folks from Nuance picked me up in a tricked-out Chrysler Pacifica loaded up with the speech recognition company's Dragon Drive automotive assistant, and I found out that Dragon Drive not only understood me perfectly but could handle two requests nearly simultaneously.   Dragon Drive uses AI as well as natural language and speech recognition to understand your commands. (Credit: Nuance)To show off Dragon Drive's ability to listen to two commands at once and understand both, Lior Ben-Gigi, a senior product manager at Nuance, told the automotive assistant to bring up a route to the Golden Gate Bridge and notify one of his contacts about our estimated arrival time. Dragon Drive produced a route and then told us a message about our arrival in an hour's time had been sent  pretty impressive stuff if you've ever struggled to make a voice-powered assistant understand just one of your commands.    Dragon Drive's trick: it's built on multiple layers that help the automotive assistant figure out and act on just exactly what you're saying. In addition to automatic speech recognition that can hear your dictated command and natural language understanding that can figure out the intent behind your words, Nuance has also added an artificial intelligence layer that's able to place your commands in context.  MORE: Siri vs. Alexa: Why Amazon Won Our 300-Question Showdown  All these things come together to make things easier on the driver, Ben-Gigi explained to me. ""As a driver, I don't have to think too much,"" he said.    Dragon Drive, which Nuance developers for automakers to include in their in-car infotainment systems, can also tap into the sensors and diagnostic information of the car itself. In another demo, we plotted a course between my home in Northern California and San Diego; the Dragon Drive system calculated a route, but also threw in stops at gas stations when it calculated we would need to refuel the Pacifica.     ""It can understand the distance [we're driving], that we have a certain amount of fuel, and it can calculate a detour that doesn't take a lot of time,"" said Ben-Gigi, pointing out that the gas stations selected by the driving assistant were conveniently located right of the freeway.    What I particularly appreciate about Dragon Drive is that it didn't seem to require a series of specific commands to recognize what you were saying. That's a byproduct of the system's prowess with natural language. Say ""I'm cold,"" explained Nuance senior director of product marketing and strategy Eric Montague, and the automotive assistant would recognize that it needed to turn on the car's heating system.     The Pacifica minivan we used in the demo had been specially modified by the Nuance team. Whereas most cars with a Dragon Drive-powered assistant contain just two microphones, this car had six  all the better to show off the voice biometrics features Nuance has developed.  MORE: Best Smart Driving Apps for Dumb Cars  After registering your voice with the Dragon Drive system much in the same way you would with Siri or Google Assistant  you basically say ""Hello, Dragon"" a few times so that the assistant can learn the sound of your voice  you're able to control the voice commands of the car. There's a Guest mode, which allows someone like a passenger or a valet to operate limited commands, but info you might not want other people to access  your contacts, say  remains accessible only if Dragon Drive recognizes your voice.   Dragon Drive uses voice recognition and seat location in its Name That Tune-style game. (Credit: Nuance)All those biometric microphones in a car can also let you and your fellow passenges pass the time on a road trip with a friendly game. To showcase the system's ability to recognize different voices, Nuance created a version of Name That Tune that could play through the Pacifica's infotainment system.  As a music clip played, my fellow passengers and I could shout out ""got it,"" and the voice-recognition system would figure out which of us had buzzed in first. The system would then only listen to that contestant's guess, ignoring any interruptions from the other players.",2017-03-26,Dragon Drive uses AI as well as natural language and speech recognition to understand your commands. (Credit: Nuance)To show off Dragon ...,I Rode Shotgun with Nuance's AI-Powered Auto Assistant,"http://www.tomsguide.com/us/nuance-dragon-drive-automotive-assistant,news-24451.html"
47,"Intelligent Virtual Assistant Market by Verticle (Automotive, Healthcare, and e-commerce) and Technology (Text-to-Speech and Speech Recognition) - Global Opportunity Analysis and Industry Forecast, 2014 - 2020.North America dominates the global intelligent virtual assistant market owing to the increased adoption of IVA in countries such as the U.S.  However, Asia-Pacific region is expected to exhibit a faster growth over the forecast period 2015-2020.  Speech recognition technology segment dominated the market in 2014, and is expected to dominate throughout the forecast period 2015-2020.  Intelligent virtual assistant (IVA) is used in various industry verticals such as automotive, healthcare, BFSI, retail and others. Among these verticals, BFSI was the highest revenue-generating segment, accounting for 39.9% share in 2014.  IVA solutions are widely used in the BFSI sector, owing to faster response time, improved customer handling and high customer satisfaction. However, the automotive segment would witness the fastest growth during the forecast period.  The growth would be driven by the fast growing in-car infotainment systems market, as IVA is an important part of these systems, which enables the vehicle driver and passengers, to surf the internet, navigate and make calls through speech recognition technology.  Further, theIVA market based on geography is bifurcated into North America, Europe, Asia-Pacific and LAMEA. North America dominated the global intelligent virtual assistant (IVA) market, accounting for 39.5% market share in 2014.  The region would maintain its dominance during the forecast period. However, Asia- Pacific would witness the highest CAGR of 38.97% during the same period.  The report also outlines the competitive scenario of the intelligent virtual assistant (IVA) market, providing a comprehensive study of the key strategies adopted by companies operating in the IVA market. Key companies profiled in the report include Next IT, Nuance Communications Inc., IntelliResponse Systems Inc., CodeBaby Corporation, Anboto Group and others.",2017-04-10,The global intelligent virtual assistant market size is the revenue generated from the summation of speech recognition solutions and ...,Global intelligent virtual assistant market forecast by 2020 just ...,https://www.whatech.com/market-research/it/285473-global-intelligent-virtual-assistant-market-forecast-by-2020-just-published
48,,2017-03-16,"iFLYTEK has been empowering developers with the best performing and easy-to-use speech-recognition service, said Jidong YU, Senior Vice ...",ST Collaborates with iFLYTEK for Chinese Voice-Recognition Cloud ...,https://en.ctimes.com.tw/DispNews.asp?O=HK13H5Q8Z2YSAA00NE
49,"Global Speech Recognition Market had made research and come up with a report that focuses on the major players in the Global Speech Recognition Market throughout the world. The report includes information like company profiles, specification and product picture, production, capacity, contact information, cost and revenue.  Likewise, equipment and upstream raw materials as well as downstream demand analysis is also tackled.  The report investigates and analyzes the Global Speech Recognition Market and shows a comprehensive evaluation of the evaluation and its specifications.  Another aspect that was taken is the cost analysis of the main products dominant in the global ice cream industry considering the profit margin for the manufacturers. Another aspect that was taken is the cost analysis of the main products dominant in the global ice cream industry considering the profit margin for the manufacturers. Through this report, the core driving factors of the Global Speech Recognition Market were identified and the business partners and end-users were also elaborated. The business sector structure, business patterns and challenges affecting the market globally were also included in the extensive analysis for this research report. Various interviews and talks were held with the prominent leaders in the industry in order to obtain reliable and updated information pertaining to the market. The report firstly introduced the Speech Recognition basics: definitions, classifications, applications and industry chain overview; industry policies and plans; product specifications; manufacturing processes; cost structures and so on. Then it analyzed the world's main region market conditions, including the product price, profit, capacity, production, capacity utilization, supply, demand and industry growth rate. In the end, the report introduced new project SWOT analysis, investment feasibility analysis, and investment return analysis.    Company profile: Research N Reports is a new age market research firm where we focus on providing information that can be effectively applied. Today being a consumer driven market, companies require information to deal with the complex and dynamic world of choices. Where relying on a sound board firm for your decisions becomes crucial.  Research N Reports specializes in industry analysis, market forecasts and as a result getting quality reports covering all verticals, whether be it gaining perspective on ...",2017-03-23,Global Speech Recognition Market had made research and come up with a report that focuses on the major players in the Global Speech ...,New study: What is the future of “global speech recognition market ...,https://www.whatech.com/market-research/consumer/277753-new-study-what-is-the-future-of-global-speech-recognition-market-in-global-industry
